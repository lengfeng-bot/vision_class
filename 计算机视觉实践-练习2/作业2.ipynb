{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:48:46.456.812 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:48:46.457.817 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:48:46.457.817 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:48:46.458.813 [mindspore\\dataset\\core\\validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:48:46.458.813 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:48:46.459.814 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:48:46.459.814 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:48:46.460.814 [mindspore\\dataset\\core\\validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n"
     ]
    }
   ],
   "source": [
    "from mindvision.dataset import Mnist\n",
    "\n",
    "# 下载并处理MNIST数据集\n",
    "download_train = Mnist(\n",
    "    path=\"./mnist\",\n",
    "    split=\"train\",\n",
    "    batch_size=32,\n",
    "    repeat_num=1,\n",
    "    shuffle=True,\n",
    "    resize=32,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "download_eval = Mnist(\n",
    "    path=\"./mnist\", split=\"test\", batch_size=32, resize=32, download=True\n",
    ")\n",
    "\n",
    "dataset_train = download_train.run()\n",
    "dataset_eval = download_eval.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindvision.classification.models import lenet\n",
    "\n",
    "network = lenet(num_classes=10, pretrained=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "\n",
    "# 定义损失函数\n",
    "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "\n",
    "# 定义优化器函数\n",
    "net_opt = nn.Momentum(network.trainable_params(), learning_rate=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig\n",
    "\n",
    "# 设置模型保存参数，模型训练保存参数的step为1875\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "\n",
    "# 应用模型保存参数\n",
    "ckpoint = ModelCheckpoint(prefix=\"lenet\", directory=\"./lenet\", config=config_ck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[  0/ 10], step:[ 1875/ 1875], loss:[0.042/0.439], time:10.000 ms, lr:0.01000\n",
      "Epoch time: 10180.968 ms, per step time: 5.430 ms, avg loss: 0.439\n",
      "Epoch:[  1/ 10], step:[ 1875/ 1875], loss:[0.017/0.059], time:9.000 ms, lr:0.01000\n",
      "Epoch time: 9887.465 ms, per step time: 5.273 ms, avg loss: 0.059\n",
      "Epoch:[  2/ 10], step:[ 1875/ 1875], loss:[0.022/0.043], time:10.000 ms, lr:0.01000\n",
      "Epoch time: 10100.705 ms, per step time: 5.387 ms, avg loss: 0.043\n",
      "Epoch:[  3/ 10], step:[ 1875/ 1875], loss:[0.004/0.032], time:9.000 ms, lr:0.01000\n",
      "Epoch time: 10261.483 ms, per step time: 5.473 ms, avg loss: 0.032\n",
      "Epoch:[  4/ 10], step:[ 1875/ 1875], loss:[0.020/0.026], time:8.999 ms, lr:0.01000\n",
      "Epoch time: 9793.327 ms, per step time: 5.223 ms, avg loss: 0.026\n",
      "Epoch:[  5/ 10], step:[ 1875/ 1875], loss:[0.025/0.024], time:9.050 ms, lr:0.01000\n",
      "Epoch time: 10186.386 ms, per step time: 5.433 ms, avg loss: 0.024\n",
      "Epoch:[  6/ 10], step:[ 1875/ 1875], loss:[0.000/0.020], time:9.001 ms, lr:0.01000\n",
      "Epoch time: 9667.573 ms, per step time: 5.156 ms, avg loss: 0.020\n",
      "Epoch:[  7/ 10], step:[ 1875/ 1875], loss:[0.001/0.017], time:8.000 ms, lr:0.01000\n",
      "Epoch time: 9733.880 ms, per step time: 5.191 ms, avg loss: 0.017\n",
      "Epoch:[  8/ 10], step:[ 1875/ 1875], loss:[0.281/0.014], time:8.999 ms, lr:0.01000\n",
      "Epoch time: 9319.870 ms, per step time: 4.971 ms, avg loss: 0.014\n",
      "Epoch:[  9/ 10], step:[ 1875/ 1875], loss:[0.001/0.015], time:8.000 ms, lr:0.01000\n",
      "Epoch time: 9839.330 ms, per step time: 5.248 ms, avg loss: 0.015\n"
     ]
    }
   ],
   "source": [
    "from mindvision.engine.callback import LossMonitor\n",
    "from mindspore.train import Model\n",
    "\n",
    "# 初始化模型参数\n",
    "model = Model(network, loss_fn=net_loss, optimizer=net_opt, metrics={\"accuracy\"})\n",
    "\n",
    "# 训练网络模型，并保存为lenet-1_1875.ckpt文件\n",
    "model.train(10, dataset_train, callbacks=[ckpoint, LossMonitor(0.01, 1875)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9898838141025641}\n"
     ]
    }
   ],
   "source": [
    "acc = model.eval(dataset_eval)\n",
    "\n",
    "print(\"{}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['global_step',\n",
       "  'learning_rate',\n",
       "  'momentum',\n",
       "  'moments.backbone.conv1.weight',\n",
       "  'moments.backbone.conv2.weight',\n",
       "  'moments.backbone.fc1.weight',\n",
       "  'moments.backbone.fc1.bias',\n",
       "  'moments.backbone.fc2.weight',\n",
       "  'moments.backbone.fc2.bias',\n",
       "  'moments.backbone.fc3.weight',\n",
       "  'moments.backbone.fc3.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "\n",
    "# 加载已经保存的用于测试的模型\n",
    "param_dict = load_checkpoint(\"./lenet/lenet-1_1875.ckpt\")\n",
    "# 加载参数到网络中\n",
    "load_param_into_net(network, param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:50:28.365.352 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:50:28.365.352 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:50:28.366.352 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-01:50:28.366.352 [mindspore\\dataset\\core\\validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"[2 8 7 0 4 6]\", Actual: \"[2 8 7 0 4 6]\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist = Mnist(\"./mnist\", split=\"train\", batch_size=6, resize=32)\n",
    "dataset_infer = mnist.run()\n",
    "ds_test = dataset_infer.create_dict_iterator()\n",
    "data = next(ds_test)\n",
    "images = data[\"image\"].asnumpy()\n",
    "labels = data[\"label\"].asnumpy()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(1, 7):\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.imshow(images[i - 1][0], interpolation=\"None\", cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# 使用函数model.predict预测image对应分类\n",
    "output = model.predict(Tensor(data[\"image\"]))\n",
    "predicted = np.argmax(output.asnumpy(), axis=1)\n",
    "\n",
    "# 输出预测分类与实际分类\n",
    "print(f'Predicted: \"{predicted}\", Actual: \"{labels}\"')\n",
    "\n",
    "\n",
    "# 从上面的打印结果可以看出，预测值与目标值完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-02:00:16.378.086 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-02:00:16.378.086 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-02:00:16.379.086 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-02:00:16.379.086 [mindspore\\dataset\\core\\validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-02:00:16.380.086 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-02:00:16.380.086 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-02:00:16.381.100 [mindspore\\dataset\\core\\validator_helpers.py:744] 'Rescale' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Rescale' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(11404:16908,MainProcess):2024-04-26-02:00:16.381.100 [mindspore\\dataset\\core\\validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Starting Training ==============\n",
      "epoch: 1 step: 1, loss is 2.319427490234375\n",
      "epoch: 1 step: 2, loss is 2.329155683517456\n",
      "epoch: 1 step: 3, loss is 2.3199281692504883\n",
      "epoch: 1 step: 4, loss is 2.298027276992798\n",
      "epoch: 1 step: 5, loss is 2.2997207641601562\n",
      "epoch: 1 step: 6, loss is 2.310328245162964\n",
      "epoch: 1 step: 7, loss is 2.2916946411132812\n",
      "epoch: 1 step: 8, loss is 2.306972026824951\n",
      "epoch: 1 step: 9, loss is 2.316160202026367\n",
      "epoch: 1 step: 10, loss is 2.3053765296936035\n",
      "epoch: 1 step: 11, loss is 2.2975194454193115\n",
      "epoch: 1 step: 12, loss is 2.3074183464050293\n",
      "epoch: 1 step: 13, loss is 2.318402051925659\n",
      "epoch: 1 step: 14, loss is 2.284738779067993\n",
      "epoch: 1 step: 15, loss is 2.2856645584106445\n",
      "epoch: 1 step: 16, loss is 2.293644428253174\n",
      "epoch: 1 step: 17, loss is 2.314279317855835\n",
      "epoch: 1 step: 18, loss is 2.316558361053467\n",
      "epoch: 1 step: 19, loss is 2.322911262512207\n",
      "epoch: 1 step: 20, loss is 2.29589581489563\n",
      "epoch: 1 step: 21, loss is 2.306579113006592\n",
      "epoch: 1 step: 22, loss is 2.301990032196045\n",
      "epoch: 1 step: 23, loss is 2.307818651199341\n",
      "epoch: 1 step: 24, loss is 2.3126792907714844\n",
      "epoch: 1 step: 25, loss is 2.294774293899536\n",
      "epoch: 1 step: 26, loss is 2.314828395843506\n",
      "epoch: 1 step: 27, loss is 2.3214876651763916\n",
      "epoch: 1 step: 28, loss is 2.30208420753479\n",
      "epoch: 1 step: 29, loss is 2.2917966842651367\n",
      "epoch: 1 step: 30, loss is 2.300333023071289\n",
      "epoch: 1 step: 31, loss is 2.2955965995788574\n",
      "epoch: 1 step: 32, loss is 2.2984819412231445\n",
      "epoch: 1 step: 33, loss is 2.2966725826263428\n",
      "epoch: 1 step: 34, loss is 2.2977163791656494\n",
      "epoch: 1 step: 35, loss is 2.307286262512207\n",
      "epoch: 1 step: 36, loss is 2.2983193397521973\n",
      "epoch: 1 step: 37, loss is 2.2962794303894043\n",
      "epoch: 1 step: 38, loss is 2.312955141067505\n",
      "epoch: 1 step: 39, loss is 2.3058221340179443\n",
      "epoch: 1 step: 40, loss is 2.292031764984131\n",
      "epoch: 1 step: 41, loss is 2.3062477111816406\n",
      "epoch: 1 step: 42, loss is 2.3177220821380615\n",
      "epoch: 1 step: 43, loss is 2.2994425296783447\n",
      "epoch: 1 step: 44, loss is 2.2884504795074463\n",
      "epoch: 1 step: 45, loss is 2.2904775142669678\n",
      "epoch: 1 step: 46, loss is 2.297905206680298\n",
      "epoch: 1 step: 47, loss is 2.287900686264038\n",
      "epoch: 1 step: 48, loss is 2.300584316253662\n",
      "epoch: 1 step: 49, loss is 2.3001883029937744\n",
      "epoch: 1 step: 50, loss is 2.3064167499542236\n",
      "epoch: 1 step: 51, loss is 2.3002283573150635\n",
      "epoch: 1 step: 52, loss is 2.3031418323516846\n",
      "epoch: 1 step: 53, loss is 2.296794891357422\n",
      "epoch: 1 step: 54, loss is 2.3099067211151123\n",
      "epoch: 1 step: 55, loss is 2.2888052463531494\n",
      "epoch: 1 step: 56, loss is 2.3028502464294434\n",
      "epoch: 1 step: 57, loss is 2.3008038997650146\n",
      "epoch: 1 step: 58, loss is 2.2899694442749023\n",
      "epoch: 1 step: 59, loss is 2.297518253326416\n",
      "epoch: 1 step: 60, loss is 2.30387544631958\n",
      "epoch: 1 step: 61, loss is 2.2851364612579346\n",
      "epoch: 1 step: 62, loss is 2.292182683944702\n",
      "epoch: 1 step: 63, loss is 2.305583953857422\n",
      "epoch: 1 step: 64, loss is 2.3130111694335938\n",
      "epoch: 1 step: 65, loss is 2.3048007488250732\n",
      "epoch: 1 step: 66, loss is 2.2949564456939697\n",
      "epoch: 1 step: 67, loss is 2.319149971008301\n",
      "epoch: 1 step: 68, loss is 2.2880003452301025\n",
      "epoch: 1 step: 69, loss is 2.294804096221924\n",
      "epoch: 1 step: 70, loss is 2.311915874481201\n",
      "epoch: 1 step: 71, loss is 2.305954694747925\n",
      "epoch: 1 step: 72, loss is 2.292588472366333\n",
      "epoch: 1 step: 73, loss is 2.3012843132019043\n",
      "epoch: 1 step: 74, loss is 2.292635440826416\n",
      "epoch: 1 step: 75, loss is 2.2937824726104736\n",
      "epoch: 1 step: 76, loss is 2.297337770462036\n",
      "epoch: 1 step: 77, loss is 2.304108142852783\n",
      "epoch: 1 step: 78, loss is 2.295175313949585\n",
      "epoch: 1 step: 79, loss is 2.304205894470215\n",
      "epoch: 1 step: 80, loss is 2.2889859676361084\n",
      "epoch: 1 step: 81, loss is 2.2919726371765137\n",
      "epoch: 1 step: 82, loss is 2.287353277206421\n",
      "epoch: 1 step: 83, loss is 2.2981033325195312\n",
      "epoch: 1 step: 84, loss is 2.28926420211792\n",
      "epoch: 1 step: 85, loss is 2.2862157821655273\n",
      "epoch: 1 step: 86, loss is 2.3009111881256104\n",
      "epoch: 1 step: 87, loss is 2.2811052799224854\n",
      "epoch: 1 step: 88, loss is 2.2950239181518555\n",
      "epoch: 1 step: 89, loss is 2.2781858444213867\n",
      "epoch: 1 step: 90, loss is 2.2761309146881104\n",
      "epoch: 1 step: 91, loss is 2.2815706729888916\n",
      "epoch: 1 step: 92, loss is 2.285374641418457\n",
      "epoch: 1 step: 93, loss is 2.279696464538574\n",
      "epoch: 1 step: 94, loss is 2.293689727783203\n",
      "epoch: 1 step: 95, loss is 2.2822322845458984\n",
      "epoch: 1 step: 96, loss is 2.285330057144165\n",
      "epoch: 1 step: 97, loss is 2.253770112991333\n",
      "epoch: 1 step: 98, loss is 2.2762954235076904\n",
      "epoch: 1 step: 99, loss is 2.260927200317383\n",
      "epoch: 1 step: 100, loss is 2.266472339630127\n",
      "epoch: 1 step: 101, loss is 2.2772185802459717\n",
      "epoch: 1 step: 102, loss is 2.2642600536346436\n",
      "epoch: 1 step: 103, loss is 2.2960848808288574\n",
      "epoch: 1 step: 104, loss is 2.2555885314941406\n",
      "epoch: 1 step: 105, loss is 2.2606520652770996\n",
      "epoch: 1 step: 106, loss is 2.259671926498413\n",
      "epoch: 1 step: 107, loss is 2.248307228088379\n",
      "epoch: 1 step: 108, loss is 2.2571794986724854\n",
      "epoch: 1 step: 109, loss is 2.2407243251800537\n",
      "epoch: 1 step: 110, loss is 2.203329086303711\n",
      "epoch: 1 step: 111, loss is 2.25187087059021\n",
      "epoch: 1 step: 112, loss is 2.2421529293060303\n",
      "epoch: 1 step: 113, loss is 2.1887576580047607\n",
      "epoch: 1 step: 114, loss is 2.2091033458709717\n",
      "epoch: 1 step: 115, loss is 2.1957128047943115\n",
      "epoch: 1 step: 116, loss is 2.1889264583587646\n",
      "epoch: 1 step: 117, loss is 2.167083740234375\n",
      "epoch: 1 step: 118, loss is 2.1491520404815674\n",
      "epoch: 1 step: 119, loss is 2.1007626056671143\n",
      "epoch: 1 step: 120, loss is 2.1324071884155273\n",
      "epoch: 1 step: 121, loss is 2.133941411972046\n",
      "epoch: 1 step: 122, loss is 2.0815494060516357\n",
      "epoch: 1 step: 123, loss is 2.0192861557006836\n",
      "epoch: 1 step: 124, loss is 2.0313732624053955\n",
      "epoch: 1 step: 125, loss is 1.8519515991210938\n",
      "epoch: 1 step: 126, loss is 1.8918949365615845\n",
      "epoch: 1 step: 127, loss is 1.8350450992584229\n",
      "epoch: 1 step: 128, loss is 1.730307698249817\n",
      "epoch: 1 step: 129, loss is 1.5697914361953735\n",
      "epoch: 1 step: 130, loss is 1.6479285955429077\n",
      "epoch: 1 step: 131, loss is 1.3725645542144775\n",
      "epoch: 1 step: 132, loss is 1.344164490699768\n",
      "epoch: 1 step: 133, loss is 1.1306779384613037\n",
      "epoch: 1 step: 134, loss is 1.1961219310760498\n",
      "epoch: 1 step: 135, loss is 1.121279001235962\n",
      "epoch: 1 step: 136, loss is 1.4398525953292847\n",
      "epoch: 1 step: 137, loss is 1.1185733079910278\n",
      "epoch: 1 step: 138, loss is 1.9202202558517456\n",
      "epoch: 1 step: 139, loss is 1.5967961549758911\n",
      "epoch: 1 step: 140, loss is 1.4916197061538696\n",
      "epoch: 1 step: 141, loss is 1.2584707736968994\n",
      "epoch: 1 step: 142, loss is 1.445403814315796\n",
      "epoch: 1 step: 143, loss is 0.9586519002914429\n",
      "epoch: 1 step: 144, loss is 0.9339654445648193\n",
      "epoch: 1 step: 145, loss is 1.1235475540161133\n",
      "epoch: 1 step: 146, loss is 1.1396169662475586\n",
      "epoch: 1 step: 147, loss is 1.1635085344314575\n",
      "epoch: 1 step: 148, loss is 1.1511284112930298\n",
      "epoch: 1 step: 149, loss is 0.9444236755371094\n",
      "epoch: 1 step: 150, loss is 1.0323106050491333\n",
      "epoch: 1 step: 151, loss is 0.7443897128105164\n",
      "epoch: 1 step: 152, loss is 0.8786020278930664\n",
      "epoch: 1 step: 153, loss is 0.8258760571479797\n",
      "epoch: 1 step: 154, loss is 0.7384645342826843\n",
      "epoch: 1 step: 155, loss is 1.0615195035934448\n",
      "epoch: 1 step: 156, loss is 1.0257121324539185\n",
      "epoch: 1 step: 157, loss is 0.8139419555664062\n",
      "epoch: 1 step: 158, loss is 1.4568657875061035\n",
      "epoch: 1 step: 159, loss is 1.1011558771133423\n",
      "epoch: 1 step: 160, loss is 0.4682457745075226\n",
      "epoch: 1 step: 161, loss is 0.8652860522270203\n",
      "epoch: 1 step: 162, loss is 0.7010059952735901\n",
      "epoch: 1 step: 163, loss is 0.47219669818878174\n",
      "epoch: 1 step: 164, loss is 0.5851203799247742\n",
      "epoch: 1 step: 165, loss is 0.8456287980079651\n",
      "epoch: 1 step: 166, loss is 0.5415191650390625\n",
      "epoch: 1 step: 167, loss is 0.6775801181793213\n",
      "epoch: 1 step: 168, loss is 0.4747622609138489\n",
      "epoch: 1 step: 169, loss is 0.41982609033584595\n",
      "epoch: 1 step: 170, loss is 0.3733251094818115\n",
      "epoch: 1 step: 171, loss is 0.8220382928848267\n",
      "epoch: 1 step: 172, loss is 0.5014687776565552\n",
      "epoch: 1 step: 173, loss is 0.4494118392467499\n",
      "epoch: 1 step: 174, loss is 0.605930507183075\n",
      "epoch: 1 step: 175, loss is 0.7452725172042847\n",
      "epoch: 1 step: 176, loss is 0.9134483337402344\n",
      "epoch: 1 step: 177, loss is 0.5028344392776489\n",
      "epoch: 1 step: 178, loss is 0.5020811557769775\n",
      "epoch: 1 step: 179, loss is 0.5250822305679321\n",
      "epoch: 1 step: 180, loss is 0.4141062796115875\n",
      "epoch: 1 step: 181, loss is 0.6971101760864258\n",
      "epoch: 1 step: 182, loss is 0.43417292833328247\n",
      "epoch: 1 step: 183, loss is 0.4902670979499817\n",
      "epoch: 1 step: 184, loss is 0.3544062376022339\n",
      "epoch: 1 step: 185, loss is 0.45065754652023315\n",
      "epoch: 1 step: 186, loss is 0.5489557385444641\n",
      "epoch: 1 step: 187, loss is 0.38882818818092346\n",
      "epoch: 1 step: 188, loss is 1.1047108173370361\n",
      "epoch: 1 step: 189, loss is 0.5032371282577515\n",
      "epoch: 1 step: 190, loss is 0.5294333100318909\n",
      "epoch: 1 step: 191, loss is 0.5441161394119263\n",
      "epoch: 1 step: 192, loss is 0.5950626730918884\n",
      "epoch: 1 step: 193, loss is 1.0013530254364014\n",
      "epoch: 1 step: 194, loss is 0.40773648023605347\n",
      "epoch: 1 step: 195, loss is 0.6131607294082642\n",
      "epoch: 1 step: 196, loss is 0.4065846800804138\n",
      "epoch: 1 step: 197, loss is 0.5840517282485962\n",
      "epoch: 1 step: 198, loss is 0.7707012891769409\n",
      "epoch: 1 step: 199, loss is 0.6165181994438171\n",
      "epoch: 1 step: 200, loss is 0.7243974804878235\n",
      "epoch: 1 step: 201, loss is 0.7483330368995667\n",
      "epoch: 1 step: 202, loss is 0.4664674699306488\n",
      "epoch: 1 step: 203, loss is 0.9319736957550049\n",
      "epoch: 1 step: 204, loss is 0.274808794260025\n",
      "epoch: 1 step: 205, loss is 0.5456839799880981\n",
      "epoch: 1 step: 206, loss is 0.5070214867591858\n",
      "epoch: 1 step: 207, loss is 0.5005847811698914\n",
      "epoch: 1 step: 208, loss is 0.5525323152542114\n",
      "epoch: 1 step: 209, loss is 0.2758564054965973\n",
      "epoch: 1 step: 210, loss is 0.40495842695236206\n",
      "epoch: 1 step: 211, loss is 0.5158289670944214\n",
      "epoch: 1 step: 212, loss is 0.5525885820388794\n",
      "epoch: 1 step: 213, loss is 0.20559832453727722\n",
      "epoch: 1 step: 214, loss is 0.4176567494869232\n",
      "epoch: 1 step: 215, loss is 0.3521881699562073\n",
      "epoch: 1 step: 216, loss is 0.30445730686187744\n",
      "epoch: 1 step: 217, loss is 0.47035494446754456\n",
      "epoch: 1 step: 218, loss is 0.8607084155082703\n",
      "epoch: 1 step: 219, loss is 0.1834246814250946\n",
      "epoch: 1 step: 220, loss is 0.613594114780426\n",
      "epoch: 1 step: 221, loss is 0.9539731740951538\n",
      "epoch: 1 step: 222, loss is 0.10406552255153656\n",
      "epoch: 1 step: 223, loss is 0.43739810585975647\n",
      "epoch: 1 step: 224, loss is 0.4122500419616699\n",
      "epoch: 1 step: 225, loss is 0.20147302746772766\n",
      "epoch: 1 step: 226, loss is 0.592848002910614\n",
      "epoch: 1 step: 227, loss is 0.352899432182312\n",
      "epoch: 1 step: 228, loss is 0.5375850796699524\n",
      "epoch: 1 step: 229, loss is 0.360500693321228\n",
      "epoch: 1 step: 230, loss is 0.20705902576446533\n",
      "epoch: 1 step: 231, loss is 0.526919424533844\n",
      "epoch: 1 step: 232, loss is 0.35047057271003723\n",
      "epoch: 1 step: 233, loss is 0.582110583782196\n",
      "epoch: 1 step: 234, loss is 0.2663924992084503\n",
      "epoch: 1 step: 235, loss is 0.3582293689250946\n",
      "epoch: 1 step: 236, loss is 0.49724045395851135\n",
      "epoch: 1 step: 237, loss is 0.2675812840461731\n",
      "epoch: 1 step: 238, loss is 0.4417322874069214\n",
      "epoch: 1 step: 239, loss is 0.3821181058883667\n",
      "epoch: 1 step: 240, loss is 0.34670931100845337\n",
      "epoch: 1 step: 241, loss is 0.5503239631652832\n",
      "epoch: 1 step: 242, loss is 0.12304611504077911\n",
      "epoch: 1 step: 243, loss is 0.46034640073776245\n",
      "epoch: 1 step: 244, loss is 0.34894081950187683\n",
      "epoch: 1 step: 245, loss is 0.08230114728212357\n",
      "epoch: 1 step: 246, loss is 0.10732230544090271\n",
      "epoch: 1 step: 247, loss is 0.4857635498046875\n",
      "epoch: 1 step: 248, loss is 0.3136960566043854\n",
      "epoch: 1 step: 249, loss is 0.5046505331993103\n",
      "epoch: 1 step: 250, loss is 0.19097116589546204\n",
      "epoch: 1 step: 251, loss is 0.7364304661750793\n",
      "epoch: 1 step: 252, loss is 0.6170752644538879\n",
      "epoch: 1 step: 253, loss is 0.23930883407592773\n",
      "epoch: 1 step: 254, loss is 0.3403239846229553\n",
      "epoch: 1 step: 255, loss is 0.19071583449840546\n",
      "epoch: 1 step: 256, loss is 0.5207450985908508\n",
      "epoch: 1 step: 257, loss is 0.12778981029987335\n",
      "epoch: 1 step: 258, loss is 0.3158693015575409\n",
      "epoch: 1 step: 259, loss is 0.4369245171546936\n",
      "epoch: 1 step: 260, loss is 0.3095913827419281\n",
      "epoch: 1 step: 261, loss is 0.355733186006546\n",
      "epoch: 1 step: 262, loss is 0.17576785385608673\n",
      "epoch: 1 step: 263, loss is 0.3579254746437073\n",
      "epoch: 1 step: 264, loss is 0.3831014931201935\n",
      "epoch: 1 step: 265, loss is 0.6251522898674011\n",
      "epoch: 1 step: 266, loss is 0.427729994058609\n",
      "epoch: 1 step: 267, loss is 0.36115360260009766\n",
      "epoch: 1 step: 268, loss is 0.20859713852405548\n",
      "epoch: 1 step: 269, loss is 0.22530190646648407\n",
      "epoch: 1 step: 270, loss is 0.49928179383277893\n",
      "epoch: 1 step: 271, loss is 0.45627352595329285\n",
      "epoch: 1 step: 272, loss is 0.6579424738883972\n",
      "epoch: 1 step: 273, loss is 0.17711715400218964\n",
      "epoch: 1 step: 274, loss is 0.2009163498878479\n",
      "epoch: 1 step: 275, loss is 0.43977755308151245\n",
      "epoch: 1 step: 276, loss is 0.28302231431007385\n",
      "epoch: 1 step: 277, loss is 0.34034186601638794\n",
      "epoch: 1 step: 278, loss is 0.3997373878955841\n",
      "epoch: 1 step: 279, loss is 0.2867247760295868\n",
      "epoch: 1 step: 280, loss is 0.35509416460990906\n",
      "epoch: 1 step: 281, loss is 0.29265645146369934\n",
      "epoch: 1 step: 282, loss is 0.24575532972812653\n",
      "epoch: 1 step: 283, loss is 0.2789851427078247\n",
      "epoch: 1 step: 284, loss is 0.6027967929840088\n",
      "epoch: 1 step: 285, loss is 0.45206350088119507\n",
      "epoch: 1 step: 286, loss is 0.20801183581352234\n",
      "epoch: 1 step: 287, loss is 0.32581979036331177\n",
      "epoch: 1 step: 288, loss is 0.15136384963989258\n",
      "epoch: 1 step: 289, loss is 0.3439443111419678\n",
      "epoch: 1 step: 290, loss is 0.4809704124927521\n",
      "epoch: 1 step: 291, loss is 0.15220168232917786\n",
      "epoch: 1 step: 292, loss is 0.6112145781517029\n",
      "epoch: 1 step: 293, loss is 0.5267757773399353\n",
      "epoch: 1 step: 294, loss is 0.2710700035095215\n",
      "epoch: 1 step: 295, loss is 0.3949891924858093\n",
      "epoch: 1 step: 296, loss is 0.5489540696144104\n",
      "epoch: 1 step: 297, loss is 0.4754487872123718\n",
      "epoch: 1 step: 298, loss is 0.40599822998046875\n",
      "epoch: 1 step: 299, loss is 0.3813641369342804\n",
      "epoch: 1 step: 300, loss is 0.780788242816925\n",
      "epoch: 1 step: 301, loss is 0.34083494544029236\n",
      "epoch: 1 step: 302, loss is 0.14889603853225708\n",
      "epoch: 1 step: 303, loss is 0.24141652882099152\n",
      "epoch: 1 step: 304, loss is 0.21104198694229126\n",
      "epoch: 1 step: 305, loss is 0.15779447555541992\n",
      "epoch: 1 step: 306, loss is 0.28345686197280884\n",
      "epoch: 1 step: 307, loss is 0.5827661752700806\n",
      "epoch: 1 step: 308, loss is 0.1614186018705368\n",
      "epoch: 1 step: 309, loss is 0.2845139801502228\n",
      "epoch: 1 step: 310, loss is 0.34080860018730164\n",
      "epoch: 1 step: 311, loss is 0.21370422840118408\n",
      "epoch: 1 step: 312, loss is 0.4870494604110718\n",
      "epoch: 1 step: 313, loss is 0.21107661724090576\n",
      "epoch: 1 step: 314, loss is 0.23934553563594818\n",
      "epoch: 1 step: 315, loss is 0.3472202718257904\n",
      "epoch: 1 step: 316, loss is 0.16581787168979645\n",
      "epoch: 1 step: 317, loss is 0.15476909279823303\n",
      "epoch: 1 step: 318, loss is 0.24959741532802582\n",
      "epoch: 1 step: 319, loss is 0.27657586336135864\n",
      "epoch: 1 step: 320, loss is 0.3436586856842041\n",
      "epoch: 1 step: 321, loss is 0.3025360703468323\n",
      "epoch: 1 step: 322, loss is 0.27135181427001953\n",
      "epoch: 1 step: 323, loss is 0.2517828941345215\n",
      "epoch: 1 step: 324, loss is 0.1321588009595871\n",
      "epoch: 1 step: 325, loss is 0.13113188743591309\n",
      "epoch: 1 step: 326, loss is 0.18083429336547852\n",
      "epoch: 1 step: 327, loss is 0.2846350073814392\n",
      "epoch: 1 step: 328, loss is 0.6512872576713562\n",
      "epoch: 1 step: 329, loss is 0.3477955460548401\n",
      "epoch: 1 step: 330, loss is 0.5971644520759583\n",
      "epoch: 1 step: 331, loss is 0.35619935393333435\n",
      "epoch: 1 step: 332, loss is 0.38118210434913635\n",
      "epoch: 1 step: 333, loss is 0.14413927495479584\n",
      "epoch: 1 step: 334, loss is 0.4308086633682251\n",
      "epoch: 1 step: 335, loss is 0.33116820454597473\n",
      "epoch: 1 step: 336, loss is 0.5717834234237671\n",
      "epoch: 1 step: 337, loss is 0.37249457836151123\n",
      "epoch: 1 step: 338, loss is 0.22563444077968597\n",
      "epoch: 1 step: 339, loss is 0.47915029525756836\n",
      "epoch: 1 step: 340, loss is 0.2915925979614258\n",
      "epoch: 1 step: 341, loss is 0.10273553431034088\n",
      "epoch: 1 step: 342, loss is 0.20434832572937012\n",
      "epoch: 1 step: 343, loss is 0.143961563706398\n",
      "epoch: 1 step: 344, loss is 0.06956162303686142\n",
      "epoch: 1 step: 345, loss is 0.2810762822628021\n",
      "epoch: 1 step: 346, loss is 0.16787093877792358\n",
      "epoch: 1 step: 347, loss is 0.5714946389198303\n",
      "epoch: 1 step: 348, loss is 0.08106997609138489\n",
      "epoch: 1 step: 349, loss is 0.29702579975128174\n",
      "epoch: 1 step: 350, loss is 0.07693422585725784\n",
      "epoch: 1 step: 351, loss is 0.06781704723834991\n",
      "epoch: 1 step: 352, loss is 0.15703298151493073\n",
      "epoch: 1 step: 353, loss is 0.17239820957183838\n",
      "epoch: 1 step: 354, loss is 0.17607761919498444\n",
      "epoch: 1 step: 355, loss is 0.22846879065036774\n",
      "epoch: 1 step: 356, loss is 0.261709988117218\n",
      "epoch: 1 step: 357, loss is 0.07453294098377228\n",
      "epoch: 1 step: 358, loss is 0.35591843724250793\n",
      "epoch: 1 step: 359, loss is 0.20631849765777588\n",
      "epoch: 1 step: 360, loss is 0.17916379868984222\n",
      "epoch: 1 step: 361, loss is 0.19193528592586517\n",
      "epoch: 1 step: 362, loss is 0.1129179522395134\n",
      "epoch: 1 step: 363, loss is 0.3937514126300812\n",
      "epoch: 1 step: 364, loss is 0.19116471707820892\n",
      "epoch: 1 step: 365, loss is 0.23981940746307373\n",
      "epoch: 1 step: 366, loss is 0.1414879560470581\n",
      "epoch: 1 step: 367, loss is 0.07516859471797943\n",
      "epoch: 1 step: 368, loss is 0.12097346037626266\n",
      "epoch: 1 step: 369, loss is 0.14433862268924713\n",
      "epoch: 1 step: 370, loss is 0.07036945968866348\n",
      "epoch: 1 step: 371, loss is 0.08045036345720291\n",
      "epoch: 1 step: 372, loss is 0.05250544846057892\n",
      "epoch: 1 step: 373, loss is 0.11673377454280853\n",
      "epoch: 1 step: 374, loss is 0.10445541888475418\n",
      "epoch: 1 step: 375, loss is 0.10664397478103638\n",
      "epoch: 1 step: 376, loss is 0.2508256733417511\n",
      "epoch: 1 step: 377, loss is 0.06530235707759857\n",
      "epoch: 1 step: 378, loss is 0.22729165852069855\n",
      "epoch: 1 step: 379, loss is 0.14781396090984344\n",
      "epoch: 1 step: 380, loss is 0.17170150578022003\n",
      "epoch: 1 step: 381, loss is 0.20609450340270996\n",
      "epoch: 1 step: 382, loss is 0.21717485785484314\n",
      "epoch: 1 step: 383, loss is 0.2972956895828247\n",
      "epoch: 1 step: 384, loss is 0.3696528971195221\n",
      "epoch: 1 step: 385, loss is 0.2821713984012604\n",
      "epoch: 1 step: 386, loss is 0.012725558131933212\n",
      "epoch: 1 step: 387, loss is 0.15394845604896545\n",
      "epoch: 1 step: 388, loss is 0.056059423834085464\n",
      "epoch: 1 step: 389, loss is 0.36526089906692505\n",
      "epoch: 1 step: 390, loss is 0.3429429233074188\n",
      "epoch: 1 step: 391, loss is 0.4626307189464569\n",
      "epoch: 1 step: 392, loss is 0.10842198133468628\n",
      "epoch: 1 step: 393, loss is 0.1916845440864563\n",
      "epoch: 1 step: 394, loss is 0.20825868844985962\n",
      "epoch: 1 step: 395, loss is 0.14676342904567719\n",
      "epoch: 1 step: 396, loss is 0.13387103378772736\n",
      "epoch: 1 step: 397, loss is 0.30640846490859985\n",
      "epoch: 1 step: 398, loss is 0.11538344621658325\n",
      "epoch: 1 step: 399, loss is 0.24780431389808655\n",
      "epoch: 1 step: 400, loss is 0.21086309850215912\n",
      "epoch: 1 step: 401, loss is 0.06140602380037308\n",
      "epoch: 1 step: 402, loss is 0.1369970291852951\n",
      "epoch: 1 step: 403, loss is 0.5033377408981323\n",
      "epoch: 1 step: 404, loss is 0.11262420564889908\n",
      "epoch: 1 step: 405, loss is 0.3609219491481781\n",
      "epoch: 1 step: 406, loss is 0.16653935611248016\n",
      "epoch: 1 step: 407, loss is 0.36220917105674744\n",
      "epoch: 1 step: 408, loss is 0.09533026814460754\n",
      "epoch: 1 step: 409, loss is 0.11081346124410629\n",
      "epoch: 1 step: 410, loss is 0.23229919373989105\n",
      "epoch: 1 step: 411, loss is 0.2721213400363922\n",
      "epoch: 1 step: 412, loss is 0.17221540212631226\n",
      "epoch: 1 step: 413, loss is 0.05742771551012993\n",
      "epoch: 1 step: 414, loss is 0.1296335756778717\n",
      "epoch: 1 step: 415, loss is 0.27784624695777893\n",
      "epoch: 1 step: 416, loss is 0.1967737376689911\n",
      "epoch: 1 step: 417, loss is 0.32024234533309937\n",
      "epoch: 1 step: 418, loss is 0.2766094207763672\n",
      "epoch: 1 step: 419, loss is 0.4408024549484253\n",
      "epoch: 1 step: 420, loss is 0.08826147764921188\n",
      "epoch: 1 step: 421, loss is 0.22713971138000488\n",
      "epoch: 1 step: 422, loss is 0.2756864130496979\n",
      "epoch: 1 step: 423, loss is 0.1321011185646057\n",
      "epoch: 1 step: 424, loss is 0.0606728158891201\n",
      "epoch: 1 step: 425, loss is 0.37210017442703247\n",
      "epoch: 1 step: 426, loss is 0.20385102927684784\n",
      "epoch: 1 step: 427, loss is 0.3937963545322418\n",
      "epoch: 1 step: 428, loss is 0.26194697618484497\n",
      "epoch: 1 step: 429, loss is 0.27752745151519775\n",
      "epoch: 1 step: 430, loss is 0.29156264662742615\n",
      "epoch: 1 step: 431, loss is 0.2771685719490051\n",
      "epoch: 1 step: 432, loss is 0.4345690608024597\n",
      "epoch: 1 step: 433, loss is 0.29483091831207275\n",
      "epoch: 1 step: 434, loss is 0.15236562490463257\n",
      "epoch: 1 step: 435, loss is 0.180022194981575\n",
      "epoch: 1 step: 436, loss is 0.08356284350156784\n",
      "epoch: 1 step: 437, loss is 0.1847969889640808\n",
      "epoch: 1 step: 438, loss is 0.12854349613189697\n",
      "epoch: 1 step: 439, loss is 0.29691198468208313\n",
      "epoch: 1 step: 440, loss is 0.26420435309410095\n",
      "epoch: 1 step: 441, loss is 0.24327564239501953\n",
      "epoch: 1 step: 442, loss is 0.2105543464422226\n",
      "epoch: 1 step: 443, loss is 0.14846757054328918\n",
      "epoch: 1 step: 444, loss is 0.08114511519670486\n",
      "epoch: 1 step: 445, loss is 0.12005185335874557\n",
      "epoch: 1 step: 446, loss is 0.073968805372715\n",
      "epoch: 1 step: 447, loss is 0.32880693674087524\n",
      "epoch: 1 step: 448, loss is 0.10354147106409073\n",
      "epoch: 1 step: 449, loss is 0.11718299239873886\n",
      "epoch: 1 step: 450, loss is 0.20183560252189636\n",
      "epoch: 1 step: 451, loss is 0.14440098404884338\n",
      "epoch: 1 step: 452, loss is 0.22770576179027557\n",
      "epoch: 1 step: 453, loss is 0.15577031672000885\n",
      "epoch: 1 step: 454, loss is 0.30259454250335693\n",
      "epoch: 1 step: 455, loss is 0.01992645673453808\n",
      "epoch: 1 step: 456, loss is 0.31763896346092224\n",
      "epoch: 1 step: 457, loss is 0.14808231592178345\n",
      "epoch: 1 step: 458, loss is 0.15419258177280426\n",
      "epoch: 1 step: 459, loss is 0.12993966042995453\n",
      "epoch: 1 step: 460, loss is 0.4852132797241211\n",
      "epoch: 1 step: 461, loss is 0.21564246714115143\n",
      "epoch: 1 step: 462, loss is 0.31804049015045166\n",
      "epoch: 1 step: 463, loss is 0.3537827730178833\n",
      "epoch: 1 step: 464, loss is 0.2816370725631714\n",
      "epoch: 1 step: 465, loss is 0.21256695687770844\n",
      "epoch: 1 step: 466, loss is 0.23753947019577026\n",
      "epoch: 1 step: 467, loss is 0.11479686200618744\n",
      "epoch: 1 step: 468, loss is 0.16264966130256653\n",
      "epoch: 1 step: 469, loss is 0.14529114961624146\n",
      "epoch: 1 step: 470, loss is 0.5646207332611084\n",
      "epoch: 1 step: 471, loss is 0.14057180285453796\n",
      "epoch: 1 step: 472, loss is 0.30555057525634766\n",
      "epoch: 1 step: 473, loss is 0.20936532318592072\n",
      "epoch: 1 step: 474, loss is 0.4070970416069031\n",
      "epoch: 1 step: 475, loss is 0.3421860337257385\n",
      "epoch: 1 step: 476, loss is 0.2488812953233719\n",
      "epoch: 1 step: 477, loss is 0.2742674946784973\n",
      "epoch: 1 step: 478, loss is 0.2077065110206604\n",
      "epoch: 1 step: 479, loss is 0.08991523087024689\n",
      "epoch: 1 step: 480, loss is 0.1317114681005478\n",
      "epoch: 1 step: 481, loss is 0.21427196264266968\n",
      "epoch: 1 step: 482, loss is 0.02043992280960083\n",
      "epoch: 1 step: 483, loss is 0.10735955089330673\n",
      "epoch: 1 step: 484, loss is 0.14769281446933746\n",
      "epoch: 1 step: 485, loss is 0.07345481216907501\n",
      "epoch: 1 step: 486, loss is 0.12353655695915222\n",
      "epoch: 1 step: 487, loss is 0.4024898409843445\n",
      "epoch: 1 step: 488, loss is 0.37773793935775757\n",
      "epoch: 1 step: 489, loss is 0.06663686037063599\n",
      "epoch: 1 step: 490, loss is 0.2836684584617615\n",
      "epoch: 1 step: 491, loss is 0.07871846854686737\n",
      "epoch: 1 step: 492, loss is 0.24675515294075012\n",
      "epoch: 1 step: 493, loss is 0.2627420723438263\n",
      "epoch: 1 step: 494, loss is 0.17577680945396423\n",
      "epoch: 1 step: 495, loss is 0.04183826223015785\n",
      "epoch: 1 step: 496, loss is 0.2686707079410553\n",
      "epoch: 1 step: 497, loss is 0.34822818636894226\n",
      "epoch: 1 step: 498, loss is 0.23866328597068787\n",
      "epoch: 1 step: 499, loss is 0.3621365427970886\n",
      "epoch: 1 step: 500, loss is 0.18823359906673431\n",
      "epoch: 1 step: 501, loss is 0.15639235079288483\n",
      "epoch: 1 step: 502, loss is 0.09185117483139038\n",
      "epoch: 1 step: 503, loss is 0.16212153434753418\n",
      "epoch: 1 step: 504, loss is 0.07705637067556381\n",
      "epoch: 1 step: 505, loss is 0.19269177317619324\n",
      "epoch: 1 step: 506, loss is 0.2647285461425781\n",
      "epoch: 1 step: 507, loss is 0.3679889142513275\n",
      "epoch: 1 step: 508, loss is 0.41481202840805054\n",
      "epoch: 1 step: 509, loss is 0.06260140985250473\n",
      "epoch: 1 step: 510, loss is 0.12708745896816254\n",
      "epoch: 1 step: 511, loss is 0.03956140950322151\n",
      "epoch: 1 step: 512, loss is 0.0888315886259079\n",
      "epoch: 1 step: 513, loss is 0.09545634686946869\n",
      "epoch: 1 step: 514, loss is 0.049326200038194656\n",
      "epoch: 1 step: 515, loss is 0.08297157287597656\n",
      "epoch: 1 step: 516, loss is 0.08592289686203003\n",
      "epoch: 1 step: 517, loss is 0.036021530628204346\n",
      "epoch: 1 step: 518, loss is 0.05650680884718895\n",
      "epoch: 1 step: 519, loss is 0.15444442629814148\n",
      "epoch: 1 step: 520, loss is 0.2765766978263855\n",
      "epoch: 1 step: 521, loss is 0.05768530070781708\n",
      "epoch: 1 step: 522, loss is 0.29469966888427734\n",
      "epoch: 1 step: 523, loss is 0.2859289050102234\n",
      "epoch: 1 step: 524, loss is 0.028803303837776184\n",
      "epoch: 1 step: 525, loss is 0.06796640157699585\n",
      "epoch: 1 step: 526, loss is 0.02053193189203739\n",
      "epoch: 1 step: 527, loss is 0.1246851310133934\n",
      "epoch: 1 step: 528, loss is 0.15326906740665436\n",
      "epoch: 1 step: 529, loss is 0.033386118710041046\n",
      "epoch: 1 step: 530, loss is 0.2713582217693329\n",
      "epoch: 1 step: 531, loss is 0.23311683535575867\n",
      "epoch: 1 step: 532, loss is 0.1470445692539215\n",
      "epoch: 1 step: 533, loss is 0.24946482479572296\n",
      "epoch: 1 step: 534, loss is 0.18093465268611908\n",
      "epoch: 1 step: 535, loss is 0.07416702806949615\n",
      "epoch: 1 step: 536, loss is 0.15285271406173706\n",
      "epoch: 1 step: 537, loss is 0.22353896498680115\n",
      "epoch: 1 step: 538, loss is 0.10966890305280685\n",
      "epoch: 1 step: 539, loss is 0.1798165738582611\n",
      "epoch: 1 step: 540, loss is 0.33686354756355286\n",
      "epoch: 1 step: 541, loss is 0.15401732921600342\n",
      "epoch: 1 step: 542, loss is 0.4802706241607666\n",
      "epoch: 1 step: 543, loss is 0.16410227119922638\n",
      "epoch: 1 step: 544, loss is 0.14848394691944122\n",
      "epoch: 1 step: 545, loss is 0.09028011560440063\n",
      "epoch: 1 step: 546, loss is 0.17071831226348877\n",
      "epoch: 1 step: 547, loss is 0.09357600659132004\n",
      "epoch: 1 step: 548, loss is 0.25802770256996155\n",
      "epoch: 1 step: 549, loss is 0.28572380542755127\n",
      "epoch: 1 step: 550, loss is 0.151303231716156\n",
      "epoch: 1 step: 551, loss is 0.20029348134994507\n",
      "epoch: 1 step: 552, loss is 0.21549919247627258\n",
      "epoch: 1 step: 553, loss is 0.10308362543582916\n",
      "epoch: 1 step: 554, loss is 0.16887794435024261\n",
      "epoch: 1 step: 555, loss is 0.3401975631713867\n",
      "epoch: 1 step: 556, loss is 0.13371963798999786\n",
      "epoch: 1 step: 557, loss is 0.04114530235528946\n",
      "epoch: 1 step: 558, loss is 0.34522736072540283\n",
      "epoch: 1 step: 559, loss is 0.364635705947876\n",
      "epoch: 1 step: 560, loss is 0.18856243789196014\n",
      "epoch: 1 step: 561, loss is 0.20210471749305725\n",
      "epoch: 1 step: 562, loss is 0.09920786321163177\n",
      "epoch: 1 step: 563, loss is 0.18550796806812286\n",
      "epoch: 1 step: 564, loss is 0.086827352643013\n",
      "epoch: 1 step: 565, loss is 0.017001070082187653\n",
      "epoch: 1 step: 566, loss is 0.12102019786834717\n",
      "epoch: 1 step: 567, loss is 0.0712701752781868\n",
      "epoch: 1 step: 568, loss is 0.029108313843607903\n",
      "epoch: 1 step: 569, loss is 0.15260639786720276\n",
      "epoch: 1 step: 570, loss is 0.017164377495646477\n",
      "epoch: 1 step: 571, loss is 0.08872281759977341\n",
      "epoch: 1 step: 572, loss is 0.05052364617586136\n",
      "epoch: 1 step: 573, loss is 0.029853887856006622\n",
      "epoch: 1 step: 574, loss is 0.09334623068571091\n",
      "epoch: 1 step: 575, loss is 0.2331179976463318\n",
      "epoch: 1 step: 576, loss is 0.24757172167301178\n",
      "epoch: 1 step: 577, loss is 0.37542131543159485\n",
      "epoch: 1 step: 578, loss is 0.08054431527853012\n",
      "epoch: 1 step: 579, loss is 0.03240664675831795\n",
      "epoch: 1 step: 580, loss is 0.39577627182006836\n",
      "epoch: 1 step: 581, loss is 0.30487534403800964\n",
      "epoch: 1 step: 582, loss is 0.03487298637628555\n",
      "epoch: 1 step: 583, loss is 0.17446045577526093\n",
      "epoch: 1 step: 584, loss is 0.16413874924182892\n",
      "epoch: 1 step: 585, loss is 0.06842450797557831\n",
      "epoch: 1 step: 586, loss is 0.09682971984148026\n",
      "epoch: 1 step: 587, loss is 0.15614183247089386\n",
      "epoch: 1 step: 588, loss is 0.17622390389442444\n",
      "epoch: 1 step: 589, loss is 0.5101990699768066\n",
      "epoch: 1 step: 590, loss is 0.24423716962337494\n",
      "epoch: 1 step: 591, loss is 0.1181274875998497\n",
      "epoch: 1 step: 592, loss is 0.2765606641769409\n",
      "epoch: 1 step: 593, loss is 0.08795223385095596\n",
      "epoch: 1 step: 594, loss is 0.2662479281425476\n",
      "epoch: 1 step: 595, loss is 0.09487254917621613\n",
      "epoch: 1 step: 596, loss is 0.1174827441573143\n",
      "epoch: 1 step: 597, loss is 0.1276596486568451\n",
      "epoch: 1 step: 598, loss is 0.218570739030838\n",
      "epoch: 1 step: 599, loss is 0.19994999468326569\n",
      "epoch: 1 step: 600, loss is 0.02450493909418583\n",
      "epoch: 1 step: 601, loss is 0.02117883786559105\n",
      "epoch: 1 step: 602, loss is 0.1252979338169098\n",
      "epoch: 1 step: 603, loss is 0.0790499672293663\n",
      "epoch: 1 step: 604, loss is 0.1717398464679718\n",
      "epoch: 1 step: 605, loss is 0.13459733128547668\n",
      "epoch: 1 step: 606, loss is 0.03590129688382149\n",
      "epoch: 1 step: 607, loss is 0.061660025268793106\n",
      "epoch: 1 step: 608, loss is 0.25132036209106445\n",
      "epoch: 1 step: 609, loss is 0.2222413271665573\n",
      "epoch: 1 step: 610, loss is 0.09723952412605286\n",
      "epoch: 1 step: 611, loss is 0.2688053548336029\n",
      "epoch: 1 step: 612, loss is 0.08114410191774368\n",
      "epoch: 1 step: 613, loss is 0.2255954146385193\n",
      "epoch: 1 step: 614, loss is 0.05416152998805046\n",
      "epoch: 1 step: 615, loss is 0.15668892860412598\n",
      "epoch: 1 step: 616, loss is 0.041952818632125854\n",
      "epoch: 1 step: 617, loss is 0.16355763375759125\n",
      "epoch: 1 step: 618, loss is 0.06144766882061958\n",
      "epoch: 1 step: 619, loss is 0.16458499431610107\n",
      "epoch: 1 step: 620, loss is 0.05786080285906792\n",
      "epoch: 1 step: 621, loss is 0.186150923371315\n",
      "epoch: 1 step: 622, loss is 0.15771615505218506\n",
      "epoch: 1 step: 623, loss is 0.03372066468000412\n",
      "epoch: 1 step: 624, loss is 0.052404675632715225\n",
      "epoch: 1 step: 625, loss is 0.078459233045578\n",
      "epoch: 1 step: 626, loss is 0.4521864950656891\n",
      "epoch: 1 step: 627, loss is 0.16854847967624664\n",
      "epoch: 1 step: 628, loss is 0.021266931667923927\n",
      "epoch: 1 step: 629, loss is 0.1517406702041626\n",
      "epoch: 1 step: 630, loss is 0.04605522379279137\n",
      "epoch: 1 step: 631, loss is 0.08530986309051514\n",
      "epoch: 1 step: 632, loss is 0.07402152568101883\n",
      "epoch: 1 step: 633, loss is 0.14208343625068665\n",
      "epoch: 1 step: 634, loss is 0.1437755823135376\n",
      "epoch: 1 step: 635, loss is 0.019178232178092003\n",
      "epoch: 1 step: 636, loss is 0.46352043747901917\n",
      "epoch: 1 step: 637, loss is 0.07766427099704742\n",
      "epoch: 1 step: 638, loss is 0.07824882864952087\n",
      "epoch: 1 step: 639, loss is 0.5231856107711792\n",
      "epoch: 1 step: 640, loss is 0.27633997797966003\n",
      "epoch: 1 step: 641, loss is 0.045774590224027634\n",
      "epoch: 1 step: 642, loss is 0.23177997767925262\n",
      "epoch: 1 step: 643, loss is 0.16609901189804077\n",
      "epoch: 1 step: 644, loss is 0.2157207429409027\n",
      "epoch: 1 step: 645, loss is 0.11201780289411545\n",
      "epoch: 1 step: 646, loss is 0.10361912846565247\n",
      "epoch: 1 step: 647, loss is 0.21328739821910858\n",
      "epoch: 1 step: 648, loss is 0.07730650156736374\n",
      "epoch: 1 step: 649, loss is 0.20418810844421387\n",
      "epoch: 1 step: 650, loss is 0.22685247659683228\n",
      "epoch: 1 step: 651, loss is 0.05606384202837944\n",
      "epoch: 1 step: 652, loss is 0.02030366100370884\n",
      "epoch: 1 step: 653, loss is 0.07299064099788666\n",
      "epoch: 1 step: 654, loss is 0.3819345533847809\n",
      "epoch: 1 step: 655, loss is 0.05552439019083977\n",
      "epoch: 1 step: 656, loss is 0.13978849351406097\n",
      "epoch: 1 step: 657, loss is 0.33133020997047424\n",
      "epoch: 1 step: 658, loss is 0.00946557056158781\n",
      "epoch: 1 step: 659, loss is 0.3657073378562927\n",
      "epoch: 1 step: 660, loss is 0.03360546752810478\n",
      "epoch: 1 step: 661, loss is 0.17374980449676514\n",
      "epoch: 1 step: 662, loss is 0.27477774024009705\n",
      "epoch: 1 step: 663, loss is 0.3139316737651825\n",
      "epoch: 1 step: 664, loss is 0.09858964383602142\n",
      "epoch: 1 step: 665, loss is 0.19350548088550568\n",
      "epoch: 1 step: 666, loss is 0.3172350525856018\n",
      "epoch: 1 step: 667, loss is 0.026478631421923637\n",
      "epoch: 1 step: 668, loss is 0.08360057324171066\n",
      "epoch: 1 step: 669, loss is 0.4161718487739563\n",
      "epoch: 1 step: 670, loss is 0.4814925789833069\n",
      "epoch: 1 step: 671, loss is 0.19982512295246124\n",
      "epoch: 1 step: 672, loss is 0.19712288677692413\n",
      "epoch: 1 step: 673, loss is 0.04768284037709236\n",
      "epoch: 1 step: 674, loss is 0.16780366003513336\n",
      "epoch: 1 step: 675, loss is 0.1841135025024414\n",
      "epoch: 1 step: 676, loss is 0.09223426878452301\n",
      "epoch: 1 step: 677, loss is 0.1612549126148224\n",
      "epoch: 1 step: 678, loss is 0.033923257142305374\n",
      "epoch: 1 step: 679, loss is 0.07967496663331985\n",
      "epoch: 1 step: 680, loss is 0.2022145837545395\n",
      "epoch: 1 step: 681, loss is 0.2629541754722595\n",
      "epoch: 1 step: 682, loss is 0.07327946275472641\n",
      "epoch: 1 step: 683, loss is 0.15231722593307495\n",
      "epoch: 1 step: 684, loss is 0.16759327054023743\n",
      "epoch: 1 step: 685, loss is 0.07532431930303574\n",
      "epoch: 1 step: 686, loss is 0.0936567485332489\n",
      "epoch: 1 step: 687, loss is 0.14765626192092896\n",
      "epoch: 1 step: 688, loss is 0.11023778468370438\n",
      "epoch: 1 step: 689, loss is 0.07887525856494904\n",
      "epoch: 1 step: 690, loss is 0.31246238946914673\n",
      "epoch: 1 step: 691, loss is 0.10124529153108597\n",
      "epoch: 1 step: 692, loss is 0.3964160978794098\n",
      "epoch: 1 step: 693, loss is 0.06812770664691925\n",
      "epoch: 1 step: 694, loss is 0.10842609405517578\n",
      "epoch: 1 step: 695, loss is 0.055281419306993484\n",
      "epoch: 1 step: 696, loss is 0.037206318229436874\n",
      "epoch: 1 step: 697, loss is 0.04202675819396973\n",
      "epoch: 1 step: 698, loss is 0.03937557339668274\n",
      "epoch: 1 step: 699, loss is 0.04937982186675072\n",
      "epoch: 1 step: 700, loss is 0.22507323324680328\n",
      "epoch: 1 step: 701, loss is 0.27451711893081665\n",
      "epoch: 1 step: 702, loss is 0.07245444506406784\n",
      "epoch: 1 step: 703, loss is 0.3368932008743286\n",
      "epoch: 1 step: 704, loss is 0.02094663865864277\n",
      "epoch: 1 step: 705, loss is 0.004281579051166773\n",
      "epoch: 1 step: 706, loss is 0.6382633447647095\n",
      "epoch: 1 step: 707, loss is 0.08029258251190186\n",
      "epoch: 1 step: 708, loss is 0.013144067488610744\n",
      "epoch: 1 step: 709, loss is 0.15386152267456055\n",
      "epoch: 1 step: 710, loss is 0.08160386234521866\n",
      "epoch: 1 step: 711, loss is 0.04420263692736626\n",
      "epoch: 1 step: 712, loss is 0.05170544981956482\n",
      "epoch: 1 step: 713, loss is 0.04144631326198578\n",
      "epoch: 1 step: 714, loss is 0.04995330423116684\n",
      "epoch: 1 step: 715, loss is 0.08678751438856125\n",
      "epoch: 1 step: 716, loss is 0.1542607843875885\n",
      "epoch: 1 step: 717, loss is 0.18840016424655914\n",
      "epoch: 1 step: 718, loss is 0.20127877593040466\n",
      "epoch: 1 step: 719, loss is 0.1998748481273651\n",
      "epoch: 1 step: 720, loss is 0.021357590332627296\n",
      "epoch: 1 step: 721, loss is 0.07913174480199814\n",
      "epoch: 1 step: 722, loss is 0.02133711241185665\n",
      "epoch: 1 step: 723, loss is 0.19060780107975006\n",
      "epoch: 1 step: 724, loss is 0.10918480157852173\n",
      "epoch: 1 step: 725, loss is 0.03473356366157532\n",
      "epoch: 1 step: 726, loss is 0.013031498529016972\n",
      "epoch: 1 step: 727, loss is 0.13777469098567963\n",
      "epoch: 1 step: 728, loss is 0.0835038498044014\n",
      "epoch: 1 step: 729, loss is 0.19630050659179688\n",
      "epoch: 1 step: 730, loss is 0.20885296165943146\n",
      "epoch: 1 step: 731, loss is 0.10629959404468536\n",
      "epoch: 1 step: 732, loss is 0.03026123344898224\n",
      "epoch: 1 step: 733, loss is 0.34967002272605896\n",
      "epoch: 1 step: 734, loss is 0.3186901807785034\n",
      "epoch: 1 step: 735, loss is 0.4687366187572479\n",
      "epoch: 1 step: 736, loss is 0.09435470402240753\n",
      "epoch: 1 step: 737, loss is 0.007008041255176067\n",
      "epoch: 1 step: 738, loss is 0.2271123230457306\n",
      "epoch: 1 step: 739, loss is 0.17637579143047333\n",
      "epoch: 1 step: 740, loss is 0.08877959102392197\n",
      "epoch: 1 step: 741, loss is 0.03895551711320877\n",
      "epoch: 1 step: 742, loss is 0.22668206691741943\n",
      "epoch: 1 step: 743, loss is 0.24552467465400696\n",
      "epoch: 1 step: 744, loss is 0.27468496561050415\n",
      "epoch: 1 step: 745, loss is 0.28161871433258057\n",
      "epoch: 1 step: 746, loss is 0.03917889669537544\n",
      "epoch: 1 step: 747, loss is 0.10481685400009155\n",
      "epoch: 1 step: 748, loss is 0.2897336483001709\n",
      "epoch: 1 step: 749, loss is 0.10289400070905685\n",
      "epoch: 1 step: 750, loss is 0.14721347391605377\n",
      "epoch: 1 step: 751, loss is 0.24490994215011597\n",
      "epoch: 1 step: 752, loss is 0.1010529026389122\n",
      "epoch: 1 step: 753, loss is 0.04339419677853584\n",
      "epoch: 1 step: 754, loss is 0.0812174379825592\n",
      "epoch: 1 step: 755, loss is 0.010607514530420303\n",
      "epoch: 1 step: 756, loss is 0.08010134845972061\n",
      "epoch: 1 step: 757, loss is 0.45090556144714355\n",
      "epoch: 1 step: 758, loss is 0.16426342725753784\n",
      "epoch: 1 step: 759, loss is 0.0752301886677742\n",
      "epoch: 1 step: 760, loss is 0.23112505674362183\n",
      "epoch: 1 step: 761, loss is 0.07676857709884644\n",
      "epoch: 1 step: 762, loss is 0.08318254351615906\n",
      "epoch: 1 step: 763, loss is 0.059236664324998856\n",
      "epoch: 1 step: 764, loss is 0.14073288440704346\n",
      "epoch: 1 step: 765, loss is 0.09236849844455719\n",
      "epoch: 1 step: 766, loss is 0.10437434911727905\n",
      "epoch: 1 step: 767, loss is 0.050098590552806854\n",
      "epoch: 1 step: 768, loss is 0.18175867199897766\n",
      "epoch: 1 step: 769, loss is 0.02233094908297062\n",
      "epoch: 1 step: 770, loss is 0.020624235272407532\n",
      "epoch: 1 step: 771, loss is 0.21255339682102203\n",
      "epoch: 1 step: 772, loss is 0.2117007076740265\n",
      "epoch: 1 step: 773, loss is 0.18157638609409332\n",
      "epoch: 1 step: 774, loss is 0.04730185493826866\n",
      "epoch: 1 step: 775, loss is 0.06858718395233154\n",
      "epoch: 1 step: 776, loss is 0.1641293466091156\n",
      "epoch: 1 step: 777, loss is 0.021801723167300224\n",
      "epoch: 1 step: 778, loss is 0.02917766198515892\n",
      "epoch: 1 step: 779, loss is 0.16309422254562378\n",
      "epoch: 1 step: 780, loss is 0.13677102327346802\n",
      "epoch: 1 step: 781, loss is 0.08449974656105042\n",
      "epoch: 1 step: 782, loss is 0.011586077511310577\n",
      "epoch: 1 step: 783, loss is 0.20129363238811493\n",
      "epoch: 1 step: 784, loss is 0.04097237065434456\n",
      "epoch: 1 step: 785, loss is 0.07592243701219559\n",
      "epoch: 1 step: 786, loss is 0.20382511615753174\n",
      "epoch: 1 step: 787, loss is 0.3919617235660553\n",
      "epoch: 1 step: 788, loss is 0.0735631063580513\n",
      "epoch: 1 step: 789, loss is 0.04911526292562485\n",
      "epoch: 1 step: 790, loss is 0.09728514403104782\n",
      "epoch: 1 step: 791, loss is 0.47966212034225464\n",
      "epoch: 1 step: 792, loss is 0.026445738971233368\n",
      "epoch: 1 step: 793, loss is 0.11157847195863724\n",
      "epoch: 1 step: 794, loss is 0.19545094668865204\n",
      "epoch: 1 step: 795, loss is 0.12757804989814758\n",
      "epoch: 1 step: 796, loss is 0.07750773429870605\n",
      "epoch: 1 step: 797, loss is 0.04218735173344612\n",
      "epoch: 1 step: 798, loss is 0.3758203089237213\n",
      "epoch: 1 step: 799, loss is 0.1921914517879486\n",
      "epoch: 1 step: 800, loss is 0.10704340040683746\n",
      "epoch: 1 step: 801, loss is 0.26382049918174744\n",
      "epoch: 1 step: 802, loss is 0.4355892241001129\n",
      "epoch: 1 step: 803, loss is 0.05023439973592758\n",
      "epoch: 1 step: 804, loss is 0.022155793383717537\n",
      "epoch: 1 step: 805, loss is 0.1747702658176422\n",
      "epoch: 1 step: 806, loss is 0.19542622566223145\n",
      "epoch: 1 step: 807, loss is 0.07427790760993958\n",
      "epoch: 1 step: 808, loss is 0.3124256134033203\n",
      "epoch: 1 step: 809, loss is 0.136952206492424\n",
      "epoch: 1 step: 810, loss is 0.021079299971461296\n",
      "epoch: 1 step: 811, loss is 0.12313639372587204\n",
      "epoch: 1 step: 812, loss is 0.036405324935913086\n",
      "epoch: 1 step: 813, loss is 0.09533258527517319\n",
      "epoch: 1 step: 814, loss is 0.11685507744550705\n",
      "epoch: 1 step: 815, loss is 0.3584708571434021\n",
      "epoch: 1 step: 816, loss is 0.0338616780936718\n",
      "epoch: 1 step: 817, loss is 0.11523909866809845\n",
      "epoch: 1 step: 818, loss is 0.19075985252857208\n",
      "epoch: 1 step: 819, loss is 0.020869266241788864\n",
      "epoch: 1 step: 820, loss is 0.07771588861942291\n",
      "epoch: 1 step: 821, loss is 0.014650605618953705\n",
      "epoch: 1 step: 822, loss is 0.08280874043703079\n",
      "epoch: 1 step: 823, loss is 0.04614664614200592\n",
      "epoch: 1 step: 824, loss is 0.07845209538936615\n",
      "epoch: 1 step: 825, loss is 0.3164377510547638\n",
      "epoch: 1 step: 826, loss is 0.2219492793083191\n",
      "epoch: 1 step: 827, loss is 0.07956863939762115\n",
      "epoch: 1 step: 828, loss is 0.07367879897356033\n",
      "epoch: 1 step: 829, loss is 0.05500541999936104\n",
      "epoch: 1 step: 830, loss is 0.07098458707332611\n",
      "epoch: 1 step: 831, loss is 0.07081561535596848\n",
      "epoch: 1 step: 832, loss is 0.19234836101531982\n",
      "epoch: 1 step: 833, loss is 0.3272745907306671\n",
      "epoch: 1 step: 834, loss is 0.01814563013613224\n",
      "epoch: 1 step: 835, loss is 0.22906102240085602\n",
      "epoch: 1 step: 836, loss is 0.22922329604625702\n",
      "epoch: 1 step: 837, loss is 0.12815451622009277\n",
      "epoch: 1 step: 838, loss is 0.10430523008108139\n",
      "epoch: 1 step: 839, loss is 0.3633933961391449\n",
      "epoch: 1 step: 840, loss is 0.15242263674736023\n",
      "epoch: 1 step: 841, loss is 0.1421540081501007\n",
      "epoch: 1 step: 842, loss is 0.06266450881958008\n",
      "epoch: 1 step: 843, loss is 0.05254566669464111\n",
      "epoch: 1 step: 844, loss is 0.13606980443000793\n",
      "epoch: 1 step: 845, loss is 0.06642871350049973\n",
      "epoch: 1 step: 846, loss is 0.22458940744400024\n",
      "epoch: 1 step: 847, loss is 0.07485054433345795\n",
      "epoch: 1 step: 848, loss is 0.1282101571559906\n",
      "epoch: 1 step: 849, loss is 0.04573255032300949\n",
      "epoch: 1 step: 850, loss is 0.0989660695195198\n",
      "epoch: 1 step: 851, loss is 0.14905095100402832\n",
      "epoch: 1 step: 852, loss is 0.05280202999711037\n",
      "epoch: 1 step: 853, loss is 0.06327775865793228\n",
      "epoch: 1 step: 854, loss is 0.02854990027844906\n",
      "epoch: 1 step: 855, loss is 0.19519513845443726\n",
      "epoch: 1 step: 856, loss is 0.27234312891960144\n",
      "epoch: 1 step: 857, loss is 0.028407014906406403\n",
      "epoch: 1 step: 858, loss is 0.037571411579847336\n",
      "epoch: 1 step: 859, loss is 0.04252500459551811\n",
      "epoch: 1 step: 860, loss is 0.07606232166290283\n",
      "epoch: 1 step: 861, loss is 0.20627129077911377\n",
      "epoch: 1 step: 862, loss is 0.16908781230449677\n",
      "epoch: 1 step: 863, loss is 0.16262198984622955\n",
      "epoch: 1 step: 864, loss is 0.312045693397522\n",
      "epoch: 1 step: 865, loss is 0.21725471317768097\n",
      "epoch: 1 step: 866, loss is 0.08197880536317825\n",
      "epoch: 1 step: 867, loss is 0.0628485307097435\n",
      "epoch: 1 step: 868, loss is 0.15535132586956024\n",
      "epoch: 1 step: 869, loss is 0.12085177004337311\n",
      "epoch: 1 step: 870, loss is 0.011831502430140972\n",
      "epoch: 1 step: 871, loss is 0.2493990957736969\n",
      "epoch: 1 step: 872, loss is 0.05043753981590271\n",
      "epoch: 1 step: 873, loss is 0.05149971321225166\n",
      "epoch: 1 step: 874, loss is 0.07963347434997559\n",
      "epoch: 1 step: 875, loss is 0.09900771081447601\n",
      "epoch: 1 step: 876, loss is 0.16047550737857819\n",
      "epoch: 1 step: 877, loss is 0.08230836689472198\n",
      "epoch: 1 step: 878, loss is 0.18841080367565155\n",
      "epoch: 1 step: 879, loss is 0.24194112420082092\n",
      "epoch: 1 step: 880, loss is 0.11768443137407303\n",
      "epoch: 1 step: 881, loss is 0.043630048632621765\n",
      "epoch: 1 step: 882, loss is 0.05746970325708389\n",
      "epoch: 1 step: 883, loss is 0.1157948300242424\n",
      "epoch: 1 step: 884, loss is 0.024897413328289986\n",
      "epoch: 1 step: 885, loss is 0.032695114612579346\n",
      "epoch: 1 step: 886, loss is 0.013208982534706593\n",
      "epoch: 1 step: 887, loss is 0.03675622120499611\n",
      "epoch: 1 step: 888, loss is 0.08116141706705093\n",
      "epoch: 1 step: 889, loss is 0.2579053044319153\n",
      "epoch: 1 step: 890, loss is 0.07275064289569855\n",
      "epoch: 1 step: 891, loss is 0.16019773483276367\n",
      "epoch: 1 step: 892, loss is 0.20372173190116882\n",
      "epoch: 1 step: 893, loss is 0.0208144448697567\n",
      "epoch: 1 step: 894, loss is 0.05758141353726387\n",
      "epoch: 1 step: 895, loss is 0.10106530785560608\n",
      "epoch: 1 step: 896, loss is 0.2702634334564209\n",
      "epoch: 1 step: 897, loss is 0.011561819352209568\n",
      "epoch: 1 step: 898, loss is 0.09546622633934021\n",
      "epoch: 1 step: 899, loss is 0.19204115867614746\n",
      "epoch: 1 step: 900, loss is 0.04149441048502922\n",
      "epoch: 1 step: 901, loss is 0.23782849311828613\n",
      "epoch: 1 step: 902, loss is 0.10099136829376221\n",
      "epoch: 1 step: 903, loss is 0.14250880479812622\n",
      "epoch: 1 step: 904, loss is 0.01814163289964199\n",
      "epoch: 1 step: 905, loss is 0.1702907234430313\n",
      "epoch: 1 step: 906, loss is 0.26051321625709534\n",
      "epoch: 1 step: 907, loss is 0.05322111397981644\n",
      "epoch: 1 step: 908, loss is 0.02370082400739193\n",
      "epoch: 1 step: 909, loss is 0.07778587937355042\n",
      "epoch: 1 step: 910, loss is 0.018320182338356972\n",
      "epoch: 1 step: 911, loss is 0.13695572316646576\n",
      "epoch: 1 step: 912, loss is 0.1238589808344841\n",
      "epoch: 1 step: 913, loss is 0.017813730984926224\n",
      "epoch: 1 step: 914, loss is 0.2541626989841461\n",
      "epoch: 1 step: 915, loss is 0.3596697747707367\n",
      "epoch: 1 step: 916, loss is 0.14648288488388062\n",
      "epoch: 1 step: 917, loss is 0.1368904709815979\n",
      "epoch: 1 step: 918, loss is 0.06275729835033417\n",
      "epoch: 1 step: 919, loss is 0.010656546801328659\n",
      "epoch: 1 step: 920, loss is 0.2399308830499649\n",
      "epoch: 1 step: 921, loss is 0.0795993059873581\n",
      "epoch: 1 step: 922, loss is 0.1469222605228424\n",
      "epoch: 1 step: 923, loss is 0.016558144241571426\n",
      "epoch: 1 step: 924, loss is 0.117841437458992\n",
      "epoch: 1 step: 925, loss is 0.058641962707042694\n",
      "epoch: 1 step: 926, loss is 0.049760494381189346\n",
      "epoch: 1 step: 927, loss is 0.07333505153656006\n",
      "epoch: 1 step: 928, loss is 0.10237649083137512\n",
      "epoch: 1 step: 929, loss is 0.14374175667762756\n",
      "epoch: 1 step: 930, loss is 0.06607156246900558\n",
      "epoch: 1 step: 931, loss is 0.18722683191299438\n",
      "epoch: 1 step: 932, loss is 0.11358549445867538\n",
      "epoch: 1 step: 933, loss is 0.14436431229114532\n",
      "epoch: 1 step: 934, loss is 0.06347916275262833\n",
      "epoch: 1 step: 935, loss is 0.06463515758514404\n",
      "epoch: 1 step: 936, loss is 0.07686017453670502\n",
      "epoch: 1 step: 937, loss is 0.07316245883703232\n",
      "epoch: 1 step: 938, loss is 0.12310659140348434\n",
      "epoch: 1 step: 939, loss is 0.3434673547744751\n",
      "epoch: 1 step: 940, loss is 0.027574723586440086\n",
      "epoch: 1 step: 941, loss is 0.009762875735759735\n",
      "epoch: 1 step: 942, loss is 0.13149923086166382\n",
      "epoch: 1 step: 943, loss is 0.054061464965343475\n",
      "epoch: 1 step: 944, loss is 0.11147985607385635\n",
      "epoch: 1 step: 945, loss is 0.014447052031755447\n",
      "epoch: 1 step: 946, loss is 0.06928281486034393\n",
      "epoch: 1 step: 947, loss is 0.21375104784965515\n",
      "epoch: 1 step: 948, loss is 0.07834302634000778\n",
      "epoch: 1 step: 949, loss is 0.079465851187706\n",
      "epoch: 1 step: 950, loss is 0.08843464404344559\n",
      "epoch: 1 step: 951, loss is 0.1976931095123291\n",
      "epoch: 1 step: 952, loss is 0.018738484010100365\n",
      "epoch: 1 step: 953, loss is 0.3329857885837555\n",
      "epoch: 1 step: 954, loss is 0.004581207409501076\n",
      "epoch: 1 step: 955, loss is 0.08822595328092575\n",
      "epoch: 1 step: 956, loss is 0.03808204084634781\n",
      "epoch: 1 step: 957, loss is 0.0055724335834383965\n",
      "epoch: 1 step: 958, loss is 0.17440776526927948\n",
      "epoch: 1 step: 959, loss is 0.06079722195863724\n",
      "epoch: 1 step: 960, loss is 0.014723091386258602\n",
      "epoch: 1 step: 961, loss is 0.04399527236819267\n",
      "epoch: 1 step: 962, loss is 0.030975019559264183\n",
      "epoch: 1 step: 963, loss is 0.014838449656963348\n",
      "epoch: 1 step: 964, loss is 0.02822227030992508\n",
      "epoch: 1 step: 965, loss is 0.05463070794939995\n",
      "epoch: 1 step: 966, loss is 0.19557204842567444\n",
      "epoch: 1 step: 967, loss is 0.048055265098810196\n",
      "epoch: 1 step: 968, loss is 0.1560031771659851\n",
      "epoch: 1 step: 969, loss is 0.11209623515605927\n",
      "epoch: 1 step: 970, loss is 0.012768372893333435\n",
      "epoch: 1 step: 971, loss is 0.027635136619210243\n",
      "epoch: 1 step: 972, loss is 0.17142215371131897\n",
      "epoch: 1 step: 973, loss is 0.40507158637046814\n",
      "epoch: 1 step: 974, loss is 0.020489094778895378\n",
      "epoch: 1 step: 975, loss is 0.15339888632297516\n",
      "epoch: 1 step: 976, loss is 0.04564090818166733\n",
      "epoch: 1 step: 977, loss is 0.09839168936014175\n",
      "epoch: 1 step: 978, loss is 0.0064788442105054855\n",
      "epoch: 1 step: 979, loss is 0.2509225010871887\n",
      "epoch: 1 step: 980, loss is 0.20632199943065643\n",
      "epoch: 1 step: 981, loss is 0.1261405348777771\n",
      "epoch: 1 step: 982, loss is 0.13442909717559814\n",
      "epoch: 1 step: 983, loss is 0.054978132247924805\n",
      "epoch: 1 step: 984, loss is 0.4873884916305542\n",
      "epoch: 1 step: 985, loss is 0.18126694858074188\n",
      "epoch: 1 step: 986, loss is 0.07860349863767624\n",
      "epoch: 1 step: 987, loss is 0.1291622519493103\n",
      "epoch: 1 step: 988, loss is 0.010222459211945534\n",
      "epoch: 1 step: 989, loss is 0.05446292459964752\n",
      "epoch: 1 step: 990, loss is 0.06242380291223526\n",
      "epoch: 1 step: 991, loss is 0.07864487916231155\n",
      "epoch: 1 step: 992, loss is 0.024207402020692825\n",
      "epoch: 1 step: 993, loss is 0.05218008533120155\n",
      "epoch: 1 step: 994, loss is 0.06376927345991135\n",
      "epoch: 1 step: 995, loss is 0.09353600442409515\n",
      "epoch: 1 step: 996, loss is 0.051318928599357605\n",
      "epoch: 1 step: 997, loss is 0.3451608717441559\n",
      "epoch: 1 step: 998, loss is 0.025926105678081512\n",
      "epoch: 1 step: 999, loss is 0.12992605566978455\n",
      "epoch: 1 step: 1000, loss is 0.027341123670339584\n",
      "epoch: 1 step: 1001, loss is 0.11981458961963654\n",
      "epoch: 1 step: 1002, loss is 0.022669972851872444\n",
      "epoch: 1 step: 1003, loss is 0.014900551177561283\n",
      "epoch: 1 step: 1004, loss is 0.09011262655258179\n",
      "epoch: 1 step: 1005, loss is 0.18652866780757904\n",
      "epoch: 1 step: 1006, loss is 0.2165137380361557\n",
      "epoch: 1 step: 1007, loss is 0.08143895864486694\n",
      "epoch: 1 step: 1008, loss is 0.00882895290851593\n",
      "epoch: 1 step: 1009, loss is 0.06531578302383423\n",
      "epoch: 1 step: 1010, loss is 0.37991955876350403\n",
      "epoch: 1 step: 1011, loss is 0.114296555519104\n",
      "epoch: 1 step: 1012, loss is 0.010511180385947227\n",
      "epoch: 1 step: 1013, loss is 0.05884148180484772\n",
      "epoch: 1 step: 1014, loss is 0.05798500031232834\n",
      "epoch: 1 step: 1015, loss is 0.43899962306022644\n",
      "epoch: 1 step: 1016, loss is 0.3263086974620819\n",
      "epoch: 1 step: 1017, loss is 0.18642960488796234\n",
      "epoch: 1 step: 1018, loss is 0.08191145211458206\n",
      "epoch: 1 step: 1019, loss is 0.13856180012226105\n",
      "epoch: 1 step: 1020, loss is 0.20646047592163086\n",
      "epoch: 1 step: 1021, loss is 0.13710862398147583\n",
      "epoch: 1 step: 1022, loss is 0.008713974617421627\n",
      "epoch: 1 step: 1023, loss is 0.20907928049564362\n",
      "epoch: 1 step: 1024, loss is 0.055863119661808014\n",
      "epoch: 1 step: 1025, loss is 0.2588513195514679\n",
      "epoch: 1 step: 1026, loss is 0.10577966272830963\n",
      "epoch: 1 step: 1027, loss is 0.07488246262073517\n",
      "epoch: 1 step: 1028, loss is 0.005286765284836292\n",
      "epoch: 1 step: 1029, loss is 0.04528703913092613\n",
      "epoch: 1 step: 1030, loss is 0.0829891636967659\n",
      "epoch: 1 step: 1031, loss is 0.08408230543136597\n",
      "epoch: 1 step: 1032, loss is 0.08778529614210129\n",
      "epoch: 1 step: 1033, loss is 0.4090602695941925\n",
      "epoch: 1 step: 1034, loss is 0.04737802967429161\n",
      "epoch: 1 step: 1035, loss is 0.051274124532938004\n",
      "epoch: 1 step: 1036, loss is 0.17612004280090332\n",
      "epoch: 1 step: 1037, loss is 0.035722702741622925\n",
      "epoch: 1 step: 1038, loss is 0.03423036262392998\n",
      "epoch: 1 step: 1039, loss is 0.3987044394016266\n",
      "epoch: 1 step: 1040, loss is 0.029902108013629913\n",
      "epoch: 1 step: 1041, loss is 0.05562816187739372\n",
      "epoch: 1 step: 1042, loss is 0.04484041780233383\n",
      "epoch: 1 step: 1043, loss is 0.03216052055358887\n",
      "epoch: 1 step: 1044, loss is 0.16158132255077362\n",
      "epoch: 1 step: 1045, loss is 0.015883823856711388\n",
      "epoch: 1 step: 1046, loss is 0.04390377923846245\n",
      "epoch: 1 step: 1047, loss is 0.3334730863571167\n",
      "epoch: 1 step: 1048, loss is 0.014237764291465282\n",
      "epoch: 1 step: 1049, loss is 0.07553263008594513\n",
      "epoch: 1 step: 1050, loss is 0.3597179055213928\n",
      "epoch: 1 step: 1051, loss is 0.02478509396314621\n",
      "epoch: 1 step: 1052, loss is 0.14448878169059753\n",
      "epoch: 1 step: 1053, loss is 0.056406330317258835\n",
      "epoch: 1 step: 1054, loss is 0.4483635425567627\n",
      "epoch: 1 step: 1055, loss is 0.19524745643138885\n",
      "epoch: 1 step: 1056, loss is 0.04012574627995491\n",
      "epoch: 1 step: 1057, loss is 0.368938684463501\n",
      "epoch: 1 step: 1058, loss is 0.21491247415542603\n",
      "epoch: 1 step: 1059, loss is 0.12481001764535904\n",
      "epoch: 1 step: 1060, loss is 0.05621379241347313\n",
      "epoch: 1 step: 1061, loss is 0.021995382383465767\n",
      "epoch: 1 step: 1062, loss is 0.3188300132751465\n",
      "epoch: 1 step: 1063, loss is 0.06316885352134705\n",
      "epoch: 1 step: 1064, loss is 0.09332999587059021\n",
      "epoch: 1 step: 1065, loss is 0.14624330401420593\n",
      "epoch: 1 step: 1066, loss is 0.04088732972741127\n",
      "epoch: 1 step: 1067, loss is 0.5551424622535706\n",
      "epoch: 1 step: 1068, loss is 0.3344794511795044\n",
      "epoch: 1 step: 1069, loss is 0.16131402552127838\n",
      "epoch: 1 step: 1070, loss is 0.11592777073383331\n",
      "epoch: 1 step: 1071, loss is 0.09092094004154205\n",
      "epoch: 1 step: 1072, loss is 0.06454681605100632\n",
      "epoch: 1 step: 1073, loss is 0.14743494987487793\n",
      "epoch: 1 step: 1074, loss is 0.025554439052939415\n",
      "epoch: 1 step: 1075, loss is 0.01909627951681614\n",
      "epoch: 1 step: 1076, loss is 0.13129714131355286\n",
      "epoch: 1 step: 1077, loss is 0.16944235563278198\n",
      "epoch: 1 step: 1078, loss is 0.12615042924880981\n",
      "epoch: 1 step: 1079, loss is 0.09231673181056976\n",
      "epoch: 1 step: 1080, loss is 0.12949958443641663\n",
      "epoch: 1 step: 1081, loss is 0.04293717443943024\n",
      "epoch: 1 step: 1082, loss is 0.03950321301817894\n",
      "epoch: 1 step: 1083, loss is 0.040927059948444366\n",
      "epoch: 1 step: 1084, loss is 0.1269809752702713\n",
      "epoch: 1 step: 1085, loss is 0.04947840794920921\n",
      "epoch: 1 step: 1086, loss is 0.10235758870840073\n",
      "epoch: 1 step: 1087, loss is 0.12117636948823929\n",
      "epoch: 1 step: 1088, loss is 0.1687171459197998\n",
      "epoch: 1 step: 1089, loss is 0.18152710795402527\n",
      "epoch: 1 step: 1090, loss is 0.01694326288998127\n",
      "epoch: 1 step: 1091, loss is 0.059403225779533386\n",
      "epoch: 1 step: 1092, loss is 0.11058761179447174\n",
      "epoch: 1 step: 1093, loss is 0.1576506346464157\n",
      "epoch: 1 step: 1094, loss is 0.02575826831161976\n",
      "epoch: 1 step: 1095, loss is 0.1744128167629242\n",
      "epoch: 1 step: 1096, loss is 0.010247849859297276\n",
      "epoch: 1 step: 1097, loss is 0.04819132015109062\n",
      "epoch: 1 step: 1098, loss is 0.05742517486214638\n",
      "epoch: 1 step: 1099, loss is 0.06596729159355164\n",
      "epoch: 1 step: 1100, loss is 0.0286706555634737\n",
      "epoch: 1 step: 1101, loss is 0.1760430634021759\n",
      "epoch: 1 step: 1102, loss is 0.2868131101131439\n",
      "epoch: 1 step: 1103, loss is 0.058120083063840866\n",
      "epoch: 1 step: 1104, loss is 0.10911694169044495\n",
      "epoch: 1 step: 1105, loss is 0.005792098119854927\n",
      "epoch: 1 step: 1106, loss is 0.12135567516088486\n",
      "epoch: 1 step: 1107, loss is 0.06280563771724701\n",
      "epoch: 1 step: 1108, loss is 0.007388210855424404\n",
      "epoch: 1 step: 1109, loss is 0.3424130976200104\n",
      "epoch: 1 step: 1110, loss is 0.2903766930103302\n",
      "epoch: 1 step: 1111, loss is 0.003817385993897915\n",
      "epoch: 1 step: 1112, loss is 0.010699729435145855\n",
      "epoch: 1 step: 1113, loss is 0.056351397186517715\n",
      "epoch: 1 step: 1114, loss is 0.0973876342177391\n",
      "epoch: 1 step: 1115, loss is 0.09065743535757065\n",
      "epoch: 1 step: 1116, loss is 0.05489867925643921\n",
      "epoch: 1 step: 1117, loss is 0.19970981776714325\n",
      "epoch: 1 step: 1118, loss is 0.011572843417525291\n",
      "epoch: 1 step: 1119, loss is 0.33937352895736694\n",
      "epoch: 1 step: 1120, loss is 0.07250108569860458\n",
      "epoch: 1 step: 1121, loss is 0.13554193079471588\n",
      "epoch: 1 step: 1122, loss is 0.1132892593741417\n",
      "epoch: 1 step: 1123, loss is 0.06493962556123734\n",
      "epoch: 1 step: 1124, loss is 0.040028154850006104\n",
      "epoch: 1 step: 1125, loss is 0.01631917990744114\n",
      "epoch: 1 step: 1126, loss is 0.032424114644527435\n",
      "epoch: 1 step: 1127, loss is 0.06428327411413193\n",
      "epoch: 1 step: 1128, loss is 0.1890771985054016\n",
      "epoch: 1 step: 1129, loss is 0.07601169496774673\n",
      "epoch: 1 step: 1130, loss is 0.14255309104919434\n",
      "epoch: 1 step: 1131, loss is 0.3249346911907196\n",
      "epoch: 1 step: 1132, loss is 0.04458269104361534\n",
      "epoch: 1 step: 1133, loss is 0.1886952668428421\n",
      "epoch: 1 step: 1134, loss is 0.15370342135429382\n",
      "epoch: 1 step: 1135, loss is 0.1694389283657074\n",
      "epoch: 1 step: 1136, loss is 0.04734952747821808\n",
      "epoch: 1 step: 1137, loss is 0.2414272278547287\n",
      "epoch: 1 step: 1138, loss is 0.1123557984828949\n",
      "epoch: 1 step: 1139, loss is 0.06814040243625641\n",
      "epoch: 1 step: 1140, loss is 0.1706748902797699\n",
      "epoch: 1 step: 1141, loss is 0.07470817118883133\n",
      "epoch: 1 step: 1142, loss is 0.2217569202184677\n",
      "epoch: 1 step: 1143, loss is 0.05675504356622696\n",
      "epoch: 1 step: 1144, loss is 0.12628173828125\n",
      "epoch: 1 step: 1145, loss is 0.09684870392084122\n",
      "epoch: 1 step: 1146, loss is 0.23944517970085144\n",
      "epoch: 1 step: 1147, loss is 0.1701023280620575\n",
      "epoch: 1 step: 1148, loss is 0.09718728810548782\n",
      "epoch: 1 step: 1149, loss is 0.18478354811668396\n",
      "epoch: 1 step: 1150, loss is 0.07865593582391739\n",
      "epoch: 1 step: 1151, loss is 0.1529979258775711\n",
      "epoch: 1 step: 1152, loss is 0.20857906341552734\n",
      "epoch: 1 step: 1153, loss is 0.08578260987997055\n",
      "epoch: 1 step: 1154, loss is 0.059914518147706985\n",
      "epoch: 1 step: 1155, loss is 0.03704451024532318\n",
      "epoch: 1 step: 1156, loss is 0.049720074981451035\n",
      "epoch: 1 step: 1157, loss is 0.02024567686021328\n",
      "epoch: 1 step: 1158, loss is 0.029207857325673103\n",
      "epoch: 1 step: 1159, loss is 0.015214907005429268\n",
      "epoch: 1 step: 1160, loss is 0.03772131726145744\n",
      "epoch: 1 step: 1161, loss is 0.28107401728630066\n",
      "epoch: 1 step: 1162, loss is 0.22324487566947937\n",
      "epoch: 1 step: 1163, loss is 0.16917994618415833\n",
      "epoch: 1 step: 1164, loss is 0.24539007246494293\n",
      "epoch: 1 step: 1165, loss is 0.11881645768880844\n",
      "epoch: 1 step: 1166, loss is 0.05548863485455513\n",
      "epoch: 1 step: 1167, loss is 0.06893354654312134\n",
      "epoch: 1 step: 1168, loss is 0.020069755613803864\n",
      "epoch: 1 step: 1169, loss is 0.05313951522111893\n",
      "epoch: 1 step: 1170, loss is 0.013040500693023205\n",
      "epoch: 1 step: 1171, loss is 0.006654943339526653\n",
      "epoch: 1 step: 1172, loss is 0.01592152565717697\n",
      "epoch: 1 step: 1173, loss is 0.17461970448493958\n",
      "epoch: 1 step: 1174, loss is 0.13264644145965576\n",
      "epoch: 1 step: 1175, loss is 0.22196051478385925\n",
      "epoch: 1 step: 1176, loss is 0.01937052048742771\n",
      "epoch: 1 step: 1177, loss is 0.020120933651924133\n",
      "epoch: 1 step: 1178, loss is 0.06378902494907379\n",
      "epoch: 1 step: 1179, loss is 0.2322189062833786\n",
      "epoch: 1 step: 1180, loss is 0.07147850096225739\n",
      "epoch: 1 step: 1181, loss is 0.041429441422224045\n",
      "epoch: 1 step: 1182, loss is 0.09910453855991364\n",
      "epoch: 1 step: 1183, loss is 0.12166018784046173\n",
      "epoch: 1 step: 1184, loss is 0.24615415930747986\n",
      "epoch: 1 step: 1185, loss is 0.09661119431257248\n",
      "epoch: 1 step: 1186, loss is 0.0912114754319191\n",
      "epoch: 1 step: 1187, loss is 0.03092300519347191\n",
      "epoch: 1 step: 1188, loss is 0.02877862937748432\n",
      "epoch: 1 step: 1189, loss is 0.061252836138010025\n",
      "epoch: 1 step: 1190, loss is 0.08226949721574783\n",
      "epoch: 1 step: 1191, loss is 0.23007678985595703\n",
      "epoch: 1 step: 1192, loss is 0.06514611840248108\n",
      "epoch: 1 step: 1193, loss is 0.04771064966917038\n",
      "epoch: 1 step: 1194, loss is 0.05817912518978119\n",
      "epoch: 1 step: 1195, loss is 0.015279199928045273\n",
      "epoch: 1 step: 1196, loss is 0.03518703952431679\n",
      "epoch: 1 step: 1197, loss is 0.06870399415493011\n",
      "epoch: 1 step: 1198, loss is 0.17964644730091095\n",
      "epoch: 1 step: 1199, loss is 0.04168097674846649\n",
      "epoch: 1 step: 1200, loss is 0.11379780620336533\n",
      "epoch: 1 step: 1201, loss is 0.2344641387462616\n",
      "epoch: 1 step: 1202, loss is 0.08071091026067734\n",
      "epoch: 1 step: 1203, loss is 0.004974784795194864\n",
      "epoch: 1 step: 1204, loss is 0.17468559741973877\n",
      "epoch: 1 step: 1205, loss is 0.061937980353832245\n",
      "epoch: 1 step: 1206, loss is 0.04651670902967453\n",
      "epoch: 1 step: 1207, loss is 0.03772994130849838\n",
      "epoch: 1 step: 1208, loss is 0.17130963504314423\n",
      "epoch: 1 step: 1209, loss is 0.03878980129957199\n",
      "epoch: 1 step: 1210, loss is 0.0895434096455574\n",
      "epoch: 1 step: 1211, loss is 0.21031467616558075\n",
      "epoch: 1 step: 1212, loss is 0.1197885125875473\n",
      "epoch: 1 step: 1213, loss is 0.09127490967512131\n",
      "epoch: 1 step: 1214, loss is 0.09710722416639328\n",
      "epoch: 1 step: 1215, loss is 0.030011000111699104\n",
      "epoch: 1 step: 1216, loss is 0.03169265016913414\n",
      "epoch: 1 step: 1217, loss is 0.15214700996875763\n",
      "epoch: 1 step: 1218, loss is 0.006686696782708168\n",
      "epoch: 1 step: 1219, loss is 0.0026446632109582424\n",
      "epoch: 1 step: 1220, loss is 0.11259142309427261\n",
      "epoch: 1 step: 1221, loss is 0.03927266597747803\n",
      "epoch: 1 step: 1222, loss is 0.060389790683984756\n",
      "epoch: 1 step: 1223, loss is 0.1231682077050209\n",
      "epoch: 1 step: 1224, loss is 0.12162088602781296\n",
      "epoch: 1 step: 1225, loss is 0.04732786864042282\n",
      "epoch: 1 step: 1226, loss is 0.025479406118392944\n",
      "epoch: 1 step: 1227, loss is 0.12468263506889343\n",
      "epoch: 1 step: 1228, loss is 0.021641124039888382\n",
      "epoch: 1 step: 1229, loss is 0.07629469782114029\n",
      "epoch: 1 step: 1230, loss is 0.007589519955217838\n",
      "epoch: 1 step: 1231, loss is 0.3395673334598541\n",
      "epoch: 1 step: 1232, loss is 0.22391866147518158\n",
      "epoch: 1 step: 1233, loss is 0.01656581647694111\n",
      "epoch: 1 step: 1234, loss is 0.28723689913749695\n",
      "epoch: 1 step: 1235, loss is 0.008025839924812317\n",
      "epoch: 1 step: 1236, loss is 0.020769620314240456\n",
      "epoch: 1 step: 1237, loss is 0.1307135820388794\n",
      "epoch: 1 step: 1238, loss is 0.016915449872612953\n",
      "epoch: 1 step: 1239, loss is 0.014705859124660492\n",
      "epoch: 1 step: 1240, loss is 0.056520234793424606\n",
      "epoch: 1 step: 1241, loss is 0.12951090931892395\n",
      "epoch: 1 step: 1242, loss is 0.05521237105131149\n",
      "epoch: 1 step: 1243, loss is 0.012760082259774208\n",
      "epoch: 1 step: 1244, loss is 0.18565279245376587\n",
      "epoch: 1 step: 1245, loss is 0.16850131750106812\n",
      "epoch: 1 step: 1246, loss is 0.0112691018730402\n",
      "epoch: 1 step: 1247, loss is 0.030471567064523697\n",
      "epoch: 1 step: 1248, loss is 0.010679586790502071\n",
      "epoch: 1 step: 1249, loss is 0.30917418003082275\n",
      "epoch: 1 step: 1250, loss is 0.039129968732595444\n",
      "epoch: 1 step: 1251, loss is 0.12425416707992554\n",
      "epoch: 1 step: 1252, loss is 0.021737098693847656\n",
      "epoch: 1 step: 1253, loss is 0.25736719369888306\n",
      "epoch: 1 step: 1254, loss is 0.08506286144256592\n",
      "epoch: 1 step: 1255, loss is 0.027595145627856255\n",
      "epoch: 1 step: 1256, loss is 0.11487887054681778\n",
      "epoch: 1 step: 1257, loss is 0.15923526883125305\n",
      "epoch: 1 step: 1258, loss is 0.020986460149288177\n",
      "epoch: 1 step: 1259, loss is 0.01506902277469635\n",
      "epoch: 1 step: 1260, loss is 0.034070227295160294\n",
      "epoch: 1 step: 1261, loss is 0.14669378101825714\n",
      "epoch: 1 step: 1262, loss is 0.020630287006497383\n",
      "epoch: 1 step: 1263, loss is 0.143571674823761\n",
      "epoch: 1 step: 1264, loss is 0.03106396086513996\n",
      "epoch: 1 step: 1265, loss is 0.06638379395008087\n",
      "epoch: 1 step: 1266, loss is 0.10084288567304611\n",
      "epoch: 1 step: 1267, loss is 0.12117559462785721\n",
      "epoch: 1 step: 1268, loss is 0.11658062040805817\n",
      "epoch: 1 step: 1269, loss is 0.10896588116884232\n",
      "epoch: 1 step: 1270, loss is 0.044548358768224716\n",
      "epoch: 1 step: 1271, loss is 0.1091637834906578\n",
      "epoch: 1 step: 1272, loss is 0.08750993758440018\n",
      "epoch: 1 step: 1273, loss is 0.051070451736450195\n",
      "epoch: 1 step: 1274, loss is 0.024563774466514587\n",
      "epoch: 1 step: 1275, loss is 0.33147725462913513\n",
      "epoch: 1 step: 1276, loss is 0.023537730798125267\n",
      "epoch: 1 step: 1277, loss is 0.021204952150583267\n",
      "epoch: 1 step: 1278, loss is 0.19102753698825836\n",
      "epoch: 1 step: 1279, loss is 0.019723642617464066\n",
      "epoch: 1 step: 1280, loss is 0.0061119720339775085\n",
      "epoch: 1 step: 1281, loss is 0.16965089738368988\n",
      "epoch: 1 step: 1282, loss is 0.0016390299424529076\n",
      "epoch: 1 step: 1283, loss is 0.18329228460788727\n",
      "epoch: 1 step: 1284, loss is 0.3323659598827362\n",
      "epoch: 1 step: 1285, loss is 0.1349373757839203\n",
      "epoch: 1 step: 1286, loss is 0.06374703347682953\n",
      "epoch: 1 step: 1287, loss is 0.13168174028396606\n",
      "epoch: 1 step: 1288, loss is 0.5248231291770935\n",
      "epoch: 1 step: 1289, loss is 0.022260092198848724\n",
      "epoch: 1 step: 1290, loss is 0.21118517220020294\n",
      "epoch: 1 step: 1291, loss is 0.0817599818110466\n",
      "epoch: 1 step: 1292, loss is 0.02701520547270775\n",
      "epoch: 1 step: 1293, loss is 0.05140455439686775\n",
      "epoch: 1 step: 1294, loss is 0.17368339002132416\n",
      "epoch: 1 step: 1295, loss is 0.08512680232524872\n",
      "epoch: 1 step: 1296, loss is 0.05208838731050491\n",
      "epoch: 1 step: 1297, loss is 0.04889639467000961\n",
      "epoch: 1 step: 1298, loss is 0.09984389692544937\n",
      "epoch: 1 step: 1299, loss is 0.04827458783984184\n",
      "epoch: 1 step: 1300, loss is 0.33920392394065857\n",
      "epoch: 1 step: 1301, loss is 0.07212894409894943\n",
      "epoch: 1 step: 1302, loss is 0.022092554718255997\n",
      "epoch: 1 step: 1303, loss is 0.021565204486250877\n",
      "epoch: 1 step: 1304, loss is 0.05534256622195244\n",
      "epoch: 1 step: 1305, loss is 0.05814647302031517\n",
      "epoch: 1 step: 1306, loss is 0.02657911367714405\n",
      "epoch: 1 step: 1307, loss is 0.381160706281662\n",
      "epoch: 1 step: 1308, loss is 0.11385169625282288\n",
      "epoch: 1 step: 1309, loss is 0.07750635594129562\n",
      "epoch: 1 step: 1310, loss is 0.1097073182463646\n",
      "epoch: 1 step: 1311, loss is 0.12304170429706573\n",
      "epoch: 1 step: 1312, loss is 0.022447949275374413\n",
      "epoch: 1 step: 1313, loss is 0.03719177469611168\n",
      "epoch: 1 step: 1314, loss is 0.040256720036268234\n",
      "epoch: 1 step: 1315, loss is 0.03728034719824791\n",
      "epoch: 1 step: 1316, loss is 0.08535023033618927\n",
      "epoch: 1 step: 1317, loss is 0.0634731724858284\n",
      "epoch: 1 step: 1318, loss is 0.08614500612020493\n",
      "epoch: 1 step: 1319, loss is 0.04637671634554863\n",
      "epoch: 1 step: 1320, loss is 0.07627718895673752\n",
      "epoch: 1 step: 1321, loss is 0.11142058670520782\n",
      "epoch: 1 step: 1322, loss is 0.15921619534492493\n",
      "epoch: 1 step: 1323, loss is 0.11890479922294617\n",
      "epoch: 1 step: 1324, loss is 0.013355416245758533\n",
      "epoch: 1 step: 1325, loss is 0.1251823604106903\n",
      "epoch: 1 step: 1326, loss is 0.011910657398402691\n",
      "epoch: 1 step: 1327, loss is 0.04405630752444267\n",
      "epoch: 1 step: 1328, loss is 0.046843841671943665\n",
      "epoch: 1 step: 1329, loss is 0.057457033544778824\n",
      "epoch: 1 step: 1330, loss is 0.039659395813941956\n",
      "epoch: 1 step: 1331, loss is 0.004895031452178955\n",
      "epoch: 1 step: 1332, loss is 0.12463527172803879\n",
      "epoch: 1 step: 1333, loss is 0.014466757886111736\n",
      "epoch: 1 step: 1334, loss is 0.008164661005139351\n",
      "epoch: 1 step: 1335, loss is 0.06640647351741791\n",
      "epoch: 1 step: 1336, loss is 0.010890557430684566\n",
      "epoch: 1 step: 1337, loss is 0.05021017789840698\n",
      "epoch: 1 step: 1338, loss is 0.01930912770330906\n",
      "epoch: 1 step: 1339, loss is 0.1213330551981926\n",
      "epoch: 1 step: 1340, loss is 0.11155203729867935\n",
      "epoch: 1 step: 1341, loss is 0.08551982045173645\n",
      "epoch: 1 step: 1342, loss is 0.019104259088635445\n",
      "epoch: 1 step: 1343, loss is 0.010785449296236038\n",
      "epoch: 1 step: 1344, loss is 0.09418126195669174\n",
      "epoch: 1 step: 1345, loss is 0.06024136021733284\n",
      "epoch: 1 step: 1346, loss is 0.08630089461803436\n",
      "epoch: 1 step: 1347, loss is 0.07388142496347427\n",
      "epoch: 1 step: 1348, loss is 0.3043631315231323\n",
      "epoch: 1 step: 1349, loss is 0.21303504705429077\n",
      "epoch: 1 step: 1350, loss is 0.1490936279296875\n",
      "epoch: 1 step: 1351, loss is 0.12008022516965866\n",
      "epoch: 1 step: 1352, loss is 0.13257843255996704\n",
      "epoch: 1 step: 1353, loss is 0.020662881433963776\n",
      "epoch: 1 step: 1354, loss is 0.19639822840690613\n",
      "epoch: 1 step: 1355, loss is 0.04978227615356445\n",
      "epoch: 1 step: 1356, loss is 0.0996057316660881\n",
      "epoch: 1 step: 1357, loss is 0.18832460045814514\n",
      "epoch: 1 step: 1358, loss is 0.27490749955177307\n",
      "epoch: 1 step: 1359, loss is 0.023462826386094093\n",
      "epoch: 1 step: 1360, loss is 0.038436148315668106\n",
      "epoch: 1 step: 1361, loss is 0.064771868288517\n",
      "epoch: 1 step: 1362, loss is 0.25539591908454895\n",
      "epoch: 1 step: 1363, loss is 0.035726312547922134\n",
      "epoch: 1 step: 1364, loss is 0.10811538994312286\n",
      "epoch: 1 step: 1365, loss is 0.15920695662498474\n",
      "epoch: 1 step: 1366, loss is 0.28431928157806396\n",
      "epoch: 1 step: 1367, loss is 0.1831359565258026\n",
      "epoch: 1 step: 1368, loss is 0.022518055513501167\n",
      "epoch: 1 step: 1369, loss is 0.050675034523010254\n",
      "epoch: 1 step: 1370, loss is 0.007691934239119291\n",
      "epoch: 1 step: 1371, loss is 0.3780187964439392\n",
      "epoch: 1 step: 1372, loss is 0.16580308973789215\n",
      "epoch: 1 step: 1373, loss is 0.14644554257392883\n",
      "epoch: 1 step: 1374, loss is 0.06374569237232208\n",
      "epoch: 1 step: 1375, loss is 0.027312183752655983\n",
      "epoch: 1 step: 1376, loss is 0.010501195676624775\n",
      "epoch: 1 step: 1377, loss is 0.03517720475792885\n",
      "epoch: 1 step: 1378, loss is 0.042918093502521515\n",
      "epoch: 1 step: 1379, loss is 0.2030736356973648\n",
      "epoch: 1 step: 1380, loss is 0.015142286196351051\n",
      "epoch: 1 step: 1381, loss is 0.09236679971218109\n",
      "epoch: 1 step: 1382, loss is 0.3450559079647064\n",
      "epoch: 1 step: 1383, loss is 0.15437589585781097\n",
      "epoch: 1 step: 1384, loss is 0.07269144803285599\n",
      "epoch: 1 step: 1385, loss is 0.1691408008337021\n",
      "epoch: 1 step: 1386, loss is 0.03118034638464451\n",
      "epoch: 1 step: 1387, loss is 0.007349831983447075\n",
      "epoch: 1 step: 1388, loss is 0.19636701047420502\n",
      "epoch: 1 step: 1389, loss is 0.07324417680501938\n",
      "epoch: 1 step: 1390, loss is 0.06296936422586441\n",
      "epoch: 1 step: 1391, loss is 0.03919704630970955\n",
      "epoch: 1 step: 1392, loss is 0.006620924919843674\n",
      "epoch: 1 step: 1393, loss is 0.009495618753135204\n",
      "epoch: 1 step: 1394, loss is 0.37431448698043823\n",
      "epoch: 1 step: 1395, loss is 0.009896617382764816\n",
      "epoch: 1 step: 1396, loss is 0.18809279799461365\n",
      "epoch: 1 step: 1397, loss is 0.292171448469162\n",
      "epoch: 1 step: 1398, loss is 0.06918743997812271\n",
      "epoch: 1 step: 1399, loss is 0.10913503170013428\n",
      "epoch: 1 step: 1400, loss is 0.26968467235565186\n",
      "epoch: 1 step: 1401, loss is 0.09862468391656876\n",
      "epoch: 1 step: 1402, loss is 0.2364039123058319\n",
      "epoch: 1 step: 1403, loss is 0.04460490867495537\n",
      "epoch: 1 step: 1404, loss is 0.021192124113440514\n",
      "epoch: 1 step: 1405, loss is 0.02477959170937538\n",
      "epoch: 1 step: 1406, loss is 0.03988850861787796\n",
      "epoch: 1 step: 1407, loss is 0.011502726003527641\n",
      "epoch: 1 step: 1408, loss is 0.006008635275065899\n",
      "epoch: 1 step: 1409, loss is 0.03203946724534035\n",
      "epoch: 1 step: 1410, loss is 0.20420843362808228\n",
      "epoch: 1 step: 1411, loss is 0.18088605999946594\n",
      "epoch: 1 step: 1412, loss is 0.005058805458247662\n",
      "epoch: 1 step: 1413, loss is 0.0651400163769722\n",
      "epoch: 1 step: 1414, loss is 0.21881328523159027\n",
      "epoch: 1 step: 1415, loss is 0.0629226490855217\n",
      "epoch: 1 step: 1416, loss is 0.10726666450500488\n",
      "epoch: 1 step: 1417, loss is 0.12014461308717728\n",
      "epoch: 1 step: 1418, loss is 0.1725795865058899\n",
      "epoch: 1 step: 1419, loss is 0.017263730987906456\n",
      "epoch: 1 step: 1420, loss is 0.22951844334602356\n",
      "epoch: 1 step: 1421, loss is 0.11206377297639847\n",
      "epoch: 1 step: 1422, loss is 0.16886919736862183\n",
      "epoch: 1 step: 1423, loss is 0.22067226469516754\n",
      "epoch: 1 step: 1424, loss is 0.083441361784935\n",
      "epoch: 1 step: 1425, loss is 0.017833929508924484\n",
      "epoch: 1 step: 1426, loss is 0.11753370612859726\n",
      "epoch: 1 step: 1427, loss is 0.07715889811515808\n",
      "epoch: 1 step: 1428, loss is 0.035957831889390945\n",
      "epoch: 1 step: 1429, loss is 0.704408586025238\n",
      "epoch: 1 step: 1430, loss is 0.02107342891395092\n",
      "epoch: 1 step: 1431, loss is 0.04209224879741669\n",
      "epoch: 1 step: 1432, loss is 0.02755909226834774\n",
      "epoch: 1 step: 1433, loss is 0.03215901553630829\n",
      "epoch: 1 step: 1434, loss is 0.09553046524524689\n",
      "epoch: 1 step: 1435, loss is 0.03556598722934723\n",
      "epoch: 1 step: 1436, loss is 0.1332334578037262\n",
      "epoch: 1 step: 1437, loss is 0.03162303566932678\n",
      "epoch: 1 step: 1438, loss is 0.07215582579374313\n",
      "epoch: 1 step: 1439, loss is 0.03670749068260193\n",
      "epoch: 1 step: 1440, loss is 0.12498392164707184\n",
      "epoch: 1 step: 1441, loss is 0.1472044587135315\n",
      "epoch: 1 step: 1442, loss is 0.01588270254433155\n",
      "epoch: 1 step: 1443, loss is 0.02747470512986183\n",
      "epoch: 1 step: 1444, loss is 0.022146007046103477\n",
      "epoch: 1 step: 1445, loss is 0.04482613876461983\n",
      "epoch: 1 step: 1446, loss is 0.19651426374912262\n",
      "epoch: 1 step: 1447, loss is 0.13507740199565887\n",
      "epoch: 1 step: 1448, loss is 0.14418424665927887\n",
      "epoch: 1 step: 1449, loss is 0.09890978783369064\n",
      "epoch: 1 step: 1450, loss is 0.11163585633039474\n",
      "epoch: 1 step: 1451, loss is 0.008757662028074265\n",
      "epoch: 1 step: 1452, loss is 0.22674770653247833\n",
      "epoch: 1 step: 1453, loss is 0.06916401535272598\n",
      "epoch: 1 step: 1454, loss is 0.2332131266593933\n",
      "epoch: 1 step: 1455, loss is 0.04193142428994179\n",
      "epoch: 1 step: 1456, loss is 0.03833451867103577\n",
      "epoch: 1 step: 1457, loss is 0.3325307071208954\n",
      "epoch: 1 step: 1458, loss is 0.05390039086341858\n",
      "epoch: 1 step: 1459, loss is 0.019443558529019356\n",
      "epoch: 1 step: 1460, loss is 0.09329091012477875\n",
      "epoch: 1 step: 1461, loss is 0.21708481013774872\n",
      "epoch: 1 step: 1462, loss is 0.11892271041870117\n",
      "epoch: 1 step: 1463, loss is 0.354387104511261\n",
      "epoch: 1 step: 1464, loss is 0.15554556250572205\n",
      "epoch: 1 step: 1465, loss is 0.07215408980846405\n",
      "epoch: 1 step: 1466, loss is 0.09893624484539032\n",
      "epoch: 1 step: 1467, loss is 0.08545053750276566\n",
      "epoch: 1 step: 1468, loss is 0.044767238199710846\n",
      "epoch: 1 step: 1469, loss is 0.1710706353187561\n",
      "epoch: 1 step: 1470, loss is 0.17905543744564056\n",
      "epoch: 1 step: 1471, loss is 0.015921492129564285\n",
      "epoch: 1 step: 1472, loss is 0.02496645785868168\n",
      "epoch: 1 step: 1473, loss is 0.10840512812137604\n",
      "epoch: 1 step: 1474, loss is 0.2608339488506317\n",
      "epoch: 1 step: 1475, loss is 0.055436693131923676\n",
      "epoch: 1 step: 1476, loss is 0.024031953886151314\n",
      "epoch: 1 step: 1477, loss is 0.5063212513923645\n",
      "epoch: 1 step: 1478, loss is 0.03431190922856331\n",
      "epoch: 1 step: 1479, loss is 0.06168760359287262\n",
      "epoch: 1 step: 1480, loss is 0.12435606122016907\n",
      "epoch: 1 step: 1481, loss is 0.023891737684607506\n",
      "epoch: 1 step: 1482, loss is 0.05802851542830467\n",
      "epoch: 1 step: 1483, loss is 0.2052524834871292\n",
      "epoch: 1 step: 1484, loss is 0.0479995496571064\n",
      "epoch: 1 step: 1485, loss is 0.13998936116695404\n",
      "epoch: 1 step: 1486, loss is 0.02942056581377983\n",
      "epoch: 1 step: 1487, loss is 0.06810861825942993\n",
      "epoch: 1 step: 1488, loss is 0.018836282193660736\n",
      "epoch: 1 step: 1489, loss is 0.02309591881930828\n",
      "epoch: 1 step: 1490, loss is 0.006535924505442381\n",
      "epoch: 1 step: 1491, loss is 0.16667678952217102\n",
      "epoch: 1 step: 1492, loss is 0.10996165871620178\n",
      "epoch: 1 step: 1493, loss is 0.21154557168483734\n",
      "epoch: 1 step: 1494, loss is 0.10235898196697235\n",
      "epoch: 1 step: 1495, loss is 0.057612862437963486\n",
      "epoch: 1 step: 1496, loss is 0.045515455305576324\n",
      "epoch: 1 step: 1497, loss is 0.00971097033470869\n",
      "epoch: 1 step: 1498, loss is 0.34770748019218445\n",
      "epoch: 1 step: 1499, loss is 0.24238555133342743\n",
      "epoch: 1 step: 1500, loss is 0.16839754581451416\n",
      "epoch: 1 step: 1501, loss is 0.25848186016082764\n",
      "epoch: 1 step: 1502, loss is 0.047663118690252304\n",
      "epoch: 1 step: 1503, loss is 0.042893651872873306\n",
      "epoch: 1 step: 1504, loss is 0.12236418575048447\n",
      "epoch: 1 step: 1505, loss is 0.010942338965833187\n",
      "epoch: 1 step: 1506, loss is 0.01988445222377777\n",
      "epoch: 1 step: 1507, loss is 0.19186021387577057\n",
      "epoch: 1 step: 1508, loss is 0.03929097205400467\n",
      "epoch: 1 step: 1509, loss is 0.18497920036315918\n",
      "epoch: 1 step: 1510, loss is 0.053736183792352676\n",
      "epoch: 1 step: 1511, loss is 0.09410693496465683\n",
      "epoch: 1 step: 1512, loss is 0.008181072771549225\n",
      "epoch: 1 step: 1513, loss is 0.12818661332130432\n",
      "epoch: 1 step: 1514, loss is 0.01480379980057478\n",
      "epoch: 1 step: 1515, loss is 0.01757471263408661\n",
      "epoch: 1 step: 1516, loss is 0.033638253808021545\n",
      "epoch: 1 step: 1517, loss is 0.02937532775104046\n",
      "epoch: 1 step: 1518, loss is 0.15549981594085693\n",
      "epoch: 1 step: 1519, loss is 0.07312300056219101\n",
      "epoch: 1 step: 1520, loss is 0.18580810725688934\n",
      "epoch: 1 step: 1521, loss is 0.03377189487218857\n",
      "epoch: 1 step: 1522, loss is 0.008240417577326298\n",
      "epoch: 1 step: 1523, loss is 0.18135978281497955\n",
      "epoch: 1 step: 1524, loss is 0.039608824998140335\n",
      "epoch: 1 step: 1525, loss is 0.0067176902666687965\n",
      "epoch: 1 step: 1526, loss is 0.006563897710293531\n",
      "epoch: 1 step: 1527, loss is 0.12235856056213379\n",
      "epoch: 1 step: 1528, loss is 0.19971993565559387\n",
      "epoch: 1 step: 1529, loss is 0.0233291182667017\n",
      "epoch: 1 step: 1530, loss is 0.04066960886120796\n",
      "epoch: 1 step: 1531, loss is 0.017348550260066986\n",
      "epoch: 1 step: 1532, loss is 0.0018744533881545067\n",
      "epoch: 1 step: 1533, loss is 0.06442826986312866\n",
      "epoch: 1 step: 1534, loss is 0.15563027560710907\n",
      "epoch: 1 step: 1535, loss is 0.01382968295365572\n",
      "epoch: 1 step: 1536, loss is 0.04622407257556915\n",
      "epoch: 1 step: 1537, loss is 0.05836739391088486\n",
      "epoch: 1 step: 1538, loss is 0.17505735158920288\n",
      "epoch: 1 step: 1539, loss is 0.030629223212599754\n",
      "epoch: 1 step: 1540, loss is 0.04475819319486618\n",
      "epoch: 1 step: 1541, loss is 0.01719881407916546\n",
      "epoch: 1 step: 1542, loss is 0.22584573924541473\n",
      "epoch: 1 step: 1543, loss is 0.07632284611463547\n",
      "epoch: 1 step: 1544, loss is 0.12850269675254822\n",
      "epoch: 1 step: 1545, loss is 0.014909151941537857\n",
      "epoch: 1 step: 1546, loss is 0.049666184931993484\n",
      "epoch: 1 step: 1547, loss is 0.015286645852029324\n",
      "epoch: 1 step: 1548, loss is 0.09225387871265411\n",
      "epoch: 1 step: 1549, loss is 0.0700942650437355\n",
      "epoch: 1 step: 1550, loss is 0.10709699243307114\n",
      "epoch: 1 step: 1551, loss is 0.007030695676803589\n",
      "epoch: 1 step: 1552, loss is 0.008781202137470245\n",
      "epoch: 1 step: 1553, loss is 0.05830540880560875\n",
      "epoch: 1 step: 1554, loss is 0.010048865340650082\n",
      "epoch: 1 step: 1555, loss is 0.20066522061824799\n",
      "epoch: 1 step: 1556, loss is 0.018279409036040306\n",
      "epoch: 1 step: 1557, loss is 0.06817036122083664\n",
      "epoch: 1 step: 1558, loss is 0.01337585598230362\n",
      "epoch: 1 step: 1559, loss is 0.07770711928606033\n",
      "epoch: 1 step: 1560, loss is 0.006718487478792667\n",
      "epoch: 1 step: 1561, loss is 0.05615601688623428\n",
      "epoch: 1 step: 1562, loss is 0.08672574162483215\n",
      "epoch: 1 step: 1563, loss is 0.05646899715065956\n",
      "epoch: 1 step: 1564, loss is 0.1519157588481903\n",
      "epoch: 1 step: 1565, loss is 0.026167243719100952\n",
      "epoch: 1 step: 1566, loss is 0.09557238966226578\n",
      "epoch: 1 step: 1567, loss is 0.037483956664800644\n",
      "epoch: 1 step: 1568, loss is 0.16657698154449463\n",
      "epoch: 1 step: 1569, loss is 0.07552096247673035\n",
      "epoch: 1 step: 1570, loss is 0.25032344460487366\n",
      "epoch: 1 step: 1571, loss is 0.017603276297450066\n",
      "epoch: 1 step: 1572, loss is 0.007208230905234814\n",
      "epoch: 1 step: 1573, loss is 0.08508458733558655\n",
      "epoch: 1 step: 1574, loss is 0.06348766386508942\n",
      "epoch: 1 step: 1575, loss is 0.059679340571165085\n",
      "epoch: 1 step: 1576, loss is 0.008213426917791367\n",
      "epoch: 1 step: 1577, loss is 0.004302077926695347\n",
      "epoch: 1 step: 1578, loss is 0.15998831391334534\n",
      "epoch: 1 step: 1579, loss is 0.1669018566608429\n",
      "epoch: 1 step: 1580, loss is 0.24954620003700256\n",
      "epoch: 1 step: 1581, loss is 0.2245074212551117\n",
      "epoch: 1 step: 1582, loss is 0.27693191170692444\n",
      "epoch: 1 step: 1583, loss is 0.02839270792901516\n",
      "epoch: 1 step: 1584, loss is 0.09916555881500244\n",
      "epoch: 1 step: 1585, loss is 0.10398966073989868\n",
      "epoch: 1 step: 1586, loss is 0.09624280780553818\n",
      "epoch: 1 step: 1587, loss is 0.022677883505821228\n",
      "epoch: 1 step: 1588, loss is 0.05549908056855202\n",
      "epoch: 1 step: 1589, loss is 0.29226163029670715\n",
      "epoch: 1 step: 1590, loss is 0.277610719203949\n",
      "epoch: 1 step: 1591, loss is 0.010319558903574944\n",
      "epoch: 1 step: 1592, loss is 0.2782599627971649\n",
      "epoch: 1 step: 1593, loss is 0.11891112476587296\n",
      "epoch: 1 step: 1594, loss is 0.045040469616651535\n",
      "epoch: 1 step: 1595, loss is 0.056105293333530426\n",
      "epoch: 1 step: 1596, loss is 0.19487963616847992\n",
      "epoch: 1 step: 1597, loss is 0.07789105921983719\n",
      "epoch: 1 step: 1598, loss is 0.14436447620391846\n",
      "epoch: 1 step: 1599, loss is 0.011761801317334175\n",
      "epoch: 1 step: 1600, loss is 0.0072988104075193405\n",
      "epoch: 1 step: 1601, loss is 0.08774867653846741\n",
      "epoch: 1 step: 1602, loss is 0.07438912242650986\n",
      "epoch: 1 step: 1603, loss is 0.07972375303506851\n",
      "epoch: 1 step: 1604, loss is 0.1338934451341629\n",
      "epoch: 1 step: 1605, loss is 0.039160121232271194\n",
      "epoch: 1 step: 1606, loss is 0.0030619304161518812\n",
      "epoch: 1 step: 1607, loss is 0.1985364556312561\n",
      "epoch: 1 step: 1608, loss is 0.1000731885433197\n",
      "epoch: 1 step: 1609, loss is 0.2629086375236511\n",
      "epoch: 1 step: 1610, loss is 0.03490390256047249\n",
      "epoch: 1 step: 1611, loss is 0.1098480224609375\n",
      "epoch: 1 step: 1612, loss is 0.01974616013467312\n",
      "epoch: 1 step: 1613, loss is 0.16333308815956116\n",
      "epoch: 1 step: 1614, loss is 0.20227564871311188\n",
      "epoch: 1 step: 1615, loss is 0.011081635020673275\n",
      "epoch: 1 step: 1616, loss is 0.18399423360824585\n",
      "epoch: 1 step: 1617, loss is 0.1212615892291069\n",
      "epoch: 1 step: 1618, loss is 0.06314563006162643\n",
      "epoch: 1 step: 1619, loss is 0.20233871042728424\n",
      "epoch: 1 step: 1620, loss is 0.009182394482195377\n",
      "epoch: 1 step: 1621, loss is 0.01807711273431778\n",
      "epoch: 1 step: 1622, loss is 0.07624636590480804\n",
      "epoch: 1 step: 1623, loss is 0.009678915143013\n",
      "epoch: 1 step: 1624, loss is 0.04705892503261566\n",
      "epoch: 1 step: 1625, loss is 0.06026078760623932\n",
      "epoch: 1 step: 1626, loss is 0.19536234438419342\n",
      "epoch: 1 step: 1627, loss is 0.006304121110588312\n",
      "epoch: 1 step: 1628, loss is 0.07586975395679474\n",
      "epoch: 1 step: 1629, loss is 0.022608624771237373\n",
      "epoch: 1 step: 1630, loss is 0.029751647263765335\n",
      "epoch: 1 step: 1631, loss is 0.012705701403319836\n",
      "epoch: 1 step: 1632, loss is 0.042616572231054306\n",
      "epoch: 1 step: 1633, loss is 0.027827883139252663\n",
      "epoch: 1 step: 1634, loss is 0.09953665733337402\n",
      "epoch: 1 step: 1635, loss is 0.03461518883705139\n",
      "epoch: 1 step: 1636, loss is 0.04006598889827728\n",
      "epoch: 1 step: 1637, loss is 0.28139424324035645\n",
      "epoch: 1 step: 1638, loss is 0.08632858842611313\n",
      "epoch: 1 step: 1639, loss is 0.02273751236498356\n",
      "epoch: 1 step: 1640, loss is 0.06434454023838043\n",
      "epoch: 1 step: 1641, loss is 0.009157057851552963\n",
      "epoch: 1 step: 1642, loss is 0.01731758378446102\n",
      "epoch: 1 step: 1643, loss is 0.002901113359257579\n",
      "epoch: 1 step: 1644, loss is 0.014823222532868385\n",
      "epoch: 1 step: 1645, loss is 0.06575122475624084\n",
      "epoch: 1 step: 1646, loss is 0.009291300550103188\n",
      "epoch: 1 step: 1647, loss is 0.2552613317966461\n",
      "epoch: 1 step: 1648, loss is 0.06754869967699051\n",
      "epoch: 1 step: 1649, loss is 0.38131484389305115\n",
      "epoch: 1 step: 1650, loss is 0.09311343729496002\n",
      "epoch: 1 step: 1651, loss is 0.29280680418014526\n",
      "epoch: 1 step: 1652, loss is 0.01718538999557495\n",
      "epoch: 1 step: 1653, loss is 0.13350088894367218\n",
      "epoch: 1 step: 1654, loss is 0.14283894002437592\n",
      "epoch: 1 step: 1655, loss is 0.14060859382152557\n",
      "epoch: 1 step: 1656, loss is 0.006708438042551279\n",
      "epoch: 1 step: 1657, loss is 0.025981305167078972\n",
      "epoch: 1 step: 1658, loss is 0.03294645994901657\n",
      "epoch: 1 step: 1659, loss is 0.049663376063108444\n",
      "epoch: 1 step: 1660, loss is 0.18087612092494965\n",
      "epoch: 1 step: 1661, loss is 0.28765010833740234\n",
      "epoch: 1 step: 1662, loss is 0.0354413203895092\n",
      "epoch: 1 step: 1663, loss is 0.19210800528526306\n",
      "epoch: 1 step: 1664, loss is 0.07038184255361557\n",
      "epoch: 1 step: 1665, loss is 0.29556146264076233\n",
      "epoch: 1 step: 1666, loss is 0.04351905360817909\n",
      "epoch: 1 step: 1667, loss is 0.05455970764160156\n",
      "epoch: 1 step: 1668, loss is 0.003852224675938487\n",
      "epoch: 1 step: 1669, loss is 0.24247030913829803\n",
      "epoch: 1 step: 1670, loss is 0.020361846312880516\n",
      "epoch: 1 step: 1671, loss is 0.11385182291269302\n",
      "epoch: 1 step: 1672, loss is 0.010360280983150005\n",
      "epoch: 1 step: 1673, loss is 0.02472045086324215\n",
      "epoch: 1 step: 1674, loss is 0.16081610321998596\n",
      "epoch: 1 step: 1675, loss is 0.06277532130479813\n",
      "epoch: 1 step: 1676, loss is 0.026633629575371742\n",
      "epoch: 1 step: 1677, loss is 0.10672701150178909\n",
      "epoch: 1 step: 1678, loss is 0.04345884546637535\n",
      "epoch: 1 step: 1679, loss is 0.34853407740592957\n",
      "epoch: 1 step: 1680, loss is 0.08495155721902847\n",
      "epoch: 1 step: 1681, loss is 0.01450007688254118\n",
      "epoch: 1 step: 1682, loss is 0.01778416894376278\n",
      "epoch: 1 step: 1683, loss is 0.13918033242225647\n",
      "epoch: 1 step: 1684, loss is 0.09731172025203705\n",
      "epoch: 1 step: 1685, loss is 0.08190473169088364\n",
      "epoch: 1 step: 1686, loss is 0.06092056259512901\n",
      "epoch: 1 step: 1687, loss is 0.10782629996538162\n",
      "epoch: 1 step: 1688, loss is 0.08799029886722565\n",
      "epoch: 1 step: 1689, loss is 0.07342406362295151\n",
      "epoch: 1 step: 1690, loss is 0.023525353521108627\n",
      "epoch: 1 step: 1691, loss is 0.03243466466665268\n",
      "epoch: 1 step: 1692, loss is 0.06834565103054047\n",
      "epoch: 1 step: 1693, loss is 0.0030923886224627495\n",
      "epoch: 1 step: 1694, loss is 0.011756759136915207\n",
      "epoch: 1 step: 1695, loss is 0.1187024712562561\n",
      "epoch: 1 step: 1696, loss is 0.013203867711126804\n",
      "epoch: 1 step: 1697, loss is 0.012871854938566685\n",
      "epoch: 1 step: 1698, loss is 0.011232206597924232\n",
      "epoch: 1 step: 1699, loss is 0.0125202015042305\n",
      "epoch: 1 step: 1700, loss is 0.13030606508255005\n",
      "epoch: 1 step: 1701, loss is 0.009399011731147766\n",
      "epoch: 1 step: 1702, loss is 0.003176432568579912\n",
      "epoch: 1 step: 1703, loss is 0.09850434213876724\n",
      "epoch: 1 step: 1704, loss is 0.08215685188770294\n",
      "epoch: 1 step: 1705, loss is 0.0426182746887207\n",
      "epoch: 1 step: 1706, loss is 0.014110374264419079\n",
      "epoch: 1 step: 1707, loss is 0.0024291393347084522\n",
      "epoch: 1 step: 1708, loss is 0.11408174782991409\n",
      "epoch: 1 step: 1709, loss is 0.029657414183020592\n",
      "epoch: 1 step: 1710, loss is 0.1560140699148178\n",
      "epoch: 1 step: 1711, loss is 0.013531314209103584\n",
      "epoch: 1 step: 1712, loss is 0.044526759535074234\n",
      "epoch: 1 step: 1713, loss is 0.053382132202386856\n",
      "epoch: 1 step: 1714, loss is 0.04197406396269798\n",
      "epoch: 1 step: 1715, loss is 0.17959171533584595\n",
      "epoch: 1 step: 1716, loss is 0.009221578016877174\n",
      "epoch: 1 step: 1717, loss is 0.28783854842185974\n",
      "epoch: 1 step: 1718, loss is 0.005410047713667154\n",
      "epoch: 1 step: 1719, loss is 0.18493987619876862\n",
      "epoch: 1 step: 1720, loss is 0.039841048419475555\n",
      "epoch: 1 step: 1721, loss is 0.25180184841156006\n",
      "epoch: 1 step: 1722, loss is 0.05385686457157135\n",
      "epoch: 1 step: 1723, loss is 0.04457546025514603\n",
      "epoch: 1 step: 1724, loss is 0.0980023443698883\n",
      "epoch: 1 step: 1725, loss is 0.011195088736712933\n",
      "epoch: 1 step: 1726, loss is 0.014882422983646393\n",
      "epoch: 1 step: 1727, loss is 0.2426939159631729\n",
      "epoch: 1 step: 1728, loss is 0.0073632970452308655\n",
      "epoch: 1 step: 1729, loss is 0.039196617901325226\n",
      "epoch: 1 step: 1730, loss is 0.21435770392417908\n",
      "epoch: 1 step: 1731, loss is 0.09892871230840683\n",
      "epoch: 1 step: 1732, loss is 0.010640440508723259\n",
      "epoch: 1 step: 1733, loss is 0.2270040065050125\n",
      "epoch: 1 step: 1734, loss is 0.16431204974651337\n",
      "epoch: 1 step: 1735, loss is 0.029766874387860298\n",
      "epoch: 1 step: 1736, loss is 0.16288013756275177\n",
      "epoch: 1 step: 1737, loss is 0.032774414867162704\n",
      "epoch: 1 step: 1738, loss is 0.045338306576013565\n",
      "epoch: 1 step: 1739, loss is 0.010253624059259892\n",
      "epoch: 1 step: 1740, loss is 0.03543548285961151\n",
      "epoch: 1 step: 1741, loss is 0.015917079523205757\n",
      "epoch: 1 step: 1742, loss is 0.0019911678973585367\n",
      "epoch: 1 step: 1743, loss is 0.037160880863666534\n",
      "epoch: 1 step: 1744, loss is 0.037692755460739136\n",
      "epoch: 1 step: 1745, loss is 0.018388565629720688\n",
      "epoch: 1 step: 1746, loss is 0.30804312229156494\n",
      "epoch: 1 step: 1747, loss is 0.04110155627131462\n",
      "epoch: 1 step: 1748, loss is 0.009565630927681923\n",
      "epoch: 1 step: 1749, loss is 0.012391632422804832\n",
      "epoch: 1 step: 1750, loss is 0.10605588555335999\n",
      "epoch: 1 step: 1751, loss is 0.10108523815870285\n",
      "epoch: 1 step: 1752, loss is 0.1297270506620407\n",
      "epoch: 1 step: 1753, loss is 0.031017694622278214\n",
      "epoch: 1 step: 1754, loss is 0.20310072600841522\n",
      "epoch: 1 step: 1755, loss is 0.07848486304283142\n",
      "epoch: 1 step: 1756, loss is 0.005773536860942841\n",
      "epoch: 1 step: 1757, loss is 0.18881604075431824\n",
      "epoch: 1 step: 1758, loss is 0.286787748336792\n",
      "epoch: 1 step: 1759, loss is 0.0104951411485672\n",
      "epoch: 1 step: 1760, loss is 0.041170816868543625\n",
      "epoch: 1 step: 1761, loss is 0.0703088566660881\n",
      "epoch: 1 step: 1762, loss is 0.013947589322924614\n",
      "epoch: 1 step: 1763, loss is 0.02581862173974514\n",
      "epoch: 1 step: 1764, loss is 0.0551769845187664\n",
      "epoch: 1 step: 1765, loss is 0.013943612575531006\n",
      "epoch: 1 step: 1766, loss is 0.0337672084569931\n",
      "epoch: 1 step: 1767, loss is 0.013028398156166077\n",
      "epoch: 1 step: 1768, loss is 0.006254429463297129\n",
      "epoch: 1 step: 1769, loss is 0.03707628697156906\n",
      "epoch: 1 step: 1770, loss is 0.1429462730884552\n",
      "epoch: 1 step: 1771, loss is 0.08284281939268112\n",
      "epoch: 1 step: 1772, loss is 0.006389244459569454\n",
      "epoch: 1 step: 1773, loss is 0.01626957766711712\n",
      "epoch: 1 step: 1774, loss is 0.005855421535670757\n",
      "epoch: 1 step: 1775, loss is 0.06759864836931229\n",
      "epoch: 1 step: 1776, loss is 0.0379297100007534\n",
      "epoch: 1 step: 1777, loss is 0.06474126130342484\n",
      "epoch: 1 step: 1778, loss is 0.004615540150552988\n",
      "epoch: 1 step: 1779, loss is 0.013878502883017063\n",
      "epoch: 1 step: 1780, loss is 0.05853242054581642\n",
      "epoch: 1 step: 1781, loss is 0.02538628689944744\n",
      "epoch: 1 step: 1782, loss is 0.052371907979249954\n",
      "epoch: 1 step: 1783, loss is 0.29634466767311096\n",
      "epoch: 1 step: 1784, loss is 0.03251928091049194\n",
      "epoch: 1 step: 1785, loss is 0.15136203169822693\n",
      "epoch: 1 step: 1786, loss is 0.03816896677017212\n",
      "epoch: 1 step: 1787, loss is 0.0010043696966022253\n",
      "epoch: 1 step: 1788, loss is 0.1342702955007553\n",
      "epoch: 1 step: 1789, loss is 0.007628186605870724\n",
      "epoch: 1 step: 1790, loss is 0.03456607460975647\n",
      "epoch: 1 step: 1791, loss is 0.05243869870901108\n",
      "epoch: 1 step: 1792, loss is 0.20125605165958405\n",
      "epoch: 1 step: 1793, loss is 0.2753843367099762\n",
      "epoch: 1 step: 1794, loss is 0.13326768577098846\n",
      "epoch: 1 step: 1795, loss is 0.004118097480386496\n",
      "epoch: 1 step: 1796, loss is 0.017439937219023705\n",
      "epoch: 1 step: 1797, loss is 0.002800466027110815\n",
      "epoch: 1 step: 1798, loss is 0.1727903187274933\n",
      "epoch: 1 step: 1799, loss is 0.04638604447245598\n",
      "epoch: 1 step: 1800, loss is 0.11413352936506271\n",
      "epoch: 1 step: 1801, loss is 0.07545488327741623\n",
      "epoch: 1 step: 1802, loss is 0.03785521537065506\n",
      "epoch: 1 step: 1803, loss is 0.06855717301368713\n",
      "epoch: 1 step: 1804, loss is 0.21121573448181152\n",
      "epoch: 1 step: 1805, loss is 0.020576490089297295\n",
      "epoch: 1 step: 1806, loss is 0.20777207612991333\n",
      "epoch: 1 step: 1807, loss is 0.11287068575620651\n",
      "epoch: 1 step: 1808, loss is 0.00284417811781168\n",
      "epoch: 1 step: 1809, loss is 0.03628458082675934\n",
      "epoch: 1 step: 1810, loss is 0.0007133449544198811\n",
      "epoch: 1 step: 1811, loss is 0.045960571616888046\n",
      "epoch: 1 step: 1812, loss is 0.004430787172168493\n",
      "epoch: 1 step: 1813, loss is 0.32419320940971375\n",
      "epoch: 1 step: 1814, loss is 0.09050629287958145\n",
      "epoch: 1 step: 1815, loss is 0.003810841590166092\n",
      "epoch: 1 step: 1816, loss is 0.029030073434114456\n",
      "epoch: 1 step: 1817, loss is 0.049207866191864014\n",
      "epoch: 1 step: 1818, loss is 0.009705123491585255\n",
      "epoch: 1 step: 1819, loss is 0.08112514019012451\n",
      "epoch: 1 step: 1820, loss is 0.07052821666002274\n",
      "epoch: 1 step: 1821, loss is 0.11077843606472015\n",
      "epoch: 1 step: 1822, loss is 0.0890299454331398\n",
      "epoch: 1 step: 1823, loss is 0.005097134970128536\n",
      "epoch: 1 step: 1824, loss is 0.17183612287044525\n",
      "epoch: 1 step: 1825, loss is 0.01972353644669056\n",
      "epoch: 1 step: 1826, loss is 0.11905510723590851\n",
      "epoch: 1 step: 1827, loss is 0.09470272809267044\n",
      "epoch: 1 step: 1828, loss is 0.1755504608154297\n",
      "epoch: 1 step: 1829, loss is 0.0646340548992157\n",
      "epoch: 1 step: 1830, loss is 0.0029476096387952566\n",
      "epoch: 1 step: 1831, loss is 0.041468702256679535\n",
      "epoch: 1 step: 1832, loss is 0.042063500732183456\n",
      "epoch: 1 step: 1833, loss is 0.004608949646353722\n",
      "epoch: 1 step: 1834, loss is 0.014052369631826878\n",
      "epoch: 1 step: 1835, loss is 0.030145494267344475\n",
      "epoch: 1 step: 1836, loss is 0.03042498230934143\n",
      "epoch: 1 step: 1837, loss is 0.021627333015203476\n",
      "epoch: 1 step: 1838, loss is 0.09739771485328674\n",
      "epoch: 1 step: 1839, loss is 0.011000634171068668\n",
      "epoch: 1 step: 1840, loss is 0.009740362875163555\n",
      "epoch: 1 step: 1841, loss is 0.04392034560441971\n",
      "epoch: 1 step: 1842, loss is 0.34444546699523926\n",
      "epoch: 1 step: 1843, loss is 0.14430417120456696\n",
      "epoch: 1 step: 1844, loss is 0.006984635256230831\n",
      "epoch: 1 step: 1845, loss is 0.023129742592573166\n",
      "epoch: 1 step: 1846, loss is 0.03216003254055977\n",
      "epoch: 1 step: 1847, loss is 0.012334456667304039\n",
      "epoch: 1 step: 1848, loss is 0.26542896032333374\n",
      "epoch: 1 step: 1849, loss is 0.08867913484573364\n",
      "epoch: 1 step: 1850, loss is 0.008527957834303379\n",
      "epoch: 1 step: 1851, loss is 0.05850714072585106\n",
      "epoch: 1 step: 1852, loss is 0.02742098830640316\n",
      "epoch: 1 step: 1853, loss is 0.01140032708644867\n",
      "epoch: 1 step: 1854, loss is 0.13385453820228577\n",
      "epoch: 1 step: 1855, loss is 0.01921071670949459\n",
      "epoch: 1 step: 1856, loss is 0.030036523938179016\n",
      "epoch: 1 step: 1857, loss is 0.049359604716300964\n",
      "epoch: 1 step: 1858, loss is 0.162160262465477\n",
      "epoch: 1 step: 1859, loss is 0.014031944796442986\n",
      "epoch: 1 step: 1860, loss is 0.01191163994371891\n",
      "epoch: 1 step: 1861, loss is 0.1816508173942566\n",
      "epoch: 1 step: 1862, loss is 0.03710625320672989\n",
      "epoch: 1 step: 1863, loss is 0.039096321910619736\n",
      "epoch: 1 step: 1864, loss is 0.14754508435726166\n",
      "epoch: 1 step: 1865, loss is 0.03121664747595787\n",
      "epoch: 1 step: 1866, loss is 0.049226902425289154\n",
      "epoch: 1 step: 1867, loss is 0.021760843694210052\n",
      "epoch: 1 step: 1868, loss is 0.01054546982049942\n",
      "epoch: 1 step: 1869, loss is 0.030794011428952217\n",
      "epoch: 1 step: 1870, loss is 0.02129032276570797\n",
      "epoch: 1 step: 1871, loss is 0.0053953444585204124\n",
      "epoch: 1 step: 1872, loss is 0.28787294030189514\n",
      "epoch: 1 step: 1873, loss is 0.007215758785605431\n",
      "epoch: 1 step: 1874, loss is 0.04439825192093849\n",
      "epoch: 1 step: 1875, loss is 0.07404784113168716\n",
      "Train epoch time: 17934.163 ms, per step time: 9.565 ms\n",
      "epoch: 2 step: 1, loss is 0.00598502904176712\n",
      "epoch: 2 step: 2, loss is 0.08480927348136902\n",
      "epoch: 2 step: 3, loss is 0.11717605590820312\n",
      "epoch: 2 step: 4, loss is 0.009308070875704288\n",
      "epoch: 2 step: 5, loss is 0.07842438668012619\n",
      "epoch: 2 step: 6, loss is 0.03475583344697952\n",
      "epoch: 2 step: 7, loss is 0.052394092082977295\n",
      "epoch: 2 step: 8, loss is 0.02368541620671749\n",
      "epoch: 2 step: 9, loss is 0.11374201625585556\n",
      "epoch: 2 step: 10, loss is 0.09308765083551407\n",
      "epoch: 2 step: 11, loss is 0.12929557263851166\n",
      "epoch: 2 step: 12, loss is 0.028708873316645622\n",
      "epoch: 2 step: 13, loss is 0.003254356561228633\n",
      "epoch: 2 step: 14, loss is 0.01280366349965334\n",
      "epoch: 2 step: 15, loss is 0.004359083250164986\n",
      "epoch: 2 step: 16, loss is 0.054195988923311234\n",
      "epoch: 2 step: 17, loss is 0.014989017508924007\n",
      "epoch: 2 step: 18, loss is 0.0026227585040032864\n",
      "epoch: 2 step: 19, loss is 0.01362746860831976\n",
      "epoch: 2 step: 20, loss is 0.00517939031124115\n",
      "epoch: 2 step: 21, loss is 0.20021478831768036\n",
      "epoch: 2 step: 22, loss is 0.01113906316459179\n",
      "epoch: 2 step: 23, loss is 0.583176851272583\n",
      "epoch: 2 step: 24, loss is 0.01032500620931387\n",
      "epoch: 2 step: 25, loss is 0.004876228515058756\n",
      "epoch: 2 step: 26, loss is 0.04022464528679848\n",
      "epoch: 2 step: 27, loss is 0.10465937107801437\n",
      "epoch: 2 step: 28, loss is 0.048901572823524475\n",
      "epoch: 2 step: 29, loss is 0.1461191624403\n",
      "epoch: 2 step: 30, loss is 0.03191946819424629\n",
      "epoch: 2 step: 31, loss is 0.015369835309684277\n",
      "epoch: 2 step: 32, loss is 0.29561781883239746\n",
      "epoch: 2 step: 33, loss is 0.008594678714871407\n",
      "epoch: 2 step: 34, loss is 0.008774044923484325\n",
      "epoch: 2 step: 35, loss is 0.043978042900562286\n",
      "epoch: 2 step: 36, loss is 0.03799650818109512\n",
      "epoch: 2 step: 37, loss is 0.15946435928344727\n",
      "epoch: 2 step: 38, loss is 0.11074142158031464\n",
      "epoch: 2 step: 39, loss is 0.03916877508163452\n",
      "epoch: 2 step: 40, loss is 0.03716374188661575\n",
      "epoch: 2 step: 41, loss is 0.03720156475901604\n",
      "epoch: 2 step: 42, loss is 0.10330889374017715\n",
      "epoch: 2 step: 43, loss is 0.1179676353931427\n",
      "epoch: 2 step: 44, loss is 0.08456457406282425\n",
      "epoch: 2 step: 45, loss is 0.007771235425025225\n",
      "epoch: 2 step: 46, loss is 0.0029793553985655308\n",
      "epoch: 2 step: 47, loss is 0.012729920446872711\n",
      "epoch: 2 step: 48, loss is 0.01478947326540947\n",
      "epoch: 2 step: 49, loss is 0.005067502148449421\n",
      "epoch: 2 step: 50, loss is 0.05902073532342911\n",
      "epoch: 2 step: 51, loss is 0.17114707827568054\n",
      "epoch: 2 step: 52, loss is 0.08790657669305801\n",
      "epoch: 2 step: 53, loss is 0.01481587067246437\n",
      "epoch: 2 step: 54, loss is 0.20225341618061066\n",
      "epoch: 2 step: 55, loss is 0.025231847539544106\n",
      "epoch: 2 step: 56, loss is 0.12252725660800934\n",
      "epoch: 2 step: 57, loss is 0.013014883734285831\n",
      "epoch: 2 step: 58, loss is 0.0008317611645907164\n",
      "epoch: 2 step: 59, loss is 0.009369393810629845\n",
      "epoch: 2 step: 60, loss is 0.11148987710475922\n",
      "epoch: 2 step: 61, loss is 0.0018029356142506003\n",
      "epoch: 2 step: 62, loss is 0.01720227114856243\n",
      "epoch: 2 step: 63, loss is 0.07748900353908539\n",
      "epoch: 2 step: 64, loss is 0.04430386424064636\n",
      "epoch: 2 step: 65, loss is 0.007306735031306744\n",
      "epoch: 2 step: 66, loss is 0.008546996861696243\n",
      "epoch: 2 step: 67, loss is 0.01919417269527912\n",
      "epoch: 2 step: 68, loss is 0.02238284982740879\n",
      "epoch: 2 step: 69, loss is 0.02061295509338379\n",
      "epoch: 2 step: 70, loss is 0.08578368276357651\n",
      "epoch: 2 step: 71, loss is 0.09372047334909439\n",
      "epoch: 2 step: 72, loss is 0.08062954246997833\n",
      "epoch: 2 step: 73, loss is 0.008182968944311142\n",
      "epoch: 2 step: 74, loss is 0.059987038373947144\n",
      "epoch: 2 step: 75, loss is 0.025335924699902534\n",
      "epoch: 2 step: 76, loss is 0.010728182271122932\n",
      "epoch: 2 step: 77, loss is 0.1809026598930359\n",
      "epoch: 2 step: 78, loss is 0.014587357640266418\n",
      "epoch: 2 step: 79, loss is 0.004063108470290899\n",
      "epoch: 2 step: 80, loss is 0.03634188696742058\n",
      "epoch: 2 step: 81, loss is 0.060310930013656616\n",
      "epoch: 2 step: 82, loss is 0.05801614746451378\n",
      "epoch: 2 step: 83, loss is 0.006731668021529913\n",
      "epoch: 2 step: 84, loss is 0.12204037606716156\n",
      "epoch: 2 step: 85, loss is 0.027190418913960457\n",
      "epoch: 2 step: 86, loss is 0.007601005490869284\n",
      "epoch: 2 step: 87, loss is 0.04617678374052048\n",
      "epoch: 2 step: 88, loss is 0.004249555058777332\n",
      "epoch: 2 step: 89, loss is 0.005402061156928539\n",
      "epoch: 2 step: 90, loss is 0.006415078882128\n",
      "epoch: 2 step: 91, loss is 0.17498987913131714\n",
      "epoch: 2 step: 92, loss is 0.2104080468416214\n",
      "epoch: 2 step: 93, loss is 0.009550376795232296\n",
      "epoch: 2 step: 94, loss is 0.07321707159280777\n",
      "epoch: 2 step: 95, loss is 0.08571816235780716\n",
      "epoch: 2 step: 96, loss is 0.012316529639065266\n",
      "epoch: 2 step: 97, loss is 0.07534431666135788\n",
      "epoch: 2 step: 98, loss is 0.00425784382969141\n",
      "epoch: 2 step: 99, loss is 0.07601316273212433\n",
      "epoch: 2 step: 100, loss is 0.022234410047531128\n",
      "epoch: 2 step: 101, loss is 0.10635589063167572\n",
      "epoch: 2 step: 102, loss is 0.14067749679088593\n",
      "epoch: 2 step: 103, loss is 0.18768583238124847\n",
      "epoch: 2 step: 104, loss is 0.07491130381822586\n",
      "epoch: 2 step: 105, loss is 0.13300761580467224\n",
      "epoch: 2 step: 106, loss is 0.21463356912136078\n",
      "epoch: 2 step: 107, loss is 0.11044783145189285\n",
      "epoch: 2 step: 108, loss is 0.00849925260990858\n",
      "epoch: 2 step: 109, loss is 0.018194258213043213\n",
      "epoch: 2 step: 110, loss is 0.0006424707244150341\n",
      "epoch: 2 step: 111, loss is 0.016248593106865883\n",
      "epoch: 2 step: 112, loss is 0.014821573160588741\n",
      "epoch: 2 step: 113, loss is 0.10096481442451477\n",
      "epoch: 2 step: 114, loss is 0.006000286899507046\n",
      "epoch: 2 step: 115, loss is 0.005586062092334032\n",
      "epoch: 2 step: 116, loss is 0.005195326171815395\n",
      "epoch: 2 step: 117, loss is 0.03613512963056564\n",
      "epoch: 2 step: 118, loss is 0.015864016488194466\n",
      "epoch: 2 step: 119, loss is 0.003916396759450436\n",
      "epoch: 2 step: 120, loss is 0.05710877850651741\n",
      "epoch: 2 step: 121, loss is 0.07866053283214569\n",
      "epoch: 2 step: 122, loss is 0.011527281254529953\n",
      "epoch: 2 step: 123, loss is 0.11292823404073715\n",
      "epoch: 2 step: 124, loss is 0.18930551409721375\n",
      "epoch: 2 step: 125, loss is 0.052578672766685486\n",
      "epoch: 2 step: 126, loss is 0.00480659818276763\n",
      "epoch: 2 step: 127, loss is 0.041102904826402664\n",
      "epoch: 2 step: 128, loss is 0.005816437304019928\n",
      "epoch: 2 step: 129, loss is 0.015704507008194923\n",
      "epoch: 2 step: 130, loss is 0.1584072858095169\n",
      "epoch: 2 step: 131, loss is 0.2906545400619507\n",
      "epoch: 2 step: 132, loss is 0.01257880125194788\n",
      "epoch: 2 step: 133, loss is 0.04844984412193298\n",
      "epoch: 2 step: 134, loss is 0.005920447409152985\n",
      "epoch: 2 step: 135, loss is 0.017211131751537323\n",
      "epoch: 2 step: 136, loss is 0.1985892951488495\n",
      "epoch: 2 step: 137, loss is 0.004768642131239176\n",
      "epoch: 2 step: 138, loss is 0.049758460372686386\n",
      "epoch: 2 step: 139, loss is 0.04794877767562866\n",
      "epoch: 2 step: 140, loss is 0.19225597381591797\n",
      "epoch: 2 step: 141, loss is 0.03192967176437378\n",
      "epoch: 2 step: 142, loss is 0.08461654931306839\n",
      "epoch: 2 step: 143, loss is 0.07047532498836517\n",
      "epoch: 2 step: 144, loss is 0.08835547417402267\n",
      "epoch: 2 step: 145, loss is 0.24411745369434357\n",
      "epoch: 2 step: 146, loss is 0.024890998378396034\n",
      "epoch: 2 step: 147, loss is 0.10446887463331223\n",
      "epoch: 2 step: 148, loss is 0.00733975088223815\n",
      "epoch: 2 step: 149, loss is 0.004971588030457497\n",
      "epoch: 2 step: 150, loss is 0.052528198808431625\n",
      "epoch: 2 step: 151, loss is 0.01629268005490303\n",
      "epoch: 2 step: 152, loss is 0.005206176079809666\n",
      "epoch: 2 step: 153, loss is 0.10119018703699112\n",
      "epoch: 2 step: 154, loss is 0.09503807872533798\n",
      "epoch: 2 step: 155, loss is 0.06566860526800156\n",
      "epoch: 2 step: 156, loss is 0.054078225046396255\n",
      "epoch: 2 step: 157, loss is 0.0038367987144738436\n",
      "epoch: 2 step: 158, loss is 0.06168714538216591\n",
      "epoch: 2 step: 159, loss is 0.01320275105535984\n",
      "epoch: 2 step: 160, loss is 0.02047339268028736\n",
      "epoch: 2 step: 161, loss is 0.17979751527309418\n",
      "epoch: 2 step: 162, loss is 0.021976573392748833\n",
      "epoch: 2 step: 163, loss is 0.025582561269402504\n",
      "epoch: 2 step: 164, loss is 0.030505891889333725\n",
      "epoch: 2 step: 165, loss is 0.02210722304880619\n",
      "epoch: 2 step: 166, loss is 0.014484441839158535\n",
      "epoch: 2 step: 167, loss is 0.0032800096087157726\n",
      "epoch: 2 step: 168, loss is 0.03491782024502754\n",
      "epoch: 2 step: 169, loss is 0.005340925883501768\n",
      "epoch: 2 step: 170, loss is 0.07935281842947006\n",
      "epoch: 2 step: 171, loss is 0.1252012401819229\n",
      "epoch: 2 step: 172, loss is 0.05841587483882904\n",
      "epoch: 2 step: 173, loss is 0.020663239061832428\n",
      "epoch: 2 step: 174, loss is 0.014562226831912994\n",
      "epoch: 2 step: 175, loss is 0.1180746853351593\n",
      "epoch: 2 step: 176, loss is 0.026930157095193863\n",
      "epoch: 2 step: 177, loss is 0.0053194258362054825\n",
      "epoch: 2 step: 178, loss is 0.25801628828048706\n",
      "epoch: 2 step: 179, loss is 0.08657622337341309\n",
      "epoch: 2 step: 180, loss is 0.08605023473501205\n",
      "epoch: 2 step: 181, loss is 0.02741687186062336\n",
      "epoch: 2 step: 182, loss is 0.03254067897796631\n",
      "epoch: 2 step: 183, loss is 0.01566503569483757\n",
      "epoch: 2 step: 184, loss is 0.018820850178599358\n",
      "epoch: 2 step: 185, loss is 0.004387971479445696\n",
      "epoch: 2 step: 186, loss is 0.2613121569156647\n",
      "epoch: 2 step: 187, loss is 0.04347774758934975\n",
      "epoch: 2 step: 188, loss is 0.0020554696675390005\n",
      "epoch: 2 step: 189, loss is 0.15527838468551636\n",
      "epoch: 2 step: 190, loss is 0.14474202692508698\n",
      "epoch: 2 step: 191, loss is 0.049397073686122894\n",
      "epoch: 2 step: 192, loss is 0.08874549716711044\n",
      "epoch: 2 step: 193, loss is 0.35332074761390686\n",
      "epoch: 2 step: 194, loss is 0.019917016848921776\n",
      "epoch: 2 step: 195, loss is 0.02855977602303028\n",
      "epoch: 2 step: 196, loss is 0.005467792972922325\n",
      "epoch: 2 step: 197, loss is 0.04146712273359299\n",
      "epoch: 2 step: 198, loss is 0.012218700721859932\n",
      "epoch: 2 step: 199, loss is 0.06042083725333214\n",
      "epoch: 2 step: 200, loss is 0.018227359279990196\n",
      "epoch: 2 step: 201, loss is 0.01166574191302061\n",
      "epoch: 2 step: 202, loss is 0.09927592426538467\n",
      "epoch: 2 step: 203, loss is 0.008073592558503151\n",
      "epoch: 2 step: 204, loss is 0.16028831899166107\n",
      "epoch: 2 step: 205, loss is 0.019468097016215324\n",
      "epoch: 2 step: 206, loss is 0.00949985720217228\n",
      "epoch: 2 step: 207, loss is 0.13773515820503235\n",
      "epoch: 2 step: 208, loss is 0.01163496170192957\n",
      "epoch: 2 step: 209, loss is 0.01389102078974247\n",
      "epoch: 2 step: 210, loss is 0.031464699655771255\n",
      "epoch: 2 step: 211, loss is 0.005430966150015593\n",
      "epoch: 2 step: 212, loss is 0.006319739390164614\n",
      "epoch: 2 step: 213, loss is 0.025791769847273827\n",
      "epoch: 2 step: 214, loss is 0.004886500537395477\n",
      "epoch: 2 step: 215, loss is 0.01141237374395132\n",
      "epoch: 2 step: 216, loss is 0.0054138158448040485\n",
      "epoch: 2 step: 217, loss is 0.16948872804641724\n",
      "epoch: 2 step: 218, loss is 0.09254219383001328\n",
      "epoch: 2 step: 219, loss is 0.006409665569663048\n",
      "epoch: 2 step: 220, loss is 0.10157608240842819\n",
      "epoch: 2 step: 221, loss is 0.12875665724277496\n",
      "epoch: 2 step: 222, loss is 0.17086786031723022\n",
      "epoch: 2 step: 223, loss is 0.202238991856575\n",
      "epoch: 2 step: 224, loss is 0.10967420786619186\n",
      "epoch: 2 step: 225, loss is 0.010880643501877785\n",
      "epoch: 2 step: 226, loss is 0.009243570268154144\n",
      "epoch: 2 step: 227, loss is 0.06482061743736267\n",
      "epoch: 2 step: 228, loss is 0.0800899863243103\n",
      "epoch: 2 step: 229, loss is 0.036818232387304306\n",
      "epoch: 2 step: 230, loss is 0.03317796811461449\n",
      "epoch: 2 step: 231, loss is 0.002786038676276803\n",
      "epoch: 2 step: 232, loss is 0.008703385479748249\n",
      "epoch: 2 step: 233, loss is 0.03462933748960495\n",
      "epoch: 2 step: 234, loss is 0.004118358250707388\n",
      "epoch: 2 step: 235, loss is 0.04579834267497063\n",
      "epoch: 2 step: 236, loss is 0.17018795013427734\n",
      "epoch: 2 step: 237, loss is 0.06325260549783707\n",
      "epoch: 2 step: 238, loss is 0.011801589280366898\n",
      "epoch: 2 step: 239, loss is 0.1836606115102768\n",
      "epoch: 2 step: 240, loss is 0.05482429265975952\n",
      "epoch: 2 step: 241, loss is 0.016059510409832\n",
      "epoch: 2 step: 242, loss is 0.006079840008169413\n",
      "epoch: 2 step: 243, loss is 0.025573955848813057\n",
      "epoch: 2 step: 244, loss is 0.18157413601875305\n",
      "epoch: 2 step: 245, loss is 0.02811470814049244\n",
      "epoch: 2 step: 246, loss is 0.05879946053028107\n",
      "epoch: 2 step: 247, loss is 0.006912900134921074\n",
      "epoch: 2 step: 248, loss is 0.011017934419214725\n",
      "epoch: 2 step: 249, loss is 0.03269665688276291\n",
      "epoch: 2 step: 250, loss is 0.01405327022075653\n",
      "epoch: 2 step: 251, loss is 0.005241943523287773\n",
      "epoch: 2 step: 252, loss is 0.044880591332912445\n",
      "epoch: 2 step: 253, loss is 0.011931566521525383\n",
      "epoch: 2 step: 254, loss is 0.012366000562906265\n",
      "epoch: 2 step: 255, loss is 0.004043082240968943\n",
      "epoch: 2 step: 256, loss is 0.0032321023754775524\n",
      "epoch: 2 step: 257, loss is 0.06308300793170929\n",
      "epoch: 2 step: 258, loss is 0.26502975821495056\n",
      "epoch: 2 step: 259, loss is 0.013263244181871414\n",
      "epoch: 2 step: 260, loss is 0.05452454835176468\n",
      "epoch: 2 step: 261, loss is 0.013148265890777111\n",
      "epoch: 2 step: 262, loss is 0.07261602580547333\n",
      "epoch: 2 step: 263, loss is 0.06394673883914948\n",
      "epoch: 2 step: 264, loss is 0.009593973867595196\n",
      "epoch: 2 step: 265, loss is 0.02592514641582966\n",
      "epoch: 2 step: 266, loss is 0.007447865791618824\n",
      "epoch: 2 step: 267, loss is 0.0895669013261795\n",
      "epoch: 2 step: 268, loss is 0.0014610799262300134\n",
      "epoch: 2 step: 269, loss is 0.004774838220328093\n",
      "epoch: 2 step: 270, loss is 0.26007571816444397\n",
      "epoch: 2 step: 271, loss is 0.002489514881744981\n",
      "epoch: 2 step: 272, loss is 0.344154953956604\n",
      "epoch: 2 step: 273, loss is 0.0015079749282449484\n",
      "epoch: 2 step: 274, loss is 0.00739268446341157\n",
      "epoch: 2 step: 275, loss is 0.06367956846952438\n",
      "epoch: 2 step: 276, loss is 0.06322570145130157\n",
      "epoch: 2 step: 277, loss is 0.16599780321121216\n",
      "epoch: 2 step: 278, loss is 0.0338042713701725\n",
      "epoch: 2 step: 279, loss is 0.04199118912220001\n",
      "epoch: 2 step: 280, loss is 0.02914360538125038\n",
      "epoch: 2 step: 281, loss is 0.24938005208969116\n",
      "epoch: 2 step: 282, loss is 0.19471818208694458\n",
      "epoch: 2 step: 283, loss is 0.033750999718904495\n",
      "epoch: 2 step: 284, loss is 0.09383875876665115\n",
      "epoch: 2 step: 285, loss is 0.00794399343430996\n",
      "epoch: 2 step: 286, loss is 0.07541121542453766\n",
      "epoch: 2 step: 287, loss is 0.1457253396511078\n",
      "epoch: 2 step: 288, loss is 0.005585996899753809\n",
      "epoch: 2 step: 289, loss is 0.03256399556994438\n",
      "epoch: 2 step: 290, loss is 0.01184418797492981\n",
      "epoch: 2 step: 291, loss is 0.009675162844359875\n",
      "epoch: 2 step: 292, loss is 0.18502062559127808\n",
      "epoch: 2 step: 293, loss is 0.11482290923595428\n",
      "epoch: 2 step: 294, loss is 0.018908312544226646\n",
      "epoch: 2 step: 295, loss is 0.004500670824199915\n",
      "epoch: 2 step: 296, loss is 0.05650050938129425\n",
      "epoch: 2 step: 297, loss is 0.006731804460287094\n",
      "epoch: 2 step: 298, loss is 0.12418565154075623\n",
      "epoch: 2 step: 299, loss is 0.05336504802107811\n",
      "epoch: 2 step: 300, loss is 0.0011192341335117817\n",
      "epoch: 2 step: 301, loss is 0.043777547776699066\n",
      "epoch: 2 step: 302, loss is 0.03185782581567764\n",
      "epoch: 2 step: 303, loss is 0.017381442710757256\n",
      "epoch: 2 step: 304, loss is 0.07471892237663269\n",
      "epoch: 2 step: 305, loss is 0.04036505147814751\n",
      "epoch: 2 step: 306, loss is 0.028481632471084595\n",
      "epoch: 2 step: 307, loss is 0.15298351645469666\n",
      "epoch: 2 step: 308, loss is 0.15021656453609467\n",
      "epoch: 2 step: 309, loss is 0.12894746661186218\n",
      "epoch: 2 step: 310, loss is 0.048396024852991104\n",
      "epoch: 2 step: 311, loss is 0.0069614602252841\n",
      "epoch: 2 step: 312, loss is 0.038465466350317\n",
      "epoch: 2 step: 313, loss is 0.23523573577404022\n",
      "epoch: 2 step: 314, loss is 0.05933452397584915\n",
      "epoch: 2 step: 315, loss is 0.038303013890981674\n",
      "epoch: 2 step: 316, loss is 0.031142089515924454\n",
      "epoch: 2 step: 317, loss is 0.010844925418496132\n",
      "epoch: 2 step: 318, loss is 0.012565392069518566\n",
      "epoch: 2 step: 319, loss is 0.12671294808387756\n",
      "epoch: 2 step: 320, loss is 0.11787055432796478\n",
      "epoch: 2 step: 321, loss is 0.2126912921667099\n",
      "epoch: 2 step: 322, loss is 0.06728564202785492\n",
      "epoch: 2 step: 323, loss is 0.06724004447460175\n",
      "epoch: 2 step: 324, loss is 0.06891611218452454\n",
      "epoch: 2 step: 325, loss is 0.29951101541519165\n",
      "epoch: 2 step: 326, loss is 0.032766193151474\n",
      "epoch: 2 step: 327, loss is 0.18785671889781952\n",
      "epoch: 2 step: 328, loss is 0.05990364030003548\n",
      "epoch: 2 step: 329, loss is 0.008198248222470284\n",
      "epoch: 2 step: 330, loss is 0.07345515489578247\n",
      "epoch: 2 step: 331, loss is 0.025119271129369736\n",
      "epoch: 2 step: 332, loss is 0.3676407039165497\n",
      "epoch: 2 step: 333, loss is 0.04120970889925957\n",
      "epoch: 2 step: 334, loss is 0.0116446977481246\n",
      "epoch: 2 step: 335, loss is 0.03791436925530434\n",
      "epoch: 2 step: 336, loss is 0.1343819946050644\n",
      "epoch: 2 step: 337, loss is 0.015637466683983803\n",
      "epoch: 2 step: 338, loss is 0.014783944934606552\n",
      "epoch: 2 step: 339, loss is 0.0981680229306221\n",
      "epoch: 2 step: 340, loss is 0.04843401536345482\n",
      "epoch: 2 step: 341, loss is 0.01223619095981121\n",
      "epoch: 2 step: 342, loss is 0.10637813806533813\n",
      "epoch: 2 step: 343, loss is 0.024697108194231987\n",
      "epoch: 2 step: 344, loss is 0.051886383444070816\n",
      "epoch: 2 step: 345, loss is 0.18940116465091705\n",
      "epoch: 2 step: 346, loss is 0.00955489557236433\n",
      "epoch: 2 step: 347, loss is 0.039079952985048294\n",
      "epoch: 2 step: 348, loss is 0.18130727112293243\n",
      "epoch: 2 step: 349, loss is 0.04059430956840515\n",
      "epoch: 2 step: 350, loss is 0.23769210278987885\n",
      "epoch: 2 step: 351, loss is 0.006868808530271053\n",
      "epoch: 2 step: 352, loss is 0.13358211517333984\n",
      "epoch: 2 step: 353, loss is 0.07633238285779953\n",
      "epoch: 2 step: 354, loss is 0.239318385720253\n",
      "epoch: 2 step: 355, loss is 0.032970160245895386\n",
      "epoch: 2 step: 356, loss is 0.07300512492656708\n",
      "epoch: 2 step: 357, loss is 0.09043533354997635\n",
      "epoch: 2 step: 358, loss is 0.04401117190718651\n",
      "epoch: 2 step: 359, loss is 0.03524436056613922\n",
      "epoch: 2 step: 360, loss is 0.02546977810561657\n",
      "epoch: 2 step: 361, loss is 0.010876581072807312\n",
      "epoch: 2 step: 362, loss is 0.026351993903517723\n",
      "epoch: 2 step: 363, loss is 0.020741991698741913\n",
      "epoch: 2 step: 364, loss is 0.05156104639172554\n",
      "epoch: 2 step: 365, loss is 0.033934686332941055\n",
      "epoch: 2 step: 366, loss is 0.005503247492015362\n",
      "epoch: 2 step: 367, loss is 0.08749571442604065\n",
      "epoch: 2 step: 368, loss is 0.024846209213137627\n",
      "epoch: 2 step: 369, loss is 0.026496896520256996\n",
      "epoch: 2 step: 370, loss is 0.006954660173505545\n",
      "epoch: 2 step: 371, loss is 0.08590695261955261\n",
      "epoch: 2 step: 372, loss is 0.05529052019119263\n",
      "epoch: 2 step: 373, loss is 0.13769516348838806\n",
      "epoch: 2 step: 374, loss is 0.03738124296069145\n",
      "epoch: 2 step: 375, loss is 0.17600402235984802\n",
      "epoch: 2 step: 376, loss is 0.06749816238880157\n",
      "epoch: 2 step: 377, loss is 0.03410230204463005\n",
      "epoch: 2 step: 378, loss is 0.0007517445483244956\n",
      "epoch: 2 step: 379, loss is 0.021761495620012283\n",
      "epoch: 2 step: 380, loss is 0.056031484156847\n",
      "epoch: 2 step: 381, loss is 0.16285432875156403\n",
      "epoch: 2 step: 382, loss is 0.01741277053952217\n",
      "epoch: 2 step: 383, loss is 0.07417337596416473\n",
      "epoch: 2 step: 384, loss is 0.14119631052017212\n",
      "epoch: 2 step: 385, loss is 0.06285741180181503\n",
      "epoch: 2 step: 386, loss is 0.052223511040210724\n",
      "epoch: 2 step: 387, loss is 0.04068094864487648\n",
      "epoch: 2 step: 388, loss is 0.10629315674304962\n",
      "epoch: 2 step: 389, loss is 0.08451097458600998\n",
      "epoch: 2 step: 390, loss is 0.024890568107366562\n",
      "epoch: 2 step: 391, loss is 0.1642717868089676\n",
      "epoch: 2 step: 392, loss is 0.010981796309351921\n",
      "epoch: 2 step: 393, loss is 0.1283256560564041\n",
      "epoch: 2 step: 394, loss is 0.06595691293478012\n",
      "epoch: 2 step: 395, loss is 0.3564358651638031\n",
      "epoch: 2 step: 396, loss is 0.004873831756412983\n",
      "epoch: 2 step: 397, loss is 0.08389125019311905\n",
      "epoch: 2 step: 398, loss is 0.032449666410684586\n",
      "epoch: 2 step: 399, loss is 0.033887580037117004\n",
      "epoch: 2 step: 400, loss is 0.010745718143880367\n",
      "epoch: 2 step: 401, loss is 0.06937646120786667\n",
      "epoch: 2 step: 402, loss is 0.04078184440732002\n",
      "epoch: 2 step: 403, loss is 0.05809204652905464\n",
      "epoch: 2 step: 404, loss is 0.1201879158616066\n",
      "epoch: 2 step: 405, loss is 0.1007043793797493\n",
      "epoch: 2 step: 406, loss is 0.002115650800988078\n",
      "epoch: 2 step: 407, loss is 0.021649353206157684\n",
      "epoch: 2 step: 408, loss is 0.0033721625804901123\n",
      "epoch: 2 step: 409, loss is 0.03324330598115921\n",
      "epoch: 2 step: 410, loss is 0.04799405485391617\n",
      "epoch: 2 step: 411, loss is 0.037077028304338455\n",
      "epoch: 2 step: 412, loss is 0.008343731053173542\n",
      "epoch: 2 step: 413, loss is 0.09244595468044281\n",
      "epoch: 2 step: 414, loss is 0.003010788466781378\n",
      "epoch: 2 step: 415, loss is 0.007584115490317345\n",
      "epoch: 2 step: 416, loss is 0.15603940188884735\n",
      "epoch: 2 step: 417, loss is 0.13149966299533844\n",
      "epoch: 2 step: 418, loss is 0.15664368867874146\n",
      "epoch: 2 step: 419, loss is 0.008824149146676064\n",
      "epoch: 2 step: 420, loss is 0.002135257236659527\n",
      "epoch: 2 step: 421, loss is 0.0017180066788569093\n",
      "epoch: 2 step: 422, loss is 0.12655866146087646\n",
      "epoch: 2 step: 423, loss is 0.01280976552516222\n",
      "epoch: 2 step: 424, loss is 0.016720743849873543\n",
      "epoch: 2 step: 425, loss is 0.001966631505638361\n",
      "epoch: 2 step: 426, loss is 0.07676500827074051\n",
      "epoch: 2 step: 427, loss is 0.025876948609948158\n",
      "epoch: 2 step: 428, loss is 0.006113430950790644\n",
      "epoch: 2 step: 429, loss is 0.10546303540468216\n",
      "epoch: 2 step: 430, loss is 0.012330138124525547\n",
      "epoch: 2 step: 431, loss is 0.028272567316889763\n",
      "epoch: 2 step: 432, loss is 0.002467432525008917\n",
      "epoch: 2 step: 433, loss is 0.008290090598165989\n",
      "epoch: 2 step: 434, loss is 0.008652185089886189\n",
      "epoch: 2 step: 435, loss is 0.12499164789915085\n",
      "epoch: 2 step: 436, loss is 0.06737200170755386\n",
      "epoch: 2 step: 437, loss is 0.05216912552714348\n",
      "epoch: 2 step: 438, loss is 0.026014866307377815\n",
      "epoch: 2 step: 439, loss is 0.003895072964951396\n",
      "epoch: 2 step: 440, loss is 0.07341744005680084\n",
      "epoch: 2 step: 441, loss is 0.0495208203792572\n",
      "epoch: 2 step: 442, loss is 0.0027574514970183372\n",
      "epoch: 2 step: 443, loss is 0.13441228866577148\n",
      "epoch: 2 step: 444, loss is 0.006320537067949772\n",
      "epoch: 2 step: 445, loss is 0.011769902892410755\n",
      "epoch: 2 step: 446, loss is 0.0006450568325817585\n",
      "epoch: 2 step: 447, loss is 0.006274590268731117\n",
      "epoch: 2 step: 448, loss is 0.03320326283574104\n",
      "epoch: 2 step: 449, loss is 0.11541330814361572\n",
      "epoch: 2 step: 450, loss is 0.015203531831502914\n",
      "epoch: 2 step: 451, loss is 0.017916854470968246\n",
      "epoch: 2 step: 452, loss is 0.04085246101021767\n",
      "epoch: 2 step: 453, loss is 0.0940660759806633\n",
      "epoch: 2 step: 454, loss is 0.20833031833171844\n",
      "epoch: 2 step: 455, loss is 0.29222503304481506\n",
      "epoch: 2 step: 456, loss is 0.004827217198908329\n",
      "epoch: 2 step: 457, loss is 0.11902936547994614\n",
      "epoch: 2 step: 458, loss is 0.004760435316711664\n",
      "epoch: 2 step: 459, loss is 0.06725458800792694\n",
      "epoch: 2 step: 460, loss is 0.30750569701194763\n",
      "epoch: 2 step: 461, loss is 0.17364734411239624\n",
      "epoch: 2 step: 462, loss is 0.0072658671997487545\n",
      "epoch: 2 step: 463, loss is 0.0909818485379219\n",
      "epoch: 2 step: 464, loss is 0.04829113930463791\n",
      "epoch: 2 step: 465, loss is 0.07563809305429459\n",
      "epoch: 2 step: 466, loss is 0.21025292575359344\n",
      "epoch: 2 step: 467, loss is 0.15344415605068207\n",
      "epoch: 2 step: 468, loss is 0.0015771932667121291\n",
      "epoch: 2 step: 469, loss is 0.01700964756309986\n",
      "epoch: 2 step: 470, loss is 0.2375211864709854\n",
      "epoch: 2 step: 471, loss is 0.011431021615862846\n",
      "epoch: 2 step: 472, loss is 0.003919082228094339\n",
      "epoch: 2 step: 473, loss is 0.20212218165397644\n",
      "epoch: 2 step: 474, loss is 0.01253453642129898\n",
      "epoch: 2 step: 475, loss is 0.055718835443258286\n",
      "epoch: 2 step: 476, loss is 0.14850559830665588\n",
      "epoch: 2 step: 477, loss is 0.033786166459321976\n",
      "epoch: 2 step: 478, loss is 0.08533003181219101\n",
      "epoch: 2 step: 479, loss is 0.013720585033297539\n",
      "epoch: 2 step: 480, loss is 0.001633411506190896\n",
      "epoch: 2 step: 481, loss is 0.005322513170540333\n",
      "epoch: 2 step: 482, loss is 0.05430247262120247\n",
      "epoch: 2 step: 483, loss is 0.02313871495425701\n",
      "epoch: 2 step: 484, loss is 0.003918597009032965\n",
      "epoch: 2 step: 485, loss is 0.011391734704375267\n",
      "epoch: 2 step: 486, loss is 0.09322325140237808\n",
      "epoch: 2 step: 487, loss is 0.05274670571088791\n",
      "epoch: 2 step: 488, loss is 0.034719206392765045\n",
      "epoch: 2 step: 489, loss is 0.007436922285705805\n",
      "epoch: 2 step: 490, loss is 0.012133659794926643\n",
      "epoch: 2 step: 491, loss is 0.0455089770257473\n",
      "epoch: 2 step: 492, loss is 0.028066381812095642\n",
      "epoch: 2 step: 493, loss is 0.016911817714571953\n",
      "epoch: 2 step: 494, loss is 0.06601240485906601\n",
      "epoch: 2 step: 495, loss is 0.012090643867850304\n",
      "epoch: 2 step: 496, loss is 0.0007682962459512055\n",
      "epoch: 2 step: 497, loss is 0.046211984008550644\n",
      "epoch: 2 step: 498, loss is 0.09939330071210861\n",
      "epoch: 2 step: 499, loss is 0.021369393914937973\n",
      "epoch: 2 step: 500, loss is 0.0038243522867560387\n",
      "epoch: 2 step: 501, loss is 0.025058992207050323\n",
      "epoch: 2 step: 502, loss is 0.006822614464908838\n",
      "epoch: 2 step: 503, loss is 0.03752513229846954\n",
      "epoch: 2 step: 504, loss is 0.1763194054365158\n",
      "epoch: 2 step: 505, loss is 0.1119278222322464\n",
      "epoch: 2 step: 506, loss is 0.2570383548736572\n",
      "epoch: 2 step: 507, loss is 0.0058319177478551865\n",
      "epoch: 2 step: 508, loss is 0.036869462579488754\n",
      "epoch: 2 step: 509, loss is 0.015445448458194733\n",
      "epoch: 2 step: 510, loss is 0.044199179857969284\n",
      "epoch: 2 step: 511, loss is 0.003609195351600647\n",
      "epoch: 2 step: 512, loss is 0.11220909655094147\n",
      "epoch: 2 step: 513, loss is 0.13218113780021667\n",
      "epoch: 2 step: 514, loss is 0.0023270382080227137\n",
      "epoch: 2 step: 515, loss is 0.05620637163519859\n",
      "epoch: 2 step: 516, loss is 0.03387867659330368\n",
      "epoch: 2 step: 517, loss is 0.02398575469851494\n",
      "epoch: 2 step: 518, loss is 0.0496467724442482\n",
      "epoch: 2 step: 519, loss is 0.010845063254237175\n",
      "epoch: 2 step: 520, loss is 0.09151794761419296\n",
      "epoch: 2 step: 521, loss is 0.0048909555189311504\n",
      "epoch: 2 step: 522, loss is 0.017748210579156876\n",
      "epoch: 2 step: 523, loss is 0.3044031262397766\n",
      "epoch: 2 step: 524, loss is 0.008731796406209469\n",
      "epoch: 2 step: 525, loss is 0.0728251188993454\n",
      "epoch: 2 step: 526, loss is 0.1318684220314026\n",
      "epoch: 2 step: 527, loss is 0.00941396877169609\n",
      "epoch: 2 step: 528, loss is 0.012381769716739655\n",
      "epoch: 2 step: 529, loss is 0.032793350517749786\n",
      "epoch: 2 step: 530, loss is 0.0024265616666525602\n",
      "epoch: 2 step: 531, loss is 0.03608198091387749\n",
      "epoch: 2 step: 532, loss is 0.012018948793411255\n",
      "epoch: 2 step: 533, loss is 0.005468024872243404\n",
      "epoch: 2 step: 534, loss is 0.3788013160228729\n",
      "epoch: 2 step: 535, loss is 0.021729497238993645\n",
      "epoch: 2 step: 536, loss is 0.033901114016771317\n",
      "epoch: 2 step: 537, loss is 0.010339291766285896\n",
      "epoch: 2 step: 538, loss is 0.027297377586364746\n",
      "epoch: 2 step: 539, loss is 0.16043052077293396\n",
      "epoch: 2 step: 540, loss is 0.021792802959680557\n",
      "epoch: 2 step: 541, loss is 0.020321782678365707\n",
      "epoch: 2 step: 542, loss is 0.12930335104465485\n",
      "epoch: 2 step: 543, loss is 0.029726149514317513\n",
      "epoch: 2 step: 544, loss is 0.011776195839047432\n",
      "epoch: 2 step: 545, loss is 0.004137062467634678\n",
      "epoch: 2 step: 546, loss is 0.010729147121310234\n",
      "epoch: 2 step: 547, loss is 0.011891381815075874\n",
      "epoch: 2 step: 548, loss is 0.011657696217298508\n",
      "epoch: 2 step: 549, loss is 0.05643675848841667\n",
      "epoch: 2 step: 550, loss is 0.04693835973739624\n",
      "epoch: 2 step: 551, loss is 0.03316952660679817\n",
      "epoch: 2 step: 552, loss is 0.012068456038832664\n",
      "epoch: 2 step: 553, loss is 0.011669347994029522\n",
      "epoch: 2 step: 554, loss is 0.009702757000923157\n",
      "epoch: 2 step: 555, loss is 0.0032361496705561876\n",
      "epoch: 2 step: 556, loss is 0.07142738252878189\n",
      "epoch: 2 step: 557, loss is 0.001192936091683805\n",
      "epoch: 2 step: 558, loss is 0.004107761196792126\n",
      "epoch: 2 step: 559, loss is 0.040266260504722595\n",
      "epoch: 2 step: 560, loss is 0.012762349098920822\n",
      "epoch: 2 step: 561, loss is 0.14635510742664337\n",
      "epoch: 2 step: 562, loss is 0.4087083637714386\n",
      "epoch: 2 step: 563, loss is 0.05502050369977951\n",
      "epoch: 2 step: 564, loss is 0.02397051639854908\n",
      "epoch: 2 step: 565, loss is 0.021464284509420395\n",
      "epoch: 2 step: 566, loss is 0.002008320763707161\n",
      "epoch: 2 step: 567, loss is 0.15047509968280792\n",
      "epoch: 2 step: 568, loss is 0.06422657519578934\n",
      "epoch: 2 step: 569, loss is 0.004962247330695391\n",
      "epoch: 2 step: 570, loss is 0.01709161140024662\n",
      "epoch: 2 step: 571, loss is 0.006118162535130978\n",
      "epoch: 2 step: 572, loss is 0.06629671156406403\n",
      "epoch: 2 step: 573, loss is 0.0067806909792125225\n",
      "epoch: 2 step: 574, loss is 0.31726300716400146\n",
      "epoch: 2 step: 575, loss is 0.17158080637454987\n",
      "epoch: 2 step: 576, loss is 0.11816813796758652\n",
      "epoch: 2 step: 577, loss is 0.12455141544342041\n",
      "epoch: 2 step: 578, loss is 0.10523293912410736\n",
      "epoch: 2 step: 579, loss is 0.053734004497528076\n",
      "epoch: 2 step: 580, loss is 0.16785462200641632\n",
      "epoch: 2 step: 581, loss is 0.24031506478786469\n",
      "epoch: 2 step: 582, loss is 0.02017597295343876\n",
      "epoch: 2 step: 583, loss is 0.18930663168430328\n",
      "epoch: 2 step: 584, loss is 0.11590131372213364\n",
      "epoch: 2 step: 585, loss is 0.030278844758868217\n",
      "epoch: 2 step: 586, loss is 0.0059946458786726\n",
      "epoch: 2 step: 587, loss is 0.04860980808734894\n",
      "epoch: 2 step: 588, loss is 0.004369702190160751\n",
      "epoch: 2 step: 589, loss is 0.08259410411119461\n",
      "epoch: 2 step: 590, loss is 0.011826867237687111\n",
      "epoch: 2 step: 591, loss is 0.13360711932182312\n",
      "epoch: 2 step: 592, loss is 0.025938572362065315\n",
      "epoch: 2 step: 593, loss is 0.03196628764271736\n",
      "epoch: 2 step: 594, loss is 0.08156470209360123\n",
      "epoch: 2 step: 595, loss is 0.021219369024038315\n",
      "epoch: 2 step: 596, loss is 0.011091405525803566\n",
      "epoch: 2 step: 597, loss is 0.11605016887187958\n",
      "epoch: 2 step: 598, loss is 0.30335697531700134\n",
      "epoch: 2 step: 599, loss is 0.094349704682827\n",
      "epoch: 2 step: 600, loss is 0.004877856932580471\n",
      "epoch: 2 step: 601, loss is 0.03755583614110947\n",
      "epoch: 2 step: 602, loss is 0.02773568220436573\n",
      "epoch: 2 step: 603, loss is 0.043454430997371674\n",
      "epoch: 2 step: 604, loss is 0.018866321071982384\n",
      "epoch: 2 step: 605, loss is 0.006930110044777393\n",
      "epoch: 2 step: 606, loss is 0.011763913556933403\n",
      "epoch: 2 step: 607, loss is 0.0026435288600623608\n",
      "epoch: 2 step: 608, loss is 0.38442450761795044\n",
      "epoch: 2 step: 609, loss is 0.007290592882782221\n",
      "epoch: 2 step: 610, loss is 0.070213183760643\n",
      "epoch: 2 step: 611, loss is 0.03679244592785835\n",
      "epoch: 2 step: 612, loss is 0.02787870727479458\n",
      "epoch: 2 step: 613, loss is 0.21733272075653076\n",
      "epoch: 2 step: 614, loss is 0.1801546812057495\n",
      "epoch: 2 step: 615, loss is 0.3372383117675781\n",
      "epoch: 2 step: 616, loss is 0.04461866617202759\n",
      "epoch: 2 step: 617, loss is 0.014623419381678104\n",
      "epoch: 2 step: 618, loss is 0.06322438269853592\n",
      "epoch: 2 step: 619, loss is 0.09432253986597061\n",
      "epoch: 2 step: 620, loss is 0.05045299604535103\n",
      "epoch: 2 step: 621, loss is 0.12376906722784042\n",
      "epoch: 2 step: 622, loss is 0.030424945056438446\n",
      "epoch: 2 step: 623, loss is 0.0715022161602974\n",
      "epoch: 2 step: 624, loss is 0.04529096558690071\n",
      "epoch: 2 step: 625, loss is 0.011006161570549011\n",
      "epoch: 2 step: 626, loss is 0.14342130720615387\n",
      "epoch: 2 step: 627, loss is 0.09332463145256042\n",
      "epoch: 2 step: 628, loss is 0.0755390003323555\n",
      "epoch: 2 step: 629, loss is 0.009092975407838821\n",
      "epoch: 2 step: 630, loss is 0.030458301305770874\n",
      "epoch: 2 step: 631, loss is 0.09794867783784866\n",
      "epoch: 2 step: 632, loss is 0.06860455125570297\n",
      "epoch: 2 step: 633, loss is 0.007203826680779457\n",
      "epoch: 2 step: 634, loss is 0.020966708660125732\n",
      "epoch: 2 step: 635, loss is 0.053306564688682556\n",
      "epoch: 2 step: 636, loss is 0.09574670344591141\n",
      "epoch: 2 step: 637, loss is 0.056937478482723236\n",
      "epoch: 2 step: 638, loss is 0.0358201302587986\n",
      "epoch: 2 step: 639, loss is 0.09842351078987122\n",
      "epoch: 2 step: 640, loss is 0.07506853342056274\n",
      "epoch: 2 step: 641, loss is 0.017763551324605942\n",
      "epoch: 2 step: 642, loss is 0.08641472458839417\n",
      "epoch: 2 step: 643, loss is 0.07865795493125916\n",
      "epoch: 2 step: 644, loss is 0.02722897380590439\n",
      "epoch: 2 step: 645, loss is 0.17859472334384918\n",
      "epoch: 2 step: 646, loss is 0.009537228383123875\n",
      "epoch: 2 step: 647, loss is 0.0020087407901883125\n",
      "epoch: 2 step: 648, loss is 0.0037469554226845503\n",
      "epoch: 2 step: 649, loss is 0.19191086292266846\n",
      "epoch: 2 step: 650, loss is 0.09185062348842621\n",
      "epoch: 2 step: 651, loss is 0.022100210189819336\n",
      "epoch: 2 step: 652, loss is 0.024390147998929024\n",
      "epoch: 2 step: 653, loss is 0.007142312824726105\n",
      "epoch: 2 step: 654, loss is 0.0598214752972126\n",
      "epoch: 2 step: 655, loss is 0.02231650799512863\n",
      "epoch: 2 step: 656, loss is 0.017242711037397385\n",
      "epoch: 2 step: 657, loss is 0.05794893950223923\n",
      "epoch: 2 step: 658, loss is 0.03945806249976158\n",
      "epoch: 2 step: 659, loss is 0.007709683384746313\n",
      "epoch: 2 step: 660, loss is 0.0563620924949646\n",
      "epoch: 2 step: 661, loss is 0.0033120792359113693\n",
      "epoch: 2 step: 662, loss is 0.07201956957578659\n",
      "epoch: 2 step: 663, loss is 0.08886748552322388\n",
      "epoch: 2 step: 664, loss is 0.05989350378513336\n",
      "epoch: 2 step: 665, loss is 0.02882017381489277\n",
      "epoch: 2 step: 666, loss is 0.01618301495909691\n",
      "epoch: 2 step: 667, loss is 0.00542085524648428\n",
      "epoch: 2 step: 668, loss is 0.004375654272735119\n",
      "epoch: 2 step: 669, loss is 0.040121451020240784\n",
      "epoch: 2 step: 670, loss is 0.021722368896007538\n",
      "epoch: 2 step: 671, loss is 0.06844576448202133\n",
      "epoch: 2 step: 672, loss is 0.01957579329609871\n",
      "epoch: 2 step: 673, loss is 0.0067209601402282715\n",
      "epoch: 2 step: 674, loss is 0.0015107522485777736\n",
      "epoch: 2 step: 675, loss is 0.05180580914020538\n",
      "epoch: 2 step: 676, loss is 0.0735829770565033\n",
      "epoch: 2 step: 677, loss is 0.014976605772972107\n",
      "epoch: 2 step: 678, loss is 0.2551858127117157\n",
      "epoch: 2 step: 679, loss is 0.04459722712635994\n",
      "epoch: 2 step: 680, loss is 0.03924333304166794\n",
      "epoch: 2 step: 681, loss is 0.09923750907182693\n",
      "epoch: 2 step: 682, loss is 0.012524238787591457\n",
      "epoch: 2 step: 683, loss is 0.0008012683829292655\n",
      "epoch: 2 step: 684, loss is 0.002461583586409688\n",
      "epoch: 2 step: 685, loss is 0.0024676525499671698\n",
      "epoch: 2 step: 686, loss is 0.00426614610478282\n",
      "epoch: 2 step: 687, loss is 0.0029655983671545982\n",
      "epoch: 2 step: 688, loss is 0.04684895649552345\n",
      "epoch: 2 step: 689, loss is 0.031207427382469177\n",
      "epoch: 2 step: 690, loss is 0.20543818175792694\n",
      "epoch: 2 step: 691, loss is 0.1120137944817543\n",
      "epoch: 2 step: 692, loss is 0.0004537922504823655\n",
      "epoch: 2 step: 693, loss is 0.013313258066773415\n",
      "epoch: 2 step: 694, loss is 0.08897503465414047\n",
      "epoch: 2 step: 695, loss is 0.039939045906066895\n",
      "epoch: 2 step: 696, loss is 0.026540018618106842\n",
      "epoch: 2 step: 697, loss is 0.09802080690860748\n",
      "epoch: 2 step: 698, loss is 0.005940059199929237\n",
      "epoch: 2 step: 699, loss is 0.01034796703606844\n",
      "epoch: 2 step: 700, loss is 0.027114983648061752\n",
      "epoch: 2 step: 701, loss is 0.027122171595692635\n",
      "epoch: 2 step: 702, loss is 0.0008462292607873678\n",
      "epoch: 2 step: 703, loss is 0.08745352178812027\n",
      "epoch: 2 step: 704, loss is 0.11877839267253876\n",
      "epoch: 2 step: 705, loss is 0.0027314703911542892\n",
      "epoch: 2 step: 706, loss is 0.06890551745891571\n",
      "epoch: 2 step: 707, loss is 0.04707896336913109\n",
      "epoch: 2 step: 708, loss is 0.21999412775039673\n",
      "epoch: 2 step: 709, loss is 0.04096606373786926\n",
      "epoch: 2 step: 710, loss is 0.0007044291123747826\n",
      "epoch: 2 step: 711, loss is 0.01761513575911522\n",
      "epoch: 2 step: 712, loss is 0.14952081441879272\n",
      "epoch: 2 step: 713, loss is 0.06371194124221802\n",
      "epoch: 2 step: 714, loss is 0.08576475828886032\n",
      "epoch: 2 step: 715, loss is 0.1766882687807083\n",
      "epoch: 2 step: 716, loss is 0.005405010189861059\n",
      "epoch: 2 step: 717, loss is 0.038919392973184586\n",
      "epoch: 2 step: 718, loss is 0.16998973488807678\n",
      "epoch: 2 step: 719, loss is 0.01925056427717209\n",
      "epoch: 2 step: 720, loss is 0.005439140368252993\n",
      "epoch: 2 step: 721, loss is 0.025422967970371246\n",
      "epoch: 2 step: 722, loss is 0.021633092314004898\n",
      "epoch: 2 step: 723, loss is 0.06033962219953537\n",
      "epoch: 2 step: 724, loss is 0.0812772661447525\n",
      "epoch: 2 step: 725, loss is 0.18770991265773773\n",
      "epoch: 2 step: 726, loss is 0.3271201550960541\n",
      "epoch: 2 step: 727, loss is 0.08204243332147598\n",
      "epoch: 2 step: 728, loss is 0.034169066697359085\n",
      "epoch: 2 step: 729, loss is 0.009152648039162159\n",
      "epoch: 2 step: 730, loss is 0.006550797261297703\n",
      "epoch: 2 step: 731, loss is 0.03188376873731613\n",
      "epoch: 2 step: 732, loss is 0.09090570360422134\n",
      "epoch: 2 step: 733, loss is 0.05297224968671799\n",
      "epoch: 2 step: 734, loss is 0.022561287507414818\n",
      "epoch: 2 step: 735, loss is 0.013474796898663044\n",
      "epoch: 2 step: 736, loss is 0.0179363414645195\n",
      "epoch: 2 step: 737, loss is 0.1354939192533493\n",
      "epoch: 2 step: 738, loss is 0.01815646141767502\n",
      "epoch: 2 step: 739, loss is 0.007245172746479511\n",
      "epoch: 2 step: 740, loss is 0.0541330985724926\n",
      "epoch: 2 step: 741, loss is 0.020285189151763916\n",
      "epoch: 2 step: 742, loss is 0.23881885409355164\n",
      "epoch: 2 step: 743, loss is 0.0019431690452620387\n",
      "epoch: 2 step: 744, loss is 0.07461849600076675\n",
      "epoch: 2 step: 745, loss is 0.0010463149519637227\n",
      "epoch: 2 step: 746, loss is 0.043123576790094376\n",
      "epoch: 2 step: 747, loss is 0.06798218190670013\n",
      "epoch: 2 step: 748, loss is 0.05723196640610695\n",
      "epoch: 2 step: 749, loss is 0.005649324040859938\n",
      "epoch: 2 step: 750, loss is 0.009383490309119225\n",
      "epoch: 2 step: 751, loss is 0.2679823637008667\n",
      "epoch: 2 step: 752, loss is 0.047894712537527084\n",
      "epoch: 2 step: 753, loss is 0.16745589673519135\n",
      "epoch: 2 step: 754, loss is 0.004947825334966183\n",
      "epoch: 2 step: 755, loss is 0.022436127066612244\n",
      "epoch: 2 step: 756, loss is 0.00464689452201128\n",
      "epoch: 2 step: 757, loss is 0.027892226353287697\n",
      "epoch: 2 step: 758, loss is 0.10264454782009125\n",
      "epoch: 2 step: 759, loss is 0.016237584874033928\n",
      "epoch: 2 step: 760, loss is 0.010213073343038559\n",
      "epoch: 2 step: 761, loss is 0.031736016273498535\n",
      "epoch: 2 step: 762, loss is 0.046548992395401\n",
      "epoch: 2 step: 763, loss is 0.03991351276636124\n",
      "epoch: 2 step: 764, loss is 0.007432848215103149\n",
      "epoch: 2 step: 765, loss is 0.028886541724205017\n",
      "epoch: 2 step: 766, loss is 0.05280280113220215\n",
      "epoch: 2 step: 767, loss is 0.20644964277744293\n",
      "epoch: 2 step: 768, loss is 0.001734175719320774\n",
      "epoch: 2 step: 769, loss is 0.10797432065010071\n",
      "epoch: 2 step: 770, loss is 0.09915722906589508\n",
      "epoch: 2 step: 771, loss is 0.02692832052707672\n",
      "epoch: 2 step: 772, loss is 0.1381177306175232\n",
      "epoch: 2 step: 773, loss is 0.0011361471842974424\n",
      "epoch: 2 step: 774, loss is 0.04377251863479614\n",
      "epoch: 2 step: 775, loss is 0.13659292459487915\n",
      "epoch: 2 step: 776, loss is 0.13848651945590973\n",
      "epoch: 2 step: 777, loss is 0.10923109948635101\n",
      "epoch: 2 step: 778, loss is 0.26418405771255493\n",
      "epoch: 2 step: 779, loss is 0.035874783992767334\n",
      "epoch: 2 step: 780, loss is 0.15429812669754028\n",
      "epoch: 2 step: 781, loss is 0.10172205418348312\n",
      "epoch: 2 step: 782, loss is 0.011274631135165691\n",
      "epoch: 2 step: 783, loss is 0.16291964054107666\n",
      "epoch: 2 step: 784, loss is 0.46934154629707336\n",
      "epoch: 2 step: 785, loss is 0.10708937793970108\n",
      "epoch: 2 step: 786, loss is 0.03520403802394867\n",
      "epoch: 2 step: 787, loss is 0.05845540761947632\n",
      "epoch: 2 step: 788, loss is 0.1102655902504921\n",
      "epoch: 2 step: 789, loss is 0.014620360918343067\n",
      "epoch: 2 step: 790, loss is 0.035539086908102036\n",
      "epoch: 2 step: 791, loss is 0.013135166838765144\n",
      "epoch: 2 step: 792, loss is 0.06032010167837143\n",
      "epoch: 2 step: 793, loss is 0.06787104159593582\n",
      "epoch: 2 step: 794, loss is 0.022198371589183807\n",
      "epoch: 2 step: 795, loss is 0.016426045447587967\n",
      "epoch: 2 step: 796, loss is 0.022455131635069847\n",
      "epoch: 2 step: 797, loss is 0.09047260135412216\n",
      "epoch: 2 step: 798, loss is 0.029546869918704033\n",
      "epoch: 2 step: 799, loss is 0.013328725472092628\n",
      "epoch: 2 step: 800, loss is 0.10210081934928894\n",
      "epoch: 2 step: 801, loss is 0.018527355045080185\n",
      "epoch: 2 step: 802, loss is 0.005029177758842707\n",
      "epoch: 2 step: 803, loss is 0.01546482089906931\n",
      "epoch: 2 step: 804, loss is 0.1817709505558014\n",
      "epoch: 2 step: 805, loss is 0.010026813484728336\n",
      "epoch: 2 step: 806, loss is 0.07535155862569809\n",
      "epoch: 2 step: 807, loss is 0.042105019092559814\n",
      "epoch: 2 step: 808, loss is 0.19317689538002014\n",
      "epoch: 2 step: 809, loss is 0.026540525257587433\n",
      "epoch: 2 step: 810, loss is 0.06316322833299637\n",
      "epoch: 2 step: 811, loss is 0.16383309662342072\n",
      "epoch: 2 step: 812, loss is 0.140034481883049\n",
      "epoch: 2 step: 813, loss is 0.03215717896819115\n",
      "epoch: 2 step: 814, loss is 0.011761904694139957\n",
      "epoch: 2 step: 815, loss is 0.020544985309243202\n",
      "epoch: 2 step: 816, loss is 0.009404780343174934\n",
      "epoch: 2 step: 817, loss is 0.033968735486269\n",
      "epoch: 2 step: 818, loss is 0.019098492339253426\n",
      "epoch: 2 step: 819, loss is 0.04659759998321533\n",
      "epoch: 2 step: 820, loss is 0.007842972874641418\n",
      "epoch: 2 step: 821, loss is 0.137762188911438\n",
      "epoch: 2 step: 822, loss is 0.017012137919664383\n",
      "epoch: 2 step: 823, loss is 0.11558946967124939\n",
      "epoch: 2 step: 824, loss is 0.014944386668503284\n",
      "epoch: 2 step: 825, loss is 0.006644947454333305\n",
      "epoch: 2 step: 826, loss is 0.1772601455450058\n",
      "epoch: 2 step: 827, loss is 0.006619301624596119\n",
      "epoch: 2 step: 828, loss is 0.026752635836601257\n",
      "epoch: 2 step: 829, loss is 0.10041912645101547\n",
      "epoch: 2 step: 830, loss is 0.006863290444016457\n",
      "epoch: 2 step: 831, loss is 0.024562254548072815\n",
      "epoch: 2 step: 832, loss is 0.12333863973617554\n",
      "epoch: 2 step: 833, loss is 0.07185438275337219\n",
      "epoch: 2 step: 834, loss is 0.07783593982458115\n",
      "epoch: 2 step: 835, loss is 0.05840155482292175\n",
      "epoch: 2 step: 836, loss is 0.00876680389046669\n",
      "epoch: 2 step: 837, loss is 0.009333877824246883\n",
      "epoch: 2 step: 838, loss is 0.06893512606620789\n",
      "epoch: 2 step: 839, loss is 0.09663393348455429\n",
      "epoch: 2 step: 840, loss is 0.0026119614485651255\n",
      "epoch: 2 step: 841, loss is 0.0068778181448578835\n",
      "epoch: 2 step: 842, loss is 0.02236349694430828\n",
      "epoch: 2 step: 843, loss is 0.04473802447319031\n",
      "epoch: 2 step: 844, loss is 0.05786332115530968\n",
      "epoch: 2 step: 845, loss is 0.04491819441318512\n",
      "epoch: 2 step: 846, loss is 0.002330825896933675\n",
      "epoch: 2 step: 847, loss is 0.007895425893366337\n",
      "epoch: 2 step: 848, loss is 0.11367123574018478\n",
      "epoch: 2 step: 849, loss is 0.0534466952085495\n",
      "epoch: 2 step: 850, loss is 0.11168384552001953\n",
      "epoch: 2 step: 851, loss is 0.031819481402635574\n",
      "epoch: 2 step: 852, loss is 0.34538915753364563\n",
      "epoch: 2 step: 853, loss is 0.019493186846375465\n",
      "epoch: 2 step: 854, loss is 0.20419621467590332\n",
      "epoch: 2 step: 855, loss is 0.1599276065826416\n",
      "epoch: 2 step: 856, loss is 0.002739753108471632\n",
      "epoch: 2 step: 857, loss is 0.1361282616853714\n",
      "epoch: 2 step: 858, loss is 0.1367681920528412\n",
      "epoch: 2 step: 859, loss is 0.02458694949746132\n",
      "epoch: 2 step: 860, loss is 0.05278276279568672\n",
      "epoch: 2 step: 861, loss is 0.008638326078653336\n",
      "epoch: 2 step: 862, loss is 0.04444338008761406\n",
      "epoch: 2 step: 863, loss is 0.013819234445691109\n",
      "epoch: 2 step: 864, loss is 0.04158546403050423\n",
      "epoch: 2 step: 865, loss is 0.020826471969485283\n",
      "epoch: 2 step: 866, loss is 0.5158627033233643\n",
      "epoch: 2 step: 867, loss is 0.038829367607831955\n",
      "epoch: 2 step: 868, loss is 0.1925915628671646\n",
      "epoch: 2 step: 869, loss is 0.009657665155827999\n",
      "epoch: 2 step: 870, loss is 0.04221293702721596\n",
      "epoch: 2 step: 871, loss is 0.048515256494283676\n",
      "epoch: 2 step: 872, loss is 0.010611847043037415\n",
      "epoch: 2 step: 873, loss is 0.032148219645023346\n",
      "epoch: 2 step: 874, loss is 0.046553049236536026\n",
      "epoch: 2 step: 875, loss is 0.006505177356302738\n",
      "epoch: 2 step: 876, loss is 0.13503561913967133\n",
      "epoch: 2 step: 877, loss is 0.015956256538629532\n",
      "epoch: 2 step: 878, loss is 0.082315593957901\n",
      "epoch: 2 step: 879, loss is 0.22485117614269257\n",
      "epoch: 2 step: 880, loss is 0.07738777995109558\n",
      "epoch: 2 step: 881, loss is 0.0955413430929184\n",
      "epoch: 2 step: 882, loss is 0.04268019646406174\n",
      "epoch: 2 step: 883, loss is 0.032619908452034\n",
      "epoch: 2 step: 884, loss is 0.13697761297225952\n",
      "epoch: 2 step: 885, loss is 0.030648892745375633\n",
      "epoch: 2 step: 886, loss is 0.4643426537513733\n",
      "epoch: 2 step: 887, loss is 0.11752310395240784\n",
      "epoch: 2 step: 888, loss is 0.006136423442512751\n",
      "epoch: 2 step: 889, loss is 0.010183288715779781\n",
      "epoch: 2 step: 890, loss is 0.06551589071750641\n",
      "epoch: 2 step: 891, loss is 0.1305234730243683\n",
      "epoch: 2 step: 892, loss is 0.005979502573609352\n",
      "epoch: 2 step: 893, loss is 0.0746513158082962\n",
      "epoch: 2 step: 894, loss is 0.006124753039330244\n",
      "epoch: 2 step: 895, loss is 0.01845315471291542\n",
      "epoch: 2 step: 896, loss is 0.05815383419394493\n",
      "epoch: 2 step: 897, loss is 0.1221412941813469\n",
      "epoch: 2 step: 898, loss is 0.025545325130224228\n",
      "epoch: 2 step: 899, loss is 0.09841957688331604\n",
      "epoch: 2 step: 900, loss is 0.005282321944832802\n",
      "epoch: 2 step: 901, loss is 0.021206416189670563\n",
      "epoch: 2 step: 902, loss is 0.13066178560256958\n",
      "epoch: 2 step: 903, loss is 0.1299314945936203\n",
      "epoch: 2 step: 904, loss is 0.1312044858932495\n",
      "epoch: 2 step: 905, loss is 0.0041097006760537624\n",
      "epoch: 2 step: 906, loss is 0.32359763979911804\n",
      "epoch: 2 step: 907, loss is 0.015609639696776867\n",
      "epoch: 2 step: 908, loss is 0.14997200667858124\n",
      "epoch: 2 step: 909, loss is 0.06294889003038406\n",
      "epoch: 2 step: 910, loss is 0.002503904514014721\n",
      "epoch: 2 step: 911, loss is 0.01782258227467537\n",
      "epoch: 2 step: 912, loss is 0.0375065878033638\n",
      "epoch: 2 step: 913, loss is 0.001843936275690794\n",
      "epoch: 2 step: 914, loss is 0.04001075401902199\n",
      "epoch: 2 step: 915, loss is 0.02357800118625164\n",
      "epoch: 2 step: 916, loss is 0.11366726458072662\n",
      "epoch: 2 step: 917, loss is 0.06888274103403091\n",
      "epoch: 2 step: 918, loss is 0.0037045935168862343\n",
      "epoch: 2 step: 919, loss is 0.17048324644565582\n",
      "epoch: 2 step: 920, loss is 0.0022783451713621616\n",
      "epoch: 2 step: 921, loss is 0.16177527606487274\n",
      "epoch: 2 step: 922, loss is 0.014108174480497837\n",
      "epoch: 2 step: 923, loss is 0.030591294169425964\n",
      "epoch: 2 step: 924, loss is 0.0012406731257215142\n",
      "epoch: 2 step: 925, loss is 0.010399643331766129\n",
      "epoch: 2 step: 926, loss is 0.13359954953193665\n",
      "epoch: 2 step: 927, loss is 0.010793470777571201\n",
      "epoch: 2 step: 928, loss is 0.09745475649833679\n",
      "epoch: 2 step: 929, loss is 0.0031938597094267607\n",
      "epoch: 2 step: 930, loss is 0.10043451189994812\n",
      "epoch: 2 step: 931, loss is 0.0050625791773200035\n",
      "epoch: 2 step: 932, loss is 0.06295326352119446\n",
      "epoch: 2 step: 933, loss is 0.024444201961159706\n",
      "epoch: 2 step: 934, loss is 0.01035541482269764\n",
      "epoch: 2 step: 935, loss is 0.10953106731176376\n",
      "epoch: 2 step: 936, loss is 0.023137511685490608\n",
      "epoch: 2 step: 937, loss is 0.00851963646709919\n",
      "epoch: 2 step: 938, loss is 0.014818314462900162\n",
      "epoch: 2 step: 939, loss is 0.048811960965394974\n",
      "epoch: 2 step: 940, loss is 0.06989692896604538\n",
      "epoch: 2 step: 941, loss is 0.09183461964130402\n",
      "epoch: 2 step: 942, loss is 0.1630207598209381\n",
      "epoch: 2 step: 943, loss is 0.09389548003673553\n",
      "epoch: 2 step: 944, loss is 0.008338958024978638\n",
      "epoch: 2 step: 945, loss is 0.003946033306419849\n",
      "epoch: 2 step: 946, loss is 0.06650898605585098\n",
      "epoch: 2 step: 947, loss is 0.07956375181674957\n",
      "epoch: 2 step: 948, loss is 0.18292713165283203\n",
      "epoch: 2 step: 949, loss is 0.007554027251899242\n",
      "epoch: 2 step: 950, loss is 0.05792301520705223\n",
      "epoch: 2 step: 951, loss is 0.03166058659553528\n",
      "epoch: 2 step: 952, loss is 0.0023680338636040688\n",
      "epoch: 2 step: 953, loss is 0.017908643931150436\n",
      "epoch: 2 step: 954, loss is 0.08185648173093796\n",
      "epoch: 2 step: 955, loss is 0.03551265224814415\n",
      "epoch: 2 step: 956, loss is 0.006089735310524702\n",
      "epoch: 2 step: 957, loss is 0.038957152515649796\n",
      "epoch: 2 step: 958, loss is 0.08985347300767899\n",
      "epoch: 2 step: 959, loss is 0.08673742413520813\n",
      "epoch: 2 step: 960, loss is 0.11813836544752121\n",
      "epoch: 2 step: 961, loss is 0.004752960987389088\n",
      "epoch: 2 step: 962, loss is 0.07510127127170563\n",
      "epoch: 2 step: 963, loss is 0.11497993767261505\n",
      "epoch: 2 step: 964, loss is 0.007845287211239338\n",
      "epoch: 2 step: 965, loss is 0.011450952850282192\n",
      "epoch: 2 step: 966, loss is 0.00679637910798192\n",
      "epoch: 2 step: 967, loss is 0.065616175532341\n",
      "epoch: 2 step: 968, loss is 0.04672873765230179\n",
      "epoch: 2 step: 969, loss is 0.03168821334838867\n",
      "epoch: 2 step: 970, loss is 0.05924183875322342\n",
      "epoch: 2 step: 971, loss is 0.00046913663391023874\n",
      "epoch: 2 step: 972, loss is 0.02275187335908413\n",
      "epoch: 2 step: 973, loss is 0.11165441572666168\n",
      "epoch: 2 step: 974, loss is 0.047795288264751434\n",
      "epoch: 2 step: 975, loss is 0.008188705891370773\n",
      "epoch: 2 step: 976, loss is 0.11813436448574066\n",
      "epoch: 2 step: 977, loss is 0.16207186877727509\n",
      "epoch: 2 step: 978, loss is 0.026920104399323463\n",
      "epoch: 2 step: 979, loss is 0.0127987340092659\n",
      "epoch: 2 step: 980, loss is 0.09497053176164627\n",
      "epoch: 2 step: 981, loss is 0.03419012576341629\n",
      "epoch: 2 step: 982, loss is 0.12682095170021057\n",
      "epoch: 2 step: 983, loss is 0.07389089465141296\n",
      "epoch: 2 step: 984, loss is 0.1283070147037506\n",
      "epoch: 2 step: 985, loss is 0.024006694555282593\n",
      "epoch: 2 step: 986, loss is 0.028932565823197365\n",
      "epoch: 2 step: 987, loss is 0.15723641216754913\n",
      "epoch: 2 step: 988, loss is 0.09463247656822205\n",
      "epoch: 2 step: 989, loss is 0.04822342097759247\n",
      "epoch: 2 step: 990, loss is 0.18902727961540222\n",
      "epoch: 2 step: 991, loss is 0.12456700950860977\n",
      "epoch: 2 step: 992, loss is 0.029147889465093613\n",
      "epoch: 2 step: 993, loss is 0.0028239295352250338\n",
      "epoch: 2 step: 994, loss is 0.02412468194961548\n",
      "epoch: 2 step: 995, loss is 0.030400093644857407\n",
      "epoch: 2 step: 996, loss is 0.04236111044883728\n",
      "epoch: 2 step: 997, loss is 0.06338761746883392\n",
      "epoch: 2 step: 998, loss is 0.1413775384426117\n",
      "epoch: 2 step: 999, loss is 0.08210922032594681\n",
      "epoch: 2 step: 1000, loss is 0.24280405044555664\n",
      "epoch: 2 step: 1001, loss is 0.11141278594732285\n",
      "epoch: 2 step: 1002, loss is 0.031481627374887466\n",
      "epoch: 2 step: 1003, loss is 0.050997503101825714\n",
      "epoch: 2 step: 1004, loss is 0.005676907487213612\n",
      "epoch: 2 step: 1005, loss is 0.03189403563737869\n",
      "epoch: 2 step: 1006, loss is 0.03520875424146652\n",
      "epoch: 2 step: 1007, loss is 0.023595286533236504\n",
      "epoch: 2 step: 1008, loss is 0.08283059298992157\n",
      "epoch: 2 step: 1009, loss is 0.03216641768813133\n",
      "epoch: 2 step: 1010, loss is 0.0032303903717547655\n",
      "epoch: 2 step: 1011, loss is 0.0025990011636167765\n",
      "epoch: 2 step: 1012, loss is 0.007279784884303808\n",
      "epoch: 2 step: 1013, loss is 0.008532584644854069\n",
      "epoch: 2 step: 1014, loss is 0.022962236776947975\n",
      "epoch: 2 step: 1015, loss is 0.002062370302155614\n",
      "epoch: 2 step: 1016, loss is 0.04508845880627632\n",
      "epoch: 2 step: 1017, loss is 0.04831743240356445\n",
      "epoch: 2 step: 1018, loss is 0.14449182152748108\n",
      "epoch: 2 step: 1019, loss is 0.014355561695992947\n",
      "epoch: 2 step: 1020, loss is 0.0036368174478411674\n",
      "epoch: 2 step: 1021, loss is 0.06207098439335823\n",
      "epoch: 2 step: 1022, loss is 0.0023741351906210184\n",
      "epoch: 2 step: 1023, loss is 0.10959061235189438\n",
      "epoch: 2 step: 1024, loss is 0.2139517068862915\n",
      "epoch: 2 step: 1025, loss is 0.0029427516274154186\n",
      "epoch: 2 step: 1026, loss is 0.0542927049100399\n",
      "epoch: 2 step: 1027, loss is 0.030087929219007492\n",
      "epoch: 2 step: 1028, loss is 0.28821951150894165\n",
      "epoch: 2 step: 1029, loss is 0.07158387452363968\n",
      "epoch: 2 step: 1030, loss is 0.09358757734298706\n",
      "epoch: 2 step: 1031, loss is 0.004103693179786205\n",
      "epoch: 2 step: 1032, loss is 0.02836725488305092\n",
      "epoch: 2 step: 1033, loss is 0.005307700484991074\n",
      "epoch: 2 step: 1034, loss is 0.02594953589141369\n",
      "epoch: 2 step: 1035, loss is 0.0014840620569884777\n",
      "epoch: 2 step: 1036, loss is 0.009808710776269436\n",
      "epoch: 2 step: 1037, loss is 0.07550010830163956\n",
      "epoch: 2 step: 1038, loss is 0.16217778623104095\n",
      "epoch: 2 step: 1039, loss is 0.0239664725959301\n",
      "epoch: 2 step: 1040, loss is 0.016550114378333092\n",
      "epoch: 2 step: 1041, loss is 0.012147016823291779\n",
      "epoch: 2 step: 1042, loss is 0.14856699109077454\n",
      "epoch: 2 step: 1043, loss is 0.004727813880890608\n",
      "epoch: 2 step: 1044, loss is 0.07201854139566422\n",
      "epoch: 2 step: 1045, loss is 0.16544902324676514\n",
      "epoch: 2 step: 1046, loss is 0.0038165519945323467\n",
      "epoch: 2 step: 1047, loss is 0.06559579819440842\n",
      "epoch: 2 step: 1048, loss is 0.00529575115069747\n",
      "epoch: 2 step: 1049, loss is 0.0027003877330571413\n",
      "epoch: 2 step: 1050, loss is 0.014034145511686802\n",
      "epoch: 2 step: 1051, loss is 0.05157994106411934\n",
      "epoch: 2 step: 1052, loss is 0.004893975332379341\n",
      "epoch: 2 step: 1053, loss is 0.027171438559889793\n",
      "epoch: 2 step: 1054, loss is 0.00358944502659142\n",
      "epoch: 2 step: 1055, loss is 0.5923476219177246\n",
      "epoch: 2 step: 1056, loss is 0.03830090910196304\n",
      "epoch: 2 step: 1057, loss is 0.019615422934293747\n",
      "epoch: 2 step: 1058, loss is 0.007694813888520002\n",
      "epoch: 2 step: 1059, loss is 0.09703418612480164\n",
      "epoch: 2 step: 1060, loss is 0.1024896502494812\n",
      "epoch: 2 step: 1061, loss is 0.24928683042526245\n",
      "epoch: 2 step: 1062, loss is 0.11757522821426392\n",
      "epoch: 2 step: 1063, loss is 0.026928821578621864\n",
      "epoch: 2 step: 1064, loss is 0.10753723978996277\n",
      "epoch: 2 step: 1065, loss is 0.1183934137225151\n",
      "epoch: 2 step: 1066, loss is 0.030735600739717484\n",
      "epoch: 2 step: 1067, loss is 0.13598504662513733\n",
      "epoch: 2 step: 1068, loss is 0.3630831837654114\n",
      "epoch: 2 step: 1069, loss is 0.0033195933792740107\n",
      "epoch: 2 step: 1070, loss is 0.0770995020866394\n",
      "epoch: 2 step: 1071, loss is 0.11765262484550476\n",
      "epoch: 2 step: 1072, loss is 0.026929182931780815\n",
      "epoch: 2 step: 1073, loss is 0.014944530092179775\n",
      "epoch: 2 step: 1074, loss is 0.014104742556810379\n",
      "epoch: 2 step: 1075, loss is 0.08873265981674194\n",
      "epoch: 2 step: 1076, loss is 0.046543143689632416\n",
      "epoch: 2 step: 1077, loss is 0.07128427177667618\n",
      "epoch: 2 step: 1078, loss is 0.007667836733162403\n",
      "epoch: 2 step: 1079, loss is 0.0635031908750534\n",
      "epoch: 2 step: 1080, loss is 0.01522984728217125\n",
      "epoch: 2 step: 1081, loss is 0.09778860956430435\n",
      "epoch: 2 step: 1082, loss is 0.058142971247434616\n",
      "epoch: 2 step: 1083, loss is 0.032930102199316025\n",
      "epoch: 2 step: 1084, loss is 0.14924679696559906\n",
      "epoch: 2 step: 1085, loss is 0.015524669550359249\n",
      "epoch: 2 step: 1086, loss is 0.05335273966193199\n",
      "epoch: 2 step: 1087, loss is 0.006588529795408249\n",
      "epoch: 2 step: 1088, loss is 0.14464804530143738\n",
      "epoch: 2 step: 1089, loss is 0.030539661645889282\n",
      "epoch: 2 step: 1090, loss is 0.047688547521829605\n",
      "epoch: 2 step: 1091, loss is 0.06976751983165741\n",
      "epoch: 2 step: 1092, loss is 0.050369810312986374\n",
      "epoch: 2 step: 1093, loss is 0.004814472980797291\n",
      "epoch: 2 step: 1094, loss is 0.06337529420852661\n",
      "epoch: 2 step: 1095, loss is 0.006471775937825441\n",
      "epoch: 2 step: 1096, loss is 0.2505715489387512\n",
      "epoch: 2 step: 1097, loss is 0.040401820093393326\n",
      "epoch: 2 step: 1098, loss is 0.21755433082580566\n",
      "epoch: 2 step: 1099, loss is 0.107017882168293\n",
      "epoch: 2 step: 1100, loss is 0.09121295064687729\n",
      "epoch: 2 step: 1101, loss is 0.006087678484618664\n",
      "epoch: 2 step: 1102, loss is 0.0200395118445158\n",
      "epoch: 2 step: 1103, loss is 0.05121823027729988\n",
      "epoch: 2 step: 1104, loss is 0.06744132190942764\n",
      "epoch: 2 step: 1105, loss is 0.010292676277458668\n",
      "epoch: 2 step: 1106, loss is 0.10397905111312866\n",
      "epoch: 2 step: 1107, loss is 0.019117990508675575\n",
      "epoch: 2 step: 1108, loss is 0.0034259858075529337\n",
      "epoch: 2 step: 1109, loss is 0.0016775410622358322\n",
      "epoch: 2 step: 1110, loss is 0.006906311493366957\n",
      "epoch: 2 step: 1111, loss is 0.03715905919671059\n",
      "epoch: 2 step: 1112, loss is 0.15725195407867432\n",
      "epoch: 2 step: 1113, loss is 0.30003592371940613\n",
      "epoch: 2 step: 1114, loss is 0.09444785863161087\n",
      "epoch: 2 step: 1115, loss is 0.1031154915690422\n",
      "epoch: 2 step: 1116, loss is 0.0977654680609703\n",
      "epoch: 2 step: 1117, loss is 0.2065422385931015\n",
      "epoch: 2 step: 1118, loss is 0.025610920041799545\n",
      "epoch: 2 step: 1119, loss is 0.028720302507281303\n",
      "epoch: 2 step: 1120, loss is 0.01144490484148264\n",
      "epoch: 2 step: 1121, loss is 0.0038142248522490263\n",
      "epoch: 2 step: 1122, loss is 0.1729504019021988\n",
      "epoch: 2 step: 1123, loss is 0.019638976082205772\n",
      "epoch: 2 step: 1124, loss is 0.10801491141319275\n",
      "epoch: 2 step: 1125, loss is 0.06906723976135254\n",
      "epoch: 2 step: 1126, loss is 0.028591256588697433\n",
      "epoch: 2 step: 1127, loss is 0.17881375551223755\n",
      "epoch: 2 step: 1128, loss is 0.162042498588562\n",
      "epoch: 2 step: 1129, loss is 0.007126772776246071\n",
      "epoch: 2 step: 1130, loss is 0.005350466817617416\n",
      "epoch: 2 step: 1131, loss is 0.049955930560827255\n",
      "epoch: 2 step: 1132, loss is 0.11661060899496078\n",
      "epoch: 2 step: 1133, loss is 0.02564944699406624\n",
      "epoch: 2 step: 1134, loss is 0.02035013772547245\n",
      "epoch: 2 step: 1135, loss is 0.03548537939786911\n",
      "epoch: 2 step: 1136, loss is 0.12796765565872192\n",
      "epoch: 2 step: 1137, loss is 0.020026270300149918\n",
      "epoch: 2 step: 1138, loss is 0.01874728687107563\n",
      "epoch: 2 step: 1139, loss is 0.02687946893274784\n",
      "epoch: 2 step: 1140, loss is 0.018203768879175186\n",
      "epoch: 2 step: 1141, loss is 0.012280896306037903\n",
      "epoch: 2 step: 1142, loss is 0.052901048213243484\n",
      "epoch: 2 step: 1143, loss is 0.02961336448788643\n",
      "epoch: 2 step: 1144, loss is 0.10452798753976822\n",
      "epoch: 2 step: 1145, loss is 0.031065920367836952\n",
      "epoch: 2 step: 1146, loss is 0.010591097176074982\n",
      "epoch: 2 step: 1147, loss is 0.19422799348831177\n",
      "epoch: 2 step: 1148, loss is 0.024340450763702393\n",
      "epoch: 2 step: 1149, loss is 0.09669654071331024\n",
      "epoch: 2 step: 1150, loss is 0.006845334079116583\n",
      "epoch: 2 step: 1151, loss is 0.11307290941476822\n",
      "epoch: 2 step: 1152, loss is 0.02083798311650753\n",
      "epoch: 2 step: 1153, loss is 0.06880608201026917\n",
      "epoch: 2 step: 1154, loss is 0.04013091325759888\n",
      "epoch: 2 step: 1155, loss is 0.04999259486794472\n",
      "epoch: 2 step: 1156, loss is 0.008251085877418518\n",
      "epoch: 2 step: 1157, loss is 0.022537734359502792\n",
      "epoch: 2 step: 1158, loss is 0.017729509621858597\n",
      "epoch: 2 step: 1159, loss is 0.03396548703312874\n",
      "epoch: 2 step: 1160, loss is 0.01164529100060463\n",
      "epoch: 2 step: 1161, loss is 0.03842600807547569\n",
      "epoch: 2 step: 1162, loss is 0.0009380939300172031\n",
      "epoch: 2 step: 1163, loss is 0.02851463481783867\n",
      "epoch: 2 step: 1164, loss is 0.00485193682834506\n",
      "epoch: 2 step: 1165, loss is 0.0937122255563736\n",
      "epoch: 2 step: 1166, loss is 0.15580351650714874\n",
      "epoch: 2 step: 1167, loss is 0.010325552895665169\n",
      "epoch: 2 step: 1168, loss is 0.01138108130544424\n",
      "epoch: 2 step: 1169, loss is 0.11049295216798782\n",
      "epoch: 2 step: 1170, loss is 0.05174187570810318\n",
      "epoch: 2 step: 1171, loss is 0.0037155032623559237\n",
      "epoch: 2 step: 1172, loss is 0.0367269441485405\n",
      "epoch: 2 step: 1173, loss is 0.009487123228609562\n",
      "epoch: 2 step: 1174, loss is 0.08824742585420609\n",
      "epoch: 2 step: 1175, loss is 0.02289745584130287\n",
      "epoch: 2 step: 1176, loss is 0.17016133666038513\n",
      "epoch: 2 step: 1177, loss is 0.037279970943927765\n",
      "epoch: 2 step: 1178, loss is 0.0038217282854020596\n",
      "epoch: 2 step: 1179, loss is 0.16836202144622803\n",
      "epoch: 2 step: 1180, loss is 0.057883378118276596\n",
      "epoch: 2 step: 1181, loss is 0.06225728988647461\n",
      "epoch: 2 step: 1182, loss is 0.045793693512678146\n",
      "epoch: 2 step: 1183, loss is 0.08337070047855377\n",
      "epoch: 2 step: 1184, loss is 0.04653250426054001\n",
      "epoch: 2 step: 1185, loss is 0.16980110108852386\n",
      "epoch: 2 step: 1186, loss is 0.016276463866233826\n",
      "epoch: 2 step: 1187, loss is 0.09293032437562943\n",
      "epoch: 2 step: 1188, loss is 0.003091081278398633\n",
      "epoch: 2 step: 1189, loss is 0.03510719910264015\n",
      "epoch: 2 step: 1190, loss is 0.037204042077064514\n",
      "epoch: 2 step: 1191, loss is 0.03423944488167763\n",
      "epoch: 2 step: 1192, loss is 0.020934725180268288\n",
      "epoch: 2 step: 1193, loss is 0.06655902415513992\n",
      "epoch: 2 step: 1194, loss is 0.004016869235783815\n",
      "epoch: 2 step: 1195, loss is 0.015281733125448227\n",
      "epoch: 2 step: 1196, loss is 0.008417627774178982\n",
      "epoch: 2 step: 1197, loss is 0.009533817879855633\n",
      "epoch: 2 step: 1198, loss is 0.08608024567365646\n",
      "epoch: 2 step: 1199, loss is 0.009269881062209606\n",
      "epoch: 2 step: 1200, loss is 0.1407189518213272\n",
      "epoch: 2 step: 1201, loss is 0.018344907090067863\n",
      "epoch: 2 step: 1202, loss is 0.007951611652970314\n",
      "epoch: 2 step: 1203, loss is 0.008297820575535297\n",
      "epoch: 2 step: 1204, loss is 0.007571720518171787\n",
      "epoch: 2 step: 1205, loss is 0.019096890464425087\n",
      "epoch: 2 step: 1206, loss is 0.03697961941361427\n",
      "epoch: 2 step: 1207, loss is 0.041826628148555756\n",
      "epoch: 2 step: 1208, loss is 0.020596588030457497\n",
      "epoch: 2 step: 1209, loss is 0.08346786350011826\n",
      "epoch: 2 step: 1210, loss is 0.16373571753501892\n",
      "epoch: 2 step: 1211, loss is 0.0039446656592190266\n",
      "epoch: 2 step: 1212, loss is 0.037699684500694275\n",
      "epoch: 2 step: 1213, loss is 0.015000992454588413\n",
      "epoch: 2 step: 1214, loss is 0.162919282913208\n",
      "epoch: 2 step: 1215, loss is 0.001400848850607872\n",
      "epoch: 2 step: 1216, loss is 0.00696289399638772\n",
      "epoch: 2 step: 1217, loss is 0.03110172413289547\n",
      "epoch: 2 step: 1218, loss is 0.0008041628170758486\n",
      "epoch: 2 step: 1219, loss is 0.031159086152911186\n",
      "epoch: 2 step: 1220, loss is 0.010209403932094574\n",
      "epoch: 2 step: 1221, loss is 0.001224203617312014\n",
      "epoch: 2 step: 1222, loss is 0.08995988219976425\n",
      "epoch: 2 step: 1223, loss is 0.13659678399562836\n",
      "epoch: 2 step: 1224, loss is 0.05689608305692673\n",
      "epoch: 2 step: 1225, loss is 0.011995803564786911\n",
      "epoch: 2 step: 1226, loss is 0.04715464264154434\n",
      "epoch: 2 step: 1227, loss is 0.20205135643482208\n",
      "epoch: 2 step: 1228, loss is 0.0251423679292202\n",
      "epoch: 2 step: 1229, loss is 0.0008690240210853517\n",
      "epoch: 2 step: 1230, loss is 0.04474956542253494\n",
      "epoch: 2 step: 1231, loss is 0.053651075810194016\n",
      "epoch: 2 step: 1232, loss is 0.050467267632484436\n",
      "epoch: 2 step: 1233, loss is 0.007357884664088488\n",
      "epoch: 2 step: 1234, loss is 0.015209769830107689\n",
      "epoch: 2 step: 1235, loss is 0.038320817053318024\n",
      "epoch: 2 step: 1236, loss is 0.07792983949184418\n",
      "epoch: 2 step: 1237, loss is 0.061374735087156296\n",
      "epoch: 2 step: 1238, loss is 0.007497341372072697\n",
      "epoch: 2 step: 1239, loss is 0.02371823415160179\n",
      "epoch: 2 step: 1240, loss is 0.021426398307085037\n",
      "epoch: 2 step: 1241, loss is 0.005759537685662508\n",
      "epoch: 2 step: 1242, loss is 0.036575376987457275\n",
      "epoch: 2 step: 1243, loss is 0.0028666763100773096\n",
      "epoch: 2 step: 1244, loss is 0.08887533843517303\n",
      "epoch: 2 step: 1245, loss is 0.02371644414961338\n",
      "epoch: 2 step: 1246, loss is 0.009278235025703907\n",
      "epoch: 2 step: 1247, loss is 0.018422165885567665\n",
      "epoch: 2 step: 1248, loss is 0.010862496681511402\n",
      "epoch: 2 step: 1249, loss is 0.12569190561771393\n",
      "epoch: 2 step: 1250, loss is 0.016118953004479408\n",
      "epoch: 2 step: 1251, loss is 0.0009255175245925784\n",
      "epoch: 2 step: 1252, loss is 0.0006213087472133338\n",
      "epoch: 2 step: 1253, loss is 0.0664527416229248\n",
      "epoch: 2 step: 1254, loss is 0.02626698650419712\n",
      "epoch: 2 step: 1255, loss is 0.008189168758690357\n",
      "epoch: 2 step: 1256, loss is 0.036864105612039566\n",
      "epoch: 2 step: 1257, loss is 0.2059626430273056\n",
      "epoch: 2 step: 1258, loss is 0.0858331024646759\n",
      "epoch: 2 step: 1259, loss is 0.021811868995428085\n",
      "epoch: 2 step: 1260, loss is 0.023668644949793816\n",
      "epoch: 2 step: 1261, loss is 0.0020094329956918955\n",
      "epoch: 2 step: 1262, loss is 0.014071713201701641\n",
      "epoch: 2 step: 1263, loss is 0.0011094401124864817\n",
      "epoch: 2 step: 1264, loss is 0.0005614575929939747\n",
      "epoch: 2 step: 1265, loss is 0.0264021884649992\n",
      "epoch: 2 step: 1266, loss is 0.012960270047187805\n",
      "epoch: 2 step: 1267, loss is 0.04351675137877464\n",
      "epoch: 2 step: 1268, loss is 0.018445352092385292\n",
      "epoch: 2 step: 1269, loss is 0.014906100928783417\n",
      "epoch: 2 step: 1270, loss is 0.007378475274890661\n",
      "epoch: 2 step: 1271, loss is 0.0006974994321353734\n",
      "epoch: 2 step: 1272, loss is 0.03974812477827072\n",
      "epoch: 2 step: 1273, loss is 0.04610263928771019\n",
      "epoch: 2 step: 1274, loss is 0.1266426295042038\n",
      "epoch: 2 step: 1275, loss is 0.010018661618232727\n",
      "epoch: 2 step: 1276, loss is 0.17177435755729675\n",
      "epoch: 2 step: 1277, loss is 0.06906434148550034\n",
      "epoch: 2 step: 1278, loss is 0.0040805949829518795\n",
      "epoch: 2 step: 1279, loss is 0.23131637275218964\n",
      "epoch: 2 step: 1280, loss is 0.04267732799053192\n",
      "epoch: 2 step: 1281, loss is 0.2801308035850525\n",
      "epoch: 2 step: 1282, loss is 0.23724304139614105\n",
      "epoch: 2 step: 1283, loss is 0.050996147096157074\n",
      "epoch: 2 step: 1284, loss is 0.0257578082382679\n",
      "epoch: 2 step: 1285, loss is 0.012847494333982468\n",
      "epoch: 2 step: 1286, loss is 0.4011721909046173\n",
      "epoch: 2 step: 1287, loss is 0.16459926962852478\n",
      "epoch: 2 step: 1288, loss is 0.02760595642030239\n",
      "epoch: 2 step: 1289, loss is 0.026437774300575256\n",
      "epoch: 2 step: 1290, loss is 0.0325775109231472\n",
      "epoch: 2 step: 1291, loss is 0.13932307064533234\n",
      "epoch: 2 step: 1292, loss is 0.07348467409610748\n",
      "epoch: 2 step: 1293, loss is 0.0069693285040557384\n",
      "epoch: 2 step: 1294, loss is 0.075749970972538\n",
      "epoch: 2 step: 1295, loss is 0.08641450107097626\n",
      "epoch: 2 step: 1296, loss is 0.0037317087408155203\n",
      "epoch: 2 step: 1297, loss is 0.018249940127134323\n",
      "epoch: 2 step: 1298, loss is 0.14504562318325043\n",
      "epoch: 2 step: 1299, loss is 0.011880919337272644\n",
      "epoch: 2 step: 1300, loss is 0.010777258314192295\n",
      "epoch: 2 step: 1301, loss is 0.0232057124376297\n",
      "epoch: 2 step: 1302, loss is 0.00852594431489706\n",
      "epoch: 2 step: 1303, loss is 0.0014843587996438146\n",
      "epoch: 2 step: 1304, loss is 0.010512996464967728\n",
      "epoch: 2 step: 1305, loss is 0.06150452420115471\n",
      "epoch: 2 step: 1306, loss is 0.15136219561100006\n",
      "epoch: 2 step: 1307, loss is 0.004104473628103733\n",
      "epoch: 2 step: 1308, loss is 0.010689168237149715\n",
      "epoch: 2 step: 1309, loss is 0.08141306787729263\n",
      "epoch: 2 step: 1310, loss is 0.12472210079431534\n",
      "epoch: 2 step: 1311, loss is 0.017328057438135147\n",
      "epoch: 2 step: 1312, loss is 0.015477410517632961\n",
      "epoch: 2 step: 1313, loss is 0.015603886917233467\n",
      "epoch: 2 step: 1314, loss is 0.04809515178203583\n",
      "epoch: 2 step: 1315, loss is 0.02348744310438633\n",
      "epoch: 2 step: 1316, loss is 0.27698856592178345\n",
      "epoch: 2 step: 1317, loss is 0.008187501691281796\n",
      "epoch: 2 step: 1318, loss is 0.27268823981285095\n",
      "epoch: 2 step: 1319, loss is 0.036971352994441986\n",
      "epoch: 2 step: 1320, loss is 0.016738055273890495\n",
      "epoch: 2 step: 1321, loss is 0.26374441385269165\n",
      "epoch: 2 step: 1322, loss is 0.011068832129240036\n",
      "epoch: 2 step: 1323, loss is 0.05545060336589813\n",
      "epoch: 2 step: 1324, loss is 0.00466818455606699\n",
      "epoch: 2 step: 1325, loss is 0.018600592389702797\n",
      "epoch: 2 step: 1326, loss is 0.12038554251194\n",
      "epoch: 2 step: 1327, loss is 0.1758405715227127\n",
      "epoch: 2 step: 1328, loss is 0.05900060385465622\n",
      "epoch: 2 step: 1329, loss is 0.013840814121067524\n",
      "epoch: 2 step: 1330, loss is 0.1461414098739624\n",
      "epoch: 2 step: 1331, loss is 0.013613900169730186\n",
      "epoch: 2 step: 1332, loss is 0.026660116389393806\n",
      "epoch: 2 step: 1333, loss is 0.0905456691980362\n",
      "epoch: 2 step: 1334, loss is 0.013520232401788235\n",
      "epoch: 2 step: 1335, loss is 0.2174498736858368\n",
      "epoch: 2 step: 1336, loss is 0.00151793600525707\n",
      "epoch: 2 step: 1337, loss is 0.1339479237794876\n",
      "epoch: 2 step: 1338, loss is 0.02542264200747013\n",
      "epoch: 2 step: 1339, loss is 0.0027333355974406004\n",
      "epoch: 2 step: 1340, loss is 0.01403759978711605\n",
      "epoch: 2 step: 1341, loss is 0.09589854627847672\n",
      "epoch: 2 step: 1342, loss is 0.21238435804843903\n",
      "epoch: 2 step: 1343, loss is 0.007255897857248783\n",
      "epoch: 2 step: 1344, loss is 0.20464248955249786\n",
      "epoch: 2 step: 1345, loss is 0.049724042415618896\n",
      "epoch: 2 step: 1346, loss is 0.008942845277488232\n",
      "epoch: 2 step: 1347, loss is 0.1325138509273529\n",
      "epoch: 2 step: 1348, loss is 0.04362852871417999\n",
      "epoch: 2 step: 1349, loss is 0.16727957129478455\n",
      "epoch: 2 step: 1350, loss is 0.005957812070846558\n",
      "epoch: 2 step: 1351, loss is 0.05606597661972046\n",
      "epoch: 2 step: 1352, loss is 0.025462377816438675\n",
      "epoch: 2 step: 1353, loss is 0.037444114685058594\n",
      "epoch: 2 step: 1354, loss is 0.1186470165848732\n",
      "epoch: 2 step: 1355, loss is 0.22329868376255035\n",
      "epoch: 2 step: 1356, loss is 0.14236915111541748\n",
      "epoch: 2 step: 1357, loss is 0.021900981664657593\n",
      "epoch: 2 step: 1358, loss is 0.012457025237381458\n",
      "epoch: 2 step: 1359, loss is 0.01694171503186226\n",
      "epoch: 2 step: 1360, loss is 0.005533433984965086\n",
      "epoch: 2 step: 1361, loss is 0.09632932394742966\n",
      "epoch: 2 step: 1362, loss is 0.033645741641521454\n",
      "epoch: 2 step: 1363, loss is 0.004731407854706049\n",
      "epoch: 2 step: 1364, loss is 0.007243696600198746\n",
      "epoch: 2 step: 1365, loss is 0.07437245547771454\n",
      "epoch: 2 step: 1366, loss is 0.005915510002523661\n",
      "epoch: 2 step: 1367, loss is 0.002249142387881875\n",
      "epoch: 2 step: 1368, loss is 0.12237410992383957\n",
      "epoch: 2 step: 1369, loss is 0.2655309736728668\n",
      "epoch: 2 step: 1370, loss is 0.01399297546595335\n",
      "epoch: 2 step: 1371, loss is 0.08325187861919403\n",
      "epoch: 2 step: 1372, loss is 0.23345422744750977\n",
      "epoch: 2 step: 1373, loss is 0.006691184360533953\n",
      "epoch: 2 step: 1374, loss is 0.008220598101615906\n",
      "epoch: 2 step: 1375, loss is 0.024154096841812134\n",
      "epoch: 2 step: 1376, loss is 0.080760158598423\n",
      "epoch: 2 step: 1377, loss is 0.045017484575510025\n",
      "epoch: 2 step: 1378, loss is 0.0135802598670125\n",
      "epoch: 2 step: 1379, loss is 0.04662514105439186\n",
      "epoch: 2 step: 1380, loss is 0.011904638260602951\n",
      "epoch: 2 step: 1381, loss is 0.004697355441749096\n",
      "epoch: 2 step: 1382, loss is 0.10067155957221985\n",
      "epoch: 2 step: 1383, loss is 0.06193525344133377\n",
      "epoch: 2 step: 1384, loss is 0.04120592772960663\n",
      "epoch: 2 step: 1385, loss is 0.01857188530266285\n",
      "epoch: 2 step: 1386, loss is 0.06597702950239182\n",
      "epoch: 2 step: 1387, loss is 0.009639080613851547\n",
      "epoch: 2 step: 1388, loss is 0.01860973984003067\n",
      "epoch: 2 step: 1389, loss is 0.004858695436269045\n",
      "epoch: 2 step: 1390, loss is 0.21022774279117584\n",
      "epoch: 2 step: 1391, loss is 0.07761064171791077\n",
      "epoch: 2 step: 1392, loss is 0.05501418560743332\n",
      "epoch: 2 step: 1393, loss is 0.09299466758966446\n",
      "epoch: 2 step: 1394, loss is 0.017271604388952255\n",
      "epoch: 2 step: 1395, loss is 0.1321101039648056\n",
      "epoch: 2 step: 1396, loss is 0.05829299986362457\n",
      "epoch: 2 step: 1397, loss is 0.09884586930274963\n",
      "epoch: 2 step: 1398, loss is 0.025316832587122917\n",
      "epoch: 2 step: 1399, loss is 0.031647760421037674\n",
      "epoch: 2 step: 1400, loss is 0.017426183447241783\n",
      "epoch: 2 step: 1401, loss is 0.01015939936041832\n",
      "epoch: 2 step: 1402, loss is 0.0010139356600120664\n",
      "epoch: 2 step: 1403, loss is 0.06694251298904419\n",
      "epoch: 2 step: 1404, loss is 0.0031769401393830776\n",
      "epoch: 2 step: 1405, loss is 0.0012298447545617819\n",
      "epoch: 2 step: 1406, loss is 0.0997452586889267\n",
      "epoch: 2 step: 1407, loss is 0.014317233115434647\n",
      "epoch: 2 step: 1408, loss is 0.006980746053159237\n",
      "epoch: 2 step: 1409, loss is 0.003975626081228256\n",
      "epoch: 2 step: 1410, loss is 0.05127868428826332\n",
      "epoch: 2 step: 1411, loss is 0.0026911450549960136\n",
      "epoch: 2 step: 1412, loss is 0.2442065328359604\n",
      "epoch: 2 step: 1413, loss is 0.022908980026841164\n",
      "epoch: 2 step: 1414, loss is 0.03174488991498947\n",
      "epoch: 2 step: 1415, loss is 0.03609089180827141\n",
      "epoch: 2 step: 1416, loss is 0.011392459273338318\n",
      "epoch: 2 step: 1417, loss is 0.1233600378036499\n",
      "epoch: 2 step: 1418, loss is 0.0005049475585110486\n",
      "epoch: 2 step: 1419, loss is 0.017935121431946754\n",
      "epoch: 2 step: 1420, loss is 0.03197255730628967\n",
      "epoch: 2 step: 1421, loss is 0.26523637771606445\n",
      "epoch: 2 step: 1422, loss is 0.008049068041145802\n",
      "epoch: 2 step: 1423, loss is 0.012622061185538769\n",
      "epoch: 2 step: 1424, loss is 0.03781377896666527\n",
      "epoch: 2 step: 1425, loss is 0.11425543576478958\n",
      "epoch: 2 step: 1426, loss is 0.004390059970319271\n",
      "epoch: 2 step: 1427, loss is 0.03627414628863335\n",
      "epoch: 2 step: 1428, loss is 0.011841410771012306\n",
      "epoch: 2 step: 1429, loss is 0.01502083521336317\n",
      "epoch: 2 step: 1430, loss is 0.16359765827655792\n",
      "epoch: 2 step: 1431, loss is 0.07312032580375671\n",
      "epoch: 2 step: 1432, loss is 0.22871044278144836\n",
      "epoch: 2 step: 1433, loss is 0.07940345257520676\n",
      "epoch: 2 step: 1434, loss is 0.019047100096940994\n",
      "epoch: 2 step: 1435, loss is 0.021650822833180428\n",
      "epoch: 2 step: 1436, loss is 0.1858273446559906\n",
      "epoch: 2 step: 1437, loss is 0.03053516522049904\n",
      "epoch: 2 step: 1438, loss is 0.008861145004630089\n",
      "epoch: 2 step: 1439, loss is 0.18411773443222046\n",
      "epoch: 2 step: 1440, loss is 0.013105092570185661\n",
      "epoch: 2 step: 1441, loss is 0.012626143172383308\n",
      "epoch: 2 step: 1442, loss is 0.005904126446694136\n",
      "epoch: 2 step: 1443, loss is 0.006229760125279427\n",
      "epoch: 2 step: 1444, loss is 0.10750386118888855\n",
      "epoch: 2 step: 1445, loss is 0.15210089087486267\n",
      "epoch: 2 step: 1446, loss is 0.03376749902963638\n",
      "epoch: 2 step: 1447, loss is 0.009448201395571232\n",
      "epoch: 2 step: 1448, loss is 0.07311869412660599\n",
      "epoch: 2 step: 1449, loss is 0.0024701361544430256\n",
      "epoch: 2 step: 1450, loss is 0.02665838971734047\n",
      "epoch: 2 step: 1451, loss is 0.010496926493942738\n",
      "epoch: 2 step: 1452, loss is 0.003407083684578538\n",
      "epoch: 2 step: 1453, loss is 0.07270331680774689\n",
      "epoch: 2 step: 1454, loss is 0.018930312246084213\n",
      "epoch: 2 step: 1455, loss is 0.0075437529012560844\n",
      "epoch: 2 step: 1456, loss is 0.0820833146572113\n",
      "epoch: 2 step: 1457, loss is 0.038876548409461975\n",
      "epoch: 2 step: 1458, loss is 0.09723647683858871\n",
      "epoch: 2 step: 1459, loss is 0.14248701930046082\n",
      "epoch: 2 step: 1460, loss is 0.04128294065594673\n",
      "epoch: 2 step: 1461, loss is 0.005635455250740051\n",
      "epoch: 2 step: 1462, loss is 0.05655413120985031\n",
      "epoch: 2 step: 1463, loss is 0.01623883657157421\n",
      "epoch: 2 step: 1464, loss is 0.033702146261930466\n",
      "epoch: 2 step: 1465, loss is 0.0023226654157042503\n",
      "epoch: 2 step: 1466, loss is 0.01531138550490141\n",
      "epoch: 2 step: 1467, loss is 0.014544880017638206\n",
      "epoch: 2 step: 1468, loss is 0.02312169410288334\n",
      "epoch: 2 step: 1469, loss is 0.029720228165388107\n",
      "epoch: 2 step: 1470, loss is 0.010401559062302113\n",
      "epoch: 2 step: 1471, loss is 0.10775808244943619\n",
      "epoch: 2 step: 1472, loss is 0.006480392534285784\n",
      "epoch: 2 step: 1473, loss is 0.0366949588060379\n",
      "epoch: 2 step: 1474, loss is 0.001422468456439674\n",
      "epoch: 2 step: 1475, loss is 0.003441542387008667\n",
      "epoch: 2 step: 1476, loss is 0.021649187430739403\n",
      "epoch: 2 step: 1477, loss is 0.004880881868302822\n",
      "epoch: 2 step: 1478, loss is 0.0014485784340649843\n",
      "epoch: 2 step: 1479, loss is 0.02979028783738613\n",
      "epoch: 2 step: 1480, loss is 0.24957244098186493\n",
      "epoch: 2 step: 1481, loss is 0.06324421614408493\n",
      "epoch: 2 step: 1482, loss is 0.002368304179981351\n",
      "epoch: 2 step: 1483, loss is 0.0016150635201483965\n",
      "epoch: 2 step: 1484, loss is 0.16142329573631287\n",
      "epoch: 2 step: 1485, loss is 0.09580405801534653\n",
      "epoch: 2 step: 1486, loss is 0.0008889717282727361\n",
      "epoch: 2 step: 1487, loss is 0.011521423235535622\n",
      "epoch: 2 step: 1488, loss is 0.015957960858941078\n",
      "epoch: 2 step: 1489, loss is 0.01219634898006916\n",
      "epoch: 2 step: 1490, loss is 0.13780289888381958\n",
      "epoch: 2 step: 1491, loss is 0.006285094190388918\n",
      "epoch: 2 step: 1492, loss is 0.04398457705974579\n",
      "epoch: 2 step: 1493, loss is 0.002831751015037298\n",
      "epoch: 2 step: 1494, loss is 0.011892535723745823\n",
      "epoch: 2 step: 1495, loss is 0.024025842547416687\n",
      "epoch: 2 step: 1496, loss is 0.06351325660943985\n",
      "epoch: 2 step: 1497, loss is 0.08685635775327682\n",
      "epoch: 2 step: 1498, loss is 0.00231427070684731\n",
      "epoch: 2 step: 1499, loss is 0.050064049661159515\n",
      "epoch: 2 step: 1500, loss is 0.026245875284075737\n",
      "epoch: 2 step: 1501, loss is 0.005593706853687763\n",
      "epoch: 2 step: 1502, loss is 0.0038115824572741985\n",
      "epoch: 2 step: 1503, loss is 0.056213218718767166\n",
      "epoch: 2 step: 1504, loss is 0.02516973949968815\n",
      "epoch: 2 step: 1505, loss is 0.07503853738307953\n",
      "epoch: 2 step: 1506, loss is 0.06511687487363815\n",
      "epoch: 2 step: 1507, loss is 0.06361478567123413\n",
      "epoch: 2 step: 1508, loss is 0.10644230246543884\n",
      "epoch: 2 step: 1509, loss is 0.08989596366882324\n",
      "epoch: 2 step: 1510, loss is 0.09336476027965546\n",
      "epoch: 2 step: 1511, loss is 0.11925455927848816\n",
      "epoch: 2 step: 1512, loss is 0.008127582259476185\n",
      "epoch: 2 step: 1513, loss is 0.04331580176949501\n",
      "epoch: 2 step: 1514, loss is 0.004119819961488247\n",
      "epoch: 2 step: 1515, loss is 0.013926579616963863\n",
      "epoch: 2 step: 1516, loss is 0.03338339552283287\n",
      "epoch: 2 step: 1517, loss is 0.07355231791734695\n",
      "epoch: 2 step: 1518, loss is 0.2792620360851288\n",
      "epoch: 2 step: 1519, loss is 0.010530856437981129\n",
      "epoch: 2 step: 1520, loss is 0.035180117934942245\n",
      "epoch: 2 step: 1521, loss is 0.0573461577296257\n",
      "epoch: 2 step: 1522, loss is 0.09888698905706406\n",
      "epoch: 2 step: 1523, loss is 0.005693238694220781\n",
      "epoch: 2 step: 1524, loss is 0.003282007994130254\n",
      "epoch: 2 step: 1525, loss is 0.016891665756702423\n",
      "epoch: 2 step: 1526, loss is 0.0026224064640700817\n",
      "epoch: 2 step: 1527, loss is 0.00041795679135248065\n",
      "epoch: 2 step: 1528, loss is 0.030197681859135628\n",
      "epoch: 2 step: 1529, loss is 0.002903181593865156\n",
      "epoch: 2 step: 1530, loss is 0.03088076412677765\n",
      "epoch: 2 step: 1531, loss is 0.25980502367019653\n",
      "epoch: 2 step: 1532, loss is 0.0014137011021375656\n",
      "epoch: 2 step: 1533, loss is 0.3529357314109802\n",
      "epoch: 2 step: 1534, loss is 0.0016594831831753254\n",
      "epoch: 2 step: 1535, loss is 0.03561427444219589\n",
      "epoch: 2 step: 1536, loss is 0.08910840004682541\n",
      "epoch: 2 step: 1537, loss is 0.45092928409576416\n",
      "epoch: 2 step: 1538, loss is 0.039923518896102905\n",
      "epoch: 2 step: 1539, loss is 0.02749565802514553\n",
      "epoch: 2 step: 1540, loss is 0.06501403450965881\n",
      "epoch: 2 step: 1541, loss is 0.022715257480740547\n",
      "epoch: 2 step: 1542, loss is 0.009586126543581486\n",
      "epoch: 2 step: 1543, loss is 0.059437915682792664\n",
      "epoch: 2 step: 1544, loss is 0.013801177963614464\n",
      "epoch: 2 step: 1545, loss is 0.017050210386514664\n",
      "epoch: 2 step: 1546, loss is 0.08696682751178741\n",
      "epoch: 2 step: 1547, loss is 0.0509597547352314\n",
      "epoch: 2 step: 1548, loss is 0.005726703442633152\n",
      "epoch: 2 step: 1549, loss is 0.007212752476334572\n",
      "epoch: 2 step: 1550, loss is 0.044716425240039825\n",
      "epoch: 2 step: 1551, loss is 0.0033254397567361593\n",
      "epoch: 2 step: 1552, loss is 0.016055282205343246\n",
      "epoch: 2 step: 1553, loss is 0.033686891198158264\n",
      "epoch: 2 step: 1554, loss is 0.2044665813446045\n",
      "epoch: 2 step: 1555, loss is 0.0009355486836284399\n",
      "epoch: 2 step: 1556, loss is 0.10429847985506058\n",
      "epoch: 2 step: 1557, loss is 0.01059789676219225\n",
      "epoch: 2 step: 1558, loss is 0.002188296988606453\n",
      "epoch: 2 step: 1559, loss is 0.1505407989025116\n",
      "epoch: 2 step: 1560, loss is 0.13707001507282257\n",
      "epoch: 2 step: 1561, loss is 0.025760386139154434\n",
      "epoch: 2 step: 1562, loss is 0.07853610068559647\n",
      "epoch: 2 step: 1563, loss is 0.0037878176663070917\n",
      "epoch: 2 step: 1564, loss is 0.0023538623936474323\n",
      "epoch: 2 step: 1565, loss is 0.1294197291135788\n",
      "epoch: 2 step: 1566, loss is 0.0009828307665884495\n",
      "epoch: 2 step: 1567, loss is 0.0037544085644185543\n",
      "epoch: 2 step: 1568, loss is 0.03651975467801094\n",
      "epoch: 2 step: 1569, loss is 0.0037520623300224543\n",
      "epoch: 2 step: 1570, loss is 0.013307715766131878\n",
      "epoch: 2 step: 1571, loss is 0.06760405749082565\n",
      "epoch: 2 step: 1572, loss is 0.1105704978108406\n",
      "epoch: 2 step: 1573, loss is 0.021748054772615433\n",
      "epoch: 2 step: 1574, loss is 0.02089688926935196\n",
      "epoch: 2 step: 1575, loss is 0.10522587597370148\n",
      "epoch: 2 step: 1576, loss is 0.16102072596549988\n",
      "epoch: 2 step: 1577, loss is 0.03292391821742058\n",
      "epoch: 2 step: 1578, loss is 0.09334252029657364\n",
      "epoch: 2 step: 1579, loss is 0.014448246918618679\n",
      "epoch: 2 step: 1580, loss is 0.052441082894802094\n",
      "epoch: 2 step: 1581, loss is 0.007268951740115881\n",
      "epoch: 2 step: 1582, loss is 0.004331900738179684\n",
      "epoch: 2 step: 1583, loss is 0.11373960971832275\n",
      "epoch: 2 step: 1584, loss is 0.0426517128944397\n",
      "epoch: 2 step: 1585, loss is 0.026714282110333443\n",
      "epoch: 2 step: 1586, loss is 0.01622711308300495\n",
      "epoch: 2 step: 1587, loss is 0.26975029706954956\n",
      "epoch: 2 step: 1588, loss is 0.0545024648308754\n",
      "epoch: 2 step: 1589, loss is 0.11193141341209412\n",
      "epoch: 2 step: 1590, loss is 0.017806842923164368\n",
      "epoch: 2 step: 1591, loss is 0.005893141031265259\n",
      "epoch: 2 step: 1592, loss is 0.0936725065112114\n",
      "epoch: 2 step: 1593, loss is 0.1977587640285492\n",
      "epoch: 2 step: 1594, loss is 0.09859304130077362\n",
      "epoch: 2 step: 1595, loss is 0.0489099882543087\n",
      "epoch: 2 step: 1596, loss is 0.20322057604789734\n",
      "epoch: 2 step: 1597, loss is 0.0973394438624382\n",
      "epoch: 2 step: 1598, loss is 0.2598423957824707\n",
      "epoch: 2 step: 1599, loss is 0.22238826751708984\n",
      "epoch: 2 step: 1600, loss is 0.14611241221427917\n",
      "epoch: 2 step: 1601, loss is 0.050867944955825806\n",
      "epoch: 2 step: 1602, loss is 0.027486523613333702\n",
      "epoch: 2 step: 1603, loss is 0.04331810399889946\n",
      "epoch: 2 step: 1604, loss is 0.13085882365703583\n",
      "epoch: 2 step: 1605, loss is 0.4189106225967407\n",
      "epoch: 2 step: 1606, loss is 0.007570553105324507\n",
      "epoch: 2 step: 1607, loss is 0.026801245287060738\n",
      "epoch: 2 step: 1608, loss is 0.13757945597171783\n",
      "epoch: 2 step: 1609, loss is 0.09492862969636917\n",
      "epoch: 2 step: 1610, loss is 0.1397135853767395\n",
      "epoch: 2 step: 1611, loss is 0.17076565325260162\n",
      "epoch: 2 step: 1612, loss is 0.04037005081772804\n",
      "epoch: 2 step: 1613, loss is 0.0537247359752655\n",
      "epoch: 2 step: 1614, loss is 0.023588184267282486\n",
      "epoch: 2 step: 1615, loss is 0.017360689118504524\n",
      "epoch: 2 step: 1616, loss is 0.053570084273815155\n",
      "epoch: 2 step: 1617, loss is 0.06559735536575317\n",
      "epoch: 2 step: 1618, loss is 0.044017497450113297\n",
      "epoch: 2 step: 1619, loss is 0.024146834388375282\n",
      "epoch: 2 step: 1620, loss is 0.047595761716365814\n",
      "epoch: 2 step: 1621, loss is 0.030070360749959946\n",
      "epoch: 2 step: 1622, loss is 0.05922805145382881\n",
      "epoch: 2 step: 1623, loss is 0.05548153072595596\n",
      "epoch: 2 step: 1624, loss is 0.03446613624691963\n",
      "epoch: 2 step: 1625, loss is 0.045504115521907806\n",
      "epoch: 2 step: 1626, loss is 0.018820779398083687\n",
      "epoch: 2 step: 1627, loss is 0.06528797000646591\n",
      "epoch: 2 step: 1628, loss is 0.022028520703315735\n",
      "epoch: 2 step: 1629, loss is 0.004395714495331049\n",
      "epoch: 2 step: 1630, loss is 0.013592395931482315\n",
      "epoch: 2 step: 1631, loss is 0.012898659333586693\n",
      "epoch: 2 step: 1632, loss is 0.0029065674170851707\n",
      "epoch: 2 step: 1633, loss is 0.03209495171904564\n",
      "epoch: 2 step: 1634, loss is 0.004752410110086203\n",
      "epoch: 2 step: 1635, loss is 0.007488124538213015\n",
      "epoch: 2 step: 1636, loss is 0.04751807078719139\n",
      "epoch: 2 step: 1637, loss is 0.01904737763106823\n",
      "epoch: 2 step: 1638, loss is 0.0016350741498172283\n",
      "epoch: 2 step: 1639, loss is 0.046573083847761154\n",
      "epoch: 2 step: 1640, loss is 0.0016326886834576726\n",
      "epoch: 2 step: 1641, loss is 0.00616775918751955\n",
      "epoch: 2 step: 1642, loss is 0.22906744480133057\n",
      "epoch: 2 step: 1643, loss is 0.042129166424274445\n",
      "epoch: 2 step: 1644, loss is 0.06572756916284561\n",
      "epoch: 2 step: 1645, loss is 0.005366894416511059\n",
      "epoch: 2 step: 1646, loss is 0.006852820049971342\n",
      "epoch: 2 step: 1647, loss is 0.18524150550365448\n",
      "epoch: 2 step: 1648, loss is 0.23860380053520203\n",
      "epoch: 2 step: 1649, loss is 0.006301061250269413\n",
      "epoch: 2 step: 1650, loss is 0.02268395945429802\n",
      "epoch: 2 step: 1651, loss is 0.01879923790693283\n",
      "epoch: 2 step: 1652, loss is 0.049348440021276474\n",
      "epoch: 2 step: 1653, loss is 0.14645186066627502\n",
      "epoch: 2 step: 1654, loss is 0.03115703910589218\n",
      "epoch: 2 step: 1655, loss is 0.003161781234666705\n",
      "epoch: 2 step: 1656, loss is 0.06127341464161873\n",
      "epoch: 2 step: 1657, loss is 0.2526750862598419\n",
      "epoch: 2 step: 1658, loss is 0.0173491258174181\n",
      "epoch: 2 step: 1659, loss is 0.004342853091657162\n",
      "epoch: 2 step: 1660, loss is 0.005313347093760967\n",
      "epoch: 2 step: 1661, loss is 0.09888005256652832\n",
      "epoch: 2 step: 1662, loss is 0.009656691923737526\n",
      "epoch: 2 step: 1663, loss is 0.07392517477273941\n",
      "epoch: 2 step: 1664, loss is 0.0023026675917208195\n",
      "epoch: 2 step: 1665, loss is 0.021808253601193428\n",
      "epoch: 2 step: 1666, loss is 0.0409729890525341\n",
      "epoch: 2 step: 1667, loss is 0.006262640003114939\n",
      "epoch: 2 step: 1668, loss is 0.08541856706142426\n",
      "epoch: 2 step: 1669, loss is 0.22683562338352203\n",
      "epoch: 2 step: 1670, loss is 0.01708921790122986\n",
      "epoch: 2 step: 1671, loss is 0.0034736646339297295\n",
      "epoch: 2 step: 1672, loss is 0.28196120262145996\n",
      "epoch: 2 step: 1673, loss is 0.005882532335817814\n",
      "epoch: 2 step: 1674, loss is 0.006895152386277914\n",
      "epoch: 2 step: 1675, loss is 0.007103350013494492\n",
      "epoch: 2 step: 1676, loss is 0.06332947313785553\n",
      "epoch: 2 step: 1677, loss is 0.022972233593463898\n",
      "epoch: 2 step: 1678, loss is 0.12283787876367569\n",
      "epoch: 2 step: 1679, loss is 0.0073374235071241856\n",
      "epoch: 2 step: 1680, loss is 0.05238884687423706\n",
      "epoch: 2 step: 1681, loss is 0.010296513326466084\n",
      "epoch: 2 step: 1682, loss is 0.03709503263235092\n",
      "epoch: 2 step: 1683, loss is 0.025474142283201218\n",
      "epoch: 2 step: 1684, loss is 0.016242753714323044\n",
      "epoch: 2 step: 1685, loss is 0.013079999014735222\n",
      "epoch: 2 step: 1686, loss is 0.032606761902570724\n",
      "epoch: 2 step: 1687, loss is 0.13996270298957825\n",
      "epoch: 2 step: 1688, loss is 0.04947001859545708\n",
      "epoch: 2 step: 1689, loss is 0.011805240996181965\n",
      "epoch: 2 step: 1690, loss is 0.13627947866916656\n",
      "epoch: 2 step: 1691, loss is 0.0014947113813832402\n",
      "epoch: 2 step: 1692, loss is 0.006155487149953842\n",
      "epoch: 2 step: 1693, loss is 0.004197593312710524\n",
      "epoch: 2 step: 1694, loss is 0.11168055981397629\n",
      "epoch: 2 step: 1695, loss is 0.006992702838033438\n",
      "epoch: 2 step: 1696, loss is 0.06490849703550339\n",
      "epoch: 2 step: 1697, loss is 0.020493779331445694\n",
      "epoch: 2 step: 1698, loss is 0.059504248201847076\n",
      "epoch: 2 step: 1699, loss is 0.003956029191613197\n",
      "epoch: 2 step: 1700, loss is 0.002317015780135989\n",
      "epoch: 2 step: 1701, loss is 0.002883264562115073\n",
      "epoch: 2 step: 1702, loss is 0.07873949408531189\n",
      "epoch: 2 step: 1703, loss is 0.007172105833888054\n",
      "epoch: 2 step: 1704, loss is 0.11267592757940292\n",
      "epoch: 2 step: 1705, loss is 0.0700346976518631\n",
      "epoch: 2 step: 1706, loss is 0.011959467083215714\n",
      "epoch: 2 step: 1707, loss is 0.006123635917901993\n",
      "epoch: 2 step: 1708, loss is 0.19894826412200928\n",
      "epoch: 2 step: 1709, loss is 0.06025942414999008\n",
      "epoch: 2 step: 1710, loss is 0.13379642367362976\n",
      "epoch: 2 step: 1711, loss is 0.23215089738368988\n",
      "epoch: 2 step: 1712, loss is 0.18306154012680054\n",
      "epoch: 2 step: 1713, loss is 0.13559135794639587\n",
      "epoch: 2 step: 1714, loss is 0.22887910902500153\n",
      "epoch: 2 step: 1715, loss is 0.0045407610014081\n",
      "epoch: 2 step: 1716, loss is 0.10298878699541092\n",
      "epoch: 2 step: 1717, loss is 0.23012425005435944\n",
      "epoch: 2 step: 1718, loss is 0.05480664595961571\n",
      "epoch: 2 step: 1719, loss is 0.004640885628759861\n",
      "epoch: 2 step: 1720, loss is 0.004214346408843994\n",
      "epoch: 2 step: 1721, loss is 0.042127132415771484\n",
      "epoch: 2 step: 1722, loss is 0.007298259064555168\n",
      "epoch: 2 step: 1723, loss is 0.2031729370355606\n",
      "epoch: 2 step: 1724, loss is 0.19175985455513\n",
      "epoch: 2 step: 1725, loss is 0.022585134953260422\n",
      "epoch: 2 step: 1726, loss is 0.03977614641189575\n",
      "epoch: 2 step: 1727, loss is 0.03737140819430351\n",
      "epoch: 2 step: 1728, loss is 0.011740642599761486\n",
      "epoch: 2 step: 1729, loss is 0.08579828590154648\n",
      "epoch: 2 step: 1730, loss is 0.04243652522563934\n",
      "epoch: 2 step: 1731, loss is 0.013019267469644547\n",
      "epoch: 2 step: 1732, loss is 0.056829676032066345\n",
      "epoch: 2 step: 1733, loss is 0.002227829536423087\n",
      "epoch: 2 step: 1734, loss is 0.010328351520001888\n",
      "epoch: 2 step: 1735, loss is 0.005783103406429291\n",
      "epoch: 2 step: 1736, loss is 0.08798437565565109\n",
      "epoch: 2 step: 1737, loss is 0.024439236149191856\n",
      "epoch: 2 step: 1738, loss is 0.12609544396400452\n",
      "epoch: 2 step: 1739, loss is 0.11395490914583206\n",
      "epoch: 2 step: 1740, loss is 0.021996479481458664\n",
      "epoch: 2 step: 1741, loss is 0.010647608898580074\n",
      "epoch: 2 step: 1742, loss is 0.011700390838086605\n",
      "epoch: 2 step: 1743, loss is 0.07360237836837769\n",
      "epoch: 2 step: 1744, loss is 0.02920420840382576\n",
      "epoch: 2 step: 1745, loss is 0.01589789241552353\n",
      "epoch: 2 step: 1746, loss is 0.115877665579319\n",
      "epoch: 2 step: 1747, loss is 0.12013566493988037\n",
      "epoch: 2 step: 1748, loss is 0.005341526120901108\n",
      "epoch: 2 step: 1749, loss is 0.005503689870238304\n",
      "epoch: 2 step: 1750, loss is 0.1158827617764473\n",
      "epoch: 2 step: 1751, loss is 0.003679950488731265\n",
      "epoch: 2 step: 1752, loss is 0.1386355608701706\n",
      "epoch: 2 step: 1753, loss is 0.054885681718587875\n",
      "epoch: 2 step: 1754, loss is 0.0826345682144165\n",
      "epoch: 2 step: 1755, loss is 0.0526936799287796\n",
      "epoch: 2 step: 1756, loss is 0.020284797996282578\n",
      "epoch: 2 step: 1757, loss is 0.05290458723902702\n",
      "epoch: 2 step: 1758, loss is 0.024307087063789368\n",
      "epoch: 2 step: 1759, loss is 0.17236138880252838\n",
      "epoch: 2 step: 1760, loss is 0.004034028388559818\n",
      "epoch: 2 step: 1761, loss is 0.10697820782661438\n",
      "epoch: 2 step: 1762, loss is 0.17418821156024933\n",
      "epoch: 2 step: 1763, loss is 0.03303230181336403\n",
      "epoch: 2 step: 1764, loss is 0.01718181185424328\n",
      "epoch: 2 step: 1765, loss is 0.19892311096191406\n",
      "epoch: 2 step: 1766, loss is 0.0985974371433258\n",
      "epoch: 2 step: 1767, loss is 0.001672934158705175\n",
      "epoch: 2 step: 1768, loss is 0.03842835873365402\n",
      "epoch: 2 step: 1769, loss is 0.009879669174551964\n",
      "epoch: 2 step: 1770, loss is 0.028985781595110893\n",
      "epoch: 2 step: 1771, loss is 0.0038526314310729504\n",
      "epoch: 2 step: 1772, loss is 0.048470087349414825\n",
      "epoch: 2 step: 1773, loss is 0.03806941956281662\n",
      "epoch: 2 step: 1774, loss is 0.1445564180612564\n",
      "epoch: 2 step: 1775, loss is 0.04336721450090408\n",
      "epoch: 2 step: 1776, loss is 0.004276354797184467\n",
      "epoch: 2 step: 1777, loss is 0.011585473082959652\n",
      "epoch: 2 step: 1778, loss is 0.0018235635943710804\n",
      "epoch: 2 step: 1779, loss is 0.04484381899237633\n",
      "epoch: 2 step: 1780, loss is 0.032904498279094696\n",
      "epoch: 2 step: 1781, loss is 0.06420187652111053\n",
      "epoch: 2 step: 1782, loss is 0.25010281801223755\n",
      "epoch: 2 step: 1783, loss is 0.02558066137135029\n",
      "epoch: 2 step: 1784, loss is 0.05324338749051094\n",
      "epoch: 2 step: 1785, loss is 0.006714902818202972\n",
      "epoch: 2 step: 1786, loss is 0.0444684736430645\n",
      "epoch: 2 step: 1787, loss is 0.011132817715406418\n",
      "epoch: 2 step: 1788, loss is 0.05306658521294594\n",
      "epoch: 2 step: 1789, loss is 0.020051393657922745\n",
      "epoch: 2 step: 1790, loss is 0.008649557828903198\n",
      "epoch: 2 step: 1791, loss is 0.0028317205142229795\n",
      "epoch: 2 step: 1792, loss is 0.0030067223124206066\n",
      "epoch: 2 step: 1793, loss is 0.007511477917432785\n",
      "epoch: 2 step: 1794, loss is 0.24834495782852173\n",
      "epoch: 2 step: 1795, loss is 0.0382971353828907\n",
      "epoch: 2 step: 1796, loss is 0.01165976282209158\n",
      "epoch: 2 step: 1797, loss is 0.001130196382291615\n",
      "epoch: 2 step: 1798, loss is 0.015828603878617287\n",
      "epoch: 2 step: 1799, loss is 0.004839309491217136\n",
      "epoch: 2 step: 1800, loss is 0.03712867572903633\n",
      "epoch: 2 step: 1801, loss is 0.06256572157144547\n",
      "epoch: 2 step: 1802, loss is 0.007184925023466349\n",
      "epoch: 2 step: 1803, loss is 0.019694548100233078\n",
      "epoch: 2 step: 1804, loss is 0.020695429295301437\n",
      "epoch: 2 step: 1805, loss is 0.001021877396851778\n",
      "epoch: 2 step: 1806, loss is 0.24084287881851196\n",
      "epoch: 2 step: 1807, loss is 0.040118977427482605\n",
      "epoch: 2 step: 1808, loss is 0.08096626400947571\n",
      "epoch: 2 step: 1809, loss is 0.05385410785675049\n",
      "epoch: 2 step: 1810, loss is 0.009199162013828754\n",
      "epoch: 2 step: 1811, loss is 0.1344907134771347\n",
      "epoch: 2 step: 1812, loss is 0.0032065659761428833\n",
      "epoch: 2 step: 1813, loss is 0.0020674739498645067\n",
      "epoch: 2 step: 1814, loss is 0.06945763528347015\n",
      "epoch: 2 step: 1815, loss is 0.08178844302892685\n",
      "epoch: 2 step: 1816, loss is 0.09626701474189758\n",
      "epoch: 2 step: 1817, loss is 0.010180022567510605\n",
      "epoch: 2 step: 1818, loss is 0.017922500148415565\n",
      "epoch: 2 step: 1819, loss is 0.009237755089998245\n",
      "epoch: 2 step: 1820, loss is 0.040422696620225906\n",
      "epoch: 2 step: 1821, loss is 0.11197633296251297\n",
      "epoch: 2 step: 1822, loss is 0.014067430049180984\n",
      "epoch: 2 step: 1823, loss is 0.013736880384385586\n",
      "epoch: 2 step: 1824, loss is 0.0011066674487665296\n",
      "epoch: 2 step: 1825, loss is 0.00831490010023117\n",
      "epoch: 2 step: 1826, loss is 0.043150413781404495\n",
      "epoch: 2 step: 1827, loss is 0.07961130142211914\n",
      "epoch: 2 step: 1828, loss is 0.0017295845318585634\n",
      "epoch: 2 step: 1829, loss is 0.24468210339546204\n",
      "epoch: 2 step: 1830, loss is 0.021398626267910004\n",
      "epoch: 2 step: 1831, loss is 0.15366016328334808\n",
      "epoch: 2 step: 1832, loss is 0.20208492875099182\n",
      "epoch: 2 step: 1833, loss is 0.0179931428283453\n",
      "epoch: 2 step: 1834, loss is 0.066134512424469\n",
      "epoch: 2 step: 1835, loss is 0.22225964069366455\n",
      "epoch: 2 step: 1836, loss is 0.040864694863557816\n",
      "epoch: 2 step: 1837, loss is 0.034258365631103516\n",
      "epoch: 2 step: 1838, loss is 0.080128975212574\n",
      "epoch: 2 step: 1839, loss is 0.01715932786464691\n",
      "epoch: 2 step: 1840, loss is 0.021460028365254402\n",
      "epoch: 2 step: 1841, loss is 0.015825053676962852\n",
      "epoch: 2 step: 1842, loss is 0.002651590621098876\n",
      "epoch: 2 step: 1843, loss is 0.18814368546009064\n",
      "epoch: 2 step: 1844, loss is 0.06826359033584595\n",
      "epoch: 2 step: 1845, loss is 0.0036669063847512007\n",
      "epoch: 2 step: 1846, loss is 0.0020633821841329336\n",
      "epoch: 2 step: 1847, loss is 0.004319950006902218\n",
      "epoch: 2 step: 1848, loss is 0.00298876385204494\n",
      "epoch: 2 step: 1849, loss is 0.13984835147857666\n",
      "epoch: 2 step: 1850, loss is 0.007090789265930653\n",
      "epoch: 2 step: 1851, loss is 0.009358375333249569\n",
      "epoch: 2 step: 1852, loss is 0.0015659971395507455\n",
      "epoch: 2 step: 1853, loss is 0.014485879801213741\n",
      "epoch: 2 step: 1854, loss is 0.027620304375886917\n",
      "epoch: 2 step: 1855, loss is 0.0024646709207445383\n",
      "epoch: 2 step: 1856, loss is 0.03137550503015518\n",
      "epoch: 2 step: 1857, loss is 0.027368485927581787\n",
      "epoch: 2 step: 1858, loss is 0.011077306233346462\n",
      "epoch: 2 step: 1859, loss is 0.09155025333166122\n",
      "epoch: 2 step: 1860, loss is 0.015646491199731827\n",
      "epoch: 2 step: 1861, loss is 0.04278721287846565\n",
      "epoch: 2 step: 1862, loss is 0.24864453077316284\n",
      "epoch: 2 step: 1863, loss is 0.019231518730521202\n",
      "epoch: 2 step: 1864, loss is 0.05379297584295273\n",
      "epoch: 2 step: 1865, loss is 0.01722380891442299\n",
      "epoch: 2 step: 1866, loss is 0.002157398732379079\n",
      "epoch: 2 step: 1867, loss is 0.013394728302955627\n",
      "epoch: 2 step: 1868, loss is 0.01557634025812149\n",
      "epoch: 2 step: 1869, loss is 0.16026194393634796\n",
      "epoch: 2 step: 1870, loss is 0.2676069438457489\n",
      "epoch: 2 step: 1871, loss is 0.013434539549052715\n",
      "epoch: 2 step: 1872, loss is 0.05946963652968407\n",
      "epoch: 2 step: 1873, loss is 0.2135196328163147\n",
      "epoch: 2 step: 1874, loss is 0.015395046211779118\n",
      "epoch: 2 step: 1875, loss is 0.10952591150999069\n",
      "Train epoch time: 13363.518 ms, per step time: 7.127 ms\n",
      "epoch: 3 step: 1, loss is 0.001920043840073049\n",
      "epoch: 3 step: 2, loss is 0.04878103360533714\n",
      "epoch: 3 step: 3, loss is 0.004252263344824314\n",
      "epoch: 3 step: 4, loss is 0.05125291645526886\n",
      "epoch: 3 step: 5, loss is 0.03452758491039276\n",
      "epoch: 3 step: 6, loss is 0.0005027884035371244\n",
      "epoch: 3 step: 7, loss is 0.04380778968334198\n",
      "epoch: 3 step: 8, loss is 0.11887002736330032\n",
      "epoch: 3 step: 9, loss is 0.005282753612846136\n",
      "epoch: 3 step: 10, loss is 0.008742913603782654\n",
      "epoch: 3 step: 11, loss is 0.0031916804146021605\n",
      "epoch: 3 step: 12, loss is 0.03796663135290146\n",
      "epoch: 3 step: 13, loss is 0.3382430672645569\n",
      "epoch: 3 step: 14, loss is 0.014028851874172688\n",
      "epoch: 3 step: 15, loss is 0.005235891789197922\n",
      "epoch: 3 step: 16, loss is 0.02700434997677803\n",
      "epoch: 3 step: 17, loss is 0.012674190104007721\n",
      "epoch: 3 step: 18, loss is 0.13834169507026672\n",
      "epoch: 3 step: 19, loss is 0.08498001843690872\n",
      "epoch: 3 step: 20, loss is 0.033673595637083054\n",
      "epoch: 3 step: 21, loss is 0.006593773141503334\n",
      "epoch: 3 step: 22, loss is 0.00713286641985178\n",
      "epoch: 3 step: 23, loss is 0.0271235890686512\n",
      "epoch: 3 step: 24, loss is 0.010573826730251312\n",
      "epoch: 3 step: 25, loss is 0.09304291754961014\n",
      "epoch: 3 step: 26, loss is 0.010312427766621113\n",
      "epoch: 3 step: 27, loss is 0.02045338787138462\n",
      "epoch: 3 step: 28, loss is 0.027701735496520996\n",
      "epoch: 3 step: 29, loss is 0.00828799232840538\n",
      "epoch: 3 step: 30, loss is 0.07267157733440399\n",
      "epoch: 3 step: 31, loss is 0.028672505170106888\n",
      "epoch: 3 step: 32, loss is 0.3060651421546936\n",
      "epoch: 3 step: 33, loss is 0.018141135573387146\n",
      "epoch: 3 step: 34, loss is 0.05259927734732628\n",
      "epoch: 3 step: 35, loss is 0.11719080060720444\n",
      "epoch: 3 step: 36, loss is 0.019222376868128777\n",
      "epoch: 3 step: 37, loss is 0.023906728252768517\n",
      "epoch: 3 step: 38, loss is 0.003251240821555257\n",
      "epoch: 3 step: 39, loss is 0.002444059355184436\n",
      "epoch: 3 step: 40, loss is 0.06361343711614609\n",
      "epoch: 3 step: 41, loss is 0.040833570063114166\n",
      "epoch: 3 step: 42, loss is 0.01388572808355093\n",
      "epoch: 3 step: 43, loss is 0.0994686484336853\n",
      "epoch: 3 step: 44, loss is 0.005685589741915464\n",
      "epoch: 3 step: 45, loss is 0.06829603016376495\n",
      "epoch: 3 step: 46, loss is 0.02915312722325325\n",
      "epoch: 3 step: 47, loss is 0.020543720573186874\n",
      "epoch: 3 step: 48, loss is 0.23258060216903687\n",
      "epoch: 3 step: 49, loss is 0.022570692002773285\n",
      "epoch: 3 step: 50, loss is 0.012218834832310677\n",
      "epoch: 3 step: 51, loss is 0.046210430562496185\n",
      "epoch: 3 step: 52, loss is 0.018566666170954704\n",
      "epoch: 3 step: 53, loss is 0.014008856378495693\n",
      "epoch: 3 step: 54, loss is 0.005919861141592264\n",
      "epoch: 3 step: 55, loss is 0.03262937813997269\n",
      "epoch: 3 step: 56, loss is 0.002714405534788966\n",
      "epoch: 3 step: 57, loss is 0.005760668311268091\n",
      "epoch: 3 step: 58, loss is 0.0009915889240801334\n",
      "epoch: 3 step: 59, loss is 0.014582870528101921\n",
      "epoch: 3 step: 60, loss is 0.020398708060383797\n",
      "epoch: 3 step: 61, loss is 0.08588618040084839\n",
      "epoch: 3 step: 62, loss is 0.06638415902853012\n",
      "epoch: 3 step: 63, loss is 0.0007880793418735266\n",
      "epoch: 3 step: 64, loss is 0.005236334633082151\n",
      "epoch: 3 step: 65, loss is 0.0022028093226253986\n",
      "epoch: 3 step: 66, loss is 0.08142335712909698\n",
      "epoch: 3 step: 67, loss is 0.006100479979068041\n",
      "epoch: 3 step: 68, loss is 0.010569173842668533\n",
      "epoch: 3 step: 69, loss is 0.00018819938122760504\n",
      "epoch: 3 step: 70, loss is 0.008225918747484684\n",
      "epoch: 3 step: 71, loss is 0.09005477279424667\n",
      "epoch: 3 step: 72, loss is 0.03225560858845711\n",
      "epoch: 3 step: 73, loss is 0.19975265860557556\n",
      "epoch: 3 step: 74, loss is 0.0423450730741024\n",
      "epoch: 3 step: 75, loss is 0.013340415433049202\n",
      "epoch: 3 step: 76, loss is 0.008959738537669182\n",
      "epoch: 3 step: 77, loss is 0.10281062871217728\n",
      "epoch: 3 step: 78, loss is 0.018477439880371094\n",
      "epoch: 3 step: 79, loss is 0.04622573405504227\n",
      "epoch: 3 step: 80, loss is 0.20468193292617798\n",
      "epoch: 3 step: 81, loss is 0.01016052532941103\n",
      "epoch: 3 step: 82, loss is 0.057359207421541214\n",
      "epoch: 3 step: 83, loss is 0.08672046661376953\n",
      "epoch: 3 step: 84, loss is 0.0013212028425186872\n",
      "epoch: 3 step: 85, loss is 0.0174211747944355\n",
      "epoch: 3 step: 86, loss is 0.029302679002285004\n",
      "epoch: 3 step: 87, loss is 0.1345970332622528\n",
      "epoch: 3 step: 88, loss is 0.01588796079158783\n",
      "epoch: 3 step: 89, loss is 0.014750604517757893\n",
      "epoch: 3 step: 90, loss is 0.06444160640239716\n",
      "epoch: 3 step: 91, loss is 0.03398333117365837\n",
      "epoch: 3 step: 92, loss is 0.19330109655857086\n",
      "epoch: 3 step: 93, loss is 0.005850296933203936\n",
      "epoch: 3 step: 94, loss is 0.00952879711985588\n",
      "epoch: 3 step: 95, loss is 0.00857395026832819\n",
      "epoch: 3 step: 96, loss is 0.039877310395240784\n",
      "epoch: 3 step: 97, loss is 0.027559999376535416\n",
      "epoch: 3 step: 98, loss is 0.00630610529333353\n",
      "epoch: 3 step: 99, loss is 0.002756870584562421\n",
      "epoch: 3 step: 100, loss is 0.009547416120767593\n",
      "epoch: 3 step: 101, loss is 0.004905856214463711\n",
      "epoch: 3 step: 102, loss is 0.006344266701489687\n",
      "epoch: 3 step: 103, loss is 0.10796315968036652\n",
      "epoch: 3 step: 104, loss is 0.0799308717250824\n",
      "epoch: 3 step: 105, loss is 0.0885554626584053\n",
      "epoch: 3 step: 106, loss is 0.03956056386232376\n",
      "epoch: 3 step: 107, loss is 0.007460113614797592\n",
      "epoch: 3 step: 108, loss is 0.36003297567367554\n",
      "epoch: 3 step: 109, loss is 0.025235943496227264\n",
      "epoch: 3 step: 110, loss is 0.15008319914340973\n",
      "epoch: 3 step: 111, loss is 0.14772935211658478\n",
      "epoch: 3 step: 112, loss is 0.09910576790571213\n",
      "epoch: 3 step: 113, loss is 0.1188405454158783\n",
      "epoch: 3 step: 114, loss is 0.05608689785003662\n",
      "epoch: 3 step: 115, loss is 0.008296278305351734\n",
      "epoch: 3 step: 116, loss is 0.03552476316690445\n",
      "epoch: 3 step: 117, loss is 0.02339778281748295\n",
      "epoch: 3 step: 118, loss is 0.006619180087000132\n",
      "epoch: 3 step: 119, loss is 0.19560401141643524\n",
      "epoch: 3 step: 120, loss is 0.032840970903635025\n",
      "epoch: 3 step: 121, loss is 0.0022146119736135006\n",
      "epoch: 3 step: 122, loss is 0.009969000704586506\n",
      "epoch: 3 step: 123, loss is 0.00941233616322279\n",
      "epoch: 3 step: 124, loss is 0.014342324808239937\n",
      "epoch: 3 step: 125, loss is 0.26853764057159424\n",
      "epoch: 3 step: 126, loss is 0.04231351613998413\n",
      "epoch: 3 step: 127, loss is 0.05562930554151535\n",
      "epoch: 3 step: 128, loss is 0.010139881633222103\n",
      "epoch: 3 step: 129, loss is 0.004733723122626543\n",
      "epoch: 3 step: 130, loss is 0.033943574875593185\n",
      "epoch: 3 step: 131, loss is 0.0029296136926859617\n",
      "epoch: 3 step: 132, loss is 0.023365719243884087\n",
      "epoch: 3 step: 133, loss is 0.15816853940486908\n",
      "epoch: 3 step: 134, loss is 0.015535194426774979\n",
      "epoch: 3 step: 135, loss is 0.03617271035909653\n",
      "epoch: 3 step: 136, loss is 0.00025299348635599017\n",
      "epoch: 3 step: 137, loss is 0.013663243502378464\n",
      "epoch: 3 step: 138, loss is 0.08211156725883484\n",
      "epoch: 3 step: 139, loss is 0.00396139407530427\n",
      "epoch: 3 step: 140, loss is 0.15551680326461792\n",
      "epoch: 3 step: 141, loss is 0.013470922596752644\n",
      "epoch: 3 step: 142, loss is 0.001120600732974708\n",
      "epoch: 3 step: 143, loss is 0.002955289091914892\n",
      "epoch: 3 step: 144, loss is 0.021520091220736504\n",
      "epoch: 3 step: 145, loss is 0.00917129497975111\n",
      "epoch: 3 step: 146, loss is 0.011958971619606018\n",
      "epoch: 3 step: 147, loss is 0.05057096481323242\n",
      "epoch: 3 step: 148, loss is 0.04858265072107315\n",
      "epoch: 3 step: 149, loss is 0.02126440405845642\n",
      "epoch: 3 step: 150, loss is 0.07211519032716751\n",
      "epoch: 3 step: 151, loss is 0.008237913250923157\n",
      "epoch: 3 step: 152, loss is 0.0018269175197929144\n",
      "epoch: 3 step: 153, loss is 0.12629355490207672\n",
      "epoch: 3 step: 154, loss is 0.25566795468330383\n",
      "epoch: 3 step: 155, loss is 0.004897026345133781\n",
      "epoch: 3 step: 156, loss is 0.02339002676308155\n",
      "epoch: 3 step: 157, loss is 0.07135692238807678\n",
      "epoch: 3 step: 158, loss is 0.0004688276385422796\n",
      "epoch: 3 step: 159, loss is 0.017704099416732788\n",
      "epoch: 3 step: 160, loss is 0.01782473549246788\n",
      "epoch: 3 step: 161, loss is 0.023919880390167236\n",
      "epoch: 3 step: 162, loss is 0.03798581659793854\n",
      "epoch: 3 step: 163, loss is 0.05017685145139694\n",
      "epoch: 3 step: 164, loss is 0.006041462533175945\n",
      "epoch: 3 step: 165, loss is 0.0010201865807175636\n",
      "epoch: 3 step: 166, loss is 0.0024076937697827816\n",
      "epoch: 3 step: 167, loss is 0.2228120118379593\n",
      "epoch: 3 step: 168, loss is 0.0008177869021892548\n",
      "epoch: 3 step: 169, loss is 0.15867945551872253\n",
      "epoch: 3 step: 170, loss is 0.03422464802861214\n",
      "epoch: 3 step: 171, loss is 0.01958504691720009\n",
      "epoch: 3 step: 172, loss is 0.0029871861916035414\n",
      "epoch: 3 step: 173, loss is 0.0038319930899888277\n",
      "epoch: 3 step: 174, loss is 0.012711583636701107\n",
      "epoch: 3 step: 175, loss is 0.02746645174920559\n",
      "epoch: 3 step: 176, loss is 0.04729320853948593\n",
      "epoch: 3 step: 177, loss is 0.00671739224344492\n",
      "epoch: 3 step: 178, loss is 0.02025999315083027\n",
      "epoch: 3 step: 179, loss is 0.0069762845523655415\n",
      "epoch: 3 step: 180, loss is 0.3328551948070526\n",
      "epoch: 3 step: 181, loss is 0.014124251902103424\n",
      "epoch: 3 step: 182, loss is 0.1997932493686676\n",
      "epoch: 3 step: 183, loss is 0.0059777176938951015\n",
      "epoch: 3 step: 184, loss is 0.05207759514451027\n",
      "epoch: 3 step: 185, loss is 0.033742111176252365\n",
      "epoch: 3 step: 186, loss is 0.3187301456928253\n",
      "epoch: 3 step: 187, loss is 0.005623281467705965\n",
      "epoch: 3 step: 188, loss is 0.012657194398343563\n",
      "epoch: 3 step: 189, loss is 0.0349741093814373\n",
      "epoch: 3 step: 190, loss is 0.02277671918272972\n",
      "epoch: 3 step: 191, loss is 0.08608260005712509\n",
      "epoch: 3 step: 192, loss is 0.08333763480186462\n",
      "epoch: 3 step: 193, loss is 0.012830566614866257\n",
      "epoch: 3 step: 194, loss is 0.03885018453001976\n",
      "epoch: 3 step: 195, loss is 0.009850612841546535\n",
      "epoch: 3 step: 196, loss is 0.010351919569075108\n",
      "epoch: 3 step: 197, loss is 0.029186872765421867\n",
      "epoch: 3 step: 198, loss is 0.00729788513854146\n",
      "epoch: 3 step: 199, loss is 0.05131546035408974\n",
      "epoch: 3 step: 200, loss is 0.054510101675987244\n",
      "epoch: 3 step: 201, loss is 0.035391729325056076\n",
      "epoch: 3 step: 202, loss is 0.002605403307825327\n",
      "epoch: 3 step: 203, loss is 0.0012394482037052512\n",
      "epoch: 3 step: 204, loss is 0.018758513033390045\n",
      "epoch: 3 step: 205, loss is 0.11637897789478302\n",
      "epoch: 3 step: 206, loss is 0.011833431199193\n",
      "epoch: 3 step: 207, loss is 0.083136647939682\n",
      "epoch: 3 step: 208, loss is 0.045429229736328125\n",
      "epoch: 3 step: 209, loss is 0.018853917717933655\n",
      "epoch: 3 step: 210, loss is 0.026195509359240532\n",
      "epoch: 3 step: 211, loss is 0.02964845299720764\n",
      "epoch: 3 step: 212, loss is 0.13032644987106323\n",
      "epoch: 3 step: 213, loss is 0.0016882311319932342\n",
      "epoch: 3 step: 214, loss is 0.0033508013002574444\n",
      "epoch: 3 step: 215, loss is 0.006318799685686827\n",
      "epoch: 3 step: 216, loss is 0.0077140070497989655\n",
      "epoch: 3 step: 217, loss is 0.00513845682144165\n",
      "epoch: 3 step: 218, loss is 0.008343543857336044\n",
      "epoch: 3 step: 219, loss is 0.05586959794163704\n",
      "epoch: 3 step: 220, loss is 0.04092088341712952\n",
      "epoch: 3 step: 221, loss is 0.027019459754228592\n",
      "epoch: 3 step: 222, loss is 0.021640503779053688\n",
      "epoch: 3 step: 223, loss is 0.0019945702515542507\n",
      "epoch: 3 step: 224, loss is 0.0037481822073459625\n",
      "epoch: 3 step: 225, loss is 0.017451731488108635\n",
      "epoch: 3 step: 226, loss is 0.0023821748327463865\n",
      "epoch: 3 step: 227, loss is 0.2219148576259613\n",
      "epoch: 3 step: 228, loss is 0.02311350218951702\n",
      "epoch: 3 step: 229, loss is 0.005160488188266754\n",
      "epoch: 3 step: 230, loss is 0.012578201480209827\n",
      "epoch: 3 step: 231, loss is 0.042400430887937546\n",
      "epoch: 3 step: 232, loss is 0.0074209896847605705\n",
      "epoch: 3 step: 233, loss is 0.0033679467160254717\n",
      "epoch: 3 step: 234, loss is 0.008822001516819\n",
      "epoch: 3 step: 235, loss is 0.13924896717071533\n",
      "epoch: 3 step: 236, loss is 0.01441384106874466\n",
      "epoch: 3 step: 237, loss is 0.008394144475460052\n",
      "epoch: 3 step: 238, loss is 0.0011685119243338704\n",
      "epoch: 3 step: 239, loss is 0.011083713732659817\n",
      "epoch: 3 step: 240, loss is 0.017297158017754555\n",
      "epoch: 3 step: 241, loss is 0.08228267729282379\n",
      "epoch: 3 step: 242, loss is 0.0006789368926547468\n",
      "epoch: 3 step: 243, loss is 0.306755006313324\n",
      "epoch: 3 step: 244, loss is 0.04308878257870674\n",
      "epoch: 3 step: 245, loss is 0.05729838088154793\n",
      "epoch: 3 step: 246, loss is 0.06408099830150604\n",
      "epoch: 3 step: 247, loss is 0.17548994719982147\n",
      "epoch: 3 step: 248, loss is 0.044849198311567307\n",
      "epoch: 3 step: 249, loss is 0.018555423244833946\n",
      "epoch: 3 step: 250, loss is 0.07382471114397049\n",
      "epoch: 3 step: 251, loss is 0.05486152693629265\n",
      "epoch: 3 step: 252, loss is 0.1255795657634735\n",
      "epoch: 3 step: 253, loss is 0.019835853949189186\n",
      "epoch: 3 step: 254, loss is 0.008287500590085983\n",
      "epoch: 3 step: 255, loss is 0.004819055087864399\n",
      "epoch: 3 step: 256, loss is 0.00305060762912035\n",
      "epoch: 3 step: 257, loss is 0.004444727674126625\n",
      "epoch: 3 step: 258, loss is 0.08081892132759094\n",
      "epoch: 3 step: 259, loss is 0.025190334767103195\n",
      "epoch: 3 step: 260, loss is 0.021114949136972427\n",
      "epoch: 3 step: 261, loss is 0.03056296519935131\n",
      "epoch: 3 step: 262, loss is 0.29762718081474304\n",
      "epoch: 3 step: 263, loss is 0.04290970414876938\n",
      "epoch: 3 step: 264, loss is 0.025064416229724884\n",
      "epoch: 3 step: 265, loss is 0.01568211428821087\n",
      "epoch: 3 step: 266, loss is 0.35292622447013855\n",
      "epoch: 3 step: 267, loss is 0.012820390984416008\n",
      "epoch: 3 step: 268, loss is 0.05816012620925903\n",
      "epoch: 3 step: 269, loss is 0.017889106646180153\n",
      "epoch: 3 step: 270, loss is 0.04100094735622406\n",
      "epoch: 3 step: 271, loss is 0.0028080972842872143\n",
      "epoch: 3 step: 272, loss is 0.0011854098411276937\n",
      "epoch: 3 step: 273, loss is 0.03927408158779144\n",
      "epoch: 3 step: 274, loss is 0.0053065926767885685\n",
      "epoch: 3 step: 275, loss is 0.0034267783630639315\n",
      "epoch: 3 step: 276, loss is 0.01855805702507496\n",
      "epoch: 3 step: 277, loss is 0.011588111519813538\n",
      "epoch: 3 step: 278, loss is 0.05033445358276367\n",
      "epoch: 3 step: 279, loss is 0.0025653215125203133\n",
      "epoch: 3 step: 280, loss is 0.0031734441872686148\n",
      "epoch: 3 step: 281, loss is 0.004081084858626127\n",
      "epoch: 3 step: 282, loss is 0.015909207984805107\n",
      "epoch: 3 step: 283, loss is 0.01358377281576395\n",
      "epoch: 3 step: 284, loss is 0.007183247711509466\n",
      "epoch: 3 step: 285, loss is 0.18169356882572174\n",
      "epoch: 3 step: 286, loss is 0.017331479117274284\n",
      "epoch: 3 step: 287, loss is 0.017500286921858788\n",
      "epoch: 3 step: 288, loss is 0.12936927378177643\n",
      "epoch: 3 step: 289, loss is 0.18559452891349792\n",
      "epoch: 3 step: 290, loss is 0.0011640245793387294\n",
      "epoch: 3 step: 291, loss is 0.07339980453252792\n",
      "epoch: 3 step: 292, loss is 0.21962910890579224\n",
      "epoch: 3 step: 293, loss is 0.008645184338092804\n",
      "epoch: 3 step: 294, loss is 0.0686754360795021\n",
      "epoch: 3 step: 295, loss is 0.0013843345222994685\n",
      "epoch: 3 step: 296, loss is 0.010501721873879433\n",
      "epoch: 3 step: 297, loss is 0.02377806045114994\n",
      "epoch: 3 step: 298, loss is 0.08161137253046036\n",
      "epoch: 3 step: 299, loss is 0.004581466782838106\n",
      "epoch: 3 step: 300, loss is 0.03081192448735237\n",
      "epoch: 3 step: 301, loss is 0.11495284736156464\n",
      "epoch: 3 step: 302, loss is 0.003499909769743681\n",
      "epoch: 3 step: 303, loss is 0.02307240664958954\n",
      "epoch: 3 step: 304, loss is 0.0016850836109369993\n",
      "epoch: 3 step: 305, loss is 0.12470939010381699\n",
      "epoch: 3 step: 306, loss is 0.061137259006500244\n",
      "epoch: 3 step: 307, loss is 0.004849138669669628\n",
      "epoch: 3 step: 308, loss is 0.07963912934064865\n",
      "epoch: 3 step: 309, loss is 0.0013656128430739045\n",
      "epoch: 3 step: 310, loss is 0.004162238910794258\n",
      "epoch: 3 step: 311, loss is 0.023467356339097023\n",
      "epoch: 3 step: 312, loss is 0.05519906058907509\n",
      "epoch: 3 step: 313, loss is 0.032811153680086136\n",
      "epoch: 3 step: 314, loss is 0.05362270772457123\n",
      "epoch: 3 step: 315, loss is 0.1498931646347046\n",
      "epoch: 3 step: 316, loss is 0.09925436973571777\n",
      "epoch: 3 step: 317, loss is 0.008337111212313175\n",
      "epoch: 3 step: 318, loss is 0.001985772280022502\n",
      "epoch: 3 step: 319, loss is 0.1564016491174698\n",
      "epoch: 3 step: 320, loss is 0.23233170807361603\n",
      "epoch: 3 step: 321, loss is 0.019045311957597733\n",
      "epoch: 3 step: 322, loss is 0.006616044323891401\n",
      "epoch: 3 step: 323, loss is 0.08965002000331879\n",
      "epoch: 3 step: 324, loss is 0.05588492751121521\n",
      "epoch: 3 step: 325, loss is 0.013186090625822544\n",
      "epoch: 3 step: 326, loss is 0.07226572185754776\n",
      "epoch: 3 step: 327, loss is 0.04532437026500702\n",
      "epoch: 3 step: 328, loss is 0.004079148638993502\n",
      "epoch: 3 step: 329, loss is 0.005633741617202759\n",
      "epoch: 3 step: 330, loss is 0.060328856110572815\n",
      "epoch: 3 step: 331, loss is 0.06586947292089462\n",
      "epoch: 3 step: 332, loss is 0.08505161851644516\n",
      "epoch: 3 step: 333, loss is 0.004181784577667713\n",
      "epoch: 3 step: 334, loss is 0.02119639702141285\n",
      "epoch: 3 step: 335, loss is 0.08162398636341095\n",
      "epoch: 3 step: 336, loss is 0.06394736468791962\n",
      "epoch: 3 step: 337, loss is 0.001780476188287139\n",
      "epoch: 3 step: 338, loss is 0.22848787903785706\n",
      "epoch: 3 step: 339, loss is 0.13841962814331055\n",
      "epoch: 3 step: 340, loss is 0.10145688056945801\n",
      "epoch: 3 step: 341, loss is 0.023347046226263046\n",
      "epoch: 3 step: 342, loss is 0.02205473557114601\n",
      "epoch: 3 step: 343, loss is 0.009209873154759407\n",
      "epoch: 3 step: 344, loss is 0.0910412147641182\n",
      "epoch: 3 step: 345, loss is 0.0019151667365804315\n",
      "epoch: 3 step: 346, loss is 0.2566889822483063\n",
      "epoch: 3 step: 347, loss is 0.28675249218940735\n",
      "epoch: 3 step: 348, loss is 0.01949932798743248\n",
      "epoch: 3 step: 349, loss is 0.022912198677659035\n",
      "epoch: 3 step: 350, loss is 0.0167621411383152\n",
      "epoch: 3 step: 351, loss is 0.048171356320381165\n",
      "epoch: 3 step: 352, loss is 0.022704310715198517\n",
      "epoch: 3 step: 353, loss is 0.041597574949264526\n",
      "epoch: 3 step: 354, loss is 0.009188251569867134\n",
      "epoch: 3 step: 355, loss is 0.01753443479537964\n",
      "epoch: 3 step: 356, loss is 0.01959935948252678\n",
      "epoch: 3 step: 357, loss is 0.0070487745106220245\n",
      "epoch: 3 step: 358, loss is 0.008977171033620834\n",
      "epoch: 3 step: 359, loss is 0.0756310373544693\n",
      "epoch: 3 step: 360, loss is 0.08317238837480545\n",
      "epoch: 3 step: 361, loss is 0.016653385013341904\n",
      "epoch: 3 step: 362, loss is 0.10847104340791702\n",
      "epoch: 3 step: 363, loss is 0.011961787939071655\n",
      "epoch: 3 step: 364, loss is 0.025910617783665657\n",
      "epoch: 3 step: 365, loss is 0.05555400624871254\n",
      "epoch: 3 step: 366, loss is 0.023003075271844864\n",
      "epoch: 3 step: 367, loss is 0.03746563196182251\n",
      "epoch: 3 step: 368, loss is 0.07295595854520798\n",
      "epoch: 3 step: 369, loss is 0.04217376932501793\n",
      "epoch: 3 step: 370, loss is 0.02110133320093155\n",
      "epoch: 3 step: 371, loss is 0.008913579396903515\n",
      "epoch: 3 step: 372, loss is 0.014600067399442196\n",
      "epoch: 3 step: 373, loss is 0.0033805335406214\n",
      "epoch: 3 step: 374, loss is 0.1546318084001541\n",
      "epoch: 3 step: 375, loss is 0.0031073486898094416\n",
      "epoch: 3 step: 376, loss is 0.020584510639309883\n",
      "epoch: 3 step: 377, loss is 0.0396868959069252\n",
      "epoch: 3 step: 378, loss is 0.04293809458613396\n",
      "epoch: 3 step: 379, loss is 0.00630283122882247\n",
      "epoch: 3 step: 380, loss is 0.0049636512994766235\n",
      "epoch: 3 step: 381, loss is 0.09333498030900955\n",
      "epoch: 3 step: 382, loss is 0.0020916336216032505\n",
      "epoch: 3 step: 383, loss is 0.024730877950787544\n",
      "epoch: 3 step: 384, loss is 0.0068589081056416035\n",
      "epoch: 3 step: 385, loss is 0.0545087493956089\n",
      "epoch: 3 step: 386, loss is 0.042505159974098206\n",
      "epoch: 3 step: 387, loss is 0.015424993820488453\n",
      "epoch: 3 step: 388, loss is 0.008230575360357761\n",
      "epoch: 3 step: 389, loss is 0.010911745019257069\n",
      "epoch: 3 step: 390, loss is 0.06774511933326721\n",
      "epoch: 3 step: 391, loss is 0.017701905220746994\n",
      "epoch: 3 step: 392, loss is 0.0038066068664193153\n",
      "epoch: 3 step: 393, loss is 0.04918171837925911\n",
      "epoch: 3 step: 394, loss is 0.0011882048565894365\n",
      "epoch: 3 step: 395, loss is 0.021655704826116562\n",
      "epoch: 3 step: 396, loss is 0.017660735175013542\n",
      "epoch: 3 step: 397, loss is 0.18145085871219635\n",
      "epoch: 3 step: 398, loss is 0.010615594685077667\n",
      "epoch: 3 step: 399, loss is 0.0028733443468809128\n",
      "epoch: 3 step: 400, loss is 0.07328667491674423\n",
      "epoch: 3 step: 401, loss is 0.004437663126736879\n",
      "epoch: 3 step: 402, loss is 0.13239382207393646\n",
      "epoch: 3 step: 403, loss is 0.09854070097208023\n",
      "epoch: 3 step: 404, loss is 0.010534788481891155\n",
      "epoch: 3 step: 405, loss is 0.17644192278385162\n",
      "epoch: 3 step: 406, loss is 0.03850699961185455\n",
      "epoch: 3 step: 407, loss is 0.03519798815250397\n",
      "epoch: 3 step: 408, loss is 0.016163697466254234\n",
      "epoch: 3 step: 409, loss is 0.0034376364201307297\n",
      "epoch: 3 step: 410, loss is 0.011844968423247337\n",
      "epoch: 3 step: 411, loss is 0.00690425094217062\n",
      "epoch: 3 step: 412, loss is 0.013296631164848804\n",
      "epoch: 3 step: 413, loss is 0.07303949445486069\n",
      "epoch: 3 step: 414, loss is 0.0009833513759076595\n",
      "epoch: 3 step: 415, loss is 0.009129692800343037\n",
      "epoch: 3 step: 416, loss is 0.01683998852968216\n",
      "epoch: 3 step: 417, loss is 0.07219105958938599\n",
      "epoch: 3 step: 418, loss is 0.07631106674671173\n",
      "epoch: 3 step: 419, loss is 0.12750619649887085\n",
      "epoch: 3 step: 420, loss is 0.21133440732955933\n",
      "epoch: 3 step: 421, loss is 0.010882855392992496\n",
      "epoch: 3 step: 422, loss is 0.0268841739743948\n",
      "epoch: 3 step: 423, loss is 0.01961633935570717\n",
      "epoch: 3 step: 424, loss is 0.012444587424397469\n",
      "epoch: 3 step: 425, loss is 0.056288283318281174\n",
      "epoch: 3 step: 426, loss is 0.002264866605401039\n",
      "epoch: 3 step: 427, loss is 0.02374217100441456\n",
      "epoch: 3 step: 428, loss is 0.06788777559995651\n",
      "epoch: 3 step: 429, loss is 0.027627648785710335\n",
      "epoch: 3 step: 430, loss is 0.04517267271876335\n",
      "epoch: 3 step: 431, loss is 0.002842335496097803\n",
      "epoch: 3 step: 432, loss is 0.0025259011890739202\n",
      "epoch: 3 step: 433, loss is 0.11253239959478378\n",
      "epoch: 3 step: 434, loss is 0.003595442045480013\n",
      "epoch: 3 step: 435, loss is 0.08657040446996689\n",
      "epoch: 3 step: 436, loss is 0.03482809290289879\n",
      "epoch: 3 step: 437, loss is 0.03106314316391945\n",
      "epoch: 3 step: 438, loss is 0.07905657589435577\n",
      "epoch: 3 step: 439, loss is 0.0715312659740448\n",
      "epoch: 3 step: 440, loss is 0.07191064208745956\n",
      "epoch: 3 step: 441, loss is 0.0025431904941797256\n",
      "epoch: 3 step: 442, loss is 0.004768846556544304\n",
      "epoch: 3 step: 443, loss is 0.0030020333360880613\n",
      "epoch: 3 step: 444, loss is 0.005313263274729252\n",
      "epoch: 3 step: 445, loss is 0.002774561755359173\n",
      "epoch: 3 step: 446, loss is 0.12572087347507477\n",
      "epoch: 3 step: 447, loss is 0.0722024068236351\n",
      "epoch: 3 step: 448, loss is 0.06101749837398529\n",
      "epoch: 3 step: 449, loss is 0.002562803914770484\n",
      "epoch: 3 step: 450, loss is 0.015744883567094803\n",
      "epoch: 3 step: 451, loss is 0.13886159658432007\n",
      "epoch: 3 step: 452, loss is 0.006331792566925287\n",
      "epoch: 3 step: 453, loss is 0.08502384275197983\n",
      "epoch: 3 step: 454, loss is 0.011736963875591755\n",
      "epoch: 3 step: 455, loss is 0.005331369116902351\n",
      "epoch: 3 step: 456, loss is 0.009044802747666836\n",
      "epoch: 3 step: 457, loss is 0.008215879090130329\n",
      "epoch: 3 step: 458, loss is 0.014752439223229885\n",
      "epoch: 3 step: 459, loss is 0.11297892779111862\n",
      "epoch: 3 step: 460, loss is 0.011801023967564106\n",
      "epoch: 3 step: 461, loss is 0.03842034190893173\n",
      "epoch: 3 step: 462, loss is 0.02323376014828682\n",
      "epoch: 3 step: 463, loss is 0.07429935783147812\n",
      "epoch: 3 step: 464, loss is 0.039498694241046906\n",
      "epoch: 3 step: 465, loss is 0.04021374508738518\n",
      "epoch: 3 step: 466, loss is 0.0038317989092320204\n",
      "epoch: 3 step: 467, loss is 0.002587485359981656\n",
      "epoch: 3 step: 468, loss is 0.05070379748940468\n",
      "epoch: 3 step: 469, loss is 0.0012481281301006675\n",
      "epoch: 3 step: 470, loss is 0.09379226714372635\n",
      "epoch: 3 step: 471, loss is 0.0053793685510754585\n",
      "epoch: 3 step: 472, loss is 0.0017708925297483802\n",
      "epoch: 3 step: 473, loss is 0.0015790145844221115\n",
      "epoch: 3 step: 474, loss is 0.05981820449233055\n",
      "epoch: 3 step: 475, loss is 0.006915140897035599\n",
      "epoch: 3 step: 476, loss is 0.0017051396425813437\n",
      "epoch: 3 step: 477, loss is 0.027424698695540428\n",
      "epoch: 3 step: 478, loss is 0.018294163048267365\n",
      "epoch: 3 step: 479, loss is 0.0067055923864245415\n",
      "epoch: 3 step: 480, loss is 0.06827348470687866\n",
      "epoch: 3 step: 481, loss is 0.0006671004230156541\n",
      "epoch: 3 step: 482, loss is 0.06354869902133942\n",
      "epoch: 3 step: 483, loss is 0.17366254329681396\n",
      "epoch: 3 step: 484, loss is 0.0016931201098486781\n",
      "epoch: 3 step: 485, loss is 0.002091500908136368\n",
      "epoch: 3 step: 486, loss is 0.0028507658280432224\n",
      "epoch: 3 step: 487, loss is 0.0030587157234549522\n",
      "epoch: 3 step: 488, loss is 0.0022729840129613876\n",
      "epoch: 3 step: 489, loss is 0.18851380050182343\n",
      "epoch: 3 step: 490, loss is 0.0722557008266449\n",
      "epoch: 3 step: 491, loss is 0.03234831616282463\n",
      "epoch: 3 step: 492, loss is 0.002775962697342038\n",
      "epoch: 3 step: 493, loss is 0.013244683854281902\n",
      "epoch: 3 step: 494, loss is 0.0017817458137869835\n",
      "epoch: 3 step: 495, loss is 0.001454691868275404\n",
      "epoch: 3 step: 496, loss is 0.0007118870271369815\n",
      "epoch: 3 step: 497, loss is 0.030418600887060165\n",
      "epoch: 3 step: 498, loss is 0.0014122652355581522\n",
      "epoch: 3 step: 499, loss is 0.06870148330926895\n",
      "epoch: 3 step: 500, loss is 0.050518136471509933\n",
      "epoch: 3 step: 501, loss is 0.0018731104210019112\n",
      "epoch: 3 step: 502, loss is 0.0220180694013834\n",
      "epoch: 3 step: 503, loss is 0.0025627678260207176\n",
      "epoch: 3 step: 504, loss is 0.003156475257128477\n",
      "epoch: 3 step: 505, loss is 0.0036925675813108683\n",
      "epoch: 3 step: 506, loss is 0.0013062302023172379\n",
      "epoch: 3 step: 507, loss is 0.26276010274887085\n",
      "epoch: 3 step: 508, loss is 0.06915731728076935\n",
      "epoch: 3 step: 509, loss is 0.008704987354576588\n",
      "epoch: 3 step: 510, loss is 0.009015245363116264\n",
      "epoch: 3 step: 511, loss is 0.013958794996142387\n",
      "epoch: 3 step: 512, loss is 0.010716616176068783\n",
      "epoch: 3 step: 513, loss is 0.0013614548370242119\n",
      "epoch: 3 step: 514, loss is 0.0050340802408754826\n",
      "epoch: 3 step: 515, loss is 0.09427499771118164\n",
      "epoch: 3 step: 516, loss is 0.055823422968387604\n",
      "epoch: 3 step: 517, loss is 0.006789990700781345\n",
      "epoch: 3 step: 518, loss is 0.003576644929125905\n",
      "epoch: 3 step: 519, loss is 0.05430558696389198\n",
      "epoch: 3 step: 520, loss is 0.03624897822737694\n",
      "epoch: 3 step: 521, loss is 0.00348385376855731\n",
      "epoch: 3 step: 522, loss is 0.004589003510773182\n",
      "epoch: 3 step: 523, loss is 0.004371187649667263\n",
      "epoch: 3 step: 524, loss is 0.007127177435904741\n",
      "epoch: 3 step: 525, loss is 0.00766282482072711\n",
      "epoch: 3 step: 526, loss is 0.05297037586569786\n",
      "epoch: 3 step: 527, loss is 0.018365707248449326\n",
      "epoch: 3 step: 528, loss is 0.01509973406791687\n",
      "epoch: 3 step: 529, loss is 0.02420515939593315\n",
      "epoch: 3 step: 530, loss is 0.0098423408344388\n",
      "epoch: 3 step: 531, loss is 0.002123722806572914\n",
      "epoch: 3 step: 532, loss is 0.22100603580474854\n",
      "epoch: 3 step: 533, loss is 0.0023085514549165964\n",
      "epoch: 3 step: 534, loss is 0.0023603884037584066\n",
      "epoch: 3 step: 535, loss is 0.018330270424485207\n",
      "epoch: 3 step: 536, loss is 0.0015849201008677483\n",
      "epoch: 3 step: 537, loss is 0.07485824823379517\n",
      "epoch: 3 step: 538, loss is 0.05027502775192261\n",
      "epoch: 3 step: 539, loss is 0.17049437761306763\n",
      "epoch: 3 step: 540, loss is 0.312387615442276\n",
      "epoch: 3 step: 541, loss is 0.004162030294537544\n",
      "epoch: 3 step: 542, loss is 0.022666318342089653\n",
      "epoch: 3 step: 543, loss is 0.007073347456753254\n",
      "epoch: 3 step: 544, loss is 0.02510206215083599\n",
      "epoch: 3 step: 545, loss is 0.004833201412111521\n",
      "epoch: 3 step: 546, loss is 0.01570066437125206\n",
      "epoch: 3 step: 547, loss is 0.12089179456233978\n",
      "epoch: 3 step: 548, loss is 0.008360332809388638\n",
      "epoch: 3 step: 549, loss is 0.007300801575183868\n",
      "epoch: 3 step: 550, loss is 0.00479131331667304\n",
      "epoch: 3 step: 551, loss is 0.005571798887103796\n",
      "epoch: 3 step: 552, loss is 0.002583483699709177\n",
      "epoch: 3 step: 553, loss is 0.009664986282587051\n",
      "epoch: 3 step: 554, loss is 0.18337862193584442\n",
      "epoch: 3 step: 555, loss is 0.2041870653629303\n",
      "epoch: 3 step: 556, loss is 0.016024911776185036\n",
      "epoch: 3 step: 557, loss is 0.0033034381922334433\n",
      "epoch: 3 step: 558, loss is 0.13335461914539337\n",
      "epoch: 3 step: 559, loss is 0.017804812639951706\n",
      "epoch: 3 step: 560, loss is 0.007298960350453854\n",
      "epoch: 3 step: 561, loss is 0.011161870323121548\n",
      "epoch: 3 step: 562, loss is 0.022544385865330696\n",
      "epoch: 3 step: 563, loss is 0.002632159972563386\n",
      "epoch: 3 step: 564, loss is 0.04109243303537369\n",
      "epoch: 3 step: 565, loss is 0.006392583716660738\n",
      "epoch: 3 step: 566, loss is 0.04670311510562897\n",
      "epoch: 3 step: 567, loss is 0.007629789412021637\n",
      "epoch: 3 step: 568, loss is 0.010802886448800564\n",
      "epoch: 3 step: 569, loss is 0.006252386141568422\n",
      "epoch: 3 step: 570, loss is 0.010719439014792442\n",
      "epoch: 3 step: 571, loss is 0.0006706403219141066\n",
      "epoch: 3 step: 572, loss is 0.007431827485561371\n",
      "epoch: 3 step: 573, loss is 0.0022409670054912567\n",
      "epoch: 3 step: 574, loss is 0.003439911175519228\n",
      "epoch: 3 step: 575, loss is 0.016567692160606384\n",
      "epoch: 3 step: 576, loss is 0.007866375148296356\n",
      "epoch: 3 step: 577, loss is 0.01737295836210251\n",
      "epoch: 3 step: 578, loss is 0.004001250956207514\n",
      "epoch: 3 step: 579, loss is 0.005711043253540993\n",
      "epoch: 3 step: 580, loss is 0.006838408298790455\n",
      "epoch: 3 step: 581, loss is 0.003442782210186124\n",
      "epoch: 3 step: 582, loss is 0.07221677899360657\n",
      "epoch: 3 step: 583, loss is 0.015962716192007065\n",
      "epoch: 3 step: 584, loss is 0.012006199918687344\n",
      "epoch: 3 step: 585, loss is 0.017170313745737076\n",
      "epoch: 3 step: 586, loss is 0.052071839570999146\n",
      "epoch: 3 step: 587, loss is 0.0053207362070679665\n",
      "epoch: 3 step: 588, loss is 0.02148212492465973\n",
      "epoch: 3 step: 589, loss is 0.030734537169337273\n",
      "epoch: 3 step: 590, loss is 0.001423699432052672\n",
      "epoch: 3 step: 591, loss is 0.02657848410308361\n",
      "epoch: 3 step: 592, loss is 0.026618052273988724\n",
      "epoch: 3 step: 593, loss is 0.042931392788887024\n",
      "epoch: 3 step: 594, loss is 9.719262016005814e-05\n",
      "epoch: 3 step: 595, loss is 0.00147086544893682\n",
      "epoch: 3 step: 596, loss is 0.0009810682386159897\n",
      "epoch: 3 step: 597, loss is 0.002376395044848323\n",
      "epoch: 3 step: 598, loss is 0.05639662221074104\n",
      "epoch: 3 step: 599, loss is 0.0004288170312065631\n",
      "epoch: 3 step: 600, loss is 0.003686213167384267\n",
      "epoch: 3 step: 601, loss is 0.00048302800860255957\n",
      "epoch: 3 step: 602, loss is 0.0024892673827707767\n",
      "epoch: 3 step: 603, loss is 0.03243709355592728\n",
      "epoch: 3 step: 604, loss is 0.003165028989315033\n",
      "epoch: 3 step: 605, loss is 0.0011299713514745235\n",
      "epoch: 3 step: 606, loss is 0.0036837486550211906\n",
      "epoch: 3 step: 607, loss is 0.012608679942786694\n",
      "epoch: 3 step: 608, loss is 0.03746744617819786\n",
      "epoch: 3 step: 609, loss is 0.0036582101602107286\n",
      "epoch: 3 step: 610, loss is 0.019546058028936386\n",
      "epoch: 3 step: 611, loss is 0.01417376846075058\n",
      "epoch: 3 step: 612, loss is 0.2360384464263916\n",
      "epoch: 3 step: 613, loss is 0.0016577295027673244\n",
      "epoch: 3 step: 614, loss is 0.005990298930555582\n",
      "epoch: 3 step: 615, loss is 0.0004700943536590785\n",
      "epoch: 3 step: 616, loss is 0.02344428561627865\n",
      "epoch: 3 step: 617, loss is 0.04007893428206444\n",
      "epoch: 3 step: 618, loss is 0.14414601027965546\n",
      "epoch: 3 step: 619, loss is 0.00018269300926476717\n",
      "epoch: 3 step: 620, loss is 0.00713790999725461\n",
      "epoch: 3 step: 621, loss is 0.0008175729890353978\n",
      "epoch: 3 step: 622, loss is 0.2990463674068451\n",
      "epoch: 3 step: 623, loss is 0.010519403964281082\n",
      "epoch: 3 step: 624, loss is 0.025632793083786964\n",
      "epoch: 3 step: 625, loss is 0.00867449026554823\n",
      "epoch: 3 step: 626, loss is 0.038309138268232346\n",
      "epoch: 3 step: 627, loss is 0.055098287761211395\n",
      "epoch: 3 step: 628, loss is 0.20911391079425812\n",
      "epoch: 3 step: 629, loss is 0.011500041000545025\n",
      "epoch: 3 step: 630, loss is 0.05861958861351013\n",
      "epoch: 3 step: 631, loss is 0.02243850566446781\n",
      "epoch: 3 step: 632, loss is 0.049361858516931534\n",
      "epoch: 3 step: 633, loss is 0.009645226411521435\n",
      "epoch: 3 step: 634, loss is 0.167377308011055\n",
      "epoch: 3 step: 635, loss is 0.04384899511933327\n",
      "epoch: 3 step: 636, loss is 0.016128728166222572\n",
      "epoch: 3 step: 637, loss is 0.03738762438297272\n",
      "epoch: 3 step: 638, loss is 0.21957159042358398\n",
      "epoch: 3 step: 639, loss is 0.00010329383076168597\n",
      "epoch: 3 step: 640, loss is 0.04452907294034958\n",
      "epoch: 3 step: 641, loss is 0.1510307341814041\n",
      "epoch: 3 step: 642, loss is 0.014621995389461517\n",
      "epoch: 3 step: 643, loss is 0.005576916970312595\n",
      "epoch: 3 step: 644, loss is 0.00446716882288456\n",
      "epoch: 3 step: 645, loss is 0.01973474584519863\n",
      "epoch: 3 step: 646, loss is 0.007717160973697901\n",
      "epoch: 3 step: 647, loss is 0.0028407690115273\n",
      "epoch: 3 step: 648, loss is 0.20724937319755554\n",
      "epoch: 3 step: 649, loss is 0.012483416125178337\n",
      "epoch: 3 step: 650, loss is 0.013282551430165768\n",
      "epoch: 3 step: 651, loss is 0.10899406671524048\n",
      "epoch: 3 step: 652, loss is 0.02403518557548523\n",
      "epoch: 3 step: 653, loss is 0.1337253600358963\n",
      "epoch: 3 step: 654, loss is 0.00940720271319151\n",
      "epoch: 3 step: 655, loss is 0.07957416772842407\n",
      "epoch: 3 step: 656, loss is 0.009438397362828255\n",
      "epoch: 3 step: 657, loss is 0.055714018642902374\n",
      "epoch: 3 step: 658, loss is 0.003454715246334672\n",
      "epoch: 3 step: 659, loss is 0.18991899490356445\n",
      "epoch: 3 step: 660, loss is 0.055219318717718124\n",
      "epoch: 3 step: 661, loss is 0.002364405198022723\n",
      "epoch: 3 step: 662, loss is 0.04071653634309769\n",
      "epoch: 3 step: 663, loss is 0.0062068249098956585\n",
      "epoch: 3 step: 664, loss is 0.024440065026283264\n",
      "epoch: 3 step: 665, loss is 0.0023879935033619404\n",
      "epoch: 3 step: 666, loss is 0.2854905128479004\n",
      "epoch: 3 step: 667, loss is 0.052691493183374405\n",
      "epoch: 3 step: 668, loss is 0.007190047763288021\n",
      "epoch: 3 step: 669, loss is 0.01638229377567768\n",
      "epoch: 3 step: 670, loss is 0.32330748438835144\n",
      "epoch: 3 step: 671, loss is 0.008145222440361977\n",
      "epoch: 3 step: 672, loss is 0.018101384863257408\n",
      "epoch: 3 step: 673, loss is 0.02559592016041279\n",
      "epoch: 3 step: 674, loss is 0.04880904778838158\n",
      "epoch: 3 step: 675, loss is 0.009562601335346699\n",
      "epoch: 3 step: 676, loss is 0.028206758201122284\n",
      "epoch: 3 step: 677, loss is 0.00458811828866601\n",
      "epoch: 3 step: 678, loss is 0.04523366317152977\n",
      "epoch: 3 step: 679, loss is 0.03752764314413071\n",
      "epoch: 3 step: 680, loss is 0.0031648881267756224\n",
      "epoch: 3 step: 681, loss is 0.05614011362195015\n",
      "epoch: 3 step: 682, loss is 0.05281857028603554\n",
      "epoch: 3 step: 683, loss is 0.0314522348344326\n",
      "epoch: 3 step: 684, loss is 0.2452901005744934\n",
      "epoch: 3 step: 685, loss is 0.0012042551534250379\n",
      "epoch: 3 step: 686, loss is 0.0143564622849226\n",
      "epoch: 3 step: 687, loss is 0.10162131488323212\n",
      "epoch: 3 step: 688, loss is 0.0739566758275032\n",
      "epoch: 3 step: 689, loss is 0.0046914685517549515\n",
      "epoch: 3 step: 690, loss is 0.06092662364244461\n",
      "epoch: 3 step: 691, loss is 0.009968443773686886\n",
      "epoch: 3 step: 692, loss is 0.018698211759328842\n",
      "epoch: 3 step: 693, loss is 0.01828787475824356\n",
      "epoch: 3 step: 694, loss is 0.1433303952217102\n",
      "epoch: 3 step: 695, loss is 0.003217685502022505\n",
      "epoch: 3 step: 696, loss is 0.10511081665754318\n",
      "epoch: 3 step: 697, loss is 0.025510288774967194\n",
      "epoch: 3 step: 698, loss is 0.010172485373914242\n",
      "epoch: 3 step: 699, loss is 0.006324737798422575\n",
      "epoch: 3 step: 700, loss is 0.019307313486933708\n",
      "epoch: 3 step: 701, loss is 0.0083843395113945\n",
      "epoch: 3 step: 702, loss is 0.0020731037948280573\n",
      "epoch: 3 step: 703, loss is 0.07014709711074829\n",
      "epoch: 3 step: 704, loss is 0.001123508089222014\n",
      "epoch: 3 step: 705, loss is 0.007039111107587814\n",
      "epoch: 3 step: 706, loss is 0.009909587912261486\n",
      "epoch: 3 step: 707, loss is 0.09414992481470108\n",
      "epoch: 3 step: 708, loss is 0.01799960806965828\n",
      "epoch: 3 step: 709, loss is 0.009909973479807377\n",
      "epoch: 3 step: 710, loss is 0.0010173296323046088\n",
      "epoch: 3 step: 711, loss is 0.09323330968618393\n",
      "epoch: 3 step: 712, loss is 0.014226173050701618\n",
      "epoch: 3 step: 713, loss is 0.07436415553092957\n",
      "epoch: 3 step: 714, loss is 0.0026429181452840567\n",
      "epoch: 3 step: 715, loss is 0.004451477434486151\n",
      "epoch: 3 step: 716, loss is 0.01299835555255413\n",
      "epoch: 3 step: 717, loss is 0.004975039046257734\n",
      "epoch: 3 step: 718, loss is 0.01885175332427025\n",
      "epoch: 3 step: 719, loss is 0.2236974835395813\n",
      "epoch: 3 step: 720, loss is 0.012818805873394012\n",
      "epoch: 3 step: 721, loss is 0.028324704617261887\n",
      "epoch: 3 step: 722, loss is 0.0961393266916275\n",
      "epoch: 3 step: 723, loss is 0.04255043342709541\n",
      "epoch: 3 step: 724, loss is 0.007038857787847519\n",
      "epoch: 3 step: 725, loss is 0.012717125006020069\n",
      "epoch: 3 step: 726, loss is 0.002352087991312146\n",
      "epoch: 3 step: 727, loss is 0.009257306344807148\n",
      "epoch: 3 step: 728, loss is 0.002466735662892461\n",
      "epoch: 3 step: 729, loss is 0.0027340364176779985\n",
      "epoch: 3 step: 730, loss is 0.0018567783990874887\n",
      "epoch: 3 step: 731, loss is 0.008239596150815487\n",
      "epoch: 3 step: 732, loss is 0.005694894585758448\n",
      "epoch: 3 step: 733, loss is 0.0024850775953382254\n",
      "epoch: 3 step: 734, loss is 0.0006822171853855252\n",
      "epoch: 3 step: 735, loss is 0.024106888100504875\n",
      "epoch: 3 step: 736, loss is 0.009413789957761765\n",
      "epoch: 3 step: 737, loss is 0.015191462822258472\n",
      "epoch: 3 step: 738, loss is 0.19088700413703918\n",
      "epoch: 3 step: 739, loss is 0.003089288016781211\n",
      "epoch: 3 step: 740, loss is 0.002869684947654605\n",
      "epoch: 3 step: 741, loss is 0.04475805163383484\n",
      "epoch: 3 step: 742, loss is 0.1575217843055725\n",
      "epoch: 3 step: 743, loss is 0.0019337208941578865\n",
      "epoch: 3 step: 744, loss is 0.04898523539304733\n",
      "epoch: 3 step: 745, loss is 0.01629471592605114\n",
      "epoch: 3 step: 746, loss is 0.056367166340351105\n",
      "epoch: 3 step: 747, loss is 0.1071259155869484\n",
      "epoch: 3 step: 748, loss is 0.11009752005338669\n",
      "epoch: 3 step: 749, loss is 0.019094569608569145\n",
      "epoch: 3 step: 750, loss is 0.05124617740511894\n",
      "epoch: 3 step: 751, loss is 0.05165483430027962\n",
      "epoch: 3 step: 752, loss is 0.03700985759496689\n",
      "epoch: 3 step: 753, loss is 0.12684273719787598\n",
      "epoch: 3 step: 754, loss is 0.027326583862304688\n",
      "epoch: 3 step: 755, loss is 0.05302475020289421\n",
      "epoch: 3 step: 756, loss is 0.02477448433637619\n",
      "epoch: 3 step: 757, loss is 0.061315201222896576\n",
      "epoch: 3 step: 758, loss is 0.0680854469537735\n",
      "epoch: 3 step: 759, loss is 0.005486167501658201\n",
      "epoch: 3 step: 760, loss is 0.051108457148075104\n",
      "epoch: 3 step: 761, loss is 0.0714501440525055\n",
      "epoch: 3 step: 762, loss is 0.003679138608276844\n",
      "epoch: 3 step: 763, loss is 0.02203303389251232\n",
      "epoch: 3 step: 764, loss is 0.0008529582992196083\n",
      "epoch: 3 step: 765, loss is 0.0032562389969825745\n",
      "epoch: 3 step: 766, loss is 0.047910336405038834\n",
      "epoch: 3 step: 767, loss is 0.0019111857982352376\n",
      "epoch: 3 step: 768, loss is 0.027482401579618454\n",
      "epoch: 3 step: 769, loss is 0.0011650797678157687\n",
      "epoch: 3 step: 770, loss is 0.11747148633003235\n",
      "epoch: 3 step: 771, loss is 0.002356469165533781\n",
      "epoch: 3 step: 772, loss is 0.05713409557938576\n",
      "epoch: 3 step: 773, loss is 0.061450473964214325\n",
      "epoch: 3 step: 774, loss is 0.007779531180858612\n",
      "epoch: 3 step: 775, loss is 0.0034024808555841446\n",
      "epoch: 3 step: 776, loss is 0.01674768328666687\n",
      "epoch: 3 step: 777, loss is 0.0034204560797661543\n",
      "epoch: 3 step: 778, loss is 0.00035676706465892494\n",
      "epoch: 3 step: 779, loss is 0.018241778016090393\n",
      "epoch: 3 step: 780, loss is 0.012301172129809856\n",
      "epoch: 3 step: 781, loss is 0.008880852721631527\n",
      "epoch: 3 step: 782, loss is 0.011346695013344288\n",
      "epoch: 3 step: 783, loss is 0.003921551164239645\n",
      "epoch: 3 step: 784, loss is 0.06569179147481918\n",
      "epoch: 3 step: 785, loss is 0.004518118686974049\n",
      "epoch: 3 step: 786, loss is 0.003110976656898856\n",
      "epoch: 3 step: 787, loss is 0.0018702358938753605\n",
      "epoch: 3 step: 788, loss is 0.02109062299132347\n",
      "epoch: 3 step: 789, loss is 0.06712804734706879\n",
      "epoch: 3 step: 790, loss is 0.011704904958605766\n",
      "epoch: 3 step: 791, loss is 0.002709117252379656\n",
      "epoch: 3 step: 792, loss is 0.037028905004262924\n",
      "epoch: 3 step: 793, loss is 0.12355107814073563\n",
      "epoch: 3 step: 794, loss is 0.010759520344436169\n",
      "epoch: 3 step: 795, loss is 0.034501515328884125\n",
      "epoch: 3 step: 796, loss is 0.015148413367569447\n",
      "epoch: 3 step: 797, loss is 0.0010246977908536792\n",
      "epoch: 3 step: 798, loss is 0.017223119735717773\n",
      "epoch: 3 step: 799, loss is 0.0002688686363399029\n",
      "epoch: 3 step: 800, loss is 0.0021197055466473103\n",
      "epoch: 3 step: 801, loss is 0.23830009996891022\n",
      "epoch: 3 step: 802, loss is 0.004205205477774143\n",
      "epoch: 3 step: 803, loss is 0.2724709212779999\n",
      "epoch: 3 step: 804, loss is 0.1717795729637146\n",
      "epoch: 3 step: 805, loss is 0.4710933566093445\n",
      "epoch: 3 step: 806, loss is 0.0021735476329922676\n",
      "epoch: 3 step: 807, loss is 0.012581880204379559\n",
      "epoch: 3 step: 808, loss is 0.0020493464544415474\n",
      "epoch: 3 step: 809, loss is 0.0016683002468198538\n",
      "epoch: 3 step: 810, loss is 0.001879807678051293\n",
      "epoch: 3 step: 811, loss is 0.03433431312441826\n",
      "epoch: 3 step: 812, loss is 0.0017819347558543086\n",
      "epoch: 3 step: 813, loss is 0.034657590091228485\n",
      "epoch: 3 step: 814, loss is 0.06930167973041534\n",
      "epoch: 3 step: 815, loss is 0.006265703588724136\n",
      "epoch: 3 step: 816, loss is 0.007231154479086399\n",
      "epoch: 3 step: 817, loss is 0.06078561767935753\n",
      "epoch: 3 step: 818, loss is 0.007188428658992052\n",
      "epoch: 3 step: 819, loss is 0.07740262150764465\n",
      "epoch: 3 step: 820, loss is 0.01987355202436447\n",
      "epoch: 3 step: 821, loss is 0.0062616546638309956\n",
      "epoch: 3 step: 822, loss is 0.013586567714810371\n",
      "epoch: 3 step: 823, loss is 0.0018374230712652206\n",
      "epoch: 3 step: 824, loss is 0.004380974918603897\n",
      "epoch: 3 step: 825, loss is 0.0117424875497818\n",
      "epoch: 3 step: 826, loss is 0.15648265182971954\n",
      "epoch: 3 step: 827, loss is 0.024257298558950424\n",
      "epoch: 3 step: 828, loss is 0.01763380691409111\n",
      "epoch: 3 step: 829, loss is 0.08721379190683365\n",
      "epoch: 3 step: 830, loss is 0.055383775383234024\n",
      "epoch: 3 step: 831, loss is 0.018245913088321686\n",
      "epoch: 3 step: 832, loss is 0.00469911890104413\n",
      "epoch: 3 step: 833, loss is 0.031941886991262436\n",
      "epoch: 3 step: 834, loss is 0.06556276977062225\n",
      "epoch: 3 step: 835, loss is 0.0036947117187082767\n",
      "epoch: 3 step: 836, loss is 0.008210679516196251\n",
      "epoch: 3 step: 837, loss is 0.0009216702310368419\n",
      "epoch: 3 step: 838, loss is 0.005011167377233505\n",
      "epoch: 3 step: 839, loss is 0.0067688836716115475\n",
      "epoch: 3 step: 840, loss is 0.0009522971813566983\n",
      "epoch: 3 step: 841, loss is 0.00402170792222023\n",
      "epoch: 3 step: 842, loss is 0.055573586374521255\n",
      "epoch: 3 step: 843, loss is 0.00869029015302658\n",
      "epoch: 3 step: 844, loss is 0.004095757380127907\n",
      "epoch: 3 step: 845, loss is 0.020814163610339165\n",
      "epoch: 3 step: 846, loss is 0.06042037159204483\n",
      "epoch: 3 step: 847, loss is 0.00043717309017665684\n",
      "epoch: 3 step: 848, loss is 0.00701984204351902\n",
      "epoch: 3 step: 849, loss is 0.0029758685268461704\n",
      "epoch: 3 step: 850, loss is 0.22036245465278625\n",
      "epoch: 3 step: 851, loss is 0.000681455188896507\n",
      "epoch: 3 step: 852, loss is 0.006218119990080595\n",
      "epoch: 3 step: 853, loss is 0.00968240574002266\n",
      "epoch: 3 step: 854, loss is 0.019343966618180275\n",
      "epoch: 3 step: 855, loss is 0.0018573803827166557\n",
      "epoch: 3 step: 856, loss is 0.0005448178271763027\n",
      "epoch: 3 step: 857, loss is 0.032817211002111435\n",
      "epoch: 3 step: 858, loss is 0.005563817452639341\n",
      "epoch: 3 step: 859, loss is 0.0887908935546875\n",
      "epoch: 3 step: 860, loss is 0.32215121388435364\n",
      "epoch: 3 step: 861, loss is 0.0013953576562926173\n",
      "epoch: 3 step: 862, loss is 0.005867581814527512\n",
      "epoch: 3 step: 863, loss is 0.04013713449239731\n",
      "epoch: 3 step: 864, loss is 0.08115721493959427\n",
      "epoch: 3 step: 865, loss is 0.027598608285188675\n",
      "epoch: 3 step: 866, loss is 0.005361257586628199\n",
      "epoch: 3 step: 867, loss is 0.020416516810655594\n",
      "epoch: 3 step: 868, loss is 0.03866087645292282\n",
      "epoch: 3 step: 869, loss is 0.03373304009437561\n",
      "epoch: 3 step: 870, loss is 0.004438449628651142\n",
      "epoch: 3 step: 871, loss is 0.01688886620104313\n",
      "epoch: 3 step: 872, loss is 0.008563515730202198\n",
      "epoch: 3 step: 873, loss is 0.008015068247914314\n",
      "epoch: 3 step: 874, loss is 0.003975450061261654\n",
      "epoch: 3 step: 875, loss is 0.017260612919926643\n",
      "epoch: 3 step: 876, loss is 0.010901952162384987\n",
      "epoch: 3 step: 877, loss is 0.11162921041250229\n",
      "epoch: 3 step: 878, loss is 0.2152094691991806\n",
      "epoch: 3 step: 879, loss is 0.001635005697607994\n",
      "epoch: 3 step: 880, loss is 0.061062365770339966\n",
      "epoch: 3 step: 881, loss is 0.04577820375561714\n",
      "epoch: 3 step: 882, loss is 0.026982495561242104\n",
      "epoch: 3 step: 883, loss is 0.00629658717662096\n",
      "epoch: 3 step: 884, loss is 0.04903513938188553\n",
      "epoch: 3 step: 885, loss is 0.01079684216529131\n",
      "epoch: 3 step: 886, loss is 0.0037173305172473192\n",
      "epoch: 3 step: 887, loss is 0.0016126242699101567\n",
      "epoch: 3 step: 888, loss is 0.005936087109148502\n",
      "epoch: 3 step: 889, loss is 0.005280223675072193\n",
      "epoch: 3 step: 890, loss is 0.07669389247894287\n",
      "epoch: 3 step: 891, loss is 0.10141148418188095\n",
      "epoch: 3 step: 892, loss is 0.01357667613774538\n",
      "epoch: 3 step: 893, loss is 0.0045148273929953575\n",
      "epoch: 3 step: 894, loss is 0.22835363447666168\n",
      "epoch: 3 step: 895, loss is 0.021088024601340294\n",
      "epoch: 3 step: 896, loss is 0.0019224659772589803\n",
      "epoch: 3 step: 897, loss is 0.13005304336547852\n",
      "epoch: 3 step: 898, loss is 0.04702019318938255\n",
      "epoch: 3 step: 899, loss is 0.008962568826973438\n",
      "epoch: 3 step: 900, loss is 0.00615903502330184\n",
      "epoch: 3 step: 901, loss is 0.015301917679607868\n",
      "epoch: 3 step: 902, loss is 0.02830374427139759\n",
      "epoch: 3 step: 903, loss is 0.09206078946590424\n",
      "epoch: 3 step: 904, loss is 0.19805465638637543\n",
      "epoch: 3 step: 905, loss is 0.0011507722083479166\n",
      "epoch: 3 step: 906, loss is 0.14626570045948029\n",
      "epoch: 3 step: 907, loss is 0.006499516777694225\n",
      "epoch: 3 step: 908, loss is 0.013304423540830612\n",
      "epoch: 3 step: 909, loss is 0.006900607608258724\n",
      "epoch: 3 step: 910, loss is 0.004942343570291996\n",
      "epoch: 3 step: 911, loss is 0.011102966032922268\n",
      "epoch: 3 step: 912, loss is 0.0038651691284030676\n",
      "epoch: 3 step: 913, loss is 0.026844123378396034\n",
      "epoch: 3 step: 914, loss is 0.2797083258628845\n",
      "epoch: 3 step: 915, loss is 0.004865873139351606\n",
      "epoch: 3 step: 916, loss is 0.00609121099114418\n",
      "epoch: 3 step: 917, loss is 0.002976710442453623\n",
      "epoch: 3 step: 918, loss is 0.0065458426252007484\n",
      "epoch: 3 step: 919, loss is 0.014950789511203766\n",
      "epoch: 3 step: 920, loss is 0.07885978370904922\n",
      "epoch: 3 step: 921, loss is 0.0031203345861285925\n",
      "epoch: 3 step: 922, loss is 0.0079331174492836\n",
      "epoch: 3 step: 923, loss is 0.02306772582232952\n",
      "epoch: 3 step: 924, loss is 0.02642255835235119\n",
      "epoch: 3 step: 925, loss is 0.10918727517127991\n",
      "epoch: 3 step: 926, loss is 0.0020932357292622328\n",
      "epoch: 3 step: 927, loss is 0.007729134522378445\n",
      "epoch: 3 step: 928, loss is 0.0822572186589241\n",
      "epoch: 3 step: 929, loss is 0.3049653470516205\n",
      "epoch: 3 step: 930, loss is 0.002138645388185978\n",
      "epoch: 3 step: 931, loss is 0.021837368607521057\n",
      "epoch: 3 step: 932, loss is 0.1371683031320572\n",
      "epoch: 3 step: 933, loss is 0.0321090929210186\n",
      "epoch: 3 step: 934, loss is 0.009942284785211086\n",
      "epoch: 3 step: 935, loss is 0.032625917345285416\n",
      "epoch: 3 step: 936, loss is 0.0027668746188282967\n",
      "epoch: 3 step: 937, loss is 0.033230800181627274\n",
      "epoch: 3 step: 938, loss is 0.00397994089871645\n",
      "epoch: 3 step: 939, loss is 0.0011294614523649216\n",
      "epoch: 3 step: 940, loss is 0.0031177427154034376\n",
      "epoch: 3 step: 941, loss is 0.010581952519714832\n",
      "epoch: 3 step: 942, loss is 0.0720730796456337\n",
      "epoch: 3 step: 943, loss is 0.06944805383682251\n",
      "epoch: 3 step: 944, loss is 0.03223579004406929\n",
      "epoch: 3 step: 945, loss is 0.0036785893607884645\n",
      "epoch: 3 step: 946, loss is 0.09356249123811722\n",
      "epoch: 3 step: 947, loss is 0.01668175682425499\n",
      "epoch: 3 step: 948, loss is 0.013064851984381676\n",
      "epoch: 3 step: 949, loss is 0.043738119304180145\n",
      "epoch: 3 step: 950, loss is 0.018247876316308975\n",
      "epoch: 3 step: 951, loss is 0.0067848265171051025\n",
      "epoch: 3 step: 952, loss is 0.027014566585421562\n",
      "epoch: 3 step: 953, loss is 0.018666772171854973\n",
      "epoch: 3 step: 954, loss is 0.04042204096913338\n",
      "epoch: 3 step: 955, loss is 0.0010781397577375174\n",
      "epoch: 3 step: 956, loss is 0.0007422604248858988\n",
      "epoch: 3 step: 957, loss is 0.0003111582191195339\n",
      "epoch: 3 step: 958, loss is 0.2737835645675659\n",
      "epoch: 3 step: 959, loss is 0.038188058882951736\n",
      "epoch: 3 step: 960, loss is 0.005305475555360317\n",
      "epoch: 3 step: 961, loss is 0.007384879514575005\n",
      "epoch: 3 step: 962, loss is 0.007335794623941183\n",
      "epoch: 3 step: 963, loss is 0.18592999875545502\n",
      "epoch: 3 step: 964, loss is 0.0044851480051875114\n",
      "epoch: 3 step: 965, loss is 0.12798471748828888\n",
      "epoch: 3 step: 966, loss is 0.06734391301870346\n",
      "epoch: 3 step: 967, loss is 0.022677091881632805\n",
      "epoch: 3 step: 968, loss is 0.03347798064351082\n",
      "epoch: 3 step: 969, loss is 0.09767736494541168\n",
      "epoch: 3 step: 970, loss is 0.1127956286072731\n",
      "epoch: 3 step: 971, loss is 0.038976848125457764\n",
      "epoch: 3 step: 972, loss is 0.0027626061346381903\n",
      "epoch: 3 step: 973, loss is 0.01515358965843916\n",
      "epoch: 3 step: 974, loss is 0.0037797787226736546\n",
      "epoch: 3 step: 975, loss is 0.06689614802598953\n",
      "epoch: 3 step: 976, loss is 0.06085420772433281\n",
      "epoch: 3 step: 977, loss is 0.04550718888640404\n",
      "epoch: 3 step: 978, loss is 0.02852010726928711\n",
      "epoch: 3 step: 979, loss is 0.0029887277632951736\n",
      "epoch: 3 step: 980, loss is 0.06818466633558273\n",
      "epoch: 3 step: 981, loss is 0.04174812138080597\n",
      "epoch: 3 step: 982, loss is 0.006228610873222351\n",
      "epoch: 3 step: 983, loss is 0.011132298037409782\n",
      "epoch: 3 step: 984, loss is 0.008084041997790337\n",
      "epoch: 3 step: 985, loss is 0.0288692694157362\n",
      "epoch: 3 step: 986, loss is 0.09463586658239365\n",
      "epoch: 3 step: 987, loss is 0.0014172500232234597\n",
      "epoch: 3 step: 988, loss is 0.11909168213605881\n",
      "epoch: 3 step: 989, loss is 0.004117138683795929\n",
      "epoch: 3 step: 990, loss is 0.01185916643589735\n",
      "epoch: 3 step: 991, loss is 0.1920478492975235\n",
      "epoch: 3 step: 992, loss is 0.021334081888198853\n",
      "epoch: 3 step: 993, loss is 0.003639006521552801\n",
      "epoch: 3 step: 994, loss is 0.26975640654563904\n",
      "epoch: 3 step: 995, loss is 0.0007064590463414788\n",
      "epoch: 3 step: 996, loss is 0.13731496036052704\n",
      "epoch: 3 step: 997, loss is 0.011069918051362038\n",
      "epoch: 3 step: 998, loss is 0.04630414396524429\n",
      "epoch: 3 step: 999, loss is 0.07039489597082138\n",
      "epoch: 3 step: 1000, loss is 0.03646211698651314\n",
      "epoch: 3 step: 1001, loss is 0.05495798587799072\n",
      "epoch: 3 step: 1002, loss is 0.01027798280119896\n",
      "epoch: 3 step: 1003, loss is 0.0031072567217051983\n",
      "epoch: 3 step: 1004, loss is 0.004102309700101614\n",
      "epoch: 3 step: 1005, loss is 0.0018187565729022026\n",
      "epoch: 3 step: 1006, loss is 0.18972229957580566\n",
      "epoch: 3 step: 1007, loss is 0.0019518790068104863\n",
      "epoch: 3 step: 1008, loss is 0.0014021344250068069\n",
      "epoch: 3 step: 1009, loss is 0.0013528725830838084\n",
      "epoch: 3 step: 1010, loss is 0.011212928220629692\n",
      "epoch: 3 step: 1011, loss is 0.02760528214275837\n",
      "epoch: 3 step: 1012, loss is 0.017241787165403366\n",
      "epoch: 3 step: 1013, loss is 0.05326050892472267\n",
      "epoch: 3 step: 1014, loss is 0.17978453636169434\n",
      "epoch: 3 step: 1015, loss is 0.000512507336679846\n",
      "epoch: 3 step: 1016, loss is 0.0034882277250289917\n",
      "epoch: 3 step: 1017, loss is 0.04742416739463806\n",
      "epoch: 3 step: 1018, loss is 0.005059056915342808\n",
      "epoch: 3 step: 1019, loss is 0.000350557646015659\n",
      "epoch: 3 step: 1020, loss is 0.17518272995948792\n",
      "epoch: 3 step: 1021, loss is 0.08468005061149597\n",
      "epoch: 3 step: 1022, loss is 0.0072745513170957565\n",
      "epoch: 3 step: 1023, loss is 0.006107463035732508\n",
      "epoch: 3 step: 1024, loss is 0.05390619859099388\n",
      "epoch: 3 step: 1025, loss is 0.04170314967632294\n",
      "epoch: 3 step: 1026, loss is 0.00029374187579378486\n",
      "epoch: 3 step: 1027, loss is 0.11848897486925125\n",
      "epoch: 3 step: 1028, loss is 0.009943393059074879\n",
      "epoch: 3 step: 1029, loss is 0.03933839127421379\n",
      "epoch: 3 step: 1030, loss is 0.004526463337242603\n",
      "epoch: 3 step: 1031, loss is 0.12514802813529968\n",
      "epoch: 3 step: 1032, loss is 0.002026605186983943\n",
      "epoch: 3 step: 1033, loss is 0.06861678510904312\n",
      "epoch: 3 step: 1034, loss is 0.007806098088622093\n",
      "epoch: 3 step: 1035, loss is 0.02277977205812931\n",
      "epoch: 3 step: 1036, loss is 0.10598931461572647\n",
      "epoch: 3 step: 1037, loss is 0.019638588652014732\n",
      "epoch: 3 step: 1038, loss is 0.010966327041387558\n",
      "epoch: 3 step: 1039, loss is 0.18213458359241486\n",
      "epoch: 3 step: 1040, loss is 0.0020228438079357147\n",
      "epoch: 3 step: 1041, loss is 0.0036547775380313396\n",
      "epoch: 3 step: 1042, loss is 0.17725618183612823\n",
      "epoch: 3 step: 1043, loss is 0.005157684441655874\n",
      "epoch: 3 step: 1044, loss is 0.03553696721792221\n",
      "epoch: 3 step: 1045, loss is 0.01816970854997635\n",
      "epoch: 3 step: 1046, loss is 0.010532822459936142\n",
      "epoch: 3 step: 1047, loss is 0.09521707892417908\n",
      "epoch: 3 step: 1048, loss is 0.08052852749824524\n",
      "epoch: 3 step: 1049, loss is 0.03257622569799423\n",
      "epoch: 3 step: 1050, loss is 0.015297623351216316\n",
      "epoch: 3 step: 1051, loss is 0.012714209966361523\n",
      "epoch: 3 step: 1052, loss is 0.028386173769831657\n",
      "epoch: 3 step: 1053, loss is 0.09999062120914459\n",
      "epoch: 3 step: 1054, loss is 0.23796811699867249\n",
      "epoch: 3 step: 1055, loss is 0.00148009043186903\n",
      "epoch: 3 step: 1056, loss is 0.046121809631586075\n",
      "epoch: 3 step: 1057, loss is 0.005594456102699041\n",
      "epoch: 3 step: 1058, loss is 0.14654947817325592\n",
      "epoch: 3 step: 1059, loss is 0.0027137044817209244\n",
      "epoch: 3 step: 1060, loss is 0.017262186855077744\n",
      "epoch: 3 step: 1061, loss is 0.007105442229658365\n",
      "epoch: 3 step: 1062, loss is 0.024981319904327393\n",
      "epoch: 3 step: 1063, loss is 0.02031751722097397\n",
      "epoch: 3 step: 1064, loss is 0.06257408857345581\n",
      "epoch: 3 step: 1065, loss is 0.24490946531295776\n",
      "epoch: 3 step: 1066, loss is 0.05759846791625023\n",
      "epoch: 3 step: 1067, loss is 0.005373828113079071\n",
      "epoch: 3 step: 1068, loss is 0.1634901762008667\n",
      "epoch: 3 step: 1069, loss is 0.04999133199453354\n",
      "epoch: 3 step: 1070, loss is 0.007823517546057701\n",
      "epoch: 3 step: 1071, loss is 0.0663640946149826\n",
      "epoch: 3 step: 1072, loss is 0.015960507094860077\n",
      "epoch: 3 step: 1073, loss is 0.10726052522659302\n",
      "epoch: 3 step: 1074, loss is 0.10064670443534851\n",
      "epoch: 3 step: 1075, loss is 0.019586533308029175\n",
      "epoch: 3 step: 1076, loss is 0.0015725962584838271\n",
      "epoch: 3 step: 1077, loss is 0.03459644317626953\n",
      "epoch: 3 step: 1078, loss is 0.07382194697856903\n",
      "epoch: 3 step: 1079, loss is 0.008317794650793076\n",
      "epoch: 3 step: 1080, loss is 0.03178711608052254\n",
      "epoch: 3 step: 1081, loss is 0.007738659158349037\n",
      "epoch: 3 step: 1082, loss is 0.010683132335543633\n",
      "epoch: 3 step: 1083, loss is 0.1435215026140213\n",
      "epoch: 3 step: 1084, loss is 0.01961325667798519\n",
      "epoch: 3 step: 1085, loss is 0.003136257641017437\n",
      "epoch: 3 step: 1086, loss is 0.15469078719615936\n",
      "epoch: 3 step: 1087, loss is 0.014429828152060509\n",
      "epoch: 3 step: 1088, loss is 0.0007622014963999391\n",
      "epoch: 3 step: 1089, loss is 0.034860286861658096\n",
      "epoch: 3 step: 1090, loss is 0.06791385263204575\n",
      "epoch: 3 step: 1091, loss is 0.044193167239427567\n",
      "epoch: 3 step: 1092, loss is 0.0074830432422459126\n",
      "epoch: 3 step: 1093, loss is 0.05698367953300476\n",
      "epoch: 3 step: 1094, loss is 0.04068107530474663\n",
      "epoch: 3 step: 1095, loss is 0.09891495853662491\n",
      "epoch: 3 step: 1096, loss is 0.12458055466413498\n",
      "epoch: 3 step: 1097, loss is 0.0011082921409979463\n",
      "epoch: 3 step: 1098, loss is 0.008861619979143143\n",
      "epoch: 3 step: 1099, loss is 0.007303618360310793\n",
      "epoch: 3 step: 1100, loss is 0.029933035373687744\n",
      "epoch: 3 step: 1101, loss is 0.024746457114815712\n",
      "epoch: 3 step: 1102, loss is 0.11503682285547256\n",
      "epoch: 3 step: 1103, loss is 0.01360415294766426\n",
      "epoch: 3 step: 1104, loss is 0.3030105531215668\n",
      "epoch: 3 step: 1105, loss is 0.07887199521064758\n",
      "epoch: 3 step: 1106, loss is 0.01155596598982811\n",
      "epoch: 3 step: 1107, loss is 0.16751247644424438\n",
      "epoch: 3 step: 1108, loss is 0.2615506052970886\n",
      "epoch: 3 step: 1109, loss is 0.06275192648172379\n",
      "epoch: 3 step: 1110, loss is 0.003454465651884675\n",
      "epoch: 3 step: 1111, loss is 0.0021480037830770016\n",
      "epoch: 3 step: 1112, loss is 0.15381817519664764\n",
      "epoch: 3 step: 1113, loss is 0.09596721082925797\n",
      "epoch: 3 step: 1114, loss is 0.008185893297195435\n",
      "epoch: 3 step: 1115, loss is 0.03590916469693184\n",
      "epoch: 3 step: 1116, loss is 0.004643382038921118\n",
      "epoch: 3 step: 1117, loss is 0.01585180126130581\n",
      "epoch: 3 step: 1118, loss is 0.006265739910304546\n",
      "epoch: 3 step: 1119, loss is 0.052466973662376404\n",
      "epoch: 3 step: 1120, loss is 0.013199139386415482\n",
      "epoch: 3 step: 1121, loss is 0.059483855962753296\n",
      "epoch: 3 step: 1122, loss is 0.04766617342829704\n",
      "epoch: 3 step: 1123, loss is 0.17794495820999146\n",
      "epoch: 3 step: 1124, loss is 0.06442133337259293\n",
      "epoch: 3 step: 1125, loss is 0.024127773940563202\n",
      "epoch: 3 step: 1126, loss is 0.00129488215316087\n",
      "epoch: 3 step: 1127, loss is 0.03640946373343468\n",
      "epoch: 3 step: 1128, loss is 0.03180273249745369\n",
      "epoch: 3 step: 1129, loss is 0.002737634116783738\n",
      "epoch: 3 step: 1130, loss is 0.0011703810887411237\n",
      "epoch: 3 step: 1131, loss is 0.11644592136144638\n",
      "epoch: 3 step: 1132, loss is 0.012675002217292786\n",
      "epoch: 3 step: 1133, loss is 0.024652700871229172\n",
      "epoch: 3 step: 1134, loss is 0.08199772983789444\n",
      "epoch: 3 step: 1135, loss is 0.010270021855831146\n",
      "epoch: 3 step: 1136, loss is 0.012798882089555264\n",
      "epoch: 3 step: 1137, loss is 0.009079228155314922\n",
      "epoch: 3 step: 1138, loss is 0.07696709781885147\n",
      "epoch: 3 step: 1139, loss is 0.050240203738212585\n",
      "epoch: 3 step: 1140, loss is 0.001713996403850615\n",
      "epoch: 3 step: 1141, loss is 0.010929558426141739\n",
      "epoch: 3 step: 1142, loss is 0.23907338082790375\n",
      "epoch: 3 step: 1143, loss is 0.007839515805244446\n",
      "epoch: 3 step: 1144, loss is 0.005024133250117302\n",
      "epoch: 3 step: 1145, loss is 0.0013307011686265469\n",
      "epoch: 3 step: 1146, loss is 0.009421342052519321\n",
      "epoch: 3 step: 1147, loss is 0.0030385274440050125\n",
      "epoch: 3 step: 1148, loss is 0.009469032287597656\n",
      "epoch: 3 step: 1149, loss is 0.01974683254957199\n",
      "epoch: 3 step: 1150, loss is 0.008577234111726284\n",
      "epoch: 3 step: 1151, loss is 0.1116233542561531\n",
      "epoch: 3 step: 1152, loss is 0.013887321576476097\n",
      "epoch: 3 step: 1153, loss is 0.004002160858362913\n",
      "epoch: 3 step: 1154, loss is 0.006627731490880251\n",
      "epoch: 3 step: 1155, loss is 0.0056254202499985695\n",
      "epoch: 3 step: 1156, loss is 0.016952814534306526\n",
      "epoch: 3 step: 1157, loss is 0.0047258008271455765\n",
      "epoch: 3 step: 1158, loss is 0.009842479601502419\n",
      "epoch: 3 step: 1159, loss is 0.002910879673436284\n",
      "epoch: 3 step: 1160, loss is 0.014613627456128597\n",
      "epoch: 3 step: 1161, loss is 0.02002665027976036\n",
      "epoch: 3 step: 1162, loss is 0.00828994158655405\n",
      "epoch: 3 step: 1163, loss is 0.022070473060011864\n",
      "epoch: 3 step: 1164, loss is 0.20714592933654785\n",
      "epoch: 3 step: 1165, loss is 0.001159248175099492\n",
      "epoch: 3 step: 1166, loss is 0.0076009235344827175\n",
      "epoch: 3 step: 1167, loss is 0.0037406960036605597\n",
      "epoch: 3 step: 1168, loss is 0.009326078929007053\n",
      "epoch: 3 step: 1169, loss is 0.010353387333452702\n",
      "epoch: 3 step: 1170, loss is 0.0007260793354362249\n",
      "epoch: 3 step: 1171, loss is 0.015553237870335579\n",
      "epoch: 3 step: 1172, loss is 0.033399201929569244\n",
      "epoch: 3 step: 1173, loss is 0.1370212584733963\n",
      "epoch: 3 step: 1174, loss is 0.09508180618286133\n",
      "epoch: 3 step: 1175, loss is 0.011756452731788158\n",
      "epoch: 3 step: 1176, loss is 0.006956954021006823\n",
      "epoch: 3 step: 1177, loss is 0.005663658492267132\n",
      "epoch: 3 step: 1178, loss is 0.007380309049040079\n",
      "epoch: 3 step: 1179, loss is 0.003801264800131321\n",
      "epoch: 3 step: 1180, loss is 0.06903606653213501\n",
      "epoch: 3 step: 1181, loss is 0.030910057947039604\n",
      "epoch: 3 step: 1182, loss is 0.1758018136024475\n",
      "epoch: 3 step: 1183, loss is 0.004242466762661934\n",
      "epoch: 3 step: 1184, loss is 0.001839923090301454\n",
      "epoch: 3 step: 1185, loss is 0.09261242300271988\n",
      "epoch: 3 step: 1186, loss is 0.022197628393769264\n",
      "epoch: 3 step: 1187, loss is 0.3255058526992798\n",
      "epoch: 3 step: 1188, loss is 0.004196691792458296\n",
      "epoch: 3 step: 1189, loss is 0.0011707621160894632\n",
      "epoch: 3 step: 1190, loss is 0.0008563383598811924\n",
      "epoch: 3 step: 1191, loss is 0.018566155806183815\n",
      "epoch: 3 step: 1192, loss is 0.03925851732492447\n",
      "epoch: 3 step: 1193, loss is 0.13660776615142822\n",
      "epoch: 3 step: 1194, loss is 0.000488727877382189\n",
      "epoch: 3 step: 1195, loss is 0.004242188297212124\n",
      "epoch: 3 step: 1196, loss is 0.0014442987740039825\n",
      "epoch: 3 step: 1197, loss is 0.006300977431237698\n",
      "epoch: 3 step: 1198, loss is 0.006305062212049961\n",
      "epoch: 3 step: 1199, loss is 0.012845680117607117\n",
      "epoch: 3 step: 1200, loss is 0.05091085284948349\n",
      "epoch: 3 step: 1201, loss is 0.0014962913701310754\n",
      "epoch: 3 step: 1202, loss is 0.003152399556711316\n",
      "epoch: 3 step: 1203, loss is 0.001262764330022037\n",
      "epoch: 3 step: 1204, loss is 0.033050935715436935\n",
      "epoch: 3 step: 1205, loss is 0.020265856757760048\n",
      "epoch: 3 step: 1206, loss is 0.09126955270767212\n",
      "epoch: 3 step: 1207, loss is 0.04201503098011017\n",
      "epoch: 3 step: 1208, loss is 0.008450276218354702\n",
      "epoch: 3 step: 1209, loss is 0.0024467669427394867\n",
      "epoch: 3 step: 1210, loss is 0.008419116027653217\n",
      "epoch: 3 step: 1211, loss is 0.043181851506233215\n",
      "epoch: 3 step: 1212, loss is 0.05120615288615227\n",
      "epoch: 3 step: 1213, loss is 0.03149699792265892\n",
      "epoch: 3 step: 1214, loss is 0.017219185829162598\n",
      "epoch: 3 step: 1215, loss is 0.0022119225468486547\n",
      "epoch: 3 step: 1216, loss is 0.005469599738717079\n",
      "epoch: 3 step: 1217, loss is 0.0017326123779639602\n",
      "epoch: 3 step: 1218, loss is 0.007267878856509924\n",
      "epoch: 3 step: 1219, loss is 0.08156927675008774\n",
      "epoch: 3 step: 1220, loss is 0.0006540853064507246\n",
      "epoch: 3 step: 1221, loss is 0.03149668499827385\n",
      "epoch: 3 step: 1222, loss is 0.24940940737724304\n",
      "epoch: 3 step: 1223, loss is 0.0038737424183636904\n",
      "epoch: 3 step: 1224, loss is 0.0010961078805848956\n",
      "epoch: 3 step: 1225, loss is 0.03478337824344635\n",
      "epoch: 3 step: 1226, loss is 0.0037101495545357466\n",
      "epoch: 3 step: 1227, loss is 0.1721373051404953\n",
      "epoch: 3 step: 1228, loss is 0.004130719229578972\n",
      "epoch: 3 step: 1229, loss is 0.004276188090443611\n",
      "epoch: 3 step: 1230, loss is 0.0014776348834857345\n",
      "epoch: 3 step: 1231, loss is 0.014134013094007969\n",
      "epoch: 3 step: 1232, loss is 0.0739588737487793\n",
      "epoch: 3 step: 1233, loss is 0.12293975800275803\n",
      "epoch: 3 step: 1234, loss is 0.0006085060886107385\n",
      "epoch: 3 step: 1235, loss is 0.0001990423770621419\n",
      "epoch: 3 step: 1236, loss is 0.04399091750383377\n",
      "epoch: 3 step: 1237, loss is 0.0011258528102189302\n",
      "epoch: 3 step: 1238, loss is 0.23577813804149628\n",
      "epoch: 3 step: 1239, loss is 0.007577816024422646\n",
      "epoch: 3 step: 1240, loss is 0.006959435995668173\n",
      "epoch: 3 step: 1241, loss is 0.004922335501760244\n",
      "epoch: 3 step: 1242, loss is 0.0004756225098390132\n",
      "epoch: 3 step: 1243, loss is 0.005193530581891537\n",
      "epoch: 3 step: 1244, loss is 0.005886806640774012\n",
      "epoch: 3 step: 1245, loss is 0.0003326927253510803\n",
      "epoch: 3 step: 1246, loss is 0.0007047370309010148\n",
      "epoch: 3 step: 1247, loss is 0.0021845963783562183\n",
      "epoch: 3 step: 1248, loss is 0.014565919525921345\n",
      "epoch: 3 step: 1249, loss is 0.004034313838928938\n",
      "epoch: 3 step: 1250, loss is 0.13364405930042267\n",
      "epoch: 3 step: 1251, loss is 0.07419244199991226\n",
      "epoch: 3 step: 1252, loss is 0.0036374644841998816\n",
      "epoch: 3 step: 1253, loss is 0.01779591664671898\n",
      "epoch: 3 step: 1254, loss is 0.03013836033642292\n",
      "epoch: 3 step: 1255, loss is 0.001780546735972166\n",
      "epoch: 3 step: 1256, loss is 0.004448822233825922\n",
      "epoch: 3 step: 1257, loss is 0.12635773420333862\n",
      "epoch: 3 step: 1258, loss is 0.00025018121232278645\n",
      "epoch: 3 step: 1259, loss is 0.005161191802471876\n",
      "epoch: 3 step: 1260, loss is 0.0033389749005436897\n",
      "epoch: 3 step: 1261, loss is 0.00018855175585485995\n",
      "epoch: 3 step: 1262, loss is 0.008015460334718227\n",
      "epoch: 3 step: 1263, loss is 0.18914757668972015\n",
      "epoch: 3 step: 1264, loss is 0.014700109139084816\n",
      "epoch: 3 step: 1265, loss is 0.011713884770870209\n",
      "epoch: 3 step: 1266, loss is 0.002479399787262082\n",
      "epoch: 3 step: 1267, loss is 0.007840701378881931\n",
      "epoch: 3 step: 1268, loss is 0.07007648050785065\n",
      "epoch: 3 step: 1269, loss is 0.0003754498320631683\n",
      "epoch: 3 step: 1270, loss is 0.008635945618152618\n",
      "epoch: 3 step: 1271, loss is 0.031209399923682213\n",
      "epoch: 3 step: 1272, loss is 0.042722009122371674\n",
      "epoch: 3 step: 1273, loss is 0.13615615665912628\n",
      "epoch: 3 step: 1274, loss is 0.028612658381462097\n",
      "epoch: 3 step: 1275, loss is 0.45111870765686035\n",
      "epoch: 3 step: 1276, loss is 0.005135844927281141\n",
      "epoch: 3 step: 1277, loss is 0.016556892544031143\n",
      "epoch: 3 step: 1278, loss is 0.0055055622942745686\n",
      "epoch: 3 step: 1279, loss is 0.0611034631729126\n",
      "epoch: 3 step: 1280, loss is 0.06992892175912857\n",
      "epoch: 3 step: 1281, loss is 0.0019399470183998346\n",
      "epoch: 3 step: 1282, loss is 0.03446494787931442\n",
      "epoch: 3 step: 1283, loss is 0.013023006729781628\n",
      "epoch: 3 step: 1284, loss is 0.002591704949736595\n",
      "epoch: 3 step: 1285, loss is 0.002499906811863184\n",
      "epoch: 3 step: 1286, loss is 0.012522924691438675\n",
      "epoch: 3 step: 1287, loss is 0.3050242066383362\n",
      "epoch: 3 step: 1288, loss is 0.00036307433038018644\n",
      "epoch: 3 step: 1289, loss is 0.002154608489945531\n",
      "epoch: 3 step: 1290, loss is 0.037662021815776825\n",
      "epoch: 3 step: 1291, loss is 0.030151287093758583\n",
      "epoch: 3 step: 1292, loss is 0.008429995737969875\n",
      "epoch: 3 step: 1293, loss is 0.020738471299409866\n",
      "epoch: 3 step: 1294, loss is 0.020039819180965424\n",
      "epoch: 3 step: 1295, loss is 0.05352824926376343\n",
      "epoch: 3 step: 1296, loss is 0.004315356258302927\n",
      "epoch: 3 step: 1297, loss is 0.0060930317267775536\n",
      "epoch: 3 step: 1298, loss is 0.001823187107220292\n",
      "epoch: 3 step: 1299, loss is 0.1305599957704544\n",
      "epoch: 3 step: 1300, loss is 0.0012543948832899332\n",
      "epoch: 3 step: 1301, loss is 0.06758663058280945\n",
      "epoch: 3 step: 1302, loss is 0.009927034378051758\n",
      "epoch: 3 step: 1303, loss is 0.0031075652223080397\n",
      "epoch: 3 step: 1304, loss is 0.03016560710966587\n",
      "epoch: 3 step: 1305, loss is 0.0029157325625419617\n",
      "epoch: 3 step: 1306, loss is 0.08894094824790955\n",
      "epoch: 3 step: 1307, loss is 0.001180916791781783\n",
      "epoch: 3 step: 1308, loss is 0.024194037541747093\n",
      "epoch: 3 step: 1309, loss is 0.0059917462058365345\n",
      "epoch: 3 step: 1310, loss is 0.009113567881286144\n",
      "epoch: 3 step: 1311, loss is 0.008264224976301193\n",
      "epoch: 3 step: 1312, loss is 0.0072481995448470116\n",
      "epoch: 3 step: 1313, loss is 0.1185688003897667\n",
      "epoch: 3 step: 1314, loss is 0.005199451930820942\n",
      "epoch: 3 step: 1315, loss is 0.03798902407288551\n",
      "epoch: 3 step: 1316, loss is 0.0014365852111950517\n",
      "epoch: 3 step: 1317, loss is 0.009772500954568386\n",
      "epoch: 3 step: 1318, loss is 0.0004554999468382448\n",
      "epoch: 3 step: 1319, loss is 0.001118996413424611\n",
      "epoch: 3 step: 1320, loss is 0.0035382204223424196\n",
      "epoch: 3 step: 1321, loss is 0.043808598071336746\n",
      "epoch: 3 step: 1322, loss is 0.14535483717918396\n",
      "epoch: 3 step: 1323, loss is 0.051878251135349274\n",
      "epoch: 3 step: 1324, loss is 0.0033904663287103176\n",
      "epoch: 3 step: 1325, loss is 0.22224688529968262\n",
      "epoch: 3 step: 1326, loss is 0.0011438063811510801\n",
      "epoch: 3 step: 1327, loss is 0.03244489058852196\n",
      "epoch: 3 step: 1328, loss is 0.1858159601688385\n",
      "epoch: 3 step: 1329, loss is 0.0037601443473249674\n",
      "epoch: 3 step: 1330, loss is 0.012430312111973763\n",
      "epoch: 3 step: 1331, loss is 0.045637086033821106\n",
      "epoch: 3 step: 1332, loss is 0.04197879508137703\n",
      "epoch: 3 step: 1333, loss is 0.10955873131752014\n",
      "epoch: 3 step: 1334, loss is 0.01972983218729496\n",
      "epoch: 3 step: 1335, loss is 0.025916241109371185\n",
      "epoch: 3 step: 1336, loss is 0.004405873361974955\n",
      "epoch: 3 step: 1337, loss is 0.012334072031080723\n",
      "epoch: 3 step: 1338, loss is 0.04160697013139725\n",
      "epoch: 3 step: 1339, loss is 0.030752278864383698\n",
      "epoch: 3 step: 1340, loss is 0.060123033821582794\n",
      "epoch: 3 step: 1341, loss is 0.004825504031032324\n",
      "epoch: 3 step: 1342, loss is 0.0034153289161622524\n",
      "epoch: 3 step: 1343, loss is 0.1026930883526802\n",
      "epoch: 3 step: 1344, loss is 0.001338653382845223\n",
      "epoch: 3 step: 1345, loss is 0.0015804123831912875\n",
      "epoch: 3 step: 1346, loss is 0.01024203933775425\n",
      "epoch: 3 step: 1347, loss is 0.05343366786837578\n",
      "epoch: 3 step: 1348, loss is 0.002684982493519783\n",
      "epoch: 3 step: 1349, loss is 0.03169494494795799\n",
      "epoch: 3 step: 1350, loss is 0.0022763116285204887\n",
      "epoch: 3 step: 1351, loss is 0.0033796720672398806\n",
      "epoch: 3 step: 1352, loss is 0.02867923490703106\n",
      "epoch: 3 step: 1353, loss is 0.005463933572173119\n",
      "epoch: 3 step: 1354, loss is 0.002404534723609686\n",
      "epoch: 3 step: 1355, loss is 0.10145203024148941\n",
      "epoch: 3 step: 1356, loss is 0.0013641647528856993\n",
      "epoch: 3 step: 1357, loss is 0.08907759934663773\n",
      "epoch: 3 step: 1358, loss is 0.0004603638080880046\n",
      "epoch: 3 step: 1359, loss is 0.25384074449539185\n",
      "epoch: 3 step: 1360, loss is 0.10206517577171326\n",
      "epoch: 3 step: 1361, loss is 0.11857890337705612\n",
      "epoch: 3 step: 1362, loss is 0.0041129994206130505\n",
      "epoch: 3 step: 1363, loss is 0.20069527626037598\n",
      "epoch: 3 step: 1364, loss is 0.011074868962168694\n",
      "epoch: 3 step: 1365, loss is 0.0012936522252857685\n",
      "epoch: 3 step: 1366, loss is 0.0028989550191909075\n",
      "epoch: 3 step: 1367, loss is 0.0012736729113385081\n",
      "epoch: 3 step: 1368, loss is 0.008089258335530758\n",
      "epoch: 3 step: 1369, loss is 0.27684155106544495\n",
      "epoch: 3 step: 1370, loss is 0.2304522842168808\n",
      "epoch: 3 step: 1371, loss is 0.02054702490568161\n",
      "epoch: 3 step: 1372, loss is 0.01665855012834072\n",
      "epoch: 3 step: 1373, loss is 0.007378047332167625\n",
      "epoch: 3 step: 1374, loss is 0.01782921329140663\n",
      "epoch: 3 step: 1375, loss is 0.001270037260837853\n",
      "epoch: 3 step: 1376, loss is 0.2741106152534485\n",
      "epoch: 3 step: 1377, loss is 0.20740102231502533\n",
      "epoch: 3 step: 1378, loss is 0.06850498914718628\n",
      "epoch: 3 step: 1379, loss is 0.13187123835086823\n",
      "epoch: 3 step: 1380, loss is 0.08509300649166107\n",
      "epoch: 3 step: 1381, loss is 0.0073354970663785934\n",
      "epoch: 3 step: 1382, loss is 0.12717771530151367\n",
      "epoch: 3 step: 1383, loss is 0.19777271151542664\n",
      "epoch: 3 step: 1384, loss is 0.016145342960953712\n",
      "epoch: 3 step: 1385, loss is 0.022534044459462166\n",
      "epoch: 3 step: 1386, loss is 0.19224964082241058\n",
      "epoch: 3 step: 1387, loss is 0.003915587905794382\n",
      "epoch: 3 step: 1388, loss is 0.12858149409294128\n",
      "epoch: 3 step: 1389, loss is 0.010053149424493313\n",
      "epoch: 3 step: 1390, loss is 0.047775998711586\n",
      "epoch: 3 step: 1391, loss is 0.026778582483530045\n",
      "epoch: 3 step: 1392, loss is 0.03523183614015579\n",
      "epoch: 3 step: 1393, loss is 0.049998585134744644\n",
      "epoch: 3 step: 1394, loss is 0.00463046645745635\n",
      "epoch: 3 step: 1395, loss is 0.061594512313604355\n",
      "epoch: 3 step: 1396, loss is 0.04875578731298447\n",
      "epoch: 3 step: 1397, loss is 0.05743318423628807\n",
      "epoch: 3 step: 1398, loss is 0.08139096945524216\n",
      "epoch: 3 step: 1399, loss is 0.031348712742328644\n",
      "epoch: 3 step: 1400, loss is 0.01805652305483818\n",
      "epoch: 3 step: 1401, loss is 0.02906588464975357\n",
      "epoch: 3 step: 1402, loss is 0.02394801564514637\n",
      "epoch: 3 step: 1403, loss is 0.009781022556126118\n",
      "epoch: 3 step: 1404, loss is 0.00951523706316948\n",
      "epoch: 3 step: 1405, loss is 0.15941430628299713\n",
      "epoch: 3 step: 1406, loss is 0.041140224784612656\n",
      "epoch: 3 step: 1407, loss is 0.030210169032216072\n",
      "epoch: 3 step: 1408, loss is 0.18297968804836273\n",
      "epoch: 3 step: 1409, loss is 0.02561223693192005\n",
      "epoch: 3 step: 1410, loss is 0.0054502710700035095\n",
      "epoch: 3 step: 1411, loss is 0.012821951881051064\n",
      "epoch: 3 step: 1412, loss is 0.11583869159221649\n",
      "epoch: 3 step: 1413, loss is 0.0030113202519714832\n",
      "epoch: 3 step: 1414, loss is 0.015114175155758858\n",
      "epoch: 3 step: 1415, loss is 0.014499467797577381\n",
      "epoch: 3 step: 1416, loss is 0.012035775929689407\n",
      "epoch: 3 step: 1417, loss is 0.023636912927031517\n",
      "epoch: 3 step: 1418, loss is 0.008917867206037045\n",
      "epoch: 3 step: 1419, loss is 0.006400578655302525\n",
      "epoch: 3 step: 1420, loss is 0.05330580472946167\n",
      "epoch: 3 step: 1421, loss is 0.0025594725739210844\n",
      "epoch: 3 step: 1422, loss is 0.0061997403390705585\n",
      "epoch: 3 step: 1423, loss is 0.09405113756656647\n",
      "epoch: 3 step: 1424, loss is 0.2600928246974945\n",
      "epoch: 3 step: 1425, loss is 0.021089795976877213\n",
      "epoch: 3 step: 1426, loss is 0.009114173240959644\n",
      "epoch: 3 step: 1427, loss is 0.002633011434227228\n",
      "epoch: 3 step: 1428, loss is 0.08573487401008606\n",
      "epoch: 3 step: 1429, loss is 0.0010691573843359947\n",
      "epoch: 3 step: 1430, loss is 0.0011070356704294682\n",
      "epoch: 3 step: 1431, loss is 0.005458532366901636\n",
      "epoch: 3 step: 1432, loss is 0.00199723057448864\n",
      "epoch: 3 step: 1433, loss is 0.0015550096286460757\n",
      "epoch: 3 step: 1434, loss is 0.0011333341244608164\n",
      "epoch: 3 step: 1435, loss is 0.009896798059344292\n",
      "epoch: 3 step: 1436, loss is 0.06410441547632217\n",
      "epoch: 3 step: 1437, loss is 0.01918472722172737\n",
      "epoch: 3 step: 1438, loss is 0.00912123080343008\n",
      "epoch: 3 step: 1439, loss is 0.004706706386059523\n",
      "epoch: 3 step: 1440, loss is 0.0013519985368475318\n",
      "epoch: 3 step: 1441, loss is 0.05236450210213661\n",
      "epoch: 3 step: 1442, loss is 0.018297869712114334\n",
      "epoch: 3 step: 1443, loss is 0.006134688854217529\n",
      "epoch: 3 step: 1444, loss is 0.010411123745143414\n",
      "epoch: 3 step: 1445, loss is 0.0004953301977366209\n",
      "epoch: 3 step: 1446, loss is 0.17711500823497772\n",
      "epoch: 3 step: 1447, loss is 0.008677595295011997\n",
      "epoch: 3 step: 1448, loss is 0.0027911895886063576\n",
      "epoch: 3 step: 1449, loss is 0.004156359937041998\n",
      "epoch: 3 step: 1450, loss is 0.001177427126094699\n",
      "epoch: 3 step: 1451, loss is 0.040708452463150024\n",
      "epoch: 3 step: 1452, loss is 0.0012952794786542654\n",
      "epoch: 3 step: 1453, loss is 0.10618843138217926\n",
      "epoch: 3 step: 1454, loss is 0.003075761953368783\n",
      "epoch: 3 step: 1455, loss is 0.005022912751883268\n",
      "epoch: 3 step: 1456, loss is 0.01829356513917446\n",
      "epoch: 3 step: 1457, loss is 0.003937110770493746\n",
      "epoch: 3 step: 1458, loss is 0.013919034041464329\n",
      "epoch: 3 step: 1459, loss is 0.04517900571227074\n",
      "epoch: 3 step: 1460, loss is 0.1016211286187172\n",
      "epoch: 3 step: 1461, loss is 0.056916963309049606\n",
      "epoch: 3 step: 1462, loss is 0.002381248865276575\n",
      "epoch: 3 step: 1463, loss is 0.0894196406006813\n",
      "epoch: 3 step: 1464, loss is 0.20508912205696106\n",
      "epoch: 3 step: 1465, loss is 0.00503971241414547\n",
      "epoch: 3 step: 1466, loss is 0.0005622610915452242\n",
      "epoch: 3 step: 1467, loss is 0.0004835712898056954\n",
      "epoch: 3 step: 1468, loss is 0.04767336696386337\n",
      "epoch: 3 step: 1469, loss is 0.010012701153755188\n",
      "epoch: 3 step: 1470, loss is 0.012845216318964958\n",
      "epoch: 3 step: 1471, loss is 0.04204023256897926\n",
      "epoch: 3 step: 1472, loss is 0.018387360498309135\n",
      "epoch: 3 step: 1473, loss is 0.006315620616078377\n",
      "epoch: 3 step: 1474, loss is 0.012444062158465385\n",
      "epoch: 3 step: 1475, loss is 0.010171569883823395\n",
      "epoch: 3 step: 1476, loss is 0.004544187802821398\n",
      "epoch: 3 step: 1477, loss is 0.01029857899993658\n",
      "epoch: 3 step: 1478, loss is 0.019483128562569618\n",
      "epoch: 3 step: 1479, loss is 0.010836572386324406\n",
      "epoch: 3 step: 1480, loss is 0.0016239165561273694\n",
      "epoch: 3 step: 1481, loss is 0.00022915205045137554\n",
      "epoch: 3 step: 1482, loss is 0.21612976491451263\n",
      "epoch: 3 step: 1483, loss is 0.056844647973775864\n",
      "epoch: 3 step: 1484, loss is 0.05271067097783089\n",
      "epoch: 3 step: 1485, loss is 0.10146832466125488\n",
      "epoch: 3 step: 1486, loss is 0.03669538348913193\n",
      "epoch: 3 step: 1487, loss is 0.04456692934036255\n",
      "epoch: 3 step: 1488, loss is 0.099527508020401\n",
      "epoch: 3 step: 1489, loss is 0.0002803398238029331\n",
      "epoch: 3 step: 1490, loss is 0.00550285167992115\n",
      "epoch: 3 step: 1491, loss is 0.05310322344303131\n",
      "epoch: 3 step: 1492, loss is 0.08713683485984802\n",
      "epoch: 3 step: 1493, loss is 0.006148821674287319\n",
      "epoch: 3 step: 1494, loss is 0.01662457548081875\n",
      "epoch: 3 step: 1495, loss is 0.43852442502975464\n",
      "epoch: 3 step: 1496, loss is 0.006020467262715101\n",
      "epoch: 3 step: 1497, loss is 0.01711065135896206\n",
      "epoch: 3 step: 1498, loss is 0.027355505153536797\n",
      "epoch: 3 step: 1499, loss is 0.02236594446003437\n",
      "epoch: 3 step: 1500, loss is 0.07546428591012955\n",
      "epoch: 3 step: 1501, loss is 0.017298106104135513\n",
      "epoch: 3 step: 1502, loss is 0.10898566991090775\n",
      "epoch: 3 step: 1503, loss is 0.07259466499090195\n",
      "epoch: 3 step: 1504, loss is 0.024042101576924324\n",
      "epoch: 3 step: 1505, loss is 0.007049428764730692\n",
      "epoch: 3 step: 1506, loss is 0.001397031475789845\n",
      "epoch: 3 step: 1507, loss is 0.0029888134449720383\n",
      "epoch: 3 step: 1508, loss is 0.13991810381412506\n",
      "epoch: 3 step: 1509, loss is 0.18193656206130981\n",
      "epoch: 3 step: 1510, loss is 0.00757629843428731\n",
      "epoch: 3 step: 1511, loss is 0.003352156840264797\n",
      "epoch: 3 step: 1512, loss is 0.018303362652659416\n",
      "epoch: 3 step: 1513, loss is 0.03746030107140541\n",
      "epoch: 3 step: 1514, loss is 0.1126028373837471\n",
      "epoch: 3 step: 1515, loss is 0.09333579987287521\n",
      "epoch: 3 step: 1516, loss is 0.10969727486371994\n",
      "epoch: 3 step: 1517, loss is 0.0006553137791343033\n",
      "epoch: 3 step: 1518, loss is 0.04736972972750664\n",
      "epoch: 3 step: 1519, loss is 0.0721013993024826\n",
      "epoch: 3 step: 1520, loss is 0.001058953464962542\n",
      "epoch: 3 step: 1521, loss is 0.010513193905353546\n",
      "epoch: 3 step: 1522, loss is 0.02423836477100849\n",
      "epoch: 3 step: 1523, loss is 0.003586712060496211\n",
      "epoch: 3 step: 1524, loss is 0.0037779787089675665\n",
      "epoch: 3 step: 1525, loss is 0.1583409160375595\n",
      "epoch: 3 step: 1526, loss is 0.035186268389225006\n",
      "epoch: 3 step: 1527, loss is 0.212262362241745\n",
      "epoch: 3 step: 1528, loss is 0.00998757965862751\n",
      "epoch: 3 step: 1529, loss is 0.01149235013872385\n",
      "epoch: 3 step: 1530, loss is 0.02007247880101204\n",
      "epoch: 3 step: 1531, loss is 0.009562885388731956\n",
      "epoch: 3 step: 1532, loss is 0.03473769500851631\n",
      "epoch: 3 step: 1533, loss is 0.2276485115289688\n",
      "epoch: 3 step: 1534, loss is 0.017785536125302315\n",
      "epoch: 3 step: 1535, loss is 0.009584618732333183\n",
      "epoch: 3 step: 1536, loss is 0.032487642019987106\n",
      "epoch: 3 step: 1537, loss is 0.2637498080730438\n",
      "epoch: 3 step: 1538, loss is 0.023488324135541916\n",
      "epoch: 3 step: 1539, loss is 0.02535131387412548\n",
      "epoch: 3 step: 1540, loss is 0.04741327464580536\n",
      "epoch: 3 step: 1541, loss is 0.01359574031084776\n",
      "epoch: 3 step: 1542, loss is 0.06231178343296051\n",
      "epoch: 3 step: 1543, loss is 0.059713080525398254\n",
      "epoch: 3 step: 1544, loss is 0.009889605455100536\n",
      "epoch: 3 step: 1545, loss is 0.004397838842123747\n",
      "epoch: 3 step: 1546, loss is 0.003440534695982933\n",
      "epoch: 3 step: 1547, loss is 0.11415915936231613\n",
      "epoch: 3 step: 1548, loss is 0.12362508475780487\n",
      "epoch: 3 step: 1549, loss is 0.0459715910255909\n",
      "epoch: 3 step: 1550, loss is 0.03297606110572815\n",
      "epoch: 3 step: 1551, loss is 0.0050917696207761765\n",
      "epoch: 3 step: 1552, loss is 0.018792152404785156\n",
      "epoch: 3 step: 1553, loss is 0.006106446962803602\n",
      "epoch: 3 step: 1554, loss is 0.04255777597427368\n",
      "epoch: 3 step: 1555, loss is 0.12588517367839813\n",
      "epoch: 3 step: 1556, loss is 0.24439944326877594\n",
      "epoch: 3 step: 1557, loss is 0.054585784673690796\n",
      "epoch: 3 step: 1558, loss is 0.04753885045647621\n",
      "epoch: 3 step: 1559, loss is 0.010050772689282894\n",
      "epoch: 3 step: 1560, loss is 0.05402478948235512\n",
      "epoch: 3 step: 1561, loss is 0.01903214491903782\n",
      "epoch: 3 step: 1562, loss is 0.018981529399752617\n",
      "epoch: 3 step: 1563, loss is 0.00556554039940238\n",
      "epoch: 3 step: 1564, loss is 0.005717883352190256\n",
      "epoch: 3 step: 1565, loss is 0.027148764580488205\n",
      "epoch: 3 step: 1566, loss is 0.0028732572682201862\n",
      "epoch: 3 step: 1567, loss is 0.07485317438840866\n",
      "epoch: 3 step: 1568, loss is 0.0014244811609387398\n",
      "epoch: 3 step: 1569, loss is 0.004342725034803152\n",
      "epoch: 3 step: 1570, loss is 0.09019778668880463\n",
      "epoch: 3 step: 1571, loss is 0.032368335872888565\n",
      "epoch: 3 step: 1572, loss is 0.09689164906740189\n",
      "epoch: 3 step: 1573, loss is 0.02180453948676586\n",
      "epoch: 3 step: 1574, loss is 0.06174987927079201\n",
      "epoch: 3 step: 1575, loss is 0.002753507811576128\n",
      "epoch: 3 step: 1576, loss is 0.13866165280342102\n",
      "epoch: 3 step: 1577, loss is 0.01042268704622984\n",
      "epoch: 3 step: 1578, loss is 0.013241393491625786\n",
      "epoch: 3 step: 1579, loss is 0.004439281299710274\n",
      "epoch: 3 step: 1580, loss is 0.029657118022441864\n",
      "epoch: 3 step: 1581, loss is 0.002484477125108242\n",
      "epoch: 3 step: 1582, loss is 0.014013790525496006\n",
      "epoch: 3 step: 1583, loss is 0.05307899788022041\n",
      "epoch: 3 step: 1584, loss is 0.01447962038218975\n",
      "epoch: 3 step: 1585, loss is 0.09613439440727234\n",
      "epoch: 3 step: 1586, loss is 0.0033289450220763683\n",
      "epoch: 3 step: 1587, loss is 0.015109682455658913\n",
      "epoch: 3 step: 1588, loss is 0.02427002228796482\n",
      "epoch: 3 step: 1589, loss is 0.06852778792381287\n",
      "epoch: 3 step: 1590, loss is 0.007653503678739071\n",
      "epoch: 3 step: 1591, loss is 0.005977504886686802\n",
      "epoch: 3 step: 1592, loss is 0.0015294095501303673\n",
      "epoch: 3 step: 1593, loss is 0.01672171987593174\n",
      "epoch: 3 step: 1594, loss is 0.0034819869324564934\n",
      "epoch: 3 step: 1595, loss is 0.08462245017290115\n",
      "epoch: 3 step: 1596, loss is 0.12444594502449036\n",
      "epoch: 3 step: 1597, loss is 0.0018768816953524947\n",
      "epoch: 3 step: 1598, loss is 0.002038596896454692\n",
      "epoch: 3 step: 1599, loss is 0.025714630261063576\n",
      "epoch: 3 step: 1600, loss is 0.0005837476346641779\n",
      "epoch: 3 step: 1601, loss is 0.099979467689991\n",
      "epoch: 3 step: 1602, loss is 0.013962536118924618\n",
      "epoch: 3 step: 1603, loss is 0.00238477298989892\n",
      "epoch: 3 step: 1604, loss is 0.0005517046665772796\n",
      "epoch: 3 step: 1605, loss is 0.0069578783586621284\n",
      "epoch: 3 step: 1606, loss is 0.00933461356908083\n",
      "epoch: 3 step: 1607, loss is 0.0014774681767448783\n",
      "epoch: 3 step: 1608, loss is 0.010855383239686489\n",
      "epoch: 3 step: 1609, loss is 0.03933556005358696\n",
      "epoch: 3 step: 1610, loss is 0.02568494714796543\n",
      "epoch: 3 step: 1611, loss is 0.016814902424812317\n",
      "epoch: 3 step: 1612, loss is 0.010163133963942528\n",
      "epoch: 3 step: 1613, loss is 0.2099730521440506\n",
      "epoch: 3 step: 1614, loss is 0.023575343191623688\n",
      "epoch: 3 step: 1615, loss is 0.04982713982462883\n",
      "epoch: 3 step: 1616, loss is 0.0006105227512307465\n",
      "epoch: 3 step: 1617, loss is 0.0006777092930860817\n",
      "epoch: 3 step: 1618, loss is 0.004032354336231947\n",
      "epoch: 3 step: 1619, loss is 0.0006209908169694245\n",
      "epoch: 3 step: 1620, loss is 0.03079502284526825\n",
      "epoch: 3 step: 1621, loss is 0.01564924418926239\n",
      "epoch: 3 step: 1622, loss is 0.01081810425966978\n",
      "epoch: 3 step: 1623, loss is 0.02870776131749153\n",
      "epoch: 3 step: 1624, loss is 0.15554268658161163\n",
      "epoch: 3 step: 1625, loss is 0.001406912924721837\n",
      "epoch: 3 step: 1626, loss is 0.0017682169564068317\n",
      "epoch: 3 step: 1627, loss is 0.0018496543634682894\n",
      "epoch: 3 step: 1628, loss is 9.107648656936362e-05\n",
      "epoch: 3 step: 1629, loss is 0.007264028303325176\n",
      "epoch: 3 step: 1630, loss is 0.0003267347638029605\n",
      "epoch: 3 step: 1631, loss is 0.0014863347169011831\n",
      "epoch: 3 step: 1632, loss is 0.0030424480792135\n",
      "epoch: 3 step: 1633, loss is 0.006197099108248949\n",
      "epoch: 3 step: 1634, loss is 0.0008870388846844435\n",
      "epoch: 3 step: 1635, loss is 0.002445961581543088\n",
      "epoch: 3 step: 1636, loss is 0.005383496638387442\n",
      "epoch: 3 step: 1637, loss is 0.09977784007787704\n",
      "epoch: 3 step: 1638, loss is 0.031093886122107506\n",
      "epoch: 3 step: 1639, loss is 0.05259597301483154\n",
      "epoch: 3 step: 1640, loss is 0.014243041165173054\n",
      "epoch: 3 step: 1641, loss is 0.006892641074955463\n",
      "epoch: 3 step: 1642, loss is 0.09889662265777588\n",
      "epoch: 3 step: 1643, loss is 0.046408720314502716\n",
      "epoch: 3 step: 1644, loss is 0.09154202044010162\n",
      "epoch: 3 step: 1645, loss is 0.011623801663517952\n",
      "epoch: 3 step: 1646, loss is 0.028249461203813553\n",
      "epoch: 3 step: 1647, loss is 0.0020588203333318233\n",
      "epoch: 3 step: 1648, loss is 0.0013657126110047102\n",
      "epoch: 3 step: 1649, loss is 0.01957475021481514\n",
      "epoch: 3 step: 1650, loss is 0.021016135811805725\n",
      "epoch: 3 step: 1651, loss is 0.05027946084737778\n",
      "epoch: 3 step: 1652, loss is 0.002701804507523775\n",
      "epoch: 3 step: 1653, loss is 0.03262174874544144\n",
      "epoch: 3 step: 1654, loss is 0.2884323000907898\n",
      "epoch: 3 step: 1655, loss is 0.0012866404140368104\n",
      "epoch: 3 step: 1656, loss is 0.0022707819007337093\n",
      "epoch: 3 step: 1657, loss is 0.020310314372181892\n",
      "epoch: 3 step: 1658, loss is 0.02709842659533024\n",
      "epoch: 3 step: 1659, loss is 0.05370509624481201\n",
      "epoch: 3 step: 1660, loss is 0.0010536236222833395\n",
      "epoch: 3 step: 1661, loss is 0.001305935438722372\n",
      "epoch: 3 step: 1662, loss is 0.006915456149727106\n",
      "epoch: 3 step: 1663, loss is 0.074470654129982\n",
      "epoch: 3 step: 1664, loss is 0.00037503583007492125\n",
      "epoch: 3 step: 1665, loss is 0.05402977392077446\n",
      "epoch: 3 step: 1666, loss is 0.002023830311372876\n",
      "epoch: 3 step: 1667, loss is 0.003336910158395767\n",
      "epoch: 3 step: 1668, loss is 0.08899480849504471\n",
      "epoch: 3 step: 1669, loss is 0.002125223632901907\n",
      "epoch: 3 step: 1670, loss is 0.053844138979911804\n",
      "epoch: 3 step: 1671, loss is 0.015753276646137238\n",
      "epoch: 3 step: 1672, loss is 0.001189401838928461\n",
      "epoch: 3 step: 1673, loss is 0.10792633146047592\n",
      "epoch: 3 step: 1674, loss is 0.14298978447914124\n",
      "epoch: 3 step: 1675, loss is 0.0032255861442536116\n",
      "epoch: 3 step: 1676, loss is 0.13019487261772156\n",
      "epoch: 3 step: 1677, loss is 0.005606153979897499\n",
      "epoch: 3 step: 1678, loss is 0.06523825973272324\n",
      "epoch: 3 step: 1679, loss is 0.0015116434078663588\n",
      "epoch: 3 step: 1680, loss is 0.0029008130077272654\n",
      "epoch: 3 step: 1681, loss is 0.01867852918803692\n",
      "epoch: 3 step: 1682, loss is 0.1336563229560852\n",
      "epoch: 3 step: 1683, loss is 9.419339039595798e-05\n",
      "epoch: 3 step: 1684, loss is 0.10807471722364426\n",
      "epoch: 3 step: 1685, loss is 0.08997953683137894\n",
      "epoch: 3 step: 1686, loss is 0.0006156692397780716\n",
      "epoch: 3 step: 1687, loss is 0.0204866174608469\n",
      "epoch: 3 step: 1688, loss is 0.03833705931901932\n",
      "epoch: 3 step: 1689, loss is 0.033024128526449203\n",
      "epoch: 3 step: 1690, loss is 0.0007588136359117925\n",
      "epoch: 3 step: 1691, loss is 0.01324845477938652\n",
      "epoch: 3 step: 1692, loss is 0.010927754454314709\n",
      "epoch: 3 step: 1693, loss is 0.03261447325348854\n",
      "epoch: 3 step: 1694, loss is 0.16192954778671265\n",
      "epoch: 3 step: 1695, loss is 0.0017488505691289902\n",
      "epoch: 3 step: 1696, loss is 0.08726368844509125\n",
      "epoch: 3 step: 1697, loss is 0.0008565598982386291\n",
      "epoch: 3 step: 1698, loss is 0.02228689007461071\n",
      "epoch: 3 step: 1699, loss is 0.0024615449365228415\n",
      "epoch: 3 step: 1700, loss is 0.017644964158535004\n",
      "epoch: 3 step: 1701, loss is 0.0036314770113676786\n",
      "epoch: 3 step: 1702, loss is 0.002167612547054887\n",
      "epoch: 3 step: 1703, loss is 0.06571018695831299\n",
      "epoch: 3 step: 1704, loss is 0.2052185982465744\n",
      "epoch: 3 step: 1705, loss is 0.0019258289830759168\n",
      "epoch: 3 step: 1706, loss is 0.025614174082875252\n",
      "epoch: 3 step: 1707, loss is 0.0003585807862691581\n",
      "epoch: 3 step: 1708, loss is 0.001109651755541563\n",
      "epoch: 3 step: 1709, loss is 0.0010525871766731143\n",
      "epoch: 3 step: 1710, loss is 0.0027440274134278297\n",
      "epoch: 3 step: 1711, loss is 0.09238849580287933\n",
      "epoch: 3 step: 1712, loss is 0.003676279913634062\n",
      "epoch: 3 step: 1713, loss is 0.05174478143453598\n",
      "epoch: 3 step: 1714, loss is 0.23587819933891296\n",
      "epoch: 3 step: 1715, loss is 0.0015504603506997228\n",
      "epoch: 3 step: 1716, loss is 0.014571757055819035\n",
      "epoch: 3 step: 1717, loss is 0.007984696887433529\n",
      "epoch: 3 step: 1718, loss is 0.019011277705430984\n",
      "epoch: 3 step: 1719, loss is 0.0032488787546753883\n",
      "epoch: 3 step: 1720, loss is 0.10832969844341278\n",
      "epoch: 3 step: 1721, loss is 0.25611454248428345\n",
      "epoch: 3 step: 1722, loss is 0.0007422073977068067\n",
      "epoch: 3 step: 1723, loss is 0.012302933260798454\n",
      "epoch: 3 step: 1724, loss is 0.0038526966236531734\n",
      "epoch: 3 step: 1725, loss is 0.026764526963233948\n",
      "epoch: 3 step: 1726, loss is 0.007878267206251621\n",
      "epoch: 3 step: 1727, loss is 0.11401215940713882\n",
      "epoch: 3 step: 1728, loss is 0.03226335346698761\n",
      "epoch: 3 step: 1729, loss is 0.009511894546449184\n",
      "epoch: 3 step: 1730, loss is 0.006089883390814066\n",
      "epoch: 3 step: 1731, loss is 0.21510878205299377\n",
      "epoch: 3 step: 1732, loss is 0.025121070444583893\n",
      "epoch: 3 step: 1733, loss is 0.006655133329331875\n",
      "epoch: 3 step: 1734, loss is 0.030187129974365234\n",
      "epoch: 3 step: 1735, loss is 0.050008341670036316\n",
      "epoch: 3 step: 1736, loss is 0.005135932005941868\n",
      "epoch: 3 step: 1737, loss is 0.057489775121212006\n",
      "epoch: 3 step: 1738, loss is 0.16414105892181396\n",
      "epoch: 3 step: 1739, loss is 0.0103289894759655\n",
      "epoch: 3 step: 1740, loss is 0.0024649121332913637\n",
      "epoch: 3 step: 1741, loss is 0.0014665148919448256\n",
      "epoch: 3 step: 1742, loss is 0.18306557834148407\n",
      "epoch: 3 step: 1743, loss is 0.05242302641272545\n",
      "epoch: 3 step: 1744, loss is 0.0033493521623313427\n",
      "epoch: 3 step: 1745, loss is 0.021025359630584717\n",
      "epoch: 3 step: 1746, loss is 0.08890248090028763\n",
      "epoch: 3 step: 1747, loss is 0.026671845465898514\n",
      "epoch: 3 step: 1748, loss is 0.09502856433391571\n",
      "epoch: 3 step: 1749, loss is 0.060810577124357224\n",
      "epoch: 3 step: 1750, loss is 0.006100899539887905\n",
      "epoch: 3 step: 1751, loss is 0.09334254264831543\n",
      "epoch: 3 step: 1752, loss is 0.03831300511956215\n",
      "epoch: 3 step: 1753, loss is 0.009135165251791477\n",
      "epoch: 3 step: 1754, loss is 0.1483108252286911\n",
      "epoch: 3 step: 1755, loss is 0.03992607444524765\n",
      "epoch: 3 step: 1756, loss is 0.009800510480999947\n",
      "epoch: 3 step: 1757, loss is 0.027889931574463844\n",
      "epoch: 3 step: 1758, loss is 0.001487674773670733\n",
      "epoch: 3 step: 1759, loss is 0.03648628294467926\n",
      "epoch: 3 step: 1760, loss is 0.06950697302818298\n",
      "epoch: 3 step: 1761, loss is 0.002913899254053831\n",
      "epoch: 3 step: 1762, loss is 0.024999793618917465\n",
      "epoch: 3 step: 1763, loss is 0.009941289201378822\n",
      "epoch: 3 step: 1764, loss is 0.00359136494807899\n",
      "epoch: 3 step: 1765, loss is 0.17495998740196228\n",
      "epoch: 3 step: 1766, loss is 0.005509877577424049\n",
      "epoch: 3 step: 1767, loss is 0.004979186225682497\n",
      "epoch: 3 step: 1768, loss is 0.0033108177594840527\n",
      "epoch: 3 step: 1769, loss is 0.10297904908657074\n",
      "epoch: 3 step: 1770, loss is 0.00841743964701891\n",
      "epoch: 3 step: 1771, loss is 0.0023704348132014275\n",
      "epoch: 3 step: 1772, loss is 0.13787193596363068\n",
      "epoch: 3 step: 1773, loss is 0.002354420255869627\n",
      "epoch: 3 step: 1774, loss is 0.00639494089409709\n",
      "epoch: 3 step: 1775, loss is 0.0026244374457746744\n",
      "epoch: 3 step: 1776, loss is 0.0020701864268630743\n",
      "epoch: 3 step: 1777, loss is 0.08096723258495331\n",
      "epoch: 3 step: 1778, loss is 0.0017751934938132763\n",
      "epoch: 3 step: 1779, loss is 0.011063175275921822\n",
      "epoch: 3 step: 1780, loss is 0.056422438472509384\n",
      "epoch: 3 step: 1781, loss is 0.0005475016660057008\n",
      "epoch: 3 step: 1782, loss is 0.017650913447141647\n",
      "epoch: 3 step: 1783, loss is 0.07476235926151276\n",
      "epoch: 3 step: 1784, loss is 0.021905042231082916\n",
      "epoch: 3 step: 1785, loss is 0.0014233221299946308\n",
      "epoch: 3 step: 1786, loss is 0.05208049714565277\n",
      "epoch: 3 step: 1787, loss is 0.004538895096629858\n",
      "epoch: 3 step: 1788, loss is 0.06636689603328705\n",
      "epoch: 3 step: 1789, loss is 0.024893194437026978\n",
      "epoch: 3 step: 1790, loss is 0.018749048933386803\n",
      "epoch: 3 step: 1791, loss is 0.0031564037781208754\n",
      "epoch: 3 step: 1792, loss is 0.0048611778765916824\n",
      "epoch: 3 step: 1793, loss is 0.0029019799549132586\n",
      "epoch: 3 step: 1794, loss is 0.027828006073832512\n",
      "epoch: 3 step: 1795, loss is 0.03444290533661842\n",
      "epoch: 3 step: 1796, loss is 0.08399321138858795\n",
      "epoch: 3 step: 1797, loss is 0.0011564918095245957\n",
      "epoch: 3 step: 1798, loss is 0.035977303981781006\n",
      "epoch: 3 step: 1799, loss is 0.21597880125045776\n",
      "epoch: 3 step: 1800, loss is 0.0020399796776473522\n",
      "epoch: 3 step: 1801, loss is 0.07084046304225922\n",
      "epoch: 3 step: 1802, loss is 0.022220104932785034\n",
      "epoch: 3 step: 1803, loss is 0.0009140679030679166\n",
      "epoch: 3 step: 1804, loss is 0.012973103672266006\n",
      "epoch: 3 step: 1805, loss is 0.003894210560247302\n",
      "epoch: 3 step: 1806, loss is 0.007902663201093674\n",
      "epoch: 3 step: 1807, loss is 0.005971445702016354\n",
      "epoch: 3 step: 1808, loss is 0.005411227699369192\n",
      "epoch: 3 step: 1809, loss is 0.024897951632738113\n",
      "epoch: 3 step: 1810, loss is 0.018466461449861526\n",
      "epoch: 3 step: 1811, loss is 0.002088249661028385\n",
      "epoch: 3 step: 1812, loss is 0.0017522159032523632\n",
      "epoch: 3 step: 1813, loss is 0.13459017872810364\n",
      "epoch: 3 step: 1814, loss is 0.002026778180152178\n",
      "epoch: 3 step: 1815, loss is 0.07971964031457901\n",
      "epoch: 3 step: 1816, loss is 0.0772097185254097\n",
      "epoch: 3 step: 1817, loss is 0.03550444543361664\n",
      "epoch: 3 step: 1818, loss is 0.02771582454442978\n",
      "epoch: 3 step: 1819, loss is 0.35713690519332886\n",
      "epoch: 3 step: 1820, loss is 0.013992249965667725\n",
      "epoch: 3 step: 1821, loss is 0.0005783205851912498\n",
      "epoch: 3 step: 1822, loss is 0.12173540145158768\n",
      "epoch: 3 step: 1823, loss is 0.12334003299474716\n",
      "epoch: 3 step: 1824, loss is 0.1533651500940323\n",
      "epoch: 3 step: 1825, loss is 0.007621626369655132\n",
      "epoch: 3 step: 1826, loss is 0.06378260254859924\n",
      "epoch: 3 step: 1827, loss is 0.0018949367804452777\n",
      "epoch: 3 step: 1828, loss is 0.010449899360537529\n",
      "epoch: 3 step: 1829, loss is 0.0029255393892526627\n",
      "epoch: 3 step: 1830, loss is 0.06872986257076263\n",
      "epoch: 3 step: 1831, loss is 0.0066684517078101635\n",
      "epoch: 3 step: 1832, loss is 0.008167685940861702\n",
      "epoch: 3 step: 1833, loss is 0.0007782863103784621\n",
      "epoch: 3 step: 1834, loss is 0.11239423602819443\n",
      "epoch: 3 step: 1835, loss is 0.001209718408063054\n",
      "epoch: 3 step: 1836, loss is 0.13014450669288635\n",
      "epoch: 3 step: 1837, loss is 0.020247573032975197\n",
      "epoch: 3 step: 1838, loss is 0.04632114991545677\n",
      "epoch: 3 step: 1839, loss is 0.0017856867052614689\n",
      "epoch: 3 step: 1840, loss is 0.003609519451856613\n",
      "epoch: 3 step: 1841, loss is 0.00899742916226387\n",
      "epoch: 3 step: 1842, loss is 0.006297403015196323\n",
      "epoch: 3 step: 1843, loss is 0.10268480330705643\n",
      "epoch: 3 step: 1844, loss is 0.017398426309227943\n",
      "epoch: 3 step: 1845, loss is 0.0029154049698263407\n",
      "epoch: 3 step: 1846, loss is 0.002372343558818102\n",
      "epoch: 3 step: 1847, loss is 0.13197161257266998\n",
      "epoch: 3 step: 1848, loss is 0.24256768822669983\n",
      "epoch: 3 step: 1849, loss is 0.05260072648525238\n",
      "epoch: 3 step: 1850, loss is 0.01377897709608078\n",
      "epoch: 3 step: 1851, loss is 0.006073670461773872\n",
      "epoch: 3 step: 1852, loss is 0.22714343667030334\n",
      "epoch: 3 step: 1853, loss is 0.030239643529057503\n",
      "epoch: 3 step: 1854, loss is 0.21844802796840668\n",
      "epoch: 3 step: 1855, loss is 0.03592986613512039\n",
      "epoch: 3 step: 1856, loss is 0.13364866375923157\n",
      "epoch: 3 step: 1857, loss is 0.05287521705031395\n",
      "epoch: 3 step: 1858, loss is 0.1383233368396759\n",
      "epoch: 3 step: 1859, loss is 0.04319918155670166\n",
      "epoch: 3 step: 1860, loss is 0.01126137375831604\n",
      "epoch: 3 step: 1861, loss is 0.02641407400369644\n",
      "epoch: 3 step: 1862, loss is 0.054387737065553665\n",
      "epoch: 3 step: 1863, loss is 0.012742002494633198\n",
      "epoch: 3 step: 1864, loss is 0.02056359499692917\n",
      "epoch: 3 step: 1865, loss is 0.021519247442483902\n",
      "epoch: 3 step: 1866, loss is 0.02312828041613102\n",
      "epoch: 3 step: 1867, loss is 0.01993926614522934\n",
      "epoch: 3 step: 1868, loss is 0.06484099477529526\n",
      "epoch: 3 step: 1869, loss is 0.00853359792381525\n",
      "epoch: 3 step: 1870, loss is 0.006342736538499594\n",
      "epoch: 3 step: 1871, loss is 0.003940124064683914\n",
      "epoch: 3 step: 1872, loss is 0.013718253932893276\n",
      "epoch: 3 step: 1873, loss is 0.0059129828587174416\n",
      "epoch: 3 step: 1874, loss is 0.0030967413913458586\n",
      "epoch: 3 step: 1875, loss is 0.007044709753245115\n",
      "Train epoch time: 14062.084 ms, per step time: 7.500 ms\n",
      "epoch: 4 step: 1, loss is 0.18007414042949677\n",
      "epoch: 4 step: 2, loss is 0.04596775397658348\n",
      "epoch: 4 step: 3, loss is 0.10723108053207397\n",
      "epoch: 4 step: 4, loss is 0.0005712855490855873\n",
      "epoch: 4 step: 5, loss is 0.001194668817333877\n",
      "epoch: 4 step: 6, loss is 0.27034512162208557\n",
      "epoch: 4 step: 7, loss is 0.003145454451441765\n",
      "epoch: 4 step: 8, loss is 0.03354749456048012\n",
      "epoch: 4 step: 9, loss is 0.013313773088157177\n",
      "epoch: 4 step: 10, loss is 0.01385442353785038\n",
      "epoch: 4 step: 11, loss is 0.014500202611088753\n",
      "epoch: 4 step: 12, loss is 0.0037637162022292614\n",
      "epoch: 4 step: 13, loss is 0.006931712385267019\n",
      "epoch: 4 step: 14, loss is 0.02734750509262085\n",
      "epoch: 4 step: 15, loss is 0.13140827417373657\n",
      "epoch: 4 step: 16, loss is 0.012782999314367771\n",
      "epoch: 4 step: 17, loss is 0.0653761699795723\n",
      "epoch: 4 step: 18, loss is 0.007771807722747326\n",
      "epoch: 4 step: 19, loss is 0.008939754217863083\n",
      "epoch: 4 step: 20, loss is 0.03853932023048401\n",
      "epoch: 4 step: 21, loss is 0.022960687056183815\n",
      "epoch: 4 step: 22, loss is 0.04936244711279869\n",
      "epoch: 4 step: 23, loss is 0.158499076962471\n",
      "epoch: 4 step: 24, loss is 0.00024042921722866595\n",
      "epoch: 4 step: 25, loss is 0.0046556200832128525\n",
      "epoch: 4 step: 26, loss is 0.018317531794309616\n",
      "epoch: 4 step: 27, loss is 0.12104830890893936\n",
      "epoch: 4 step: 28, loss is 0.013733898289501667\n",
      "epoch: 4 step: 29, loss is 0.00038165212026797235\n",
      "epoch: 4 step: 30, loss is 0.0020918999798595905\n",
      "epoch: 4 step: 31, loss is 0.0007521457737311721\n",
      "epoch: 4 step: 32, loss is 0.21474671363830566\n",
      "epoch: 4 step: 33, loss is 0.0051007806323468685\n",
      "epoch: 4 step: 34, loss is 0.020571066066622734\n",
      "epoch: 4 step: 35, loss is 0.05581541359424591\n",
      "epoch: 4 step: 36, loss is 0.003536434844136238\n",
      "epoch: 4 step: 37, loss is 0.014747774228453636\n",
      "epoch: 4 step: 38, loss is 0.004669766407459974\n",
      "epoch: 4 step: 39, loss is 0.0010049584088847041\n",
      "epoch: 4 step: 40, loss is 0.0338578000664711\n",
      "epoch: 4 step: 41, loss is 0.0199036356061697\n",
      "epoch: 4 step: 42, loss is 0.033674467355012894\n",
      "epoch: 4 step: 43, loss is 0.004729558248072863\n",
      "epoch: 4 step: 44, loss is 0.006443872582167387\n",
      "epoch: 4 step: 45, loss is 0.009266023524105549\n",
      "epoch: 4 step: 46, loss is 0.0077868178486824036\n",
      "epoch: 4 step: 47, loss is 0.025699136778712273\n",
      "epoch: 4 step: 48, loss is 0.041755422949790955\n",
      "epoch: 4 step: 49, loss is 0.0007504543755203485\n",
      "epoch: 4 step: 50, loss is 0.2275426834821701\n",
      "epoch: 4 step: 51, loss is 0.0011246248614042997\n",
      "epoch: 4 step: 52, loss is 0.03738752007484436\n",
      "epoch: 4 step: 53, loss is 0.013336154632270336\n",
      "epoch: 4 step: 54, loss is 0.0012893271632492542\n",
      "epoch: 4 step: 55, loss is 0.002627911511808634\n",
      "epoch: 4 step: 56, loss is 0.00045395156485028565\n",
      "epoch: 4 step: 57, loss is 0.018702564761042595\n",
      "epoch: 4 step: 58, loss is 0.0046227676793932915\n",
      "epoch: 4 step: 59, loss is 0.0010511149885132909\n",
      "epoch: 4 step: 60, loss is 0.0016509018605574965\n",
      "epoch: 4 step: 61, loss is 0.019022740423679352\n",
      "epoch: 4 step: 62, loss is 0.11515472829341888\n",
      "epoch: 4 step: 63, loss is 0.12210392206907272\n",
      "epoch: 4 step: 64, loss is 0.004338469821959734\n",
      "epoch: 4 step: 65, loss is 0.023830845952033997\n",
      "epoch: 4 step: 66, loss is 0.06973584741353989\n",
      "epoch: 4 step: 67, loss is 0.004152123350650072\n",
      "epoch: 4 step: 68, loss is 0.03622817620635033\n",
      "epoch: 4 step: 69, loss is 0.016786906868219376\n",
      "epoch: 4 step: 70, loss is 0.05387910455465317\n",
      "epoch: 4 step: 71, loss is 0.021667372435331345\n",
      "epoch: 4 step: 72, loss is 0.059197138994932175\n",
      "epoch: 4 step: 73, loss is 0.004937911406159401\n",
      "epoch: 4 step: 74, loss is 0.0012967765796929598\n",
      "epoch: 4 step: 75, loss is 0.08430740237236023\n",
      "epoch: 4 step: 76, loss is 0.04727758839726448\n",
      "epoch: 4 step: 77, loss is 0.0010096306214109063\n",
      "epoch: 4 step: 78, loss is 0.003079057903960347\n",
      "epoch: 4 step: 79, loss is 0.013484419323503971\n",
      "epoch: 4 step: 80, loss is 0.0006986596272327006\n",
      "epoch: 4 step: 81, loss is 0.003870953107252717\n",
      "epoch: 4 step: 82, loss is 0.11107933521270752\n",
      "epoch: 4 step: 83, loss is 0.015897640958428383\n",
      "epoch: 4 step: 84, loss is 0.12280191481113434\n",
      "epoch: 4 step: 85, loss is 0.16748230159282684\n",
      "epoch: 4 step: 86, loss is 0.09824972599744797\n",
      "epoch: 4 step: 87, loss is 0.012756673619151115\n",
      "epoch: 4 step: 88, loss is 0.0024842352140694857\n",
      "epoch: 4 step: 89, loss is 0.061472151428461075\n",
      "epoch: 4 step: 90, loss is 0.11006570607423782\n",
      "epoch: 4 step: 91, loss is 0.008056304417550564\n",
      "epoch: 4 step: 92, loss is 0.006241295486688614\n",
      "epoch: 4 step: 93, loss is 0.008845197036862373\n",
      "epoch: 4 step: 94, loss is 0.017657088115811348\n",
      "epoch: 4 step: 95, loss is 0.007183847017586231\n",
      "epoch: 4 step: 96, loss is 0.027489246800541878\n",
      "epoch: 4 step: 97, loss is 0.01600009948015213\n",
      "epoch: 4 step: 98, loss is 0.0007030446431599557\n",
      "epoch: 4 step: 99, loss is 0.0018011181382462382\n",
      "epoch: 4 step: 100, loss is 0.004399198107421398\n",
      "epoch: 4 step: 101, loss is 0.0024081887677311897\n",
      "epoch: 4 step: 102, loss is 0.012752577662467957\n",
      "epoch: 4 step: 103, loss is 0.002573818899691105\n",
      "epoch: 4 step: 104, loss is 0.01912245713174343\n",
      "epoch: 4 step: 105, loss is 0.005962358321994543\n",
      "epoch: 4 step: 106, loss is 0.08542437106370926\n",
      "epoch: 4 step: 107, loss is 0.024284010753035545\n",
      "epoch: 4 step: 108, loss is 0.031015222892165184\n",
      "epoch: 4 step: 109, loss is 0.022054608911275864\n",
      "epoch: 4 step: 110, loss is 0.0013154917396605015\n",
      "epoch: 4 step: 111, loss is 0.058613672852516174\n",
      "epoch: 4 step: 112, loss is 0.0010693056974560022\n",
      "epoch: 4 step: 113, loss is 0.020616916939616203\n",
      "epoch: 4 step: 114, loss is 0.0009493532124906778\n",
      "epoch: 4 step: 115, loss is 0.016078252345323563\n",
      "epoch: 4 step: 116, loss is 0.045029159635305405\n",
      "epoch: 4 step: 117, loss is 0.05189971253275871\n",
      "epoch: 4 step: 118, loss is 0.10541840642690659\n",
      "epoch: 4 step: 119, loss is 0.003326531732454896\n",
      "epoch: 4 step: 120, loss is 0.00837557390332222\n",
      "epoch: 4 step: 121, loss is 0.18279510736465454\n",
      "epoch: 4 step: 122, loss is 0.09085302799940109\n",
      "epoch: 4 step: 123, loss is 0.011913041584193707\n",
      "epoch: 4 step: 124, loss is 0.013339255005121231\n",
      "epoch: 4 step: 125, loss is 0.0008161361329257488\n",
      "epoch: 4 step: 126, loss is 0.021477852016687393\n",
      "epoch: 4 step: 127, loss is 0.12965089082717896\n",
      "epoch: 4 step: 128, loss is 0.0007813674164935946\n",
      "epoch: 4 step: 129, loss is 0.004049379378557205\n",
      "epoch: 4 step: 130, loss is 0.003310170490294695\n",
      "epoch: 4 step: 131, loss is 0.0013973417226225138\n",
      "epoch: 4 step: 132, loss is 0.10787653177976608\n",
      "epoch: 4 step: 133, loss is 0.002949162619188428\n",
      "epoch: 4 step: 134, loss is 0.01545695960521698\n",
      "epoch: 4 step: 135, loss is 0.0026662293821573257\n",
      "epoch: 4 step: 136, loss is 0.0024844517465680838\n",
      "epoch: 4 step: 137, loss is 0.045177724212408066\n",
      "epoch: 4 step: 138, loss is 0.02086758054792881\n",
      "epoch: 4 step: 139, loss is 0.00478357495740056\n",
      "epoch: 4 step: 140, loss is 0.05321158468723297\n",
      "epoch: 4 step: 141, loss is 0.020524710416793823\n",
      "epoch: 4 step: 142, loss is 0.01912926323711872\n",
      "epoch: 4 step: 143, loss is 0.04913399741053581\n",
      "epoch: 4 step: 144, loss is 0.09507285803556442\n",
      "epoch: 4 step: 145, loss is 0.019842831417918205\n",
      "epoch: 4 step: 146, loss is 0.0533154122531414\n",
      "epoch: 4 step: 147, loss is 0.008350123651325703\n",
      "epoch: 4 step: 148, loss is 0.028912147507071495\n",
      "epoch: 4 step: 149, loss is 0.10883818566799164\n",
      "epoch: 4 step: 150, loss is 0.0025245426222682\n",
      "epoch: 4 step: 151, loss is 0.04994450882077217\n",
      "epoch: 4 step: 152, loss is 0.09955894201993942\n",
      "epoch: 4 step: 153, loss is 0.07259301096200943\n",
      "epoch: 4 step: 154, loss is 0.0005066592711955309\n",
      "epoch: 4 step: 155, loss is 0.009794482961297035\n",
      "epoch: 4 step: 156, loss is 0.03455788642168045\n",
      "epoch: 4 step: 157, loss is 0.010368011891841888\n",
      "epoch: 4 step: 158, loss is 0.0015731460880488157\n",
      "epoch: 4 step: 159, loss is 0.014121152460575104\n",
      "epoch: 4 step: 160, loss is 0.009347671642899513\n",
      "epoch: 4 step: 161, loss is 0.0006636691396124661\n",
      "epoch: 4 step: 162, loss is 0.001863199518993497\n",
      "epoch: 4 step: 163, loss is 0.007191684562712908\n",
      "epoch: 4 step: 164, loss is 0.009476819075644016\n",
      "epoch: 4 step: 165, loss is 0.00064979208400473\n",
      "epoch: 4 step: 166, loss is 0.005101615097373724\n",
      "epoch: 4 step: 167, loss is 0.0014660956803709269\n",
      "epoch: 4 step: 168, loss is 0.02845817431807518\n",
      "epoch: 4 step: 169, loss is 0.001232415554113686\n",
      "epoch: 4 step: 170, loss is 0.07498648017644882\n",
      "epoch: 4 step: 171, loss is 0.002867691218852997\n",
      "epoch: 4 step: 172, loss is 0.004285136703401804\n",
      "epoch: 4 step: 173, loss is 0.002506940858438611\n",
      "epoch: 4 step: 174, loss is 0.028361760079860687\n",
      "epoch: 4 step: 175, loss is 0.01923290826380253\n",
      "epoch: 4 step: 176, loss is 0.006649782881140709\n",
      "epoch: 4 step: 177, loss is 0.003999780863523483\n",
      "epoch: 4 step: 178, loss is 0.15594862401485443\n",
      "epoch: 4 step: 179, loss is 0.007460663095116615\n",
      "epoch: 4 step: 180, loss is 0.0008861646638251841\n",
      "epoch: 4 step: 181, loss is 0.0004834733554162085\n",
      "epoch: 4 step: 182, loss is 0.003183836815878749\n",
      "epoch: 4 step: 183, loss is 0.0018767842557281256\n",
      "epoch: 4 step: 184, loss is 0.004728784319013357\n",
      "epoch: 4 step: 185, loss is 0.08586710691452026\n",
      "epoch: 4 step: 186, loss is 0.14596903324127197\n",
      "epoch: 4 step: 187, loss is 0.05698968097567558\n",
      "epoch: 4 step: 188, loss is 0.00864928774535656\n",
      "epoch: 4 step: 189, loss is 0.003077405272051692\n",
      "epoch: 4 step: 190, loss is 0.09127189964056015\n",
      "epoch: 4 step: 191, loss is 0.029380111023783684\n",
      "epoch: 4 step: 192, loss is 0.06781090795993805\n",
      "epoch: 4 step: 193, loss is 0.003605879144743085\n",
      "epoch: 4 step: 194, loss is 0.0800723135471344\n",
      "epoch: 4 step: 195, loss is 0.009003481827676296\n",
      "epoch: 4 step: 196, loss is 0.013676843605935574\n",
      "epoch: 4 step: 197, loss is 0.007514489348977804\n",
      "epoch: 4 step: 198, loss is 0.008524916134774685\n",
      "epoch: 4 step: 199, loss is 0.001116834580898285\n",
      "epoch: 4 step: 200, loss is 0.008218851871788502\n",
      "epoch: 4 step: 201, loss is 0.005022614728659391\n",
      "epoch: 4 step: 202, loss is 0.0057860687375068665\n",
      "epoch: 4 step: 203, loss is 0.03976692259311676\n",
      "epoch: 4 step: 204, loss is 0.01696772873401642\n",
      "epoch: 4 step: 205, loss is 0.021797891706228256\n",
      "epoch: 4 step: 206, loss is 0.006757660768926144\n",
      "epoch: 4 step: 207, loss is 0.0018049228237941861\n",
      "epoch: 4 step: 208, loss is 0.1427576243877411\n",
      "epoch: 4 step: 209, loss is 0.0038273236714303493\n",
      "epoch: 4 step: 210, loss is 0.0007383823394775391\n",
      "epoch: 4 step: 211, loss is 0.004037244711071253\n",
      "epoch: 4 step: 212, loss is 0.004527080338448286\n",
      "epoch: 4 step: 213, loss is 0.04823999106884003\n",
      "epoch: 4 step: 214, loss is 0.004373094066977501\n",
      "epoch: 4 step: 215, loss is 0.0008723815553821623\n",
      "epoch: 4 step: 216, loss is 0.0035133438650518656\n",
      "epoch: 4 step: 217, loss is 0.0007198394741863012\n",
      "epoch: 4 step: 218, loss is 0.051799140870571136\n",
      "epoch: 4 step: 219, loss is 0.052971452474594116\n",
      "epoch: 4 step: 220, loss is 0.01353407371789217\n",
      "epoch: 4 step: 221, loss is 0.0526222288608551\n",
      "epoch: 4 step: 222, loss is 0.011146201752126217\n",
      "epoch: 4 step: 223, loss is 0.0003107592056039721\n",
      "epoch: 4 step: 224, loss is 0.04474300891160965\n",
      "epoch: 4 step: 225, loss is 0.0384732224047184\n",
      "epoch: 4 step: 226, loss is 0.002079228637740016\n",
      "epoch: 4 step: 227, loss is 0.004759679548442364\n",
      "epoch: 4 step: 228, loss is 0.011842655017971992\n",
      "epoch: 4 step: 229, loss is 0.006940825842320919\n",
      "epoch: 4 step: 230, loss is 0.0791710913181305\n",
      "epoch: 4 step: 231, loss is 0.12427286803722382\n",
      "epoch: 4 step: 232, loss is 0.004437363240867853\n",
      "epoch: 4 step: 233, loss is 0.0006983759813010693\n",
      "epoch: 4 step: 234, loss is 0.0014297717716544867\n",
      "epoch: 4 step: 235, loss is 0.011240290477871895\n",
      "epoch: 4 step: 236, loss is 0.11620007455348969\n",
      "epoch: 4 step: 237, loss is 0.0008914940408430994\n",
      "epoch: 4 step: 238, loss is 0.003280727891251445\n",
      "epoch: 4 step: 239, loss is 0.06399640440940857\n",
      "epoch: 4 step: 240, loss is 0.0024866065941751003\n",
      "epoch: 4 step: 241, loss is 0.03122335486114025\n",
      "epoch: 4 step: 242, loss is 0.0007199184619821608\n",
      "epoch: 4 step: 243, loss is 0.0395415760576725\n",
      "epoch: 4 step: 244, loss is 0.05268094688653946\n",
      "epoch: 4 step: 245, loss is 0.04204966500401497\n",
      "epoch: 4 step: 246, loss is 0.0006600673659704626\n",
      "epoch: 4 step: 247, loss is 0.01868840493261814\n",
      "epoch: 4 step: 248, loss is 0.0018197495955973864\n",
      "epoch: 4 step: 249, loss is 0.010388690046966076\n",
      "epoch: 4 step: 250, loss is 0.00055578479077667\n",
      "epoch: 4 step: 251, loss is 0.0018282205564901233\n",
      "epoch: 4 step: 252, loss is 0.017029397189617157\n",
      "epoch: 4 step: 253, loss is 0.14730502665042877\n",
      "epoch: 4 step: 254, loss is 0.022518008947372437\n",
      "epoch: 4 step: 255, loss is 0.013881685212254524\n",
      "epoch: 4 step: 256, loss is 7.524661486968398e-05\n",
      "epoch: 4 step: 257, loss is 0.19632746279239655\n",
      "epoch: 4 step: 258, loss is 0.0020019435323774815\n",
      "epoch: 4 step: 259, loss is 0.010898834094405174\n",
      "epoch: 4 step: 260, loss is 0.007240235805511475\n",
      "epoch: 4 step: 261, loss is 0.07279486954212189\n",
      "epoch: 4 step: 262, loss is 0.04878338426351547\n",
      "epoch: 4 step: 263, loss is 0.0006283627008087933\n",
      "epoch: 4 step: 264, loss is 0.017855944111943245\n",
      "epoch: 4 step: 265, loss is 0.018235325813293457\n",
      "epoch: 4 step: 266, loss is 0.0007982581737451255\n",
      "epoch: 4 step: 267, loss is 0.01629583165049553\n",
      "epoch: 4 step: 268, loss is 0.08764554560184479\n",
      "epoch: 4 step: 269, loss is 0.008184175007045269\n",
      "epoch: 4 step: 270, loss is 0.006720573175698519\n",
      "epoch: 4 step: 271, loss is 0.0237740445882082\n",
      "epoch: 4 step: 272, loss is 0.001361717120744288\n",
      "epoch: 4 step: 273, loss is 0.001271975226700306\n",
      "epoch: 4 step: 274, loss is 0.00155582744628191\n",
      "epoch: 4 step: 275, loss is 0.031371865421533585\n",
      "epoch: 4 step: 276, loss is 0.041354019194841385\n",
      "epoch: 4 step: 277, loss is 0.010649589821696281\n",
      "epoch: 4 step: 278, loss is 0.12059146910905838\n",
      "epoch: 4 step: 279, loss is 0.00674990052357316\n",
      "epoch: 4 step: 280, loss is 0.0016332257073372602\n",
      "epoch: 4 step: 281, loss is 0.15488938987255096\n",
      "epoch: 4 step: 282, loss is 0.06553442776203156\n",
      "epoch: 4 step: 283, loss is 0.0009004388703033328\n",
      "epoch: 4 step: 284, loss is 0.004196677356958389\n",
      "epoch: 4 step: 285, loss is 0.1336967945098877\n",
      "epoch: 4 step: 286, loss is 0.0004242398717906326\n",
      "epoch: 4 step: 287, loss is 0.11417709290981293\n",
      "epoch: 4 step: 288, loss is 0.0007808533846400678\n",
      "epoch: 4 step: 289, loss is 0.0019957353360950947\n",
      "epoch: 4 step: 290, loss is 0.2841506898403168\n",
      "epoch: 4 step: 291, loss is 0.003353244625031948\n",
      "epoch: 4 step: 292, loss is 0.02963745780289173\n",
      "epoch: 4 step: 293, loss is 0.21363696455955505\n",
      "epoch: 4 step: 294, loss is 0.007262660190463066\n",
      "epoch: 4 step: 295, loss is 0.0006023653550073504\n",
      "epoch: 4 step: 296, loss is 0.021233368664979935\n",
      "epoch: 4 step: 297, loss is 0.038839295506477356\n",
      "epoch: 4 step: 298, loss is 0.0077061341144144535\n",
      "epoch: 4 step: 299, loss is 0.21708163619041443\n",
      "epoch: 4 step: 300, loss is 0.12251570075750351\n",
      "epoch: 4 step: 301, loss is 0.01511167548596859\n",
      "epoch: 4 step: 302, loss is 0.003659578040242195\n",
      "epoch: 4 step: 303, loss is 0.016122320666909218\n",
      "epoch: 4 step: 304, loss is 0.011093896813690662\n",
      "epoch: 4 step: 305, loss is 0.008273174986243248\n",
      "epoch: 4 step: 306, loss is 0.017089316621422768\n",
      "epoch: 4 step: 307, loss is 0.07110138982534409\n",
      "epoch: 4 step: 308, loss is 0.0027914345264434814\n",
      "epoch: 4 step: 309, loss is 0.012568666599690914\n",
      "epoch: 4 step: 310, loss is 0.014546569436788559\n",
      "epoch: 4 step: 311, loss is 0.0005511535564437509\n",
      "epoch: 4 step: 312, loss is 0.00036495618405751884\n",
      "epoch: 4 step: 313, loss is 0.0007611868786625564\n",
      "epoch: 4 step: 314, loss is 0.12583382427692413\n",
      "epoch: 4 step: 315, loss is 0.03782568871974945\n",
      "epoch: 4 step: 316, loss is 0.07717905938625336\n",
      "epoch: 4 step: 317, loss is 0.0005333867156878114\n",
      "epoch: 4 step: 318, loss is 0.006510211620479822\n",
      "epoch: 4 step: 319, loss is 0.060346320271492004\n",
      "epoch: 4 step: 320, loss is 0.003287613857537508\n",
      "epoch: 4 step: 321, loss is 0.0020271576941013336\n",
      "epoch: 4 step: 322, loss is 0.004839525558054447\n",
      "epoch: 4 step: 323, loss is 0.0022043560165911913\n",
      "epoch: 4 step: 324, loss is 0.0011170108336955309\n",
      "epoch: 4 step: 325, loss is 0.001372867845930159\n",
      "epoch: 4 step: 326, loss is 0.06836788356304169\n",
      "epoch: 4 step: 327, loss is 0.016190266236662865\n",
      "epoch: 4 step: 328, loss is 0.0020313826389610767\n",
      "epoch: 4 step: 329, loss is 0.001171376439742744\n",
      "epoch: 4 step: 330, loss is 0.020935509353876114\n",
      "epoch: 4 step: 331, loss is 0.2797653079032898\n",
      "epoch: 4 step: 332, loss is 0.004821299575269222\n",
      "epoch: 4 step: 333, loss is 0.01721535436809063\n",
      "epoch: 4 step: 334, loss is 0.18263694643974304\n",
      "epoch: 4 step: 335, loss is 0.0004589136515278369\n",
      "epoch: 4 step: 336, loss is 0.07968077063560486\n",
      "epoch: 4 step: 337, loss is 0.19980821013450623\n",
      "epoch: 4 step: 338, loss is 0.008829026482999325\n",
      "epoch: 4 step: 339, loss is 0.002507953206077218\n",
      "epoch: 4 step: 340, loss is 0.028220534324645996\n",
      "epoch: 4 step: 341, loss is 0.005450064316391945\n",
      "epoch: 4 step: 342, loss is 0.2004835158586502\n",
      "epoch: 4 step: 343, loss is 0.01237633265554905\n",
      "epoch: 4 step: 344, loss is 0.00553957698866725\n",
      "epoch: 4 step: 345, loss is 0.0016665072180330753\n",
      "epoch: 4 step: 346, loss is 0.010611496865749359\n",
      "epoch: 4 step: 347, loss is 0.054448388516902924\n",
      "epoch: 4 step: 348, loss is 0.06014316529035568\n",
      "epoch: 4 step: 349, loss is 0.021651023998856544\n",
      "epoch: 4 step: 350, loss is 0.004235273692756891\n",
      "epoch: 4 step: 351, loss is 0.03804916515946388\n",
      "epoch: 4 step: 352, loss is 0.06187921017408371\n",
      "epoch: 4 step: 353, loss is 0.007390535436570644\n",
      "epoch: 4 step: 354, loss is 0.005412569269537926\n",
      "epoch: 4 step: 355, loss is 0.19078239798545837\n",
      "epoch: 4 step: 356, loss is 0.0018764501437544823\n",
      "epoch: 4 step: 357, loss is 0.004449289757758379\n",
      "epoch: 4 step: 358, loss is 0.00213686958886683\n",
      "epoch: 4 step: 359, loss is 0.0017778183100745082\n",
      "epoch: 4 step: 360, loss is 0.1453612893819809\n",
      "epoch: 4 step: 361, loss is 0.007036788389086723\n",
      "epoch: 4 step: 362, loss is 0.10645811259746552\n",
      "epoch: 4 step: 363, loss is 0.001932361163198948\n",
      "epoch: 4 step: 364, loss is 0.0030502488370984793\n",
      "epoch: 4 step: 365, loss is 0.004184008110314608\n",
      "epoch: 4 step: 366, loss is 0.007442761212587357\n",
      "epoch: 4 step: 367, loss is 0.00121147814206779\n",
      "epoch: 4 step: 368, loss is 0.0363103449344635\n",
      "epoch: 4 step: 369, loss is 0.012632470577955246\n",
      "epoch: 4 step: 370, loss is 0.010349318385124207\n",
      "epoch: 4 step: 371, loss is 0.025944611057639122\n",
      "epoch: 4 step: 372, loss is 0.09461501985788345\n",
      "epoch: 4 step: 373, loss is 0.0028597849886864424\n",
      "epoch: 4 step: 374, loss is 0.0010800568852573633\n",
      "epoch: 4 step: 375, loss is 0.00721327681094408\n",
      "epoch: 4 step: 376, loss is 0.010809357278048992\n",
      "epoch: 4 step: 377, loss is 0.008892549201846123\n",
      "epoch: 4 step: 378, loss is 0.03646298870444298\n",
      "epoch: 4 step: 379, loss is 0.0022762215230613947\n",
      "epoch: 4 step: 380, loss is 0.01369483396410942\n",
      "epoch: 4 step: 381, loss is 0.0014136957470327616\n",
      "epoch: 4 step: 382, loss is 0.004950346890836954\n",
      "epoch: 4 step: 383, loss is 0.032178472727537155\n",
      "epoch: 4 step: 384, loss is 0.05716424062848091\n",
      "epoch: 4 step: 385, loss is 0.005952860694378614\n",
      "epoch: 4 step: 386, loss is 0.09863200783729553\n",
      "epoch: 4 step: 387, loss is 0.0041205305606126785\n",
      "epoch: 4 step: 388, loss is 0.053607143461704254\n",
      "epoch: 4 step: 389, loss is 0.105584055185318\n",
      "epoch: 4 step: 390, loss is 0.0657045990228653\n",
      "epoch: 4 step: 391, loss is 0.002800222486257553\n",
      "epoch: 4 step: 392, loss is 0.0010558819631114602\n",
      "epoch: 4 step: 393, loss is 0.000680549768730998\n",
      "epoch: 4 step: 394, loss is 0.013403155840933323\n",
      "epoch: 4 step: 395, loss is 0.00479644862934947\n",
      "epoch: 4 step: 396, loss is 0.0431647002696991\n",
      "epoch: 4 step: 397, loss is 0.013299663551151752\n",
      "epoch: 4 step: 398, loss is 0.02811606228351593\n",
      "epoch: 4 step: 399, loss is 0.07626853883266449\n",
      "epoch: 4 step: 400, loss is 0.001050679013133049\n",
      "epoch: 4 step: 401, loss is 0.008972466923296452\n",
      "epoch: 4 step: 402, loss is 0.004445571452379227\n",
      "epoch: 4 step: 403, loss is 0.0005258160526864231\n",
      "epoch: 4 step: 404, loss is 0.0017440274823457003\n",
      "epoch: 4 step: 405, loss is 0.0015320794191211462\n",
      "epoch: 4 step: 406, loss is 0.00199695210903883\n",
      "epoch: 4 step: 407, loss is 0.013180173933506012\n",
      "epoch: 4 step: 408, loss is 0.008539375849068165\n",
      "epoch: 4 step: 409, loss is 0.002165272831916809\n",
      "epoch: 4 step: 410, loss is 0.0013148008147254586\n",
      "epoch: 4 step: 411, loss is 0.00125234539154917\n",
      "epoch: 4 step: 412, loss is 0.0014008289435878396\n",
      "epoch: 4 step: 413, loss is 0.0016502849757671356\n",
      "epoch: 4 step: 414, loss is 0.009604865685105324\n",
      "epoch: 4 step: 415, loss is 0.005448261275887489\n",
      "epoch: 4 step: 416, loss is 0.040117718279361725\n",
      "epoch: 4 step: 417, loss is 0.012782817706465721\n",
      "epoch: 4 step: 418, loss is 0.0018822744023054838\n",
      "epoch: 4 step: 419, loss is 0.02944968082010746\n",
      "epoch: 4 step: 420, loss is 0.0007948832935653627\n",
      "epoch: 4 step: 421, loss is 0.0016193375922739506\n",
      "epoch: 4 step: 422, loss is 0.04900037497282028\n",
      "epoch: 4 step: 423, loss is 0.001135423080995679\n",
      "epoch: 4 step: 424, loss is 0.15012827515602112\n",
      "epoch: 4 step: 425, loss is 0.005390371195971966\n",
      "epoch: 4 step: 426, loss is 0.022624341771006584\n",
      "epoch: 4 step: 427, loss is 0.0033748429268598557\n",
      "epoch: 4 step: 428, loss is 0.0017637612763792276\n",
      "epoch: 4 step: 429, loss is 0.013758804649114609\n",
      "epoch: 4 step: 430, loss is 0.0005135995452292264\n",
      "epoch: 4 step: 431, loss is 0.003543887520208955\n",
      "epoch: 4 step: 432, loss is 0.005156488157808781\n",
      "epoch: 4 step: 433, loss is 0.0008099852129817009\n",
      "epoch: 4 step: 434, loss is 0.0016771105583757162\n",
      "epoch: 4 step: 435, loss is 0.08417034149169922\n",
      "epoch: 4 step: 436, loss is 0.01602773182094097\n",
      "epoch: 4 step: 437, loss is 0.008819435723125935\n",
      "epoch: 4 step: 438, loss is 0.0006301426910795271\n",
      "epoch: 4 step: 439, loss is 0.004597358871251345\n",
      "epoch: 4 step: 440, loss is 0.004572024568915367\n",
      "epoch: 4 step: 441, loss is 0.10638059675693512\n",
      "epoch: 4 step: 442, loss is 0.033377550542354584\n",
      "epoch: 4 step: 443, loss is 0.002554530743509531\n",
      "epoch: 4 step: 444, loss is 0.028084516525268555\n",
      "epoch: 4 step: 445, loss is 0.0012113220291212201\n",
      "epoch: 4 step: 446, loss is 0.04004933685064316\n",
      "epoch: 4 step: 447, loss is 0.03246082365512848\n",
      "epoch: 4 step: 448, loss is 0.16197176277637482\n",
      "epoch: 4 step: 449, loss is 0.24254566431045532\n",
      "epoch: 4 step: 450, loss is 0.042570844292640686\n",
      "epoch: 4 step: 451, loss is 0.0578448660671711\n",
      "epoch: 4 step: 452, loss is 0.006257166620343924\n",
      "epoch: 4 step: 453, loss is 0.23686331510543823\n",
      "epoch: 4 step: 454, loss is 0.13008062541484833\n",
      "epoch: 4 step: 455, loss is 0.001059465343132615\n",
      "epoch: 4 step: 456, loss is 0.012958046048879623\n",
      "epoch: 4 step: 457, loss is 0.02067416161298752\n",
      "epoch: 4 step: 458, loss is 0.003261114936321974\n",
      "epoch: 4 step: 459, loss is 0.0039270613342523575\n",
      "epoch: 4 step: 460, loss is 0.010585220530629158\n",
      "epoch: 4 step: 461, loss is 0.002521513495594263\n",
      "epoch: 4 step: 462, loss is 0.1617662012577057\n",
      "epoch: 4 step: 463, loss is 0.017886687070131302\n",
      "epoch: 4 step: 464, loss is 0.005831591319292784\n",
      "epoch: 4 step: 465, loss is 0.004175870679318905\n",
      "epoch: 4 step: 466, loss is 0.11680780351161957\n",
      "epoch: 4 step: 467, loss is 0.015530415810644627\n",
      "epoch: 4 step: 468, loss is 0.014331860467791557\n",
      "epoch: 4 step: 469, loss is 0.00043173966696485877\n",
      "epoch: 4 step: 470, loss is 0.1329759657382965\n",
      "epoch: 4 step: 471, loss is 0.024293968454003334\n",
      "epoch: 4 step: 472, loss is 0.005141318775713444\n",
      "epoch: 4 step: 473, loss is 0.08465450257062912\n",
      "epoch: 4 step: 474, loss is 0.019091228023171425\n",
      "epoch: 4 step: 475, loss is 0.014044863171875477\n",
      "epoch: 4 step: 476, loss is 0.0006822195718996227\n",
      "epoch: 4 step: 477, loss is 0.0007725178729742765\n",
      "epoch: 4 step: 478, loss is 0.21289078891277313\n",
      "epoch: 4 step: 479, loss is 0.07440955191850662\n",
      "epoch: 4 step: 480, loss is 0.018986035138368607\n",
      "epoch: 4 step: 481, loss is 0.00993766076862812\n",
      "epoch: 4 step: 482, loss is 0.001259488519281149\n",
      "epoch: 4 step: 483, loss is 0.04688649624586105\n",
      "epoch: 4 step: 484, loss is 0.0009892141679301858\n",
      "epoch: 4 step: 485, loss is 0.0026894251350313425\n",
      "epoch: 4 step: 486, loss is 0.01751922257244587\n",
      "epoch: 4 step: 487, loss is 0.017558658495545387\n",
      "epoch: 4 step: 488, loss is 0.022943848744034767\n",
      "epoch: 4 step: 489, loss is 0.01041516475379467\n",
      "epoch: 4 step: 490, loss is 0.046526648104190826\n",
      "epoch: 4 step: 491, loss is 0.010105716064572334\n",
      "epoch: 4 step: 492, loss is 0.023580657318234444\n",
      "epoch: 4 step: 493, loss is 0.0056325653567910194\n",
      "epoch: 4 step: 494, loss is 0.014718537218868732\n",
      "epoch: 4 step: 495, loss is 0.0008347289403900504\n",
      "epoch: 4 step: 496, loss is 0.025266142562031746\n",
      "epoch: 4 step: 497, loss is 0.0232180617749691\n",
      "epoch: 4 step: 498, loss is 0.0014304319629445672\n",
      "epoch: 4 step: 499, loss is 0.0011110396590083838\n",
      "epoch: 4 step: 500, loss is 0.006592737510800362\n",
      "epoch: 4 step: 501, loss is 0.14500871300697327\n",
      "epoch: 4 step: 502, loss is 0.07630486786365509\n",
      "epoch: 4 step: 503, loss is 0.26591765880584717\n",
      "epoch: 4 step: 504, loss is 0.002524701878428459\n",
      "epoch: 4 step: 505, loss is 0.004323175642639399\n",
      "epoch: 4 step: 506, loss is 0.0015501889865845442\n",
      "epoch: 4 step: 507, loss is 0.0007469892734661698\n",
      "epoch: 4 step: 508, loss is 0.0011715368600562215\n",
      "epoch: 4 step: 509, loss is 0.2877589464187622\n",
      "epoch: 4 step: 510, loss is 0.0073205819353461266\n",
      "epoch: 4 step: 511, loss is 0.15515227615833282\n",
      "epoch: 4 step: 512, loss is 0.0002639672311488539\n",
      "epoch: 4 step: 513, loss is 0.03929406404495239\n",
      "epoch: 4 step: 514, loss is 0.1694032847881317\n",
      "epoch: 4 step: 515, loss is 0.006770788691937923\n",
      "epoch: 4 step: 516, loss is 0.08936653286218643\n",
      "epoch: 4 step: 517, loss is 0.00022769358474761248\n",
      "epoch: 4 step: 518, loss is 0.041092757135629654\n",
      "epoch: 4 step: 519, loss is 0.008197552524507046\n",
      "epoch: 4 step: 520, loss is 0.0011508703464642167\n",
      "epoch: 4 step: 521, loss is 0.0003972431877627969\n",
      "epoch: 4 step: 522, loss is 0.07836610823869705\n",
      "epoch: 4 step: 523, loss is 0.03655317425727844\n",
      "epoch: 4 step: 524, loss is 0.0014690293464809656\n",
      "epoch: 4 step: 525, loss is 0.0029883405659347773\n",
      "epoch: 4 step: 526, loss is 0.03363310173153877\n",
      "epoch: 4 step: 527, loss is 0.0009729537414386868\n",
      "epoch: 4 step: 528, loss is 0.007459473796188831\n",
      "epoch: 4 step: 529, loss is 0.0025196343194693327\n",
      "epoch: 4 step: 530, loss is 0.04356684908270836\n",
      "epoch: 4 step: 531, loss is 0.058187466114759445\n",
      "epoch: 4 step: 532, loss is 0.06254224479198456\n",
      "epoch: 4 step: 533, loss is 0.04591074585914612\n",
      "epoch: 4 step: 534, loss is 0.0220639668405056\n",
      "epoch: 4 step: 535, loss is 0.010363022796809673\n",
      "epoch: 4 step: 536, loss is 0.005584696773439646\n",
      "epoch: 4 step: 537, loss is 0.03511710837483406\n",
      "epoch: 4 step: 538, loss is 0.06139616295695305\n",
      "epoch: 4 step: 539, loss is 0.0757603645324707\n",
      "epoch: 4 step: 540, loss is 0.00025742416619323194\n",
      "epoch: 4 step: 541, loss is 0.03131274878978729\n",
      "epoch: 4 step: 542, loss is 0.001808256027288735\n",
      "epoch: 4 step: 543, loss is 0.0005545749445445836\n",
      "epoch: 4 step: 544, loss is 0.0027238575275987387\n",
      "epoch: 4 step: 545, loss is 0.000789000594522804\n",
      "epoch: 4 step: 546, loss is 0.01562930829823017\n",
      "epoch: 4 step: 547, loss is 0.07858899235725403\n",
      "epoch: 4 step: 548, loss is 0.019940104335546494\n",
      "epoch: 4 step: 549, loss is 0.002423208672553301\n",
      "epoch: 4 step: 550, loss is 0.014677044004201889\n",
      "epoch: 4 step: 551, loss is 0.001734082237817347\n",
      "epoch: 4 step: 552, loss is 0.07995153218507767\n",
      "epoch: 4 step: 553, loss is 0.0004612202465068549\n",
      "epoch: 4 step: 554, loss is 0.0044705187901854515\n",
      "epoch: 4 step: 555, loss is 0.001552510540932417\n",
      "epoch: 4 step: 556, loss is 0.012295364402234554\n",
      "epoch: 4 step: 557, loss is 0.025474121794104576\n",
      "epoch: 4 step: 558, loss is 0.03600192815065384\n",
      "epoch: 4 step: 559, loss is 0.0050438446924090385\n",
      "epoch: 4 step: 560, loss is 0.00127621169667691\n",
      "epoch: 4 step: 561, loss is 0.08999070525169373\n",
      "epoch: 4 step: 562, loss is 0.05715639144182205\n",
      "epoch: 4 step: 563, loss is 0.02397965081036091\n",
      "epoch: 4 step: 564, loss is 0.015101747587323189\n",
      "epoch: 4 step: 565, loss is 0.0001779917802195996\n",
      "epoch: 4 step: 566, loss is 0.008015409111976624\n",
      "epoch: 4 step: 567, loss is 0.0002363927342230454\n",
      "epoch: 4 step: 568, loss is 0.021458415314555168\n",
      "epoch: 4 step: 569, loss is 0.11798586696386337\n",
      "epoch: 4 step: 570, loss is 0.00022638376685790718\n",
      "epoch: 4 step: 571, loss is 0.002289252355694771\n",
      "epoch: 4 step: 572, loss is 0.2521900534629822\n",
      "epoch: 4 step: 573, loss is 0.002581840381026268\n",
      "epoch: 4 step: 574, loss is 0.0015352461487054825\n",
      "epoch: 4 step: 575, loss is 0.014132394455373287\n",
      "epoch: 4 step: 576, loss is 0.010680888779461384\n",
      "epoch: 4 step: 577, loss is 0.009371782653033733\n",
      "epoch: 4 step: 578, loss is 7.08663064870052e-05\n",
      "epoch: 4 step: 579, loss is 0.021596236154437065\n",
      "epoch: 4 step: 580, loss is 0.01378064975142479\n",
      "epoch: 4 step: 581, loss is 0.14530906081199646\n",
      "epoch: 4 step: 582, loss is 0.0015478964196518064\n",
      "epoch: 4 step: 583, loss is 0.07104246318340302\n",
      "epoch: 4 step: 584, loss is 0.06913672387599945\n",
      "epoch: 4 step: 585, loss is 0.014548156410455704\n",
      "epoch: 4 step: 586, loss is 0.003755583195015788\n",
      "epoch: 4 step: 587, loss is 0.12430234253406525\n",
      "epoch: 4 step: 588, loss is 0.0006058914004825056\n",
      "epoch: 4 step: 589, loss is 0.0005558297270908952\n",
      "epoch: 4 step: 590, loss is 0.13780103623867035\n",
      "epoch: 4 step: 591, loss is 0.00023589607735630125\n",
      "epoch: 4 step: 592, loss is 0.20560906827449799\n",
      "epoch: 4 step: 593, loss is 0.013763997703790665\n",
      "epoch: 4 step: 594, loss is 0.004125499166548252\n",
      "epoch: 4 step: 595, loss is 0.020580749958753586\n",
      "epoch: 4 step: 596, loss is 0.01072631124407053\n",
      "epoch: 4 step: 597, loss is 0.021773671731352806\n",
      "epoch: 4 step: 598, loss is 0.005717091262340546\n",
      "epoch: 4 step: 599, loss is 0.01041189394891262\n",
      "epoch: 4 step: 600, loss is 0.09774225950241089\n",
      "epoch: 4 step: 601, loss is 0.09674981236457825\n",
      "epoch: 4 step: 602, loss is 0.016791697591543198\n",
      "epoch: 4 step: 603, loss is 0.003770748618990183\n",
      "epoch: 4 step: 604, loss is 0.004835010971873999\n",
      "epoch: 4 step: 605, loss is 0.13927367329597473\n",
      "epoch: 4 step: 606, loss is 0.10135388374328613\n",
      "epoch: 4 step: 607, loss is 0.016405243426561356\n",
      "epoch: 4 step: 608, loss is 0.009430229663848877\n",
      "epoch: 4 step: 609, loss is 0.05273756757378578\n",
      "epoch: 4 step: 610, loss is 0.0018613869324326515\n",
      "epoch: 4 step: 611, loss is 0.03376826271414757\n",
      "epoch: 4 step: 612, loss is 0.0018751887837424874\n",
      "epoch: 4 step: 613, loss is 0.3947070837020874\n",
      "epoch: 4 step: 614, loss is 0.0091649005189538\n",
      "epoch: 4 step: 615, loss is 0.008301529102027416\n",
      "epoch: 4 step: 616, loss is 0.006966867484152317\n",
      "epoch: 4 step: 617, loss is 0.01151347067207098\n",
      "epoch: 4 step: 618, loss is 0.0015721727395430207\n",
      "epoch: 4 step: 619, loss is 0.017608264461159706\n",
      "epoch: 4 step: 620, loss is 0.17657847702503204\n",
      "epoch: 4 step: 621, loss is 0.009759890846908092\n",
      "epoch: 4 step: 622, loss is 0.001173415221273899\n",
      "epoch: 4 step: 623, loss is 0.001387819298543036\n",
      "epoch: 4 step: 624, loss is 0.12811970710754395\n",
      "epoch: 4 step: 625, loss is 0.001813456416130066\n",
      "epoch: 4 step: 626, loss is 0.01183701679110527\n",
      "epoch: 4 step: 627, loss is 0.0234536062926054\n",
      "epoch: 4 step: 628, loss is 0.043811459094285965\n",
      "epoch: 4 step: 629, loss is 0.002793109742924571\n",
      "epoch: 4 step: 630, loss is 0.06391070783138275\n",
      "epoch: 4 step: 631, loss is 0.1447480469942093\n",
      "epoch: 4 step: 632, loss is 0.00735863484442234\n",
      "epoch: 4 step: 633, loss is 0.006412701681256294\n",
      "epoch: 4 step: 634, loss is 0.000646553176920861\n",
      "epoch: 4 step: 635, loss is 0.0015749877784401178\n",
      "epoch: 4 step: 636, loss is 0.056226301938295364\n",
      "epoch: 4 step: 637, loss is 0.010542702861130238\n",
      "epoch: 4 step: 638, loss is 0.22183926403522491\n",
      "epoch: 4 step: 639, loss is 0.07632085680961609\n",
      "epoch: 4 step: 640, loss is 0.0088705625385046\n",
      "epoch: 4 step: 641, loss is 0.30022409558296204\n",
      "epoch: 4 step: 642, loss is 0.06050163134932518\n",
      "epoch: 4 step: 643, loss is 0.05255415663123131\n",
      "epoch: 4 step: 644, loss is 0.007341137155890465\n",
      "epoch: 4 step: 645, loss is 0.006140657234936953\n",
      "epoch: 4 step: 646, loss is 0.04490635544061661\n",
      "epoch: 4 step: 647, loss is 0.004793947096914053\n",
      "epoch: 4 step: 648, loss is 0.0058770813047885895\n",
      "epoch: 4 step: 649, loss is 0.019138028845191002\n",
      "epoch: 4 step: 650, loss is 0.0057558901607990265\n",
      "epoch: 4 step: 651, loss is 0.002787370001897216\n",
      "epoch: 4 step: 652, loss is 0.10440076142549515\n",
      "epoch: 4 step: 653, loss is 0.10332980006933212\n",
      "epoch: 4 step: 654, loss is 0.019861478358507156\n",
      "epoch: 4 step: 655, loss is 0.0007231083000078797\n",
      "epoch: 4 step: 656, loss is 0.08802244812250137\n",
      "epoch: 4 step: 657, loss is 0.01521567814052105\n",
      "epoch: 4 step: 658, loss is 0.004226601216942072\n",
      "epoch: 4 step: 659, loss is 0.05899772793054581\n",
      "epoch: 4 step: 660, loss is 0.0062611945904791355\n",
      "epoch: 4 step: 661, loss is 0.01562921144068241\n",
      "epoch: 4 step: 662, loss is 0.0015792613849043846\n",
      "epoch: 4 step: 663, loss is 0.031180454418063164\n",
      "epoch: 4 step: 664, loss is 0.05325750261545181\n",
      "epoch: 4 step: 665, loss is 0.012474766932427883\n",
      "epoch: 4 step: 666, loss is 0.30182769894599915\n",
      "epoch: 4 step: 667, loss is 0.061494309455156326\n",
      "epoch: 4 step: 668, loss is 0.0010179252130910754\n",
      "epoch: 4 step: 669, loss is 0.0015652150614187121\n",
      "epoch: 4 step: 670, loss is 0.020139414817094803\n",
      "epoch: 4 step: 671, loss is 0.034842755645513535\n",
      "epoch: 4 step: 672, loss is 0.08187366276979446\n",
      "epoch: 4 step: 673, loss is 0.014208625070750713\n",
      "epoch: 4 step: 674, loss is 0.04598059877753258\n",
      "epoch: 4 step: 675, loss is 0.005143192131072283\n",
      "epoch: 4 step: 676, loss is 0.0024991915561258793\n",
      "epoch: 4 step: 677, loss is 0.05741923302412033\n",
      "epoch: 4 step: 678, loss is 0.0031284336000680923\n",
      "epoch: 4 step: 679, loss is 0.021822016686201096\n",
      "epoch: 4 step: 680, loss is 0.02195006050169468\n",
      "epoch: 4 step: 681, loss is 0.0007202852284535766\n",
      "epoch: 4 step: 682, loss is 0.014308958314359188\n",
      "epoch: 4 step: 683, loss is 0.03819318115711212\n",
      "epoch: 4 step: 684, loss is 0.002120565390214324\n",
      "epoch: 4 step: 685, loss is 0.07796067744493484\n",
      "epoch: 4 step: 686, loss is 0.028680287301540375\n",
      "epoch: 4 step: 687, loss is 0.021717214956879616\n",
      "epoch: 4 step: 688, loss is 0.06217481195926666\n",
      "epoch: 4 step: 689, loss is 0.021044854074716568\n",
      "epoch: 4 step: 690, loss is 0.004979567136615515\n",
      "epoch: 4 step: 691, loss is 0.00036470693885348737\n",
      "epoch: 4 step: 692, loss is 0.0006540357717312872\n",
      "epoch: 4 step: 693, loss is 0.009987549856305122\n",
      "epoch: 4 step: 694, loss is 0.03240547329187393\n",
      "epoch: 4 step: 695, loss is 0.00790022499859333\n",
      "epoch: 4 step: 696, loss is 0.000616671924944967\n",
      "epoch: 4 step: 697, loss is 0.045119863003492355\n",
      "epoch: 4 step: 698, loss is 0.015035359188914299\n",
      "epoch: 4 step: 699, loss is 0.0005188051145523787\n",
      "epoch: 4 step: 700, loss is 0.12271609902381897\n",
      "epoch: 4 step: 701, loss is 0.04511982947587967\n",
      "epoch: 4 step: 702, loss is 0.00121008837595582\n",
      "epoch: 4 step: 703, loss is 0.02688298001885414\n",
      "epoch: 4 step: 704, loss is 0.0013257258106023073\n",
      "epoch: 4 step: 705, loss is 0.008524805307388306\n",
      "epoch: 4 step: 706, loss is 0.011834259144961834\n",
      "epoch: 4 step: 707, loss is 0.05586607754230499\n",
      "epoch: 4 step: 708, loss is 0.07067383825778961\n",
      "epoch: 4 step: 709, loss is 0.004561584442853928\n",
      "epoch: 4 step: 710, loss is 0.0026958484668284655\n",
      "epoch: 4 step: 711, loss is 0.0038273322861641645\n",
      "epoch: 4 step: 712, loss is 0.004438292700797319\n",
      "epoch: 4 step: 713, loss is 0.015111714601516724\n",
      "epoch: 4 step: 714, loss is 0.007679180707782507\n",
      "epoch: 4 step: 715, loss is 0.001416824641637504\n",
      "epoch: 4 step: 716, loss is 0.0008671816904097795\n",
      "epoch: 4 step: 717, loss is 0.0003008476342074573\n",
      "epoch: 4 step: 718, loss is 0.03211136907339096\n",
      "epoch: 4 step: 719, loss is 0.00285970326513052\n",
      "epoch: 4 step: 720, loss is 0.00018566254584584385\n",
      "epoch: 4 step: 721, loss is 0.00045332557056099176\n",
      "epoch: 4 step: 722, loss is 0.02023685723543167\n",
      "epoch: 4 step: 723, loss is 0.03275524824857712\n",
      "epoch: 4 step: 724, loss is 0.018415581434965134\n",
      "epoch: 4 step: 725, loss is 0.000793757732026279\n",
      "epoch: 4 step: 726, loss is 0.0003798050165642053\n",
      "epoch: 4 step: 727, loss is 0.030045993626117706\n",
      "epoch: 4 step: 728, loss is 0.0766725242137909\n",
      "epoch: 4 step: 729, loss is 0.0675511360168457\n",
      "epoch: 4 step: 730, loss is 0.06362562626600266\n",
      "epoch: 4 step: 731, loss is 0.002331916708499193\n",
      "epoch: 4 step: 732, loss is 0.17212827503681183\n",
      "epoch: 4 step: 733, loss is 0.00552121177315712\n",
      "epoch: 4 step: 734, loss is 0.0017210070509463549\n",
      "epoch: 4 step: 735, loss is 0.10309988260269165\n",
      "epoch: 4 step: 736, loss is 0.009317409247159958\n",
      "epoch: 4 step: 737, loss is 0.17277570068836212\n",
      "epoch: 4 step: 738, loss is 0.0013876882148906589\n",
      "epoch: 4 step: 739, loss is 0.05812523141503334\n",
      "epoch: 4 step: 740, loss is 0.013928914442658424\n",
      "epoch: 4 step: 741, loss is 0.05889790877699852\n",
      "epoch: 4 step: 742, loss is 0.00996419321745634\n",
      "epoch: 4 step: 743, loss is 0.003286715131253004\n",
      "epoch: 4 step: 744, loss is 0.055034004151821136\n",
      "epoch: 4 step: 745, loss is 0.00542219914495945\n",
      "epoch: 4 step: 746, loss is 0.0032769208773970604\n",
      "epoch: 4 step: 747, loss is 0.0008661478641442955\n",
      "epoch: 4 step: 748, loss is 0.0013053853763267398\n",
      "epoch: 4 step: 749, loss is 0.051862530410289764\n",
      "epoch: 4 step: 750, loss is 0.005032224580645561\n",
      "epoch: 4 step: 751, loss is 0.01976669579744339\n",
      "epoch: 4 step: 752, loss is 0.05297766625881195\n",
      "epoch: 4 step: 753, loss is 0.025151800364255905\n",
      "epoch: 4 step: 754, loss is 0.004362661857157946\n",
      "epoch: 4 step: 755, loss is 0.012338092550635338\n",
      "epoch: 4 step: 756, loss is 0.04754960536956787\n",
      "epoch: 4 step: 757, loss is 0.01960947923362255\n",
      "epoch: 4 step: 758, loss is 0.0030800916720181704\n",
      "epoch: 4 step: 759, loss is 0.006586697418242693\n",
      "epoch: 4 step: 760, loss is 0.04681548848748207\n",
      "epoch: 4 step: 761, loss is 0.0025445264764130116\n",
      "epoch: 4 step: 762, loss is 0.004249208606779575\n",
      "epoch: 4 step: 763, loss is 0.0009318591328337789\n",
      "epoch: 4 step: 764, loss is 0.002055014483630657\n",
      "epoch: 4 step: 765, loss is 0.07383140921592712\n",
      "epoch: 4 step: 766, loss is 0.001196730532683432\n",
      "epoch: 4 step: 767, loss is 0.0016002586344256997\n",
      "epoch: 4 step: 768, loss is 0.0033907119650393724\n",
      "epoch: 4 step: 769, loss is 0.07635375112295151\n",
      "epoch: 4 step: 770, loss is 0.020066246390342712\n",
      "epoch: 4 step: 771, loss is 0.0016495792660862207\n",
      "epoch: 4 step: 772, loss is 0.0006018837448209524\n",
      "epoch: 4 step: 773, loss is 0.004278190899640322\n",
      "epoch: 4 step: 774, loss is 0.005913375876843929\n",
      "epoch: 4 step: 775, loss is 0.00011508074385346845\n",
      "epoch: 4 step: 776, loss is 0.006052641663700342\n",
      "epoch: 4 step: 777, loss is 0.021241258829832077\n",
      "epoch: 4 step: 778, loss is 0.1345670372247696\n",
      "epoch: 4 step: 779, loss is 0.025508606806397438\n",
      "epoch: 4 step: 780, loss is 0.020716972649097443\n",
      "epoch: 4 step: 781, loss is 0.006276634521782398\n",
      "epoch: 4 step: 782, loss is 0.01968214102089405\n",
      "epoch: 4 step: 783, loss is 0.0038891970179975033\n",
      "epoch: 4 step: 784, loss is 0.017332397401332855\n",
      "epoch: 4 step: 785, loss is 0.00564536964520812\n",
      "epoch: 4 step: 786, loss is 0.028609829023480415\n",
      "epoch: 4 step: 787, loss is 0.00216829776763916\n",
      "epoch: 4 step: 788, loss is 0.008896654471755028\n",
      "epoch: 4 step: 789, loss is 0.023943990468978882\n",
      "epoch: 4 step: 790, loss is 0.0002640250895638019\n",
      "epoch: 4 step: 791, loss is 0.01155491266399622\n",
      "epoch: 4 step: 792, loss is 0.001155843143351376\n",
      "epoch: 4 step: 793, loss is 0.0013499446213245392\n",
      "epoch: 4 step: 794, loss is 0.001467037247493863\n",
      "epoch: 4 step: 795, loss is 0.0026620926801115274\n",
      "epoch: 4 step: 796, loss is 0.003884249133989215\n",
      "epoch: 4 step: 797, loss is 0.0034243050031363964\n",
      "epoch: 4 step: 798, loss is 0.029952561482787132\n",
      "epoch: 4 step: 799, loss is 0.00016369327204301953\n",
      "epoch: 4 step: 800, loss is 0.059168748557567596\n",
      "epoch: 4 step: 801, loss is 0.17543278634548187\n",
      "epoch: 4 step: 802, loss is 0.003968504257500172\n",
      "epoch: 4 step: 803, loss is 0.016856996342539787\n",
      "epoch: 4 step: 804, loss is 0.030830925330519676\n",
      "epoch: 4 step: 805, loss is 0.09369885921478271\n",
      "epoch: 4 step: 806, loss is 0.00039078400004655123\n",
      "epoch: 4 step: 807, loss is 0.0001993587356992066\n",
      "epoch: 4 step: 808, loss is 0.00040226647979579866\n",
      "epoch: 4 step: 809, loss is 0.0033936197869479656\n",
      "epoch: 4 step: 810, loss is 0.02574286423623562\n",
      "epoch: 4 step: 811, loss is 0.004219728522002697\n",
      "epoch: 4 step: 812, loss is 0.022716663777828217\n",
      "epoch: 4 step: 813, loss is 0.0004092830640729517\n",
      "epoch: 4 step: 814, loss is 0.004273585043847561\n",
      "epoch: 4 step: 815, loss is 0.0039016292430460453\n",
      "epoch: 4 step: 816, loss is 0.0015624940861016512\n",
      "epoch: 4 step: 817, loss is 0.020983530208468437\n",
      "epoch: 4 step: 818, loss is 0.009870902635157108\n",
      "epoch: 4 step: 819, loss is 0.002437208080664277\n",
      "epoch: 4 step: 820, loss is 0.01569773256778717\n",
      "epoch: 4 step: 821, loss is 0.00017970030603464693\n",
      "epoch: 4 step: 822, loss is 0.000934045878238976\n",
      "epoch: 4 step: 823, loss is 0.0006094069103710353\n",
      "epoch: 4 step: 824, loss is 0.0026698156725615263\n",
      "epoch: 4 step: 825, loss is 0.048624057322740555\n",
      "epoch: 4 step: 826, loss is 0.00020265036437194794\n",
      "epoch: 4 step: 827, loss is 0.007999933324754238\n",
      "epoch: 4 step: 828, loss is 0.004719449672847986\n",
      "epoch: 4 step: 829, loss is 0.0010376739082857966\n",
      "epoch: 4 step: 830, loss is 0.002809613011777401\n",
      "epoch: 4 step: 831, loss is 0.0007980879163369536\n",
      "epoch: 4 step: 832, loss is 0.02505357377231121\n",
      "epoch: 4 step: 833, loss is 0.0032816496677696705\n",
      "epoch: 4 step: 834, loss is 0.02172059752047062\n",
      "epoch: 4 step: 835, loss is 0.0001789204397937283\n",
      "epoch: 4 step: 836, loss is 0.015696296468377113\n",
      "epoch: 4 step: 837, loss is 0.0009405245655216277\n",
      "epoch: 4 step: 838, loss is 0.005307449027895927\n",
      "epoch: 4 step: 839, loss is 0.001891443389467895\n",
      "epoch: 4 step: 840, loss is 0.14701341092586517\n",
      "epoch: 4 step: 841, loss is 0.000680296856444329\n",
      "epoch: 4 step: 842, loss is 0.00021454866509884596\n",
      "epoch: 4 step: 843, loss is 0.04613080248236656\n",
      "epoch: 4 step: 844, loss is 0.11946724355220795\n",
      "epoch: 4 step: 845, loss is 0.1587463915348053\n",
      "epoch: 4 step: 846, loss is 0.010412751697003841\n",
      "epoch: 4 step: 847, loss is 0.0006793843349441886\n",
      "epoch: 4 step: 848, loss is 0.006306741386651993\n",
      "epoch: 4 step: 849, loss is 0.08000145107507706\n",
      "epoch: 4 step: 850, loss is 0.08874781429767609\n",
      "epoch: 4 step: 851, loss is 0.08538925647735596\n",
      "epoch: 4 step: 852, loss is 0.061035096645355225\n",
      "epoch: 4 step: 853, loss is 0.00032640821882523596\n",
      "epoch: 4 step: 854, loss is 0.027731282636523247\n",
      "epoch: 4 step: 855, loss is 0.0011966410093009472\n",
      "epoch: 4 step: 856, loss is 0.004717111121863127\n",
      "epoch: 4 step: 857, loss is 0.0008577316766604781\n",
      "epoch: 4 step: 858, loss is 0.0005809605354443192\n",
      "epoch: 4 step: 859, loss is 0.00031233998015522957\n",
      "epoch: 4 step: 860, loss is 0.06943013519048691\n",
      "epoch: 4 step: 861, loss is 0.0019279244588688016\n",
      "epoch: 4 step: 862, loss is 0.005173142999410629\n",
      "epoch: 4 step: 863, loss is 0.0007977599161677063\n",
      "epoch: 4 step: 864, loss is 0.0007316803093999624\n",
      "epoch: 4 step: 865, loss is 0.004128145053982735\n",
      "epoch: 4 step: 866, loss is 0.001119472086429596\n",
      "epoch: 4 step: 867, loss is 0.023631474003195763\n",
      "epoch: 4 step: 868, loss is 0.31680309772491455\n",
      "epoch: 4 step: 869, loss is 0.040510211139917374\n",
      "epoch: 4 step: 870, loss is 0.008450553752481937\n",
      "epoch: 4 step: 871, loss is 0.007135395426303148\n",
      "epoch: 4 step: 872, loss is 0.0005086068995296955\n",
      "epoch: 4 step: 873, loss is 0.002594953402876854\n",
      "epoch: 4 step: 874, loss is 0.1382935494184494\n",
      "epoch: 4 step: 875, loss is 0.01841764897108078\n",
      "epoch: 4 step: 876, loss is 0.07004640996456146\n",
      "epoch: 4 step: 877, loss is 0.03900114446878433\n",
      "epoch: 4 step: 878, loss is 0.15550129115581512\n",
      "epoch: 4 step: 879, loss is 0.06192146614193916\n",
      "epoch: 4 step: 880, loss is 0.006914322264492512\n",
      "epoch: 4 step: 881, loss is 0.22127369046211243\n",
      "epoch: 4 step: 882, loss is 0.00016160309314727783\n",
      "epoch: 4 step: 883, loss is 0.0007502056541852653\n",
      "epoch: 4 step: 884, loss is 0.06876606494188309\n",
      "epoch: 4 step: 885, loss is 0.0035267663188278675\n",
      "epoch: 4 step: 886, loss is 0.04412850737571716\n",
      "epoch: 4 step: 887, loss is 0.014972878620028496\n",
      "epoch: 4 step: 888, loss is 0.02387816272675991\n",
      "epoch: 4 step: 889, loss is 0.006724242586642504\n",
      "epoch: 4 step: 890, loss is 0.06924517452716827\n",
      "epoch: 4 step: 891, loss is 0.008692300878465176\n",
      "epoch: 4 step: 892, loss is 0.08645287901163101\n",
      "epoch: 4 step: 893, loss is 0.035401735454797745\n",
      "epoch: 4 step: 894, loss is 0.003946902696043253\n",
      "epoch: 4 step: 895, loss is 0.0015600998885929585\n",
      "epoch: 4 step: 896, loss is 0.0007892708526924253\n",
      "epoch: 4 step: 897, loss is 0.044335849583148956\n",
      "epoch: 4 step: 898, loss is 0.08640839159488678\n",
      "epoch: 4 step: 899, loss is 0.022977769374847412\n",
      "epoch: 4 step: 900, loss is 0.005085245706140995\n",
      "epoch: 4 step: 901, loss is 0.0011423659743741155\n",
      "epoch: 4 step: 902, loss is 0.0022468576207756996\n",
      "epoch: 4 step: 903, loss is 0.0010861065238714218\n",
      "epoch: 4 step: 904, loss is 0.019251609221100807\n",
      "epoch: 4 step: 905, loss is 0.00035274712718091905\n",
      "epoch: 4 step: 906, loss is 0.0011070140171796083\n",
      "epoch: 4 step: 907, loss is 0.00040636514313519\n",
      "epoch: 4 step: 908, loss is 0.016264690086245537\n",
      "epoch: 4 step: 909, loss is 0.002178572118282318\n",
      "epoch: 4 step: 910, loss is 0.00014373545127455145\n",
      "epoch: 4 step: 911, loss is 0.026126155629754066\n",
      "epoch: 4 step: 912, loss is 0.01019668485969305\n",
      "epoch: 4 step: 913, loss is 0.04851052910089493\n",
      "epoch: 4 step: 914, loss is 0.015337998978793621\n",
      "epoch: 4 step: 915, loss is 0.005595912225544453\n",
      "epoch: 4 step: 916, loss is 0.20210474729537964\n",
      "epoch: 4 step: 917, loss is 0.001439273008145392\n",
      "epoch: 4 step: 918, loss is 0.18356677889823914\n",
      "epoch: 4 step: 919, loss is 0.000882064807228744\n",
      "epoch: 4 step: 920, loss is 0.1521805226802826\n",
      "epoch: 4 step: 921, loss is 0.049150608479976654\n",
      "epoch: 4 step: 922, loss is 0.017142977565526962\n",
      "epoch: 4 step: 923, loss is 0.017229847609996796\n",
      "epoch: 4 step: 924, loss is 0.006080486346036196\n",
      "epoch: 4 step: 925, loss is 0.0127745745703578\n",
      "epoch: 4 step: 926, loss is 0.03296705707907677\n",
      "epoch: 4 step: 927, loss is 0.005136000923812389\n",
      "epoch: 4 step: 928, loss is 0.0853862389922142\n",
      "epoch: 4 step: 929, loss is 0.004142425954341888\n",
      "epoch: 4 step: 930, loss is 0.015058428980410099\n",
      "epoch: 4 step: 931, loss is 0.050750311464071274\n",
      "epoch: 4 step: 932, loss is 0.04792550578713417\n",
      "epoch: 4 step: 933, loss is 0.0008294216240756214\n",
      "epoch: 4 step: 934, loss is 0.006986191961914301\n",
      "epoch: 4 step: 935, loss is 0.0005107852630317211\n",
      "epoch: 4 step: 936, loss is 0.001508032437413931\n",
      "epoch: 4 step: 937, loss is 0.008950842544436455\n",
      "epoch: 4 step: 938, loss is 0.009923990815877914\n",
      "epoch: 4 step: 939, loss is 0.0020709773525595665\n",
      "epoch: 4 step: 940, loss is 0.009047926403582096\n",
      "epoch: 4 step: 941, loss is 0.0836997702717781\n",
      "epoch: 4 step: 942, loss is 0.0029838469345122576\n",
      "epoch: 4 step: 943, loss is 0.002787345787510276\n",
      "epoch: 4 step: 944, loss is 0.14940804243087769\n",
      "epoch: 4 step: 945, loss is 0.16156618297100067\n",
      "epoch: 4 step: 946, loss is 0.03596527874469757\n",
      "epoch: 4 step: 947, loss is 0.031006932258605957\n",
      "epoch: 4 step: 948, loss is 0.01050432026386261\n",
      "epoch: 4 step: 949, loss is 0.0018985255155712366\n",
      "epoch: 4 step: 950, loss is 0.0011621257290244102\n",
      "epoch: 4 step: 951, loss is 0.10604709386825562\n",
      "epoch: 4 step: 952, loss is 0.00518818898126483\n",
      "epoch: 4 step: 953, loss is 0.017805470153689384\n",
      "epoch: 4 step: 954, loss is 0.02440020628273487\n",
      "epoch: 4 step: 955, loss is 0.044559404253959656\n",
      "epoch: 4 step: 956, loss is 0.00045764612150378525\n",
      "epoch: 4 step: 957, loss is 0.07119525969028473\n",
      "epoch: 4 step: 958, loss is 0.0004148543521296233\n",
      "epoch: 4 step: 959, loss is 0.0005225027562119067\n",
      "epoch: 4 step: 960, loss is 0.040912169963121414\n",
      "epoch: 4 step: 961, loss is 0.015324683859944344\n",
      "epoch: 4 step: 962, loss is 0.00645703449845314\n",
      "epoch: 4 step: 963, loss is 0.0005193553515709937\n",
      "epoch: 4 step: 964, loss is 0.04988773912191391\n",
      "epoch: 4 step: 965, loss is 0.0008098362013697624\n",
      "epoch: 4 step: 966, loss is 0.09165897220373154\n",
      "epoch: 4 step: 967, loss is 0.014900639653205872\n",
      "epoch: 4 step: 968, loss is 0.012133360840380192\n",
      "epoch: 4 step: 969, loss is 0.0024621589109301567\n",
      "epoch: 4 step: 970, loss is 0.007162090390920639\n",
      "epoch: 4 step: 971, loss is 0.002947533503174782\n",
      "epoch: 4 step: 972, loss is 0.006388073321431875\n",
      "epoch: 4 step: 973, loss is 0.018017230555415154\n",
      "epoch: 4 step: 974, loss is 0.030608464032411575\n",
      "epoch: 4 step: 975, loss is 0.048283010721206665\n",
      "epoch: 4 step: 976, loss is 0.003696521045640111\n",
      "epoch: 4 step: 977, loss is 0.020091215148568153\n",
      "epoch: 4 step: 978, loss is 0.10864397883415222\n",
      "epoch: 4 step: 979, loss is 0.0005398006760515273\n",
      "epoch: 4 step: 980, loss is 0.0010067314142361283\n",
      "epoch: 4 step: 981, loss is 0.0008018542430363595\n",
      "epoch: 4 step: 982, loss is 0.0013432521373033524\n",
      "epoch: 4 step: 983, loss is 0.0016844442579895258\n",
      "epoch: 4 step: 984, loss is 0.001229439047165215\n",
      "epoch: 4 step: 985, loss is 0.1219959482550621\n",
      "epoch: 4 step: 986, loss is 0.0014617502456530929\n",
      "epoch: 4 step: 987, loss is 0.007544661406427622\n",
      "epoch: 4 step: 988, loss is 0.03989085927605629\n",
      "epoch: 4 step: 989, loss is 0.030214985832571983\n",
      "epoch: 4 step: 990, loss is 0.13605211675167084\n",
      "epoch: 4 step: 991, loss is 0.02464391104876995\n",
      "epoch: 4 step: 992, loss is 0.05121024325489998\n",
      "epoch: 4 step: 993, loss is 0.007500923238694668\n",
      "epoch: 4 step: 994, loss is 0.0005555442185141146\n",
      "epoch: 4 step: 995, loss is 0.0011461921967566013\n",
      "epoch: 4 step: 996, loss is 0.0006937786820344627\n",
      "epoch: 4 step: 997, loss is 0.009221619926393032\n",
      "epoch: 4 step: 998, loss is 0.14110229909420013\n",
      "epoch: 4 step: 999, loss is 0.011551815085113049\n",
      "epoch: 4 step: 1000, loss is 0.02092120051383972\n",
      "epoch: 4 step: 1001, loss is 0.05358009785413742\n",
      "epoch: 4 step: 1002, loss is 0.03423710912466049\n",
      "epoch: 4 step: 1003, loss is 0.012711922638118267\n",
      "epoch: 4 step: 1004, loss is 0.08200710266828537\n",
      "epoch: 4 step: 1005, loss is 0.07147113233804703\n",
      "epoch: 4 step: 1006, loss is 0.05666737258434296\n",
      "epoch: 4 step: 1007, loss is 0.0034248451702296734\n",
      "epoch: 4 step: 1008, loss is 0.10386534780263901\n",
      "epoch: 4 step: 1009, loss is 0.0833410769701004\n",
      "epoch: 4 step: 1010, loss is 0.0042890929616987705\n",
      "epoch: 4 step: 1011, loss is 0.12881635129451752\n",
      "epoch: 4 step: 1012, loss is 0.020823797211050987\n",
      "epoch: 4 step: 1013, loss is 0.0014128572074696422\n",
      "epoch: 4 step: 1014, loss is 0.0037553070578724146\n",
      "epoch: 4 step: 1015, loss is 0.05224894732236862\n",
      "epoch: 4 step: 1016, loss is 0.02503373473882675\n",
      "epoch: 4 step: 1017, loss is 0.20997431874275208\n",
      "epoch: 4 step: 1018, loss is 0.006117889191955328\n",
      "epoch: 4 step: 1019, loss is 0.008294123224914074\n",
      "epoch: 4 step: 1020, loss is 0.01117327343672514\n",
      "epoch: 4 step: 1021, loss is 0.03740154951810837\n",
      "epoch: 4 step: 1022, loss is 0.0022640335373580456\n",
      "epoch: 4 step: 1023, loss is 0.05308881029486656\n",
      "epoch: 4 step: 1024, loss is 0.02196185290813446\n",
      "epoch: 4 step: 1025, loss is 0.00143424142152071\n",
      "epoch: 4 step: 1026, loss is 0.11094854027032852\n",
      "epoch: 4 step: 1027, loss is 0.0009610677370801568\n",
      "epoch: 4 step: 1028, loss is 0.0031480060424655676\n",
      "epoch: 4 step: 1029, loss is 0.09110940992832184\n",
      "epoch: 4 step: 1030, loss is 0.004358867183327675\n",
      "epoch: 4 step: 1031, loss is 0.2182205319404602\n",
      "epoch: 4 step: 1032, loss is 0.0030646224040538073\n",
      "epoch: 4 step: 1033, loss is 0.01669537089765072\n",
      "epoch: 4 step: 1034, loss is 0.004938684869557619\n",
      "epoch: 4 step: 1035, loss is 0.06687626242637634\n",
      "epoch: 4 step: 1036, loss is 0.029501253738999367\n",
      "epoch: 4 step: 1037, loss is 0.0005316328606568277\n",
      "epoch: 4 step: 1038, loss is 0.0036302104126662016\n",
      "epoch: 4 step: 1039, loss is 0.0427895113825798\n",
      "epoch: 4 step: 1040, loss is 0.013029362075030804\n",
      "epoch: 4 step: 1041, loss is 0.0014674775302410126\n",
      "epoch: 4 step: 1042, loss is 0.0015984242781996727\n",
      "epoch: 4 step: 1043, loss is 0.015730591490864754\n",
      "epoch: 4 step: 1044, loss is 0.007429015822708607\n",
      "epoch: 4 step: 1045, loss is 0.012337635271251202\n",
      "epoch: 4 step: 1046, loss is 0.036571893841028214\n",
      "epoch: 4 step: 1047, loss is 0.003214630065485835\n",
      "epoch: 4 step: 1048, loss is 0.003031291998922825\n",
      "epoch: 4 step: 1049, loss is 0.019227545708417892\n",
      "epoch: 4 step: 1050, loss is 0.003742692293599248\n",
      "epoch: 4 step: 1051, loss is 0.0027722823433578014\n",
      "epoch: 4 step: 1052, loss is 0.0030690347775816917\n",
      "epoch: 4 step: 1053, loss is 0.0005201999447308481\n",
      "epoch: 4 step: 1054, loss is 0.0011052078334614635\n",
      "epoch: 4 step: 1055, loss is 0.0396309569478035\n",
      "epoch: 4 step: 1056, loss is 0.11169754713773727\n",
      "epoch: 4 step: 1057, loss is 0.029744930565357208\n",
      "epoch: 4 step: 1058, loss is 0.05112864077091217\n",
      "epoch: 4 step: 1059, loss is 0.04489584267139435\n",
      "epoch: 4 step: 1060, loss is 0.022840622812509537\n",
      "epoch: 4 step: 1061, loss is 0.0012518727453425527\n",
      "epoch: 4 step: 1062, loss is 0.00022996503685135394\n",
      "epoch: 4 step: 1063, loss is 0.004415424074977636\n",
      "epoch: 4 step: 1064, loss is 0.00458279624581337\n",
      "epoch: 4 step: 1065, loss is 0.08361060172319412\n",
      "epoch: 4 step: 1066, loss is 0.024347979575395584\n",
      "epoch: 4 step: 1067, loss is 0.01052404846996069\n",
      "epoch: 4 step: 1068, loss is 0.0016184933483600616\n",
      "epoch: 4 step: 1069, loss is 0.08053144812583923\n",
      "epoch: 4 step: 1070, loss is 0.0004939066129736602\n",
      "epoch: 4 step: 1071, loss is 0.03261835128068924\n",
      "epoch: 4 step: 1072, loss is 0.0004592278564814478\n",
      "epoch: 4 step: 1073, loss is 0.017716651782393456\n",
      "epoch: 4 step: 1074, loss is 0.03346012905240059\n",
      "epoch: 4 step: 1075, loss is 0.006147013045847416\n",
      "epoch: 4 step: 1076, loss is 0.0039400761015713215\n",
      "epoch: 4 step: 1077, loss is 0.00019402483303565532\n",
      "epoch: 4 step: 1078, loss is 0.05539294332265854\n",
      "epoch: 4 step: 1079, loss is 0.007989986799657345\n",
      "epoch: 4 step: 1080, loss is 0.004599422682076693\n",
      "epoch: 4 step: 1081, loss is 0.06791935116052628\n",
      "epoch: 4 step: 1082, loss is 0.0009068097569979727\n",
      "epoch: 4 step: 1083, loss is 0.0010142114479094744\n",
      "epoch: 4 step: 1084, loss is 0.008911047130823135\n",
      "epoch: 4 step: 1085, loss is 0.02258518524467945\n",
      "epoch: 4 step: 1086, loss is 0.003429556731134653\n",
      "epoch: 4 step: 1087, loss is 0.0010314135579392314\n",
      "epoch: 4 step: 1088, loss is 0.12867054343223572\n",
      "epoch: 4 step: 1089, loss is 0.028562981635332108\n",
      "epoch: 4 step: 1090, loss is 0.06467980891466141\n",
      "epoch: 4 step: 1091, loss is 0.002130940556526184\n",
      "epoch: 4 step: 1092, loss is 0.08186501264572144\n",
      "epoch: 4 step: 1093, loss is 0.005308809224516153\n",
      "epoch: 4 step: 1094, loss is 0.020847387611865997\n",
      "epoch: 4 step: 1095, loss is 0.0030823289416730404\n",
      "epoch: 4 step: 1096, loss is 0.011561982333660126\n",
      "epoch: 4 step: 1097, loss is 0.010065888985991478\n",
      "epoch: 4 step: 1098, loss is 0.08397664874792099\n",
      "epoch: 4 step: 1099, loss is 0.00014783322694711387\n",
      "epoch: 4 step: 1100, loss is 0.1043350026011467\n",
      "epoch: 4 step: 1101, loss is 0.022636108100414276\n",
      "epoch: 4 step: 1102, loss is 0.008037964813411236\n",
      "epoch: 4 step: 1103, loss is 0.015140585601329803\n",
      "epoch: 4 step: 1104, loss is 0.15883328020572662\n",
      "epoch: 4 step: 1105, loss is 0.003481288906186819\n",
      "epoch: 4 step: 1106, loss is 0.014538167975842953\n",
      "epoch: 4 step: 1107, loss is 0.008819752372801304\n",
      "epoch: 4 step: 1108, loss is 0.0661243349313736\n",
      "epoch: 4 step: 1109, loss is 0.057142313569784164\n",
      "epoch: 4 step: 1110, loss is 0.04410989210009575\n",
      "epoch: 4 step: 1111, loss is 0.005103241186589003\n",
      "epoch: 4 step: 1112, loss is 0.002767625730484724\n",
      "epoch: 4 step: 1113, loss is 0.043877582997083664\n",
      "epoch: 4 step: 1114, loss is 0.026697218418121338\n",
      "epoch: 4 step: 1115, loss is 0.008359457366168499\n",
      "epoch: 4 step: 1116, loss is 0.00090251729125157\n",
      "epoch: 4 step: 1117, loss is 0.01149671245366335\n",
      "epoch: 4 step: 1118, loss is 0.00019714073278009892\n",
      "epoch: 4 step: 1119, loss is 0.0013894439907744527\n",
      "epoch: 4 step: 1120, loss is 0.056343693286180496\n",
      "epoch: 4 step: 1121, loss is 0.004208499100059271\n",
      "epoch: 4 step: 1122, loss is 0.0012773729395121336\n",
      "epoch: 4 step: 1123, loss is 0.19886212050914764\n",
      "epoch: 4 step: 1124, loss is 0.010599536821246147\n",
      "epoch: 4 step: 1125, loss is 0.0004653270007111132\n",
      "epoch: 4 step: 1126, loss is 0.011253094300627708\n",
      "epoch: 4 step: 1127, loss is 0.1482412964105606\n",
      "epoch: 4 step: 1128, loss is 0.00020557634707074612\n",
      "epoch: 4 step: 1129, loss is 0.007711115293204784\n",
      "epoch: 4 step: 1130, loss is 0.0011623246828094125\n",
      "epoch: 4 step: 1131, loss is 0.003684067400172353\n",
      "epoch: 4 step: 1132, loss is 0.0032239467836916447\n",
      "epoch: 4 step: 1133, loss is 0.022744296118617058\n",
      "epoch: 4 step: 1134, loss is 0.016758417710661888\n",
      "epoch: 4 step: 1135, loss is 0.01758500747382641\n",
      "epoch: 4 step: 1136, loss is 0.07627634704113007\n",
      "epoch: 4 step: 1137, loss is 0.00017316256707999855\n",
      "epoch: 4 step: 1138, loss is 0.09345141053199768\n",
      "epoch: 4 step: 1139, loss is 0.0022160084918141365\n",
      "epoch: 4 step: 1140, loss is 0.001583611243404448\n",
      "epoch: 4 step: 1141, loss is 0.0012996840523555875\n",
      "epoch: 4 step: 1142, loss is 0.004917016252875328\n",
      "epoch: 4 step: 1143, loss is 0.001462609856389463\n",
      "epoch: 4 step: 1144, loss is 0.03657199814915657\n",
      "epoch: 4 step: 1145, loss is 0.012733631767332554\n",
      "epoch: 4 step: 1146, loss is 0.01209784485399723\n",
      "epoch: 4 step: 1147, loss is 0.035882722586393356\n",
      "epoch: 4 step: 1148, loss is 0.12753842771053314\n",
      "epoch: 4 step: 1149, loss is 0.0008374371100217104\n",
      "epoch: 4 step: 1150, loss is 0.05444527417421341\n",
      "epoch: 4 step: 1151, loss is 0.000967118889093399\n",
      "epoch: 4 step: 1152, loss is 0.000173027568962425\n",
      "epoch: 4 step: 1153, loss is 0.004671390168368816\n",
      "epoch: 4 step: 1154, loss is 0.0019694280344992876\n",
      "epoch: 4 step: 1155, loss is 0.0730653703212738\n",
      "epoch: 4 step: 1156, loss is 0.0654272511601448\n",
      "epoch: 4 step: 1157, loss is 0.06763682514429092\n",
      "epoch: 4 step: 1158, loss is 0.04543435573577881\n",
      "epoch: 4 step: 1159, loss is 0.015543650835752487\n",
      "epoch: 4 step: 1160, loss is 0.0008080346160568297\n",
      "epoch: 4 step: 1161, loss is 0.004932504612952471\n",
      "epoch: 4 step: 1162, loss is 0.12136557698249817\n",
      "epoch: 4 step: 1163, loss is 0.0069106281735002995\n",
      "epoch: 4 step: 1164, loss is 0.0004078679485246539\n",
      "epoch: 4 step: 1165, loss is 0.023384442552924156\n",
      "epoch: 4 step: 1166, loss is 0.002527079079300165\n",
      "epoch: 4 step: 1167, loss is 0.2268155813217163\n",
      "epoch: 4 step: 1168, loss is 0.12196993827819824\n",
      "epoch: 4 step: 1169, loss is 0.48464101552963257\n",
      "epoch: 4 step: 1170, loss is 0.026190996170043945\n",
      "epoch: 4 step: 1171, loss is 0.02596311829984188\n",
      "epoch: 4 step: 1172, loss is 0.03953641653060913\n",
      "epoch: 4 step: 1173, loss is 0.006401597987860441\n",
      "epoch: 4 step: 1174, loss is 0.04211672767996788\n",
      "epoch: 4 step: 1175, loss is 0.006171452812850475\n",
      "epoch: 4 step: 1176, loss is 0.08457738161087036\n",
      "epoch: 4 step: 1177, loss is 0.005987267475575209\n",
      "epoch: 4 step: 1178, loss is 0.20391638576984406\n",
      "epoch: 4 step: 1179, loss is 0.006819458678364754\n",
      "epoch: 4 step: 1180, loss is 0.000522644491866231\n",
      "epoch: 4 step: 1181, loss is 0.039624907076358795\n",
      "epoch: 4 step: 1182, loss is 0.0010669594630599022\n",
      "epoch: 4 step: 1183, loss is 0.002301874803379178\n",
      "epoch: 4 step: 1184, loss is 0.04365542158484459\n",
      "epoch: 4 step: 1185, loss is 0.005864288192242384\n",
      "epoch: 4 step: 1186, loss is 0.0006563879433088005\n",
      "epoch: 4 step: 1187, loss is 0.1801762580871582\n",
      "epoch: 4 step: 1188, loss is 0.00512432586401701\n",
      "epoch: 4 step: 1189, loss is 0.20049110054969788\n",
      "epoch: 4 step: 1190, loss is 0.004568571224808693\n",
      "epoch: 4 step: 1191, loss is 0.01322017703205347\n",
      "epoch: 4 step: 1192, loss is 0.0006715317722409964\n",
      "epoch: 4 step: 1193, loss is 0.00206275493837893\n",
      "epoch: 4 step: 1194, loss is 0.032505594193935394\n",
      "epoch: 4 step: 1195, loss is 0.002450265921652317\n",
      "epoch: 4 step: 1196, loss is 0.0015102144097909331\n",
      "epoch: 4 step: 1197, loss is 0.003559034550562501\n",
      "epoch: 4 step: 1198, loss is 0.006882230285555124\n",
      "epoch: 4 step: 1199, loss is 0.04060060903429985\n",
      "epoch: 4 step: 1200, loss is 0.01660432294011116\n",
      "epoch: 4 step: 1201, loss is 0.2904670834541321\n",
      "epoch: 4 step: 1202, loss is 0.004376132506877184\n",
      "epoch: 4 step: 1203, loss is 0.0009680642979219556\n",
      "epoch: 4 step: 1204, loss is 0.268985778093338\n",
      "epoch: 4 step: 1205, loss is 0.13405129313468933\n",
      "epoch: 4 step: 1206, loss is 0.02329271472990513\n",
      "epoch: 4 step: 1207, loss is 0.022557882592082024\n",
      "epoch: 4 step: 1208, loss is 0.0614747516810894\n",
      "epoch: 4 step: 1209, loss is 0.11901247501373291\n",
      "epoch: 4 step: 1210, loss is 0.23574309051036835\n",
      "epoch: 4 step: 1211, loss is 0.006403996609151363\n",
      "epoch: 4 step: 1212, loss is 0.07999555766582489\n",
      "epoch: 4 step: 1213, loss is 0.013310249894857407\n",
      "epoch: 4 step: 1214, loss is 0.0049689458683133125\n",
      "epoch: 4 step: 1215, loss is 0.002243715338408947\n",
      "epoch: 4 step: 1216, loss is 0.05002913624048233\n",
      "epoch: 4 step: 1217, loss is 0.1426454782485962\n",
      "epoch: 4 step: 1218, loss is 0.006955813616514206\n",
      "epoch: 4 step: 1219, loss is 0.0721488669514656\n",
      "epoch: 4 step: 1220, loss is 0.08101876825094223\n",
      "epoch: 4 step: 1221, loss is 0.0020045293495059013\n",
      "epoch: 4 step: 1222, loss is 0.0036110219080001116\n",
      "epoch: 4 step: 1223, loss is 0.0012750623282045126\n",
      "epoch: 4 step: 1224, loss is 0.02820192277431488\n",
      "epoch: 4 step: 1225, loss is 0.061337292194366455\n",
      "epoch: 4 step: 1226, loss is 0.030328378081321716\n",
      "epoch: 4 step: 1227, loss is 0.005263458006083965\n",
      "epoch: 4 step: 1228, loss is 0.09760479629039764\n",
      "epoch: 4 step: 1229, loss is 0.010849724523723125\n",
      "epoch: 4 step: 1230, loss is 0.008859601803123951\n",
      "epoch: 4 step: 1231, loss is 0.031030679121613503\n",
      "epoch: 4 step: 1232, loss is 0.04727931320667267\n",
      "epoch: 4 step: 1233, loss is 0.0481223538517952\n",
      "epoch: 4 step: 1234, loss is 0.10387483984231949\n",
      "epoch: 4 step: 1235, loss is 0.03316563740372658\n",
      "epoch: 4 step: 1236, loss is 0.02232973650097847\n",
      "epoch: 4 step: 1237, loss is 0.015186131000518799\n",
      "epoch: 4 step: 1238, loss is 0.00510068703442812\n",
      "epoch: 4 step: 1239, loss is 0.008835244923830032\n",
      "epoch: 4 step: 1240, loss is 0.008285543881356716\n",
      "epoch: 4 step: 1241, loss is 0.01106309425085783\n",
      "epoch: 4 step: 1242, loss is 0.00520783755928278\n",
      "epoch: 4 step: 1243, loss is 0.006163597106933594\n",
      "epoch: 4 step: 1244, loss is 0.0049742585979402065\n",
      "epoch: 4 step: 1245, loss is 0.008557070977985859\n",
      "epoch: 4 step: 1246, loss is 0.002589686308056116\n",
      "epoch: 4 step: 1247, loss is 0.012395950965583324\n",
      "epoch: 4 step: 1248, loss is 0.0027616729494184256\n",
      "epoch: 4 step: 1249, loss is 0.0012941589811816812\n",
      "epoch: 4 step: 1250, loss is 0.0018133569974452257\n",
      "epoch: 4 step: 1251, loss is 0.046691883355379105\n",
      "epoch: 4 step: 1252, loss is 0.013458466157317162\n",
      "epoch: 4 step: 1253, loss is 0.0038110155146569014\n",
      "epoch: 4 step: 1254, loss is 0.06187465414404869\n",
      "epoch: 4 step: 1255, loss is 0.032397929579019547\n",
      "epoch: 4 step: 1256, loss is 0.02155209705233574\n",
      "epoch: 4 step: 1257, loss is 0.021420296281576157\n",
      "epoch: 4 step: 1258, loss is 0.011225280351936817\n",
      "epoch: 4 step: 1259, loss is 0.027636229991912842\n",
      "epoch: 4 step: 1260, loss is 0.0004589380987454206\n",
      "epoch: 4 step: 1261, loss is 0.001039876602590084\n",
      "epoch: 4 step: 1262, loss is 0.0004611665790434927\n",
      "epoch: 4 step: 1263, loss is 0.002301747677847743\n",
      "epoch: 4 step: 1264, loss is 0.014013754203915596\n",
      "epoch: 4 step: 1265, loss is 0.0032343282364308834\n",
      "epoch: 4 step: 1266, loss is 0.005266321823000908\n",
      "epoch: 4 step: 1267, loss is 0.01920330710709095\n",
      "epoch: 4 step: 1268, loss is 0.18136243522167206\n",
      "epoch: 4 step: 1269, loss is 0.008103634230792522\n",
      "epoch: 4 step: 1270, loss is 0.0052097453735768795\n",
      "epoch: 4 step: 1271, loss is 0.03480599820613861\n",
      "epoch: 4 step: 1272, loss is 0.01272230688482523\n",
      "epoch: 4 step: 1273, loss is 0.06078342720866203\n",
      "epoch: 4 step: 1274, loss is 0.01040689367800951\n",
      "epoch: 4 step: 1275, loss is 0.0005203611799515784\n",
      "epoch: 4 step: 1276, loss is 0.022969385609030724\n",
      "epoch: 4 step: 1277, loss is 0.10205362737178802\n",
      "epoch: 4 step: 1278, loss is 0.15940622985363007\n",
      "epoch: 4 step: 1279, loss is 0.02340620569884777\n",
      "epoch: 4 step: 1280, loss is 0.08361417055130005\n",
      "epoch: 4 step: 1281, loss is 0.05128081887960434\n",
      "epoch: 4 step: 1282, loss is 0.014827762730419636\n",
      "epoch: 4 step: 1283, loss is 0.0030238181352615356\n",
      "epoch: 4 step: 1284, loss is 0.008977136574685574\n",
      "epoch: 4 step: 1285, loss is 0.0018112885300070047\n",
      "epoch: 4 step: 1286, loss is 0.0014964377041906118\n",
      "epoch: 4 step: 1287, loss is 0.0088096484541893\n",
      "epoch: 4 step: 1288, loss is 0.30011919140815735\n",
      "epoch: 4 step: 1289, loss is 0.0006939428858458996\n",
      "epoch: 4 step: 1290, loss is 0.1309247463941574\n",
      "epoch: 4 step: 1291, loss is 0.00030428150785155594\n",
      "epoch: 4 step: 1292, loss is 0.035623371601104736\n",
      "epoch: 4 step: 1293, loss is 0.00802911352366209\n",
      "epoch: 4 step: 1294, loss is 0.0255555622279644\n",
      "epoch: 4 step: 1295, loss is 0.005061695817857981\n",
      "epoch: 4 step: 1296, loss is 0.0337662436068058\n",
      "epoch: 4 step: 1297, loss is 0.0001356061256956309\n",
      "epoch: 4 step: 1298, loss is 0.05127403140068054\n",
      "epoch: 4 step: 1299, loss is 0.0017726094229146838\n",
      "epoch: 4 step: 1300, loss is 0.00681284349411726\n",
      "epoch: 4 step: 1301, loss is 0.017981769517064095\n",
      "epoch: 4 step: 1302, loss is 0.00684875575825572\n",
      "epoch: 4 step: 1303, loss is 0.029190320521593094\n",
      "epoch: 4 step: 1304, loss is 0.0014352404978126287\n",
      "epoch: 4 step: 1305, loss is 0.019412025809288025\n",
      "epoch: 4 step: 1306, loss is 0.09873088449239731\n",
      "epoch: 4 step: 1307, loss is 0.0014213661197572947\n",
      "epoch: 4 step: 1308, loss is 0.11011455953121185\n",
      "epoch: 4 step: 1309, loss is 0.10162022709846497\n",
      "epoch: 4 step: 1310, loss is 0.005315664689987898\n",
      "epoch: 4 step: 1311, loss is 0.06182090565562248\n",
      "epoch: 4 step: 1312, loss is 0.017314603552222252\n",
      "epoch: 4 step: 1313, loss is 0.24259057641029358\n",
      "epoch: 4 step: 1314, loss is 0.08910918980836868\n",
      "epoch: 4 step: 1315, loss is 0.1985548883676529\n",
      "epoch: 4 step: 1316, loss is 0.0012501340825110674\n",
      "epoch: 4 step: 1317, loss is 0.15871873497962952\n",
      "epoch: 4 step: 1318, loss is 0.00352651858702302\n",
      "epoch: 4 step: 1319, loss is 0.009545634500682354\n",
      "epoch: 4 step: 1320, loss is 0.031230105087161064\n",
      "epoch: 4 step: 1321, loss is 0.01312382984906435\n",
      "epoch: 4 step: 1322, loss is 0.002054663375020027\n",
      "epoch: 4 step: 1323, loss is 0.030640751123428345\n",
      "epoch: 4 step: 1324, loss is 0.05948290601372719\n",
      "epoch: 4 step: 1325, loss is 0.005746046081185341\n",
      "epoch: 4 step: 1326, loss is 0.02935800887644291\n",
      "epoch: 4 step: 1327, loss is 0.016706904396414757\n",
      "epoch: 4 step: 1328, loss is 0.017536727711558342\n",
      "epoch: 4 step: 1329, loss is 0.006662511732429266\n",
      "epoch: 4 step: 1330, loss is 0.004795456770807505\n",
      "epoch: 4 step: 1331, loss is 0.030695203691720963\n",
      "epoch: 4 step: 1332, loss is 0.002510072896257043\n",
      "epoch: 4 step: 1333, loss is 0.005448112264275551\n",
      "epoch: 4 step: 1334, loss is 0.0012840060517191887\n",
      "epoch: 4 step: 1335, loss is 0.0011224199552088976\n",
      "epoch: 4 step: 1336, loss is 0.006825830787420273\n",
      "epoch: 4 step: 1337, loss is 0.022871531546115875\n",
      "epoch: 4 step: 1338, loss is 0.039292801171541214\n",
      "epoch: 4 step: 1339, loss is 0.11727704852819443\n",
      "epoch: 4 step: 1340, loss is 0.3018757104873657\n",
      "epoch: 4 step: 1341, loss is 0.1650826334953308\n",
      "epoch: 4 step: 1342, loss is 0.00294962665066123\n",
      "epoch: 4 step: 1343, loss is 0.029508041217923164\n",
      "epoch: 4 step: 1344, loss is 0.004983190912753344\n",
      "epoch: 4 step: 1345, loss is 0.011761728674173355\n",
      "epoch: 4 step: 1346, loss is 0.023256992921233177\n",
      "epoch: 4 step: 1347, loss is 0.010631103999912739\n",
      "epoch: 4 step: 1348, loss is 0.08324457705020905\n",
      "epoch: 4 step: 1349, loss is 0.0013983577955514193\n",
      "epoch: 4 step: 1350, loss is 0.11618190258741379\n",
      "epoch: 4 step: 1351, loss is 0.05307713523507118\n",
      "epoch: 4 step: 1352, loss is 0.0015827108873054385\n",
      "epoch: 4 step: 1353, loss is 0.013906188309192657\n",
      "epoch: 4 step: 1354, loss is 0.0009432558435946703\n",
      "epoch: 4 step: 1355, loss is 0.050893642008304596\n",
      "epoch: 4 step: 1356, loss is 0.001618240145035088\n",
      "epoch: 4 step: 1357, loss is 0.004468144848942757\n",
      "epoch: 4 step: 1358, loss is 0.07003811001777649\n",
      "epoch: 4 step: 1359, loss is 0.0002618434082251042\n",
      "epoch: 4 step: 1360, loss is 0.029548581689596176\n",
      "epoch: 4 step: 1361, loss is 0.00510057806968689\n",
      "epoch: 4 step: 1362, loss is 0.2253987193107605\n",
      "epoch: 4 step: 1363, loss is 0.0029248169157654047\n",
      "epoch: 4 step: 1364, loss is 0.32741600275039673\n",
      "epoch: 4 step: 1365, loss is 0.03222285583615303\n",
      "epoch: 4 step: 1366, loss is 0.021645210683345795\n",
      "epoch: 4 step: 1367, loss is 0.011817516759037971\n",
      "epoch: 4 step: 1368, loss is 0.05601290985941887\n",
      "epoch: 4 step: 1369, loss is 0.003961178939789534\n",
      "epoch: 4 step: 1370, loss is 0.0040894425474107265\n",
      "epoch: 4 step: 1371, loss is 0.008135915733873844\n",
      "epoch: 4 step: 1372, loss is 0.03208329528570175\n",
      "epoch: 4 step: 1373, loss is 0.006699454504996538\n",
      "epoch: 4 step: 1374, loss is 0.004900232423096895\n",
      "epoch: 4 step: 1375, loss is 0.030303148552775383\n",
      "epoch: 4 step: 1376, loss is 0.05487934872508049\n",
      "epoch: 4 step: 1377, loss is 0.003061384428292513\n",
      "epoch: 4 step: 1378, loss is 0.0005037473165430129\n",
      "epoch: 4 step: 1379, loss is 0.026316823437809944\n",
      "epoch: 4 step: 1380, loss is 0.11669248342514038\n",
      "epoch: 4 step: 1381, loss is 0.005649234633892775\n",
      "epoch: 4 step: 1382, loss is 0.018594639375805855\n",
      "epoch: 4 step: 1383, loss is 0.1472432166337967\n",
      "epoch: 4 step: 1384, loss is 0.002782554365694523\n",
      "epoch: 4 step: 1385, loss is 0.0154781648889184\n",
      "epoch: 4 step: 1386, loss is 0.05772709473967552\n",
      "epoch: 4 step: 1387, loss is 0.006525964476168156\n",
      "epoch: 4 step: 1388, loss is 0.005469847936183214\n",
      "epoch: 4 step: 1389, loss is 0.15230323374271393\n",
      "epoch: 4 step: 1390, loss is 0.03842921182513237\n",
      "epoch: 4 step: 1391, loss is 0.15823225677013397\n",
      "epoch: 4 step: 1392, loss is 0.0018524989718571305\n",
      "epoch: 4 step: 1393, loss is 0.006458866875618696\n",
      "epoch: 4 step: 1394, loss is 0.006584743037819862\n",
      "epoch: 4 step: 1395, loss is 0.00523605290800333\n",
      "epoch: 4 step: 1396, loss is 0.0007903703371994197\n",
      "epoch: 4 step: 1397, loss is 0.0039002965204417706\n",
      "epoch: 4 step: 1398, loss is 0.0017274408601224422\n",
      "epoch: 4 step: 1399, loss is 0.07249823957681656\n",
      "epoch: 4 step: 1400, loss is 0.18252964317798615\n",
      "epoch: 4 step: 1401, loss is 4.6989924157969654e-05\n",
      "epoch: 4 step: 1402, loss is 0.07067476212978363\n",
      "epoch: 4 step: 1403, loss is 0.002858660416677594\n",
      "epoch: 4 step: 1404, loss is 0.015164228156208992\n",
      "epoch: 4 step: 1405, loss is 0.0008017040090635419\n",
      "epoch: 4 step: 1406, loss is 0.003782107261940837\n",
      "epoch: 4 step: 1407, loss is 0.02505452185869217\n",
      "epoch: 4 step: 1408, loss is 0.019334513694047928\n",
      "epoch: 4 step: 1409, loss is 0.10934879630804062\n",
      "epoch: 4 step: 1410, loss is 0.0034049253445118666\n",
      "epoch: 4 step: 1411, loss is 0.014766963198781013\n",
      "epoch: 4 step: 1412, loss is 0.20663130283355713\n",
      "epoch: 4 step: 1413, loss is 0.0019061444327235222\n",
      "epoch: 4 step: 1414, loss is 0.22229483723640442\n",
      "epoch: 4 step: 1415, loss is 0.02087150141596794\n",
      "epoch: 4 step: 1416, loss is 0.002718110801652074\n",
      "epoch: 4 step: 1417, loss is 0.013313780538737774\n",
      "epoch: 4 step: 1418, loss is 0.004252089653164148\n",
      "epoch: 4 step: 1419, loss is 0.006561852525919676\n",
      "epoch: 4 step: 1420, loss is 0.09783356636762619\n",
      "epoch: 4 step: 1421, loss is 0.21584589779376984\n",
      "epoch: 4 step: 1422, loss is 8.93338437890634e-05\n",
      "epoch: 4 step: 1423, loss is 0.07366915047168732\n",
      "epoch: 4 step: 1424, loss is 0.034852541983127594\n",
      "epoch: 4 step: 1425, loss is 0.0007549001020379364\n",
      "epoch: 4 step: 1426, loss is 0.0061053065583109856\n",
      "epoch: 4 step: 1427, loss is 0.053416907787323\n",
      "epoch: 4 step: 1428, loss is 0.15029941499233246\n",
      "epoch: 4 step: 1429, loss is 0.0012016295222565532\n",
      "epoch: 4 step: 1430, loss is 0.0058214180171489716\n",
      "epoch: 4 step: 1431, loss is 0.010346845723688602\n",
      "epoch: 4 step: 1432, loss is 0.0006174139562062919\n",
      "epoch: 4 step: 1433, loss is 0.015854593366384506\n",
      "epoch: 4 step: 1434, loss is 0.04384611174464226\n",
      "epoch: 4 step: 1435, loss is 0.016468700021505356\n",
      "epoch: 4 step: 1436, loss is 0.00482324929907918\n",
      "epoch: 4 step: 1437, loss is 0.005786641035228968\n",
      "epoch: 4 step: 1438, loss is 0.005977999418973923\n",
      "epoch: 4 step: 1439, loss is 0.07014435529708862\n",
      "epoch: 4 step: 1440, loss is 0.03337433934211731\n",
      "epoch: 4 step: 1441, loss is 0.023775897920131683\n",
      "epoch: 4 step: 1442, loss is 0.03459540009498596\n",
      "epoch: 4 step: 1443, loss is 0.0011155852116644382\n",
      "epoch: 4 step: 1444, loss is 0.004896246362477541\n",
      "epoch: 4 step: 1445, loss is 0.0028483073692768812\n",
      "epoch: 4 step: 1446, loss is 0.01179890800267458\n",
      "epoch: 4 step: 1447, loss is 0.021037179976701736\n",
      "epoch: 4 step: 1448, loss is 0.001633877051062882\n",
      "epoch: 4 step: 1449, loss is 0.014681016094982624\n",
      "epoch: 4 step: 1450, loss is 0.008845800533890724\n",
      "epoch: 4 step: 1451, loss is 0.07707441598176956\n",
      "epoch: 4 step: 1452, loss is 0.0624609999358654\n",
      "epoch: 4 step: 1453, loss is 0.035764116793870926\n",
      "epoch: 4 step: 1454, loss is 0.08373752981424332\n",
      "epoch: 4 step: 1455, loss is 0.0650494322180748\n",
      "epoch: 4 step: 1456, loss is 0.0034621404483914375\n",
      "epoch: 4 step: 1457, loss is 0.00106388283893466\n",
      "epoch: 4 step: 1458, loss is 0.07094407081604004\n",
      "epoch: 4 step: 1459, loss is 0.051940083503723145\n",
      "epoch: 4 step: 1460, loss is 0.03167712688446045\n",
      "epoch: 4 step: 1461, loss is 0.006294300314038992\n",
      "epoch: 4 step: 1462, loss is 0.019895099103450775\n",
      "epoch: 4 step: 1463, loss is 0.0447344146668911\n",
      "epoch: 4 step: 1464, loss is 0.015402771532535553\n",
      "epoch: 4 step: 1465, loss is 0.0019530822755768895\n",
      "epoch: 4 step: 1466, loss is 0.0015467528719455004\n",
      "epoch: 4 step: 1467, loss is 0.0018428897019475698\n",
      "epoch: 4 step: 1468, loss is 0.0016917490866035223\n",
      "epoch: 4 step: 1469, loss is 0.0016496388707309961\n",
      "epoch: 4 step: 1470, loss is 0.03359735384583473\n",
      "epoch: 4 step: 1471, loss is 0.23808753490447998\n",
      "epoch: 4 step: 1472, loss is 0.00045463009155355394\n",
      "epoch: 4 step: 1473, loss is 0.01778241991996765\n",
      "epoch: 4 step: 1474, loss is 0.0977531373500824\n",
      "epoch: 4 step: 1475, loss is 0.009765882045030594\n",
      "epoch: 4 step: 1476, loss is 0.08575471490621567\n",
      "epoch: 4 step: 1477, loss is 0.07887846231460571\n",
      "epoch: 4 step: 1478, loss is 0.00041973558836616576\n",
      "epoch: 4 step: 1479, loss is 0.0017607583431527019\n",
      "epoch: 4 step: 1480, loss is 0.0022599357180297375\n",
      "epoch: 4 step: 1481, loss is 0.010713838040828705\n",
      "epoch: 4 step: 1482, loss is 0.021286433562636375\n",
      "epoch: 4 step: 1483, loss is 0.1263362467288971\n",
      "epoch: 4 step: 1484, loss is 0.013755671679973602\n",
      "epoch: 4 step: 1485, loss is 0.00801124982535839\n",
      "epoch: 4 step: 1486, loss is 0.03735658898949623\n",
      "epoch: 4 step: 1487, loss is 0.14836756885051727\n",
      "epoch: 4 step: 1488, loss is 0.0017572806682437658\n",
      "epoch: 4 step: 1489, loss is 0.1852722465991974\n",
      "epoch: 4 step: 1490, loss is 0.0026527487207204103\n",
      "epoch: 4 step: 1491, loss is 0.03227929025888443\n",
      "epoch: 4 step: 1492, loss is 0.009832975454628468\n",
      "epoch: 4 step: 1493, loss is 0.06808894127607346\n",
      "epoch: 4 step: 1494, loss is 0.007993824779987335\n",
      "epoch: 4 step: 1495, loss is 0.016812223941087723\n",
      "epoch: 4 step: 1496, loss is 0.01590120978653431\n",
      "epoch: 4 step: 1497, loss is 0.0020018781069666147\n",
      "epoch: 4 step: 1498, loss is 0.01017655897885561\n",
      "epoch: 4 step: 1499, loss is 0.13835303485393524\n",
      "epoch: 4 step: 1500, loss is 0.0038838875479996204\n",
      "epoch: 4 step: 1501, loss is 0.0007795919664204121\n",
      "epoch: 4 step: 1502, loss is 0.018143566325306892\n",
      "epoch: 4 step: 1503, loss is 0.016407985240221024\n",
      "epoch: 4 step: 1504, loss is 0.03758086636662483\n",
      "epoch: 4 step: 1505, loss is 0.004691657144576311\n",
      "epoch: 4 step: 1506, loss is 0.08762011677026749\n",
      "epoch: 4 step: 1507, loss is 0.1285180002450943\n",
      "epoch: 4 step: 1508, loss is 0.004535306245088577\n",
      "epoch: 4 step: 1509, loss is 0.010445114225149155\n",
      "epoch: 4 step: 1510, loss is 0.07885284721851349\n",
      "epoch: 4 step: 1511, loss is 0.008458861149847507\n",
      "epoch: 4 step: 1512, loss is 0.07194967567920685\n",
      "epoch: 4 step: 1513, loss is 0.06259150803089142\n",
      "epoch: 4 step: 1514, loss is 0.020323127508163452\n",
      "epoch: 4 step: 1515, loss is 0.04166289046406746\n",
      "epoch: 4 step: 1516, loss is 0.07821059972047806\n",
      "epoch: 4 step: 1517, loss is 0.021241014823317528\n",
      "epoch: 4 step: 1518, loss is 0.2172141671180725\n",
      "epoch: 4 step: 1519, loss is 0.05493408441543579\n",
      "epoch: 4 step: 1520, loss is 0.01754102110862732\n",
      "epoch: 4 step: 1521, loss is 0.011183146387338638\n",
      "epoch: 4 step: 1522, loss is 0.0029799367766827345\n",
      "epoch: 4 step: 1523, loss is 0.06558400392532349\n",
      "epoch: 4 step: 1524, loss is 0.0034164409153163433\n",
      "epoch: 4 step: 1525, loss is 0.00217644264921546\n",
      "epoch: 4 step: 1526, loss is 0.0022197780199348927\n",
      "epoch: 4 step: 1527, loss is 0.000803035160060972\n",
      "epoch: 4 step: 1528, loss is 0.0037111868150532246\n",
      "epoch: 4 step: 1529, loss is 0.005318192299455404\n",
      "epoch: 4 step: 1530, loss is 0.010846203193068504\n",
      "epoch: 4 step: 1531, loss is 0.017531180754303932\n",
      "epoch: 4 step: 1532, loss is 0.0006672398885712028\n",
      "epoch: 4 step: 1533, loss is 0.04360423982143402\n",
      "epoch: 4 step: 1534, loss is 0.003259984776377678\n",
      "epoch: 4 step: 1535, loss is 0.030881846323609352\n",
      "epoch: 4 step: 1536, loss is 0.00469637755304575\n",
      "epoch: 4 step: 1537, loss is 0.01725872978568077\n",
      "epoch: 4 step: 1538, loss is 0.0016164731932803988\n",
      "epoch: 4 step: 1539, loss is 0.06663309037685394\n",
      "epoch: 4 step: 1540, loss is 0.003275232156738639\n",
      "epoch: 4 step: 1541, loss is 0.0038321861065924168\n",
      "epoch: 4 step: 1542, loss is 0.006681953556835651\n",
      "epoch: 4 step: 1543, loss is 0.012182315811514854\n",
      "epoch: 4 step: 1544, loss is 0.004052409436553717\n",
      "epoch: 4 step: 1545, loss is 0.0018527571810409427\n",
      "epoch: 4 step: 1546, loss is 0.003683812450617552\n",
      "epoch: 4 step: 1547, loss is 0.011474932543933392\n",
      "epoch: 4 step: 1548, loss is 0.0017103906720876694\n",
      "epoch: 4 step: 1549, loss is 0.001925964024849236\n",
      "epoch: 4 step: 1550, loss is 0.08205480873584747\n",
      "epoch: 4 step: 1551, loss is 0.0005277732270769775\n",
      "epoch: 4 step: 1552, loss is 0.08328674733638763\n",
      "epoch: 4 step: 1553, loss is 0.0020373461302369833\n",
      "epoch: 4 step: 1554, loss is 0.0038081430830061436\n",
      "epoch: 4 step: 1555, loss is 0.08342185616493225\n",
      "epoch: 4 step: 1556, loss is 0.0014595254324376583\n",
      "epoch: 4 step: 1557, loss is 0.0004800482711289078\n",
      "epoch: 4 step: 1558, loss is 0.1541716754436493\n",
      "epoch: 4 step: 1559, loss is 0.0003068792575504631\n",
      "epoch: 4 step: 1560, loss is 0.0003375486994627863\n",
      "epoch: 4 step: 1561, loss is 0.07250309735536575\n",
      "epoch: 4 step: 1562, loss is 0.0010403023334220052\n",
      "epoch: 4 step: 1563, loss is 0.0010738471755757928\n",
      "epoch: 4 step: 1564, loss is 0.24700455367565155\n",
      "epoch: 4 step: 1565, loss is 0.0008940867846831679\n",
      "epoch: 4 step: 1566, loss is 0.0005579493590630591\n",
      "epoch: 4 step: 1567, loss is 0.011397191323339939\n",
      "epoch: 4 step: 1568, loss is 0.0022359034046530724\n",
      "epoch: 4 step: 1569, loss is 0.004135276190936565\n",
      "epoch: 4 step: 1570, loss is 0.0008132025832310319\n",
      "epoch: 4 step: 1571, loss is 0.013329162262380123\n",
      "epoch: 4 step: 1572, loss is 0.009235345758497715\n",
      "epoch: 4 step: 1573, loss is 0.10787899792194366\n",
      "epoch: 4 step: 1574, loss is 0.0032882073428481817\n",
      "epoch: 4 step: 1575, loss is 0.0004711388610303402\n",
      "epoch: 4 step: 1576, loss is 0.0054535698145627975\n",
      "epoch: 4 step: 1577, loss is 0.0016450012335553765\n",
      "epoch: 4 step: 1578, loss is 0.053321473300457\n",
      "epoch: 4 step: 1579, loss is 0.008890231139957905\n",
      "epoch: 4 step: 1580, loss is 0.0004181765834800899\n",
      "epoch: 4 step: 1581, loss is 0.010000030510127544\n",
      "epoch: 4 step: 1582, loss is 0.001062166178599\n",
      "epoch: 4 step: 1583, loss is 0.0012656956678256392\n",
      "epoch: 4 step: 1584, loss is 0.0007177555235102773\n",
      "epoch: 4 step: 1585, loss is 0.029739098623394966\n",
      "epoch: 4 step: 1586, loss is 0.0031398746650666\n",
      "epoch: 4 step: 1587, loss is 0.024839326739311218\n",
      "epoch: 4 step: 1588, loss is 0.13344533741474152\n",
      "epoch: 4 step: 1589, loss is 0.04308166354894638\n",
      "epoch: 4 step: 1590, loss is 0.010924352332949638\n",
      "epoch: 4 step: 1591, loss is 0.02486327663064003\n",
      "epoch: 4 step: 1592, loss is 0.05267784371972084\n",
      "epoch: 4 step: 1593, loss is 0.020725511014461517\n",
      "epoch: 4 step: 1594, loss is 0.0003656046756077558\n",
      "epoch: 4 step: 1595, loss is 0.010296830907464027\n",
      "epoch: 4 step: 1596, loss is 0.05180366709828377\n",
      "epoch: 4 step: 1597, loss is 0.00039425003342330456\n",
      "epoch: 4 step: 1598, loss is 0.001637576031498611\n",
      "epoch: 4 step: 1599, loss is 0.10390286892652512\n",
      "epoch: 4 step: 1600, loss is 0.001460577710531652\n",
      "epoch: 4 step: 1601, loss is 0.0014263694174587727\n",
      "epoch: 4 step: 1602, loss is 0.00044164873543195426\n",
      "epoch: 4 step: 1603, loss is 0.010589818470180035\n",
      "epoch: 4 step: 1604, loss is 0.0036895975936204195\n",
      "epoch: 4 step: 1605, loss is 0.0007447245880030096\n",
      "epoch: 4 step: 1606, loss is 0.00110846315510571\n",
      "epoch: 4 step: 1607, loss is 0.006377352401614189\n",
      "epoch: 4 step: 1608, loss is 0.013445817865431309\n",
      "epoch: 4 step: 1609, loss is 0.006626844871789217\n",
      "epoch: 4 step: 1610, loss is 0.03348960354924202\n",
      "epoch: 4 step: 1611, loss is 0.002059310209006071\n",
      "epoch: 4 step: 1612, loss is 0.09493601322174072\n",
      "epoch: 4 step: 1613, loss is 0.0023806949611753225\n",
      "epoch: 4 step: 1614, loss is 0.003362541552633047\n",
      "epoch: 4 step: 1615, loss is 8.63330060383305e-05\n",
      "epoch: 4 step: 1616, loss is 0.024539127945899963\n",
      "epoch: 4 step: 1617, loss is 0.04576646164059639\n",
      "epoch: 4 step: 1618, loss is 0.05018698796629906\n",
      "epoch: 4 step: 1619, loss is 0.2656078338623047\n",
      "epoch: 4 step: 1620, loss is 0.009543242864310741\n",
      "epoch: 4 step: 1621, loss is 0.002963091479614377\n",
      "epoch: 4 step: 1622, loss is 0.0510864220559597\n",
      "epoch: 4 step: 1623, loss is 0.18002167344093323\n",
      "epoch: 4 step: 1624, loss is 0.03929799795150757\n",
      "epoch: 4 step: 1625, loss is 0.002167308237403631\n",
      "epoch: 4 step: 1626, loss is 0.023823510855436325\n",
      "epoch: 4 step: 1627, loss is 0.01154247671365738\n",
      "epoch: 4 step: 1628, loss is 0.021368173882365227\n",
      "epoch: 4 step: 1629, loss is 0.007579403463751078\n",
      "epoch: 4 step: 1630, loss is 0.00832445826381445\n",
      "epoch: 4 step: 1631, loss is 0.04346708208322525\n",
      "epoch: 4 step: 1632, loss is 0.0343468151986599\n",
      "epoch: 4 step: 1633, loss is 0.005669150035828352\n",
      "epoch: 4 step: 1634, loss is 0.1998617798089981\n",
      "epoch: 4 step: 1635, loss is 0.01200791634619236\n",
      "epoch: 4 step: 1636, loss is 0.01430832501500845\n",
      "epoch: 4 step: 1637, loss is 0.012298685498535633\n",
      "epoch: 4 step: 1638, loss is 0.01866261288523674\n",
      "epoch: 4 step: 1639, loss is 0.00017118723189923912\n",
      "epoch: 4 step: 1640, loss is 0.1377226561307907\n",
      "epoch: 4 step: 1641, loss is 0.08486080914735794\n",
      "epoch: 4 step: 1642, loss is 0.054464228451251984\n",
      "epoch: 4 step: 1643, loss is 0.00027185355429537594\n",
      "epoch: 4 step: 1644, loss is 0.0005222443724051118\n",
      "epoch: 4 step: 1645, loss is 0.02011392079293728\n",
      "epoch: 4 step: 1646, loss is 0.09465230256319046\n",
      "epoch: 4 step: 1647, loss is 0.056808117777109146\n",
      "epoch: 4 step: 1648, loss is 0.015590238384902477\n",
      "epoch: 4 step: 1649, loss is 0.003491972805932164\n",
      "epoch: 4 step: 1650, loss is 0.0028099899645894766\n",
      "epoch: 4 step: 1651, loss is 0.028018012642860413\n",
      "epoch: 4 step: 1652, loss is 0.024797415360808372\n",
      "epoch: 4 step: 1653, loss is 0.1470874547958374\n",
      "epoch: 4 step: 1654, loss is 0.03048400767147541\n",
      "epoch: 4 step: 1655, loss is 0.004371051676571369\n",
      "epoch: 4 step: 1656, loss is 0.0461990162730217\n",
      "epoch: 4 step: 1657, loss is 0.007229188457131386\n",
      "epoch: 4 step: 1658, loss is 0.0010204006684944034\n",
      "epoch: 4 step: 1659, loss is 0.03595401719212532\n",
      "epoch: 4 step: 1660, loss is 0.04363477602601051\n",
      "epoch: 4 step: 1661, loss is 0.006642780266702175\n",
      "epoch: 4 step: 1662, loss is 0.02571573294699192\n",
      "epoch: 4 step: 1663, loss is 0.0033976712729781866\n",
      "epoch: 4 step: 1664, loss is 0.08282499760389328\n",
      "epoch: 4 step: 1665, loss is 0.03477361425757408\n",
      "epoch: 4 step: 1666, loss is 0.0057433550246059895\n",
      "epoch: 4 step: 1667, loss is 0.0001933119783643633\n",
      "epoch: 4 step: 1668, loss is 0.0011970000341534615\n",
      "epoch: 4 step: 1669, loss is 0.031113198027014732\n",
      "epoch: 4 step: 1670, loss is 0.02609100006520748\n",
      "epoch: 4 step: 1671, loss is 0.0027353528421372175\n",
      "epoch: 4 step: 1672, loss is 0.0143459876999259\n",
      "epoch: 4 step: 1673, loss is 0.015755394473671913\n",
      "epoch: 4 step: 1674, loss is 0.035247139632701874\n",
      "epoch: 4 step: 1675, loss is 0.07374387979507446\n",
      "epoch: 4 step: 1676, loss is 0.12407748401165009\n",
      "epoch: 4 step: 1677, loss is 0.006531988270580769\n",
      "epoch: 4 step: 1678, loss is 0.2245911806821823\n",
      "epoch: 4 step: 1679, loss is 0.027104318141937256\n",
      "epoch: 4 step: 1680, loss is 0.039392370730638504\n",
      "epoch: 4 step: 1681, loss is 0.1167459636926651\n",
      "epoch: 4 step: 1682, loss is 0.03558480739593506\n",
      "epoch: 4 step: 1683, loss is 0.012303038500249386\n",
      "epoch: 4 step: 1684, loss is 0.0935685932636261\n",
      "epoch: 4 step: 1685, loss is 0.005335232708603144\n",
      "epoch: 4 step: 1686, loss is 0.012968613766133785\n",
      "epoch: 4 step: 1687, loss is 0.005003337748348713\n",
      "epoch: 4 step: 1688, loss is 0.20141276717185974\n",
      "epoch: 4 step: 1689, loss is 0.004045101813971996\n",
      "epoch: 4 step: 1690, loss is 0.1288115680217743\n",
      "epoch: 4 step: 1691, loss is 0.03577097877860069\n",
      "epoch: 4 step: 1692, loss is 0.0335557758808136\n",
      "epoch: 4 step: 1693, loss is 0.0029910567682236433\n",
      "epoch: 4 step: 1694, loss is 0.0008961376152001321\n",
      "epoch: 4 step: 1695, loss is 0.009244043380022049\n",
      "epoch: 4 step: 1696, loss is 0.004120983649045229\n",
      "epoch: 4 step: 1697, loss is 0.03646291047334671\n",
      "epoch: 4 step: 1698, loss is 0.1264430731534958\n",
      "epoch: 4 step: 1699, loss is 0.03152962028980255\n",
      "epoch: 4 step: 1700, loss is 0.010441428050398827\n",
      "epoch: 4 step: 1701, loss is 0.2059434950351715\n",
      "epoch: 4 step: 1702, loss is 0.0013879884500056505\n",
      "epoch: 4 step: 1703, loss is 0.041347112506628036\n",
      "epoch: 4 step: 1704, loss is 0.0332290343940258\n",
      "epoch: 4 step: 1705, loss is 0.005286710802465677\n",
      "epoch: 4 step: 1706, loss is 0.01035823579877615\n",
      "epoch: 4 step: 1707, loss is 0.04297662526369095\n",
      "epoch: 4 step: 1708, loss is 0.001604148419573903\n",
      "epoch: 4 step: 1709, loss is 0.014241497032344341\n",
      "epoch: 4 step: 1710, loss is 0.01588929258286953\n",
      "epoch: 4 step: 1711, loss is 0.16354601085186005\n",
      "epoch: 4 step: 1712, loss is 0.00905532855540514\n",
      "epoch: 4 step: 1713, loss is 0.1314847469329834\n",
      "epoch: 4 step: 1714, loss is 0.02267850749194622\n",
      "epoch: 4 step: 1715, loss is 0.010331177152693272\n",
      "epoch: 4 step: 1716, loss is 0.0022432359401136637\n",
      "epoch: 4 step: 1717, loss is 0.12213576585054398\n",
      "epoch: 4 step: 1718, loss is 0.00897727906703949\n",
      "epoch: 4 step: 1719, loss is 0.08477000892162323\n",
      "epoch: 4 step: 1720, loss is 0.0005011428729631007\n",
      "epoch: 4 step: 1721, loss is 0.12671557068824768\n",
      "epoch: 4 step: 1722, loss is 0.1630164384841919\n",
      "epoch: 4 step: 1723, loss is 0.012301662936806679\n",
      "epoch: 4 step: 1724, loss is 0.024305950850248337\n",
      "epoch: 4 step: 1725, loss is 0.002016797661781311\n",
      "epoch: 4 step: 1726, loss is 0.002971053123474121\n",
      "epoch: 4 step: 1727, loss is 0.041134610772132874\n",
      "epoch: 4 step: 1728, loss is 0.05087511986494064\n",
      "epoch: 4 step: 1729, loss is 0.006131093483418226\n",
      "epoch: 4 step: 1730, loss is 0.03390396013855934\n",
      "epoch: 4 step: 1731, loss is 0.006747779902070761\n",
      "epoch: 4 step: 1732, loss is 0.23622249066829681\n",
      "epoch: 4 step: 1733, loss is 0.006701628677546978\n",
      "epoch: 4 step: 1734, loss is 0.0365881472826004\n",
      "epoch: 4 step: 1735, loss is 0.008163458667695522\n",
      "epoch: 4 step: 1736, loss is 0.003958946093916893\n",
      "epoch: 4 step: 1737, loss is 0.0009507379727438092\n",
      "epoch: 4 step: 1738, loss is 0.06275025755167007\n",
      "epoch: 4 step: 1739, loss is 0.0016464288346469402\n",
      "epoch: 4 step: 1740, loss is 0.1416623443365097\n",
      "epoch: 4 step: 1741, loss is 0.025363286957144737\n",
      "epoch: 4 step: 1742, loss is 0.022133368998765945\n",
      "epoch: 4 step: 1743, loss is 0.026500146836042404\n",
      "epoch: 4 step: 1744, loss is 0.05520322918891907\n",
      "epoch: 4 step: 1745, loss is 0.0012024918105453253\n",
      "epoch: 4 step: 1746, loss is 0.0174088254570961\n",
      "epoch: 4 step: 1747, loss is 0.04275868460536003\n",
      "epoch: 4 step: 1748, loss is 0.005603827070444822\n",
      "epoch: 4 step: 1749, loss is 0.07486401498317719\n",
      "epoch: 4 step: 1750, loss is 0.25178951025009155\n",
      "epoch: 4 step: 1751, loss is 0.007017279975116253\n",
      "epoch: 4 step: 1752, loss is 0.019095074385404587\n",
      "epoch: 4 step: 1753, loss is 0.04453565180301666\n",
      "epoch: 4 step: 1754, loss is 0.09867756068706512\n",
      "epoch: 4 step: 1755, loss is 0.12771086394786835\n",
      "epoch: 4 step: 1756, loss is 0.008107125759124756\n",
      "epoch: 4 step: 1757, loss is 0.01285545900464058\n",
      "epoch: 4 step: 1758, loss is 0.05552280694246292\n",
      "epoch: 4 step: 1759, loss is 0.002915935590863228\n",
      "epoch: 4 step: 1760, loss is 0.05656670033931732\n",
      "epoch: 4 step: 1761, loss is 0.040201857686042786\n",
      "epoch: 4 step: 1762, loss is 0.009168252348899841\n",
      "epoch: 4 step: 1763, loss is 0.0569419339299202\n",
      "epoch: 4 step: 1764, loss is 0.10731560736894608\n",
      "epoch: 4 step: 1765, loss is 0.0027121645398437977\n",
      "epoch: 4 step: 1766, loss is 0.09166210144758224\n",
      "epoch: 4 step: 1767, loss is 0.027159573510289192\n",
      "epoch: 4 step: 1768, loss is 0.07490111887454987\n",
      "epoch: 4 step: 1769, loss is 0.020695384591817856\n",
      "epoch: 4 step: 1770, loss is 0.006115337833762169\n",
      "epoch: 4 step: 1771, loss is 0.0069780959747731686\n",
      "epoch: 4 step: 1772, loss is 0.0015036390395835042\n",
      "epoch: 4 step: 1773, loss is 0.040721721947193146\n",
      "epoch: 4 step: 1774, loss is 0.06294257193803787\n",
      "epoch: 4 step: 1775, loss is 0.007274674251675606\n",
      "epoch: 4 step: 1776, loss is 0.024257833138108253\n",
      "epoch: 4 step: 1777, loss is 0.0006680942606180906\n",
      "epoch: 4 step: 1778, loss is 0.029575277119874954\n",
      "epoch: 4 step: 1779, loss is 0.04115995764732361\n",
      "epoch: 4 step: 1780, loss is 0.06536149978637695\n",
      "epoch: 4 step: 1781, loss is 0.012529300525784492\n",
      "epoch: 4 step: 1782, loss is 0.023495091125369072\n",
      "epoch: 4 step: 1783, loss is 0.002525627613067627\n",
      "epoch: 4 step: 1784, loss is 0.010758042335510254\n",
      "epoch: 4 step: 1785, loss is 0.0019159895600751042\n",
      "epoch: 4 step: 1786, loss is 0.0022872870322316885\n",
      "epoch: 4 step: 1787, loss is 0.0008100923150777817\n",
      "epoch: 4 step: 1788, loss is 0.01090139988809824\n",
      "epoch: 4 step: 1789, loss is 0.00047698395792394876\n",
      "epoch: 4 step: 1790, loss is 0.0034666950814425945\n",
      "epoch: 4 step: 1791, loss is 0.0063849384896457195\n",
      "epoch: 4 step: 1792, loss is 0.024301674216985703\n",
      "epoch: 4 step: 1793, loss is 0.0003313319175504148\n",
      "epoch: 4 step: 1794, loss is 0.0004964375984854996\n",
      "epoch: 4 step: 1795, loss is 0.006824948359280825\n",
      "epoch: 4 step: 1796, loss is 0.0006759013049304485\n",
      "epoch: 4 step: 1797, loss is 0.00019302374857943505\n",
      "epoch: 4 step: 1798, loss is 0.0006690311711281538\n",
      "epoch: 4 step: 1799, loss is 0.147111177444458\n",
      "epoch: 4 step: 1800, loss is 0.048884306102991104\n",
      "epoch: 4 step: 1801, loss is 0.002108889864757657\n",
      "epoch: 4 step: 1802, loss is 0.0001438126346329227\n",
      "epoch: 4 step: 1803, loss is 0.011772200465202332\n",
      "epoch: 4 step: 1804, loss is 0.004469355568289757\n",
      "epoch: 4 step: 1805, loss is 0.0019250392215326428\n",
      "epoch: 4 step: 1806, loss is 0.008841429837048054\n",
      "epoch: 4 step: 1807, loss is 0.11780201643705368\n",
      "epoch: 4 step: 1808, loss is 0.015749499201774597\n",
      "epoch: 4 step: 1809, loss is 0.08430307358503342\n",
      "epoch: 4 step: 1810, loss is 0.005353831220418215\n",
      "epoch: 4 step: 1811, loss is 0.0037066920194774866\n",
      "epoch: 4 step: 1812, loss is 0.0001033058506436646\n",
      "epoch: 4 step: 1813, loss is 0.0397350937128067\n",
      "epoch: 4 step: 1814, loss is 0.00412653898820281\n",
      "epoch: 4 step: 1815, loss is 0.16488827764987946\n",
      "epoch: 4 step: 1816, loss is 0.078938789665699\n",
      "epoch: 4 step: 1817, loss is 0.03631230816245079\n",
      "epoch: 4 step: 1818, loss is 0.1632654070854187\n",
      "epoch: 4 step: 1819, loss is 0.0006197726470418274\n",
      "epoch: 4 step: 1820, loss is 0.3018918037414551\n",
      "epoch: 4 step: 1821, loss is 0.0035363510251045227\n",
      "epoch: 4 step: 1822, loss is 0.0018360286485403776\n",
      "epoch: 4 step: 1823, loss is 0.0021494992543011904\n",
      "epoch: 4 step: 1824, loss is 0.04007026180624962\n",
      "epoch: 4 step: 1825, loss is 0.055874764919281006\n",
      "epoch: 4 step: 1826, loss is 0.010863219387829304\n",
      "epoch: 4 step: 1827, loss is 0.04895159602165222\n",
      "epoch: 4 step: 1828, loss is 0.0895458310842514\n",
      "epoch: 4 step: 1829, loss is 0.12137746065855026\n",
      "epoch: 4 step: 1830, loss is 0.004761622752994299\n",
      "epoch: 4 step: 1831, loss is 0.01749325357377529\n",
      "epoch: 4 step: 1832, loss is 0.03309112787246704\n",
      "epoch: 4 step: 1833, loss is 0.010707693174481392\n",
      "epoch: 4 step: 1834, loss is 0.19811223447322845\n",
      "epoch: 4 step: 1835, loss is 0.0014586452161893249\n",
      "epoch: 4 step: 1836, loss is 0.04807569831609726\n",
      "epoch: 4 step: 1837, loss is 0.02404794469475746\n",
      "epoch: 4 step: 1838, loss is 0.0016905275406315923\n",
      "epoch: 4 step: 1839, loss is 0.22171340882778168\n",
      "epoch: 4 step: 1840, loss is 0.0323999859392643\n",
      "epoch: 4 step: 1841, loss is 0.003663480281829834\n",
      "epoch: 4 step: 1842, loss is 0.12106504291296005\n",
      "epoch: 4 step: 1843, loss is 0.015124626457691193\n",
      "epoch: 4 step: 1844, loss is 0.006221797317266464\n",
      "epoch: 4 step: 1845, loss is 0.013424446806311607\n",
      "epoch: 4 step: 1846, loss is 0.002171536907553673\n",
      "epoch: 4 step: 1847, loss is 0.0014870509039610624\n",
      "epoch: 4 step: 1848, loss is 0.060820311307907104\n",
      "epoch: 4 step: 1849, loss is 0.06485258042812347\n",
      "epoch: 4 step: 1850, loss is 0.0019004691857844591\n",
      "epoch: 4 step: 1851, loss is 0.008769799023866653\n",
      "epoch: 4 step: 1852, loss is 0.027023550122976303\n",
      "epoch: 4 step: 1853, loss is 0.0011263315100222826\n",
      "epoch: 4 step: 1854, loss is 0.004563434049487114\n",
      "epoch: 4 step: 1855, loss is 0.008457295596599579\n",
      "epoch: 4 step: 1856, loss is 0.007893278263509274\n",
      "epoch: 4 step: 1857, loss is 0.0003909262304659933\n",
      "epoch: 4 step: 1858, loss is 0.0007101051160134375\n",
      "epoch: 4 step: 1859, loss is 0.0015998110175132751\n",
      "epoch: 4 step: 1860, loss is 0.0029363243374973536\n",
      "epoch: 4 step: 1861, loss is 0.004796280059963465\n",
      "epoch: 4 step: 1862, loss is 0.0021543216425925493\n",
      "epoch: 4 step: 1863, loss is 0.04994509741663933\n",
      "epoch: 4 step: 1864, loss is 0.0005284554208628833\n",
      "epoch: 4 step: 1865, loss is 0.0716601312160492\n",
      "epoch: 4 step: 1866, loss is 0.09730713069438934\n",
      "epoch: 4 step: 1867, loss is 0.0242915116250515\n",
      "epoch: 4 step: 1868, loss is 0.004024358466267586\n",
      "epoch: 4 step: 1869, loss is 0.004406055901199579\n",
      "epoch: 4 step: 1870, loss is 0.0007035300950519741\n",
      "epoch: 4 step: 1871, loss is 0.0696491152048111\n",
      "epoch: 4 step: 1872, loss is 0.0003638371708802879\n",
      "epoch: 4 step: 1873, loss is 0.014210054650902748\n",
      "epoch: 4 step: 1874, loss is 0.0016248064348474145\n",
      "epoch: 4 step: 1875, loss is 0.038181304931640625\n",
      "Train epoch time: 14791.638 ms, per step time: 7.889 ms\n",
      "epoch: 5 step: 1, loss is 0.021421194076538086\n",
      "epoch: 5 step: 2, loss is 0.000923492363654077\n",
      "epoch: 5 step: 3, loss is 0.0020160700660198927\n",
      "epoch: 5 step: 4, loss is 0.0009702108800411224\n",
      "epoch: 5 step: 5, loss is 0.06384088844060898\n",
      "epoch: 5 step: 6, loss is 0.039560817182064056\n",
      "epoch: 5 step: 7, loss is 0.01574787124991417\n",
      "epoch: 5 step: 8, loss is 0.07360170036554337\n",
      "epoch: 5 step: 9, loss is 0.04687736555933952\n",
      "epoch: 5 step: 10, loss is 0.0006463731406256557\n",
      "epoch: 5 step: 11, loss is 0.004214240238070488\n",
      "epoch: 5 step: 12, loss is 0.011176769621670246\n",
      "epoch: 5 step: 13, loss is 0.052708547562360764\n",
      "epoch: 5 step: 14, loss is 0.017687233164906502\n",
      "epoch: 5 step: 15, loss is 0.0029705206397920847\n",
      "epoch: 5 step: 16, loss is 0.0023591844365000725\n",
      "epoch: 5 step: 17, loss is 0.0005321635399013758\n",
      "epoch: 5 step: 18, loss is 0.09445542097091675\n",
      "epoch: 5 step: 19, loss is 0.03159264475107193\n",
      "epoch: 5 step: 20, loss is 0.007553554140031338\n",
      "epoch: 5 step: 21, loss is 0.00807283166795969\n",
      "epoch: 5 step: 22, loss is 0.012825842946767807\n",
      "epoch: 5 step: 23, loss is 0.023020276799798012\n",
      "epoch: 5 step: 24, loss is 0.0006095942808315158\n",
      "epoch: 5 step: 25, loss is 0.025095025077462196\n",
      "epoch: 5 step: 26, loss is 0.02766544744372368\n",
      "epoch: 5 step: 27, loss is 0.006907270755618811\n",
      "epoch: 5 step: 28, loss is 0.007310478948056698\n",
      "epoch: 5 step: 29, loss is 0.04881816729903221\n",
      "epoch: 5 step: 30, loss is 0.0002589439391158521\n",
      "epoch: 5 step: 31, loss is 0.03215741366147995\n",
      "epoch: 5 step: 32, loss is 0.0055572036653757095\n",
      "epoch: 5 step: 33, loss is 0.0022387090139091015\n",
      "epoch: 5 step: 34, loss is 0.01942707598209381\n",
      "epoch: 5 step: 35, loss is 0.017885472625494003\n",
      "epoch: 5 step: 36, loss is 0.0017416574992239475\n",
      "epoch: 5 step: 37, loss is 0.0032448864076286554\n",
      "epoch: 5 step: 38, loss is 0.03929324448108673\n",
      "epoch: 5 step: 39, loss is 0.001792287570424378\n",
      "epoch: 5 step: 40, loss is 0.0018724107649177313\n",
      "epoch: 5 step: 41, loss is 0.0005661732284352183\n",
      "epoch: 5 step: 42, loss is 0.0010493845911696553\n",
      "epoch: 5 step: 43, loss is 0.10898924618959427\n",
      "epoch: 5 step: 44, loss is 0.04193685203790665\n",
      "epoch: 5 step: 45, loss is 0.0001436625316273421\n",
      "epoch: 5 step: 46, loss is 0.03329711779952049\n",
      "epoch: 5 step: 47, loss is 0.03833645209670067\n",
      "epoch: 5 step: 48, loss is 0.0005063078715465963\n",
      "epoch: 5 step: 49, loss is 0.017180386930704117\n",
      "epoch: 5 step: 50, loss is 0.04171323403716087\n",
      "epoch: 5 step: 51, loss is 0.0007066960097290576\n",
      "epoch: 5 step: 52, loss is 0.0009204007801599801\n",
      "epoch: 5 step: 53, loss is 0.000355254189344123\n",
      "epoch: 5 step: 54, loss is 0.012020776979625225\n",
      "epoch: 5 step: 55, loss is 0.000765800999943167\n",
      "epoch: 5 step: 56, loss is 0.0054734451696276665\n",
      "epoch: 5 step: 57, loss is 0.002314206212759018\n",
      "epoch: 5 step: 58, loss is 0.004463978577405214\n",
      "epoch: 5 step: 59, loss is 0.00159145996440202\n",
      "epoch: 5 step: 60, loss is 0.00017296901205554605\n",
      "epoch: 5 step: 61, loss is 0.13566356897354126\n",
      "epoch: 5 step: 62, loss is 0.01744900271296501\n",
      "epoch: 5 step: 63, loss is 0.03288440778851509\n",
      "epoch: 5 step: 64, loss is 0.0010930737480521202\n",
      "epoch: 5 step: 65, loss is 0.005784910172224045\n",
      "epoch: 5 step: 66, loss is 0.08667945116758347\n",
      "epoch: 5 step: 67, loss is 0.03615495190024376\n",
      "epoch: 5 step: 68, loss is 8.015224011614919e-05\n",
      "epoch: 5 step: 69, loss is 0.0019915064331144094\n",
      "epoch: 5 step: 70, loss is 0.005019171629101038\n",
      "epoch: 5 step: 71, loss is 0.007016981951892376\n",
      "epoch: 5 step: 72, loss is 0.0006187501712702215\n",
      "epoch: 5 step: 73, loss is 0.022021563723683357\n",
      "epoch: 5 step: 74, loss is 0.008612739853560925\n",
      "epoch: 5 step: 75, loss is 0.0033625192008912563\n",
      "epoch: 5 step: 76, loss is 0.0006055334233678877\n",
      "epoch: 5 step: 77, loss is 0.0018065772019326687\n",
      "epoch: 5 step: 78, loss is 0.004997560288757086\n",
      "epoch: 5 step: 79, loss is 0.11766217648983002\n",
      "epoch: 5 step: 80, loss is 0.0029750708490610123\n",
      "epoch: 5 step: 81, loss is 0.036479923874139786\n",
      "epoch: 5 step: 82, loss is 0.0012150054099038243\n",
      "epoch: 5 step: 83, loss is 0.010297629050910473\n",
      "epoch: 5 step: 84, loss is 0.0014804562088102102\n",
      "epoch: 5 step: 85, loss is 0.08876361697912216\n",
      "epoch: 5 step: 86, loss is 0.007607756182551384\n",
      "epoch: 5 step: 87, loss is 0.0003479905426502228\n",
      "epoch: 5 step: 88, loss is 0.009122315794229507\n",
      "epoch: 5 step: 89, loss is 0.0068245395086705685\n",
      "epoch: 5 step: 90, loss is 0.0009283127728849649\n",
      "epoch: 5 step: 91, loss is 0.013738330453634262\n",
      "epoch: 5 step: 92, loss is 0.09865652769804001\n",
      "epoch: 5 step: 93, loss is 0.0027043772861361504\n",
      "epoch: 5 step: 94, loss is 0.35511279106140137\n",
      "epoch: 5 step: 95, loss is 0.007596946787089109\n",
      "epoch: 5 step: 96, loss is 0.004868192132562399\n",
      "epoch: 5 step: 97, loss is 0.0013550235889852047\n",
      "epoch: 5 step: 98, loss is 0.002088428707793355\n",
      "epoch: 5 step: 99, loss is 0.03345656022429466\n",
      "epoch: 5 step: 100, loss is 0.0006769124884158373\n",
      "epoch: 5 step: 101, loss is 0.013449439778923988\n",
      "epoch: 5 step: 102, loss is 0.003991017118096352\n",
      "epoch: 5 step: 103, loss is 0.05399240180850029\n",
      "epoch: 5 step: 104, loss is 0.0015587820671498775\n",
      "epoch: 5 step: 105, loss is 0.03002438135445118\n",
      "epoch: 5 step: 106, loss is 0.00047232754877768457\n",
      "epoch: 5 step: 107, loss is 0.0029329967219382524\n",
      "epoch: 5 step: 108, loss is 0.00011835272016469389\n",
      "epoch: 5 step: 109, loss is 0.013719694688916206\n",
      "epoch: 5 step: 110, loss is 0.000673011178150773\n",
      "epoch: 5 step: 111, loss is 0.007572957314550877\n",
      "epoch: 5 step: 112, loss is 0.0007370397797785699\n",
      "epoch: 5 step: 113, loss is 0.059336062520742416\n",
      "epoch: 5 step: 114, loss is 0.0009929966181516647\n",
      "epoch: 5 step: 115, loss is 0.028476648032665253\n",
      "epoch: 5 step: 116, loss is 0.0010216517839580774\n",
      "epoch: 5 step: 117, loss is 0.012552693486213684\n",
      "epoch: 5 step: 118, loss is 0.0035850643180310726\n",
      "epoch: 5 step: 119, loss is 0.001467296970076859\n",
      "epoch: 5 step: 120, loss is 0.043682895600795746\n",
      "epoch: 5 step: 121, loss is 0.0005842340760864317\n",
      "epoch: 5 step: 122, loss is 0.02686627209186554\n",
      "epoch: 5 step: 123, loss is 0.15803086757659912\n",
      "epoch: 5 step: 124, loss is 0.0022659224923700094\n",
      "epoch: 5 step: 125, loss is 0.0005006124265491962\n",
      "epoch: 5 step: 126, loss is 0.0010251518106088042\n",
      "epoch: 5 step: 127, loss is 0.0022116671316325665\n",
      "epoch: 5 step: 128, loss is 0.014856737107038498\n",
      "epoch: 5 step: 129, loss is 0.09708283841609955\n",
      "epoch: 5 step: 130, loss is 0.0025634076446294785\n",
      "epoch: 5 step: 131, loss is 0.0048059080727398396\n",
      "epoch: 5 step: 132, loss is 0.0006645034300163388\n",
      "epoch: 5 step: 133, loss is 0.031133798882365227\n",
      "epoch: 5 step: 134, loss is 0.0022582197561860085\n",
      "epoch: 5 step: 135, loss is 0.006221185438334942\n",
      "epoch: 5 step: 136, loss is 0.004956299904733896\n",
      "epoch: 5 step: 137, loss is 0.0008998619159683585\n",
      "epoch: 5 step: 138, loss is 0.008406302891671658\n",
      "epoch: 5 step: 139, loss is 0.14771419763565063\n",
      "epoch: 5 step: 140, loss is 0.005201202351599932\n",
      "epoch: 5 step: 141, loss is 0.18552665412425995\n",
      "epoch: 5 step: 142, loss is 0.06775835156440735\n",
      "epoch: 5 step: 143, loss is 0.008874882943928242\n",
      "epoch: 5 step: 144, loss is 0.0027868456672877073\n",
      "epoch: 5 step: 145, loss is 0.0002859284868463874\n",
      "epoch: 5 step: 146, loss is 0.03010338358581066\n",
      "epoch: 5 step: 147, loss is 0.0065276180393993855\n",
      "epoch: 5 step: 148, loss is 0.026215465739369392\n",
      "epoch: 5 step: 149, loss is 0.0010447193635627627\n",
      "epoch: 5 step: 150, loss is 0.0055195027962327\n",
      "epoch: 5 step: 151, loss is 0.016270533204078674\n",
      "epoch: 5 step: 152, loss is 0.015128287486732006\n",
      "epoch: 5 step: 153, loss is 0.01628533937036991\n",
      "epoch: 5 step: 154, loss is 0.01894761063158512\n",
      "epoch: 5 step: 155, loss is 0.032784510403871536\n",
      "epoch: 5 step: 156, loss is 0.006901784334331751\n",
      "epoch: 5 step: 157, loss is 0.0015944644110277295\n",
      "epoch: 5 step: 158, loss is 0.005277544260025024\n",
      "epoch: 5 step: 159, loss is 0.00011745451047318056\n",
      "epoch: 5 step: 160, loss is 0.005311758257448673\n",
      "epoch: 5 step: 161, loss is 0.0034105447120964527\n",
      "epoch: 5 step: 162, loss is 0.02799440734088421\n",
      "epoch: 5 step: 163, loss is 0.03409753367304802\n",
      "epoch: 5 step: 164, loss is 0.0455993227660656\n",
      "epoch: 5 step: 165, loss is 0.019892465323209763\n",
      "epoch: 5 step: 166, loss is 0.0017139145638793707\n",
      "epoch: 5 step: 167, loss is 0.0031164446845650673\n",
      "epoch: 5 step: 168, loss is 0.001276698661968112\n",
      "epoch: 5 step: 169, loss is 0.001363609335385263\n",
      "epoch: 5 step: 170, loss is 0.00872510764747858\n",
      "epoch: 5 step: 171, loss is 0.0006259757792577147\n",
      "epoch: 5 step: 172, loss is 0.1784556806087494\n",
      "epoch: 5 step: 173, loss is 0.045401863753795624\n",
      "epoch: 5 step: 174, loss is 0.0013448948739096522\n",
      "epoch: 5 step: 175, loss is 0.0004105418920516968\n",
      "epoch: 5 step: 176, loss is 0.008856147527694702\n",
      "epoch: 5 step: 177, loss is 0.00015428247570525855\n",
      "epoch: 5 step: 178, loss is 0.04002058878540993\n",
      "epoch: 5 step: 179, loss is 9.285335545428097e-05\n",
      "epoch: 5 step: 180, loss is 0.0035866289399564266\n",
      "epoch: 5 step: 181, loss is 0.0019130853470414877\n",
      "epoch: 5 step: 182, loss is 0.002830864628776908\n",
      "epoch: 5 step: 183, loss is 0.0025634746998548508\n",
      "epoch: 5 step: 184, loss is 0.002356777898967266\n",
      "epoch: 5 step: 185, loss is 0.0030344314873218536\n",
      "epoch: 5 step: 186, loss is 0.00010154192568734288\n",
      "epoch: 5 step: 187, loss is 0.04745454713702202\n",
      "epoch: 5 step: 188, loss is 0.00869030226022005\n",
      "epoch: 5 step: 189, loss is 0.046722862869501114\n",
      "epoch: 5 step: 190, loss is 0.21150267124176025\n",
      "epoch: 5 step: 191, loss is 0.00615435466170311\n",
      "epoch: 5 step: 192, loss is 0.0034664738923311234\n",
      "epoch: 5 step: 193, loss is 0.0011150353820994496\n",
      "epoch: 5 step: 194, loss is 0.062091678380966187\n",
      "epoch: 5 step: 195, loss is 0.0012665960239246488\n",
      "epoch: 5 step: 196, loss is 0.0008627609349787235\n",
      "epoch: 5 step: 197, loss is 0.007134127430617809\n",
      "epoch: 5 step: 198, loss is 0.002097800839692354\n",
      "epoch: 5 step: 199, loss is 0.010878227651119232\n",
      "epoch: 5 step: 200, loss is 0.006012254860252142\n",
      "epoch: 5 step: 201, loss is 0.0008810273720882833\n",
      "epoch: 5 step: 202, loss is 0.01149125024676323\n",
      "epoch: 5 step: 203, loss is 0.010859204456210136\n",
      "epoch: 5 step: 204, loss is 0.011415062472224236\n",
      "epoch: 5 step: 205, loss is 0.006955147720873356\n",
      "epoch: 5 step: 206, loss is 0.0029169030021876097\n",
      "epoch: 5 step: 207, loss is 0.002603684552013874\n",
      "epoch: 5 step: 208, loss is 0.030470799654722214\n",
      "epoch: 5 step: 209, loss is 0.0007434518192894757\n",
      "epoch: 5 step: 210, loss is 0.0815039798617363\n",
      "epoch: 5 step: 211, loss is 0.15033318102359772\n",
      "epoch: 5 step: 212, loss is 0.20632532238960266\n",
      "epoch: 5 step: 213, loss is 0.0012017188128083944\n",
      "epoch: 5 step: 214, loss is 0.0021308427676558495\n",
      "epoch: 5 step: 215, loss is 0.0016894523287191987\n",
      "epoch: 5 step: 216, loss is 0.06752225756645203\n",
      "epoch: 5 step: 217, loss is 0.0013256921665742993\n",
      "epoch: 5 step: 218, loss is 0.05716792866587639\n",
      "epoch: 5 step: 219, loss is 0.0011283829808235168\n",
      "epoch: 5 step: 220, loss is 0.024043839424848557\n",
      "epoch: 5 step: 221, loss is 0.003043622011318803\n",
      "epoch: 5 step: 222, loss is 0.07253464311361313\n",
      "epoch: 5 step: 223, loss is 0.05067207291722298\n",
      "epoch: 5 step: 224, loss is 0.04367845878005028\n",
      "epoch: 5 step: 225, loss is 0.002429876010864973\n",
      "epoch: 5 step: 226, loss is 0.0013193335616961122\n",
      "epoch: 5 step: 227, loss is 0.03834681957960129\n",
      "epoch: 5 step: 228, loss is 0.043104130774736404\n",
      "epoch: 5 step: 229, loss is 0.2243940234184265\n",
      "epoch: 5 step: 230, loss is 0.00059468112885952\n",
      "epoch: 5 step: 231, loss is 0.0053900969214737415\n",
      "epoch: 5 step: 232, loss is 0.03509778156876564\n",
      "epoch: 5 step: 233, loss is 0.0036798620130866766\n",
      "epoch: 5 step: 234, loss is 0.003579414915293455\n",
      "epoch: 5 step: 235, loss is 0.0323641300201416\n",
      "epoch: 5 step: 236, loss is 0.015325778163969517\n",
      "epoch: 5 step: 237, loss is 0.0008297414751723409\n",
      "epoch: 5 step: 238, loss is 0.001923847827129066\n",
      "epoch: 5 step: 239, loss is 0.001607753918506205\n",
      "epoch: 5 step: 240, loss is 0.0009351835469715297\n",
      "epoch: 5 step: 241, loss is 0.0010509726125746965\n",
      "epoch: 5 step: 242, loss is 0.0011827859561890364\n",
      "epoch: 5 step: 243, loss is 0.008144744671881199\n",
      "epoch: 5 step: 244, loss is 0.005142261739820242\n",
      "epoch: 5 step: 245, loss is 0.0011502557899802923\n",
      "epoch: 5 step: 246, loss is 0.06636989116668701\n",
      "epoch: 5 step: 247, loss is 0.040115922689437866\n",
      "epoch: 5 step: 248, loss is 0.004088708199560642\n",
      "epoch: 5 step: 249, loss is 0.0020173806697130203\n",
      "epoch: 5 step: 250, loss is 0.000992408487945795\n",
      "epoch: 5 step: 251, loss is 0.000254254846367985\n",
      "epoch: 5 step: 252, loss is 0.0021082430612295866\n",
      "epoch: 5 step: 253, loss is 0.03400985524058342\n",
      "epoch: 5 step: 254, loss is 0.0008462881087325513\n",
      "epoch: 5 step: 255, loss is 0.0024542431347072124\n",
      "epoch: 5 step: 256, loss is 0.010644752532243729\n",
      "epoch: 5 step: 257, loss is 0.00017997903341893107\n",
      "epoch: 5 step: 258, loss is 0.09708961844444275\n",
      "epoch: 5 step: 259, loss is 0.00040094947325997055\n",
      "epoch: 5 step: 260, loss is 0.04037540778517723\n",
      "epoch: 5 step: 261, loss is 0.06200476735830307\n",
      "epoch: 5 step: 262, loss is 0.0015864656306803226\n",
      "epoch: 5 step: 263, loss is 0.13598856329917908\n",
      "epoch: 5 step: 264, loss is 0.0016980123473331332\n",
      "epoch: 5 step: 265, loss is 0.00183933658991009\n",
      "epoch: 5 step: 266, loss is 0.0028910143300890923\n",
      "epoch: 5 step: 267, loss is 0.007013908587396145\n",
      "epoch: 5 step: 268, loss is 0.0048052663914859295\n",
      "epoch: 5 step: 269, loss is 0.0033096212428063154\n",
      "epoch: 5 step: 270, loss is 0.002618654863908887\n",
      "epoch: 5 step: 271, loss is 0.003663965268060565\n",
      "epoch: 5 step: 272, loss is 0.006035920232534409\n",
      "epoch: 5 step: 273, loss is 0.19538868963718414\n",
      "epoch: 5 step: 274, loss is 0.005321950651705265\n",
      "epoch: 5 step: 275, loss is 0.0073417131789028645\n",
      "epoch: 5 step: 276, loss is 0.15435174107551575\n",
      "epoch: 5 step: 277, loss is 0.0037145232781767845\n",
      "epoch: 5 step: 278, loss is 0.00034269990283064544\n",
      "epoch: 5 step: 279, loss is 0.010689201764762402\n",
      "epoch: 5 step: 280, loss is 0.005268845707178116\n",
      "epoch: 5 step: 281, loss is 0.050147656351327896\n",
      "epoch: 5 step: 282, loss is 0.0018269523279741406\n",
      "epoch: 5 step: 283, loss is 0.0014241214375942945\n",
      "epoch: 5 step: 284, loss is 0.0023570384364575148\n",
      "epoch: 5 step: 285, loss is 0.02561485394835472\n",
      "epoch: 5 step: 286, loss is 0.0030557967256754637\n",
      "epoch: 5 step: 287, loss is 0.001078890054486692\n",
      "epoch: 5 step: 288, loss is 0.0010252956999465823\n",
      "epoch: 5 step: 289, loss is 0.004321844317018986\n",
      "epoch: 5 step: 290, loss is 0.0009239986538887024\n",
      "epoch: 5 step: 291, loss is 0.00011362108489265665\n",
      "epoch: 5 step: 292, loss is 0.15913927555084229\n",
      "epoch: 5 step: 293, loss is 0.002569524571299553\n",
      "epoch: 5 step: 294, loss is 0.021219415590167046\n",
      "epoch: 5 step: 295, loss is 0.04946453496813774\n",
      "epoch: 5 step: 296, loss is 0.09855938702821732\n",
      "epoch: 5 step: 297, loss is 0.004188549239188433\n",
      "epoch: 5 step: 298, loss is 0.0027674136217683554\n",
      "epoch: 5 step: 299, loss is 0.002546221949160099\n",
      "epoch: 5 step: 300, loss is 0.0028766694013029337\n",
      "epoch: 5 step: 301, loss is 0.0007519974024035037\n",
      "epoch: 5 step: 302, loss is 0.0007319665746763349\n",
      "epoch: 5 step: 303, loss is 0.006056707818061113\n",
      "epoch: 5 step: 304, loss is 0.0017913737101480365\n",
      "epoch: 5 step: 305, loss is 0.0036074514500796795\n",
      "epoch: 5 step: 306, loss is 0.00017351802671328187\n",
      "epoch: 5 step: 307, loss is 0.00018925571930594742\n",
      "epoch: 5 step: 308, loss is 0.005775472614914179\n",
      "epoch: 5 step: 309, loss is 0.06404478847980499\n",
      "epoch: 5 step: 310, loss is 0.009418642148375511\n",
      "epoch: 5 step: 311, loss is 0.0012166333617642522\n",
      "epoch: 5 step: 312, loss is 0.08167189359664917\n",
      "epoch: 5 step: 313, loss is 0.0006123969797044992\n",
      "epoch: 5 step: 314, loss is 0.0012037361739203334\n",
      "epoch: 5 step: 315, loss is 0.03213917836546898\n",
      "epoch: 5 step: 316, loss is 0.06337779760360718\n",
      "epoch: 5 step: 317, loss is 0.004284829832613468\n",
      "epoch: 5 step: 318, loss is 0.01820162683725357\n",
      "epoch: 5 step: 319, loss is 0.0003306071739643812\n",
      "epoch: 5 step: 320, loss is 0.0024452439974993467\n",
      "epoch: 5 step: 321, loss is 0.0008164396276697516\n",
      "epoch: 5 step: 322, loss is 0.0374445840716362\n",
      "epoch: 5 step: 323, loss is 0.003784916363656521\n",
      "epoch: 5 step: 324, loss is 0.0044048866257071495\n",
      "epoch: 5 step: 325, loss is 0.003972897306084633\n",
      "epoch: 5 step: 326, loss is 0.00033669391996227205\n",
      "epoch: 5 step: 327, loss is 0.022103656083345413\n",
      "epoch: 5 step: 328, loss is 0.04896947368979454\n",
      "epoch: 5 step: 329, loss is 0.0007118359208106995\n",
      "epoch: 5 step: 330, loss is 0.0008312298450618982\n",
      "epoch: 5 step: 331, loss is 0.00468018651008606\n",
      "epoch: 5 step: 332, loss is 0.026939043775200844\n",
      "epoch: 5 step: 333, loss is 0.0004228598263580352\n",
      "epoch: 5 step: 334, loss is 0.006985630374401808\n",
      "epoch: 5 step: 335, loss is 0.0023160811979323626\n",
      "epoch: 5 step: 336, loss is 0.0020897143986076117\n",
      "epoch: 5 step: 337, loss is 0.012309560552239418\n",
      "epoch: 5 step: 338, loss is 0.026598745957016945\n",
      "epoch: 5 step: 339, loss is 0.0014832682209089398\n",
      "epoch: 5 step: 340, loss is 0.07772830873727798\n",
      "epoch: 5 step: 341, loss is 0.0014319892507046461\n",
      "epoch: 5 step: 342, loss is 0.0028728351462632418\n",
      "epoch: 5 step: 343, loss is 0.010076546110212803\n",
      "epoch: 5 step: 344, loss is 0.05154236778616905\n",
      "epoch: 5 step: 345, loss is 0.0022452513221651316\n",
      "epoch: 5 step: 346, loss is 0.003315478563308716\n",
      "epoch: 5 step: 347, loss is 0.04676736146211624\n",
      "epoch: 5 step: 348, loss is 0.0007501698564738035\n",
      "epoch: 5 step: 349, loss is 0.0005493206554092467\n",
      "epoch: 5 step: 350, loss is 0.001205328619107604\n",
      "epoch: 5 step: 351, loss is 0.002334799151867628\n",
      "epoch: 5 step: 352, loss is 0.06536238640546799\n",
      "epoch: 5 step: 353, loss is 0.03357849270105362\n",
      "epoch: 5 step: 354, loss is 0.0013798141153529286\n",
      "epoch: 5 step: 355, loss is 0.003219258738681674\n",
      "epoch: 5 step: 356, loss is 0.008273888379335403\n",
      "epoch: 5 step: 357, loss is 0.09015801548957825\n",
      "epoch: 5 step: 358, loss is 0.07616224139928818\n",
      "epoch: 5 step: 359, loss is 0.00014007691061124206\n",
      "epoch: 5 step: 360, loss is 0.03100229613482952\n",
      "epoch: 5 step: 361, loss is 0.22596463561058044\n",
      "epoch: 5 step: 362, loss is 0.007264481857419014\n",
      "epoch: 5 step: 363, loss is 0.004932175390422344\n",
      "epoch: 5 step: 364, loss is 0.0001018577822833322\n",
      "epoch: 5 step: 365, loss is 0.0037140664644539356\n",
      "epoch: 5 step: 366, loss is 0.04132218286395073\n",
      "epoch: 5 step: 367, loss is 0.00044931162847205997\n",
      "epoch: 5 step: 368, loss is 0.030036555603146553\n",
      "epoch: 5 step: 369, loss is 0.05423012375831604\n",
      "epoch: 5 step: 370, loss is 0.10821013897657394\n",
      "epoch: 5 step: 371, loss is 0.004862696398049593\n",
      "epoch: 5 step: 372, loss is 0.0014928380260244012\n",
      "epoch: 5 step: 373, loss is 0.0025763714220374823\n",
      "epoch: 5 step: 374, loss is 0.28167811036109924\n",
      "epoch: 5 step: 375, loss is 0.00294998986646533\n",
      "epoch: 5 step: 376, loss is 0.040600892156362534\n",
      "epoch: 5 step: 377, loss is 0.0028048702515661716\n",
      "epoch: 5 step: 378, loss is 0.005452171433717012\n",
      "epoch: 5 step: 379, loss is 0.0055444808676838875\n",
      "epoch: 5 step: 380, loss is 0.004172042477875948\n",
      "epoch: 5 step: 381, loss is 0.20556388795375824\n",
      "epoch: 5 step: 382, loss is 0.04144350066781044\n",
      "epoch: 5 step: 383, loss is 0.1153399646282196\n",
      "epoch: 5 step: 384, loss is 0.028821879997849464\n",
      "epoch: 5 step: 385, loss is 0.0023168581537902355\n",
      "epoch: 5 step: 386, loss is 0.0014705758076161146\n",
      "epoch: 5 step: 387, loss is 0.0010418836027383804\n",
      "epoch: 5 step: 388, loss is 0.001773092313669622\n",
      "epoch: 5 step: 389, loss is 0.0737239345908165\n",
      "epoch: 5 step: 390, loss is 0.005178755149245262\n",
      "epoch: 5 step: 391, loss is 0.000166241021361202\n",
      "epoch: 5 step: 392, loss is 0.0020314629655331373\n",
      "epoch: 5 step: 393, loss is 0.0023306286893785\n",
      "epoch: 5 step: 394, loss is 0.021288422867655754\n",
      "epoch: 5 step: 395, loss is 0.004235371947288513\n",
      "epoch: 5 step: 396, loss is 0.08091530948877335\n",
      "epoch: 5 step: 397, loss is 0.08579745143651962\n",
      "epoch: 5 step: 398, loss is 0.05595898628234863\n",
      "epoch: 5 step: 399, loss is 0.024494748562574387\n",
      "epoch: 5 step: 400, loss is 0.00046721205580979586\n",
      "epoch: 5 step: 401, loss is 0.16732041537761688\n",
      "epoch: 5 step: 402, loss is 0.0024399193935096264\n",
      "epoch: 5 step: 403, loss is 0.01137885358184576\n",
      "epoch: 5 step: 404, loss is 0.011460800655186176\n",
      "epoch: 5 step: 405, loss is 0.07265415042638779\n",
      "epoch: 5 step: 406, loss is 0.004624932073056698\n",
      "epoch: 5 step: 407, loss is 0.01079845055937767\n",
      "epoch: 5 step: 408, loss is 0.0004491734434850514\n",
      "epoch: 5 step: 409, loss is 0.0014887457946315408\n",
      "epoch: 5 step: 410, loss is 0.002787421690300107\n",
      "epoch: 5 step: 411, loss is 0.0028824235778301954\n",
      "epoch: 5 step: 412, loss is 0.0007696805987507105\n",
      "epoch: 5 step: 413, loss is 0.0039456309750676155\n",
      "epoch: 5 step: 414, loss is 0.004151534754782915\n",
      "epoch: 5 step: 415, loss is 0.08948106318712234\n",
      "epoch: 5 step: 416, loss is 0.00128763762768358\n",
      "epoch: 5 step: 417, loss is 0.13694006204605103\n",
      "epoch: 5 step: 418, loss is 0.07620252668857574\n",
      "epoch: 5 step: 419, loss is 0.022682098671793938\n",
      "epoch: 5 step: 420, loss is 0.0011124939192086458\n",
      "epoch: 5 step: 421, loss is 0.0006806033779866993\n",
      "epoch: 5 step: 422, loss is 0.039559781551361084\n",
      "epoch: 5 step: 423, loss is 0.024636248126626015\n",
      "epoch: 5 step: 424, loss is 0.01521033700555563\n",
      "epoch: 5 step: 425, loss is 0.0011250479146838188\n",
      "epoch: 5 step: 426, loss is 0.012484443373978138\n",
      "epoch: 5 step: 427, loss is 0.0005563559825532138\n",
      "epoch: 5 step: 428, loss is 0.007619649637490511\n",
      "epoch: 5 step: 429, loss is 0.028599776327610016\n",
      "epoch: 5 step: 430, loss is 0.0026990054175257683\n",
      "epoch: 5 step: 431, loss is 6.47728011244908e-05\n",
      "epoch: 5 step: 432, loss is 0.014452981762588024\n",
      "epoch: 5 step: 433, loss is 0.0002462789707351476\n",
      "epoch: 5 step: 434, loss is 0.029528126120567322\n",
      "epoch: 5 step: 435, loss is 0.004274074453860521\n",
      "epoch: 5 step: 436, loss is 0.11121831089258194\n",
      "epoch: 5 step: 437, loss is 0.0005108212935738266\n",
      "epoch: 5 step: 438, loss is 0.03187339007854462\n",
      "epoch: 5 step: 439, loss is 0.0023795017041265965\n",
      "epoch: 5 step: 440, loss is 0.004675284959375858\n",
      "epoch: 5 step: 441, loss is 0.0006272867904044688\n",
      "epoch: 5 step: 442, loss is 0.009133544750511646\n",
      "epoch: 5 step: 443, loss is 0.02053426019847393\n",
      "epoch: 5 step: 444, loss is 0.009272852912545204\n",
      "epoch: 5 step: 445, loss is 0.022859623655676842\n",
      "epoch: 5 step: 446, loss is 0.005757015664130449\n",
      "epoch: 5 step: 447, loss is 0.002907045418396592\n",
      "epoch: 5 step: 448, loss is 0.0033887403551489115\n",
      "epoch: 5 step: 449, loss is 0.002501602051779628\n",
      "epoch: 5 step: 450, loss is 0.00044058426283299923\n",
      "epoch: 5 step: 451, loss is 0.0013308284105733037\n",
      "epoch: 5 step: 452, loss is 0.00360888778232038\n",
      "epoch: 5 step: 453, loss is 0.003182441694661975\n",
      "epoch: 5 step: 454, loss is 0.0008144248276948929\n",
      "epoch: 5 step: 455, loss is 0.0015225927345454693\n",
      "epoch: 5 step: 456, loss is 0.0005726962699554861\n",
      "epoch: 5 step: 457, loss is 0.0009042318561114371\n",
      "epoch: 5 step: 458, loss is 0.0008732097921893001\n",
      "epoch: 5 step: 459, loss is 0.002310318872332573\n",
      "epoch: 5 step: 460, loss is 0.1158130094408989\n",
      "epoch: 5 step: 461, loss is 0.032790638506412506\n",
      "epoch: 5 step: 462, loss is 0.0018923281459137797\n",
      "epoch: 5 step: 463, loss is 0.00334981014020741\n",
      "epoch: 5 step: 464, loss is 0.00019429459644015878\n",
      "epoch: 5 step: 465, loss is 0.008820012211799622\n",
      "epoch: 5 step: 466, loss is 0.002616188023239374\n",
      "epoch: 5 step: 467, loss is 0.0005311874556355178\n",
      "epoch: 5 step: 468, loss is 0.011957290582358837\n",
      "epoch: 5 step: 469, loss is 0.00030191088444553316\n",
      "epoch: 5 step: 470, loss is 0.0047560082748532295\n",
      "epoch: 5 step: 471, loss is 0.00018488340720068663\n",
      "epoch: 5 step: 472, loss is 0.08574312180280685\n",
      "epoch: 5 step: 473, loss is 0.1349196881055832\n",
      "epoch: 5 step: 474, loss is 0.037073370069265366\n",
      "epoch: 5 step: 475, loss is 0.01131418813019991\n",
      "epoch: 5 step: 476, loss is 0.0017033892218023539\n",
      "epoch: 5 step: 477, loss is 4.5423719711834565e-05\n",
      "epoch: 5 step: 478, loss is 0.015000227838754654\n",
      "epoch: 5 step: 479, loss is 7.276629912666976e-05\n",
      "epoch: 5 step: 480, loss is 0.003986714407801628\n",
      "epoch: 5 step: 481, loss is 0.0007984574185684323\n",
      "epoch: 5 step: 482, loss is 0.028576206415891647\n",
      "epoch: 5 step: 483, loss is 0.00953059270977974\n",
      "epoch: 5 step: 484, loss is 0.04154396057128906\n",
      "epoch: 5 step: 485, loss is 0.0016198735684156418\n",
      "epoch: 5 step: 486, loss is 0.008142989128828049\n",
      "epoch: 5 step: 487, loss is 0.0009905933402478695\n",
      "epoch: 5 step: 488, loss is 0.0022893825080245733\n",
      "epoch: 5 step: 489, loss is 0.00043092045234516263\n",
      "epoch: 5 step: 490, loss is 0.002991572255268693\n",
      "epoch: 5 step: 491, loss is 0.05284778028726578\n",
      "epoch: 5 step: 492, loss is 0.0007071203435771167\n",
      "epoch: 5 step: 493, loss is 0.0020435494370758533\n",
      "epoch: 5 step: 494, loss is 0.006743208039551973\n",
      "epoch: 5 step: 495, loss is 0.018869802355766296\n",
      "epoch: 5 step: 496, loss is 0.011420614086091518\n",
      "epoch: 5 step: 497, loss is 0.00039815870695747435\n",
      "epoch: 5 step: 498, loss is 0.0024767203722149134\n",
      "epoch: 5 step: 499, loss is 0.01575709879398346\n",
      "epoch: 5 step: 500, loss is 0.02982107177376747\n",
      "epoch: 5 step: 501, loss is 0.15771439671516418\n",
      "epoch: 5 step: 502, loss is 0.034598734229803085\n",
      "epoch: 5 step: 503, loss is 0.0049865031614899635\n",
      "epoch: 5 step: 504, loss is 0.09990474581718445\n",
      "epoch: 5 step: 505, loss is 0.007564116735011339\n",
      "epoch: 5 step: 506, loss is 0.07152856141328812\n",
      "epoch: 5 step: 507, loss is 0.0001536194613436237\n",
      "epoch: 5 step: 508, loss is 0.08397072553634644\n",
      "epoch: 5 step: 509, loss is 0.001398091553710401\n",
      "epoch: 5 step: 510, loss is 0.023379847407341003\n",
      "epoch: 5 step: 511, loss is 0.023056767880916595\n",
      "epoch: 5 step: 512, loss is 0.0015314476331695914\n",
      "epoch: 5 step: 513, loss is 0.0008769468986429274\n",
      "epoch: 5 step: 514, loss is 0.015371488407254219\n",
      "epoch: 5 step: 515, loss is 0.012699306011199951\n",
      "epoch: 5 step: 516, loss is 0.1572958379983902\n",
      "epoch: 5 step: 517, loss is 0.00942552462220192\n",
      "epoch: 5 step: 518, loss is 0.0008691250695846975\n",
      "epoch: 5 step: 519, loss is 0.0016923496732488275\n",
      "epoch: 5 step: 520, loss is 0.0007076801848597825\n",
      "epoch: 5 step: 521, loss is 0.13299600780010223\n",
      "epoch: 5 step: 522, loss is 0.007849419489502907\n",
      "epoch: 5 step: 523, loss is 0.00250146328471601\n",
      "epoch: 5 step: 524, loss is 0.005232291761785746\n",
      "epoch: 5 step: 525, loss is 0.0012108299415558577\n",
      "epoch: 5 step: 526, loss is 0.022621335461735725\n",
      "epoch: 5 step: 527, loss is 0.000705867656506598\n",
      "epoch: 5 step: 528, loss is 0.14304301142692566\n",
      "epoch: 5 step: 529, loss is 0.020640235394239426\n",
      "epoch: 5 step: 530, loss is 0.029337579384446144\n",
      "epoch: 5 step: 531, loss is 0.012906580232083797\n",
      "epoch: 5 step: 532, loss is 0.003934881184250116\n",
      "epoch: 5 step: 533, loss is 0.005405556410551071\n",
      "epoch: 5 step: 534, loss is 0.003451449330896139\n",
      "epoch: 5 step: 535, loss is 0.007973704487085342\n",
      "epoch: 5 step: 536, loss is 0.0350135862827301\n",
      "epoch: 5 step: 537, loss is 0.0014465113636106253\n",
      "epoch: 5 step: 538, loss is 0.01639053039252758\n",
      "epoch: 5 step: 539, loss is 0.00012432067887857556\n",
      "epoch: 5 step: 540, loss is 0.0169582050293684\n",
      "epoch: 5 step: 541, loss is 0.0001707771880319342\n",
      "epoch: 5 step: 542, loss is 0.050261400640010834\n",
      "epoch: 5 step: 543, loss is 0.00041246548062190413\n",
      "epoch: 5 step: 544, loss is 0.07154159247875214\n",
      "epoch: 5 step: 545, loss is 0.0008403065148741007\n",
      "epoch: 5 step: 546, loss is 0.11263682693243027\n",
      "epoch: 5 step: 547, loss is 0.0506233312189579\n",
      "epoch: 5 step: 548, loss is 0.1615486592054367\n",
      "epoch: 5 step: 549, loss is 0.05309727042913437\n",
      "epoch: 5 step: 550, loss is 0.09789237380027771\n",
      "epoch: 5 step: 551, loss is 0.008495465852320194\n",
      "epoch: 5 step: 552, loss is 0.0028560517821460962\n",
      "epoch: 5 step: 553, loss is 0.000509724544826895\n",
      "epoch: 5 step: 554, loss is 0.0029776603914797306\n",
      "epoch: 5 step: 555, loss is 0.00036343405372463167\n",
      "epoch: 5 step: 556, loss is 0.045195501297712326\n",
      "epoch: 5 step: 557, loss is 0.0563364215195179\n",
      "epoch: 5 step: 558, loss is 0.02700101211667061\n",
      "epoch: 5 step: 559, loss is 0.009834302589297295\n",
      "epoch: 5 step: 560, loss is 0.0009026371990330517\n",
      "epoch: 5 step: 561, loss is 0.011612237431108952\n",
      "epoch: 5 step: 562, loss is 0.04784040525555611\n",
      "epoch: 5 step: 563, loss is 0.0023236589040607214\n",
      "epoch: 5 step: 564, loss is 0.005243789404630661\n",
      "epoch: 5 step: 565, loss is 0.03782473877072334\n",
      "epoch: 5 step: 566, loss is 0.0020513220224529505\n",
      "epoch: 5 step: 567, loss is 0.0025387820787727833\n",
      "epoch: 5 step: 568, loss is 0.0031134430319070816\n",
      "epoch: 5 step: 569, loss is 0.0005478843813762069\n",
      "epoch: 5 step: 570, loss is 0.034149158746004105\n",
      "epoch: 5 step: 571, loss is 0.002386889886111021\n",
      "epoch: 5 step: 572, loss is 0.031831130385398865\n",
      "epoch: 5 step: 573, loss is 0.19888003170490265\n",
      "epoch: 5 step: 574, loss is 0.00040765339508652687\n",
      "epoch: 5 step: 575, loss is 0.0056314850226044655\n",
      "epoch: 5 step: 576, loss is 0.01832345686852932\n",
      "epoch: 5 step: 577, loss is 0.002638189820572734\n",
      "epoch: 5 step: 578, loss is 0.03094770386815071\n",
      "epoch: 5 step: 579, loss is 0.1396884173154831\n",
      "epoch: 5 step: 580, loss is 0.0012107841903343797\n",
      "epoch: 5 step: 581, loss is 0.001211944967508316\n",
      "epoch: 5 step: 582, loss is 0.0021866264287382364\n",
      "epoch: 5 step: 583, loss is 0.000747563608456403\n",
      "epoch: 5 step: 584, loss is 0.04641944169998169\n",
      "epoch: 5 step: 585, loss is 0.08373752236366272\n",
      "epoch: 5 step: 586, loss is 0.006839220877736807\n",
      "epoch: 5 step: 587, loss is 0.0003887585480697453\n",
      "epoch: 5 step: 588, loss is 0.0003756668302230537\n",
      "epoch: 5 step: 589, loss is 0.05273149162530899\n",
      "epoch: 5 step: 590, loss is 0.017991410568356514\n",
      "epoch: 5 step: 591, loss is 0.012853886932134628\n",
      "epoch: 5 step: 592, loss is 0.0009725996060296893\n",
      "epoch: 5 step: 593, loss is 0.0085905771702528\n",
      "epoch: 5 step: 594, loss is 0.006298532243818045\n",
      "epoch: 5 step: 595, loss is 0.00021345625282265246\n",
      "epoch: 5 step: 596, loss is 0.002119096228852868\n",
      "epoch: 5 step: 597, loss is 0.03237321972846985\n",
      "epoch: 5 step: 598, loss is 0.0007893909350968897\n",
      "epoch: 5 step: 599, loss is 0.007128048688173294\n",
      "epoch: 5 step: 600, loss is 0.020065976306796074\n",
      "epoch: 5 step: 601, loss is 0.21785172820091248\n",
      "epoch: 5 step: 602, loss is 0.000783157127443701\n",
      "epoch: 5 step: 603, loss is 0.0001312053354922682\n",
      "epoch: 5 step: 604, loss is 0.018108362331986427\n",
      "epoch: 5 step: 605, loss is 0.0021308495197445154\n",
      "epoch: 5 step: 606, loss is 0.003648260608315468\n",
      "epoch: 5 step: 607, loss is 0.005953111220151186\n",
      "epoch: 5 step: 608, loss is 0.0014184791361913085\n",
      "epoch: 5 step: 609, loss is 0.15674179792404175\n",
      "epoch: 5 step: 610, loss is 0.0023801454808562994\n",
      "epoch: 5 step: 611, loss is 0.0285591259598732\n",
      "epoch: 5 step: 612, loss is 0.0029531517066061497\n",
      "epoch: 5 step: 613, loss is 0.000526315241586417\n",
      "epoch: 5 step: 614, loss is 0.0575488917529583\n",
      "epoch: 5 step: 615, loss is 0.04301079735159874\n",
      "epoch: 5 step: 616, loss is 0.0017232279060408473\n",
      "epoch: 5 step: 617, loss is 0.020409371703863144\n",
      "epoch: 5 step: 618, loss is 0.1543685495853424\n",
      "epoch: 5 step: 619, loss is 0.03739411011338234\n",
      "epoch: 5 step: 620, loss is 0.010580725967884064\n",
      "epoch: 5 step: 621, loss is 0.0023075195495039225\n",
      "epoch: 5 step: 622, loss is 0.032879021018743515\n",
      "epoch: 5 step: 623, loss is 0.0029596546664834023\n",
      "epoch: 5 step: 624, loss is 0.05392320081591606\n",
      "epoch: 5 step: 625, loss is 0.00028352224035188556\n",
      "epoch: 5 step: 626, loss is 0.005984573159366846\n",
      "epoch: 5 step: 627, loss is 0.0027582519687712193\n",
      "epoch: 5 step: 628, loss is 0.29231080412864685\n",
      "epoch: 5 step: 629, loss is 0.13815411925315857\n",
      "epoch: 5 step: 630, loss is 0.004143886733800173\n",
      "epoch: 5 step: 631, loss is 0.03154020011425018\n",
      "epoch: 5 step: 632, loss is 0.004278529901057482\n",
      "epoch: 5 step: 633, loss is 0.008587163873016834\n",
      "epoch: 5 step: 634, loss is 0.007063392549753189\n",
      "epoch: 5 step: 635, loss is 0.006581000983715057\n",
      "epoch: 5 step: 636, loss is 0.0500226765871048\n",
      "epoch: 5 step: 637, loss is 0.000314825854729861\n",
      "epoch: 5 step: 638, loss is 0.00822440441697836\n",
      "epoch: 5 step: 639, loss is 0.27522343397140503\n",
      "epoch: 5 step: 640, loss is 0.005092883948236704\n",
      "epoch: 5 step: 641, loss is 0.015348032116889954\n",
      "epoch: 5 step: 642, loss is 0.010688032023608685\n",
      "epoch: 5 step: 643, loss is 0.043746013194322586\n",
      "epoch: 5 step: 644, loss is 0.01308844331651926\n",
      "epoch: 5 step: 645, loss is 0.002250684192404151\n",
      "epoch: 5 step: 646, loss is 0.0021507078781723976\n",
      "epoch: 5 step: 647, loss is 0.03437076881527901\n",
      "epoch: 5 step: 648, loss is 0.007419789209961891\n",
      "epoch: 5 step: 649, loss is 0.009046444669365883\n",
      "epoch: 5 step: 650, loss is 0.006884642876684666\n",
      "epoch: 5 step: 651, loss is 4.689503111876547e-05\n",
      "epoch: 5 step: 652, loss is 0.024418486282229424\n",
      "epoch: 5 step: 653, loss is 0.0037368200719356537\n",
      "epoch: 5 step: 654, loss is 0.008293266408145428\n",
      "epoch: 5 step: 655, loss is 0.01128925010561943\n",
      "epoch: 5 step: 656, loss is 0.015322903171181679\n",
      "epoch: 5 step: 657, loss is 0.017579736188054085\n",
      "epoch: 5 step: 658, loss is 0.0009682777454145253\n",
      "epoch: 5 step: 659, loss is 0.011502656154334545\n",
      "epoch: 5 step: 660, loss is 0.0056007783859968185\n",
      "epoch: 5 step: 661, loss is 0.0001374263665638864\n",
      "epoch: 5 step: 662, loss is 0.0001610265317140147\n",
      "epoch: 5 step: 663, loss is 0.0036012802738696337\n",
      "epoch: 5 step: 664, loss is 0.0029405313543975353\n",
      "epoch: 5 step: 665, loss is 0.022012809291481972\n",
      "epoch: 5 step: 666, loss is 0.13569419085979462\n",
      "epoch: 5 step: 667, loss is 0.011886228807270527\n",
      "epoch: 5 step: 668, loss is 0.04957647994160652\n",
      "epoch: 5 step: 669, loss is 0.015538150444626808\n",
      "epoch: 5 step: 670, loss is 0.00019794798572547734\n",
      "epoch: 5 step: 671, loss is 0.0008453330956399441\n",
      "epoch: 5 step: 672, loss is 0.025458475574851036\n",
      "epoch: 5 step: 673, loss is 0.017000021412968636\n",
      "epoch: 5 step: 674, loss is 0.00042750913416966796\n",
      "epoch: 5 step: 675, loss is 0.12502489984035492\n",
      "epoch: 5 step: 676, loss is 0.008945589885115623\n",
      "epoch: 5 step: 677, loss is 0.00034900393802672625\n",
      "epoch: 5 step: 678, loss is 0.0021971999667584896\n",
      "epoch: 5 step: 679, loss is 0.011769190430641174\n",
      "epoch: 5 step: 680, loss is 0.001359367510303855\n",
      "epoch: 5 step: 681, loss is 0.03316868096590042\n",
      "epoch: 5 step: 682, loss is 0.0023021986708045006\n",
      "epoch: 5 step: 683, loss is 0.01139548234641552\n",
      "epoch: 5 step: 684, loss is 0.0007483430672436953\n",
      "epoch: 5 step: 685, loss is 0.06983549147844315\n",
      "epoch: 5 step: 686, loss is 0.0002984447928611189\n",
      "epoch: 5 step: 687, loss is 0.10144080966711044\n",
      "epoch: 5 step: 688, loss is 0.07993529736995697\n",
      "epoch: 5 step: 689, loss is 0.018248820677399635\n",
      "epoch: 5 step: 690, loss is 0.03995134308934212\n",
      "epoch: 5 step: 691, loss is 0.005941248498857021\n",
      "epoch: 5 step: 692, loss is 0.11255677044391632\n",
      "epoch: 5 step: 693, loss is 0.026292093098163605\n",
      "epoch: 5 step: 694, loss is 0.004288561176508665\n",
      "epoch: 5 step: 695, loss is 0.0027352424804121256\n",
      "epoch: 5 step: 696, loss is 0.04332119598984718\n",
      "epoch: 5 step: 697, loss is 0.0026791677810251713\n",
      "epoch: 5 step: 698, loss is 0.04520145803689957\n",
      "epoch: 5 step: 699, loss is 0.003152930410578847\n",
      "epoch: 5 step: 700, loss is 0.07491949200630188\n",
      "epoch: 5 step: 701, loss is 0.0005175695405341685\n",
      "epoch: 5 step: 702, loss is 0.004709703382104635\n",
      "epoch: 5 step: 703, loss is 0.0006750441389158368\n",
      "epoch: 5 step: 704, loss is 0.0007012231508269906\n",
      "epoch: 5 step: 705, loss is 0.016762368381023407\n",
      "epoch: 5 step: 706, loss is 0.0023476877249777317\n",
      "epoch: 5 step: 707, loss is 0.05744170770049095\n",
      "epoch: 5 step: 708, loss is 0.013704784214496613\n",
      "epoch: 5 step: 709, loss is 0.001566803315654397\n",
      "epoch: 5 step: 710, loss is 0.006470642518252134\n",
      "epoch: 5 step: 711, loss is 0.11623013019561768\n",
      "epoch: 5 step: 712, loss is 0.11472471058368683\n",
      "epoch: 5 step: 713, loss is 0.005191729869693518\n",
      "epoch: 5 step: 714, loss is 0.010219075717031956\n",
      "epoch: 5 step: 715, loss is 0.024322139099240303\n",
      "epoch: 5 step: 716, loss is 0.0027910417411476374\n",
      "epoch: 5 step: 717, loss is 0.04824274033308029\n",
      "epoch: 5 step: 718, loss is 0.003907019272446632\n",
      "epoch: 5 step: 719, loss is 0.0019233617931604385\n",
      "epoch: 5 step: 720, loss is 0.12939263880252838\n",
      "epoch: 5 step: 721, loss is 0.004808035213500261\n",
      "epoch: 5 step: 722, loss is 0.000944581173826009\n",
      "epoch: 5 step: 723, loss is 0.001043557422235608\n",
      "epoch: 5 step: 724, loss is 0.06661663949489594\n",
      "epoch: 5 step: 725, loss is 0.1015101745724678\n",
      "epoch: 5 step: 726, loss is 0.25984951853752136\n",
      "epoch: 5 step: 727, loss is 0.000278318184427917\n",
      "epoch: 5 step: 728, loss is 0.004906664602458477\n",
      "epoch: 5 step: 729, loss is 0.03327154368162155\n",
      "epoch: 5 step: 730, loss is 0.041949305683374405\n",
      "epoch: 5 step: 731, loss is 0.00042998723802156746\n",
      "epoch: 5 step: 732, loss is 0.018459685146808624\n",
      "epoch: 5 step: 733, loss is 0.08498946577310562\n",
      "epoch: 5 step: 734, loss is 0.008719493634998798\n",
      "epoch: 5 step: 735, loss is 0.017565682530403137\n",
      "epoch: 5 step: 736, loss is 0.0601888969540596\n",
      "epoch: 5 step: 737, loss is 0.2188611477613449\n",
      "epoch: 5 step: 738, loss is 0.001923042698763311\n",
      "epoch: 5 step: 739, loss is 0.07999895513057709\n",
      "epoch: 5 step: 740, loss is 0.003884566482156515\n",
      "epoch: 5 step: 741, loss is 0.00876698736101389\n",
      "epoch: 5 step: 742, loss is 0.0018260766519233584\n",
      "epoch: 5 step: 743, loss is 0.006229158956557512\n",
      "epoch: 5 step: 744, loss is 0.013735342770814896\n",
      "epoch: 5 step: 745, loss is 0.0031494759023189545\n",
      "epoch: 5 step: 746, loss is 0.04038389399647713\n",
      "epoch: 5 step: 747, loss is 0.018800094723701477\n",
      "epoch: 5 step: 748, loss is 0.0758465826511383\n",
      "epoch: 5 step: 749, loss is 0.026692429557442665\n",
      "epoch: 5 step: 750, loss is 0.006067053880542517\n",
      "epoch: 5 step: 751, loss is 0.012955664657056332\n",
      "epoch: 5 step: 752, loss is 0.018092190846800804\n",
      "epoch: 5 step: 753, loss is 0.01237594522535801\n",
      "epoch: 5 step: 754, loss is 0.0039050611667335033\n",
      "epoch: 5 step: 755, loss is 0.11489354074001312\n",
      "epoch: 5 step: 756, loss is 0.05949448421597481\n",
      "epoch: 5 step: 757, loss is 0.006036815699189901\n",
      "epoch: 5 step: 758, loss is 0.051958467811346054\n",
      "epoch: 5 step: 759, loss is 0.00610370235517621\n",
      "epoch: 5 step: 760, loss is 0.0011348319239914417\n",
      "epoch: 5 step: 761, loss is 0.006067594513297081\n",
      "epoch: 5 step: 762, loss is 0.004730224143713713\n",
      "epoch: 5 step: 763, loss is 0.03165200725197792\n",
      "epoch: 5 step: 764, loss is 0.008197548799216747\n",
      "epoch: 5 step: 765, loss is 0.029755204916000366\n",
      "epoch: 5 step: 766, loss is 0.0009365727310068905\n",
      "epoch: 5 step: 767, loss is 0.0032768489327281713\n",
      "epoch: 5 step: 768, loss is 0.0003761553962249309\n",
      "epoch: 5 step: 769, loss is 0.006569826975464821\n",
      "epoch: 5 step: 770, loss is 0.0046416050754487514\n",
      "epoch: 5 step: 771, loss is 0.0034998359624296427\n",
      "epoch: 5 step: 772, loss is 0.1598120629787445\n",
      "epoch: 5 step: 773, loss is 0.006561560556292534\n",
      "epoch: 5 step: 774, loss is 0.0014240015298128128\n",
      "epoch: 5 step: 775, loss is 0.020518293604254723\n",
      "epoch: 5 step: 776, loss is 0.012186281383037567\n",
      "epoch: 5 step: 777, loss is 0.03526925668120384\n",
      "epoch: 5 step: 778, loss is 0.002346249297261238\n",
      "epoch: 5 step: 779, loss is 0.008161338046193123\n",
      "epoch: 5 step: 780, loss is 0.019395267590880394\n",
      "epoch: 5 step: 781, loss is 0.12489966303110123\n",
      "epoch: 5 step: 782, loss is 0.04141661524772644\n",
      "epoch: 5 step: 783, loss is 0.010110858827829361\n",
      "epoch: 5 step: 784, loss is 0.05158599093556404\n",
      "epoch: 5 step: 785, loss is 0.059262532740831375\n",
      "epoch: 5 step: 786, loss is 0.0009867873741313815\n",
      "epoch: 5 step: 787, loss is 0.004694948438555002\n",
      "epoch: 5 step: 788, loss is 0.031671833246946335\n",
      "epoch: 5 step: 789, loss is 0.0002566039329394698\n",
      "epoch: 5 step: 790, loss is 0.008289765566587448\n",
      "epoch: 5 step: 791, loss is 0.00017645426851231605\n",
      "epoch: 5 step: 792, loss is 0.2659318745136261\n",
      "epoch: 5 step: 793, loss is 0.004595065955072641\n",
      "epoch: 5 step: 794, loss is 0.000497695873491466\n",
      "epoch: 5 step: 795, loss is 0.09083081036806107\n",
      "epoch: 5 step: 796, loss is 0.00018287115381099284\n",
      "epoch: 5 step: 797, loss is 0.0015132997650653124\n",
      "epoch: 5 step: 798, loss is 0.011930865235626698\n",
      "epoch: 5 step: 799, loss is 0.001991249155253172\n",
      "epoch: 5 step: 800, loss is 0.009903176687657833\n",
      "epoch: 5 step: 801, loss is 0.00343449623323977\n",
      "epoch: 5 step: 802, loss is 0.006578511092811823\n",
      "epoch: 5 step: 803, loss is 0.0006546580698341131\n",
      "epoch: 5 step: 804, loss is 0.0006537165609188378\n",
      "epoch: 5 step: 805, loss is 0.004183970391750336\n",
      "epoch: 5 step: 806, loss is 0.00014504771388601512\n",
      "epoch: 5 step: 807, loss is 0.010432888753712177\n",
      "epoch: 5 step: 808, loss is 0.0017556373495608568\n",
      "epoch: 5 step: 809, loss is 0.0008113316143862903\n",
      "epoch: 5 step: 810, loss is 0.001250964356586337\n",
      "epoch: 5 step: 811, loss is 0.006677594967186451\n",
      "epoch: 5 step: 812, loss is 0.0015101921744644642\n",
      "epoch: 5 step: 813, loss is 0.01384123507887125\n",
      "epoch: 5 step: 814, loss is 0.0024827870074659586\n",
      "epoch: 5 step: 815, loss is 0.0010700104758143425\n",
      "epoch: 5 step: 816, loss is 0.0036894353106617928\n",
      "epoch: 5 step: 817, loss is 0.00040805203025229275\n",
      "epoch: 5 step: 818, loss is 0.05159555748105049\n",
      "epoch: 5 step: 819, loss is 0.04843828082084656\n",
      "epoch: 5 step: 820, loss is 0.022499343380331993\n",
      "epoch: 5 step: 821, loss is 0.23992404341697693\n",
      "epoch: 5 step: 822, loss is 0.09586701542139053\n",
      "epoch: 5 step: 823, loss is 0.0031813853420317173\n",
      "epoch: 5 step: 824, loss is 0.006508042104542255\n",
      "epoch: 5 step: 825, loss is 0.0009755597566254437\n",
      "epoch: 5 step: 826, loss is 0.007776465266942978\n",
      "epoch: 5 step: 827, loss is 0.0042717051692306995\n",
      "epoch: 5 step: 828, loss is 0.0068138292990624905\n",
      "epoch: 5 step: 829, loss is 0.15323683619499207\n",
      "epoch: 5 step: 830, loss is 0.007673101499676704\n",
      "epoch: 5 step: 831, loss is 0.000393160356907174\n",
      "epoch: 5 step: 832, loss is 0.0046645416878163815\n",
      "epoch: 5 step: 833, loss is 0.003716046456247568\n",
      "epoch: 5 step: 834, loss is 0.0028157555498182774\n",
      "epoch: 5 step: 835, loss is 0.0010581760434433818\n",
      "epoch: 5 step: 836, loss is 0.007792909163981676\n",
      "epoch: 5 step: 837, loss is 0.010343686677515507\n",
      "epoch: 5 step: 838, loss is 0.008632049895823002\n",
      "epoch: 5 step: 839, loss is 0.028061097487807274\n",
      "epoch: 5 step: 840, loss is 0.0024547793436795473\n",
      "epoch: 5 step: 841, loss is 0.0035087617579847574\n",
      "epoch: 5 step: 842, loss is 0.02949610911309719\n",
      "epoch: 5 step: 843, loss is 0.14983436465263367\n",
      "epoch: 5 step: 844, loss is 0.03328463062644005\n",
      "epoch: 5 step: 845, loss is 0.10721085965633392\n",
      "epoch: 5 step: 846, loss is 0.01244112104177475\n",
      "epoch: 5 step: 847, loss is 0.011799964122474194\n",
      "epoch: 5 step: 848, loss is 0.0017746194498613477\n",
      "epoch: 5 step: 849, loss is 0.0004212218918837607\n",
      "epoch: 5 step: 850, loss is 0.003732325043529272\n",
      "epoch: 5 step: 851, loss is 0.0005107104661874473\n",
      "epoch: 5 step: 852, loss is 0.020570402964949608\n",
      "epoch: 5 step: 853, loss is 0.014303362928330898\n",
      "epoch: 5 step: 854, loss is 0.03076193295419216\n",
      "epoch: 5 step: 855, loss is 0.011002191342413425\n",
      "epoch: 5 step: 856, loss is 0.03658106178045273\n",
      "epoch: 5 step: 857, loss is 0.01134143490344286\n",
      "epoch: 5 step: 858, loss is 0.06285250186920166\n",
      "epoch: 5 step: 859, loss is 0.0026219829451292753\n",
      "epoch: 5 step: 860, loss is 0.007236359640955925\n",
      "epoch: 5 step: 861, loss is 0.03958233818411827\n",
      "epoch: 5 step: 862, loss is 0.002678052755072713\n",
      "epoch: 5 step: 863, loss is 0.011305775493383408\n",
      "epoch: 5 step: 864, loss is 0.00018468040798325092\n",
      "epoch: 5 step: 865, loss is 0.0025498303584754467\n",
      "epoch: 5 step: 866, loss is 0.1701420396566391\n",
      "epoch: 5 step: 867, loss is 0.25192582607269287\n",
      "epoch: 5 step: 868, loss is 0.001293300068937242\n",
      "epoch: 5 step: 869, loss is 0.021449562162160873\n",
      "epoch: 5 step: 870, loss is 0.0005406312993727624\n",
      "epoch: 5 step: 871, loss is 0.004241059999912977\n",
      "epoch: 5 step: 872, loss is 0.01814042031764984\n",
      "epoch: 5 step: 873, loss is 0.0002926329616457224\n",
      "epoch: 5 step: 874, loss is 8.612003148300573e-05\n",
      "epoch: 5 step: 875, loss is 0.01119283027946949\n",
      "epoch: 5 step: 876, loss is 0.12575794756412506\n",
      "epoch: 5 step: 877, loss is 0.1875499188899994\n",
      "epoch: 5 step: 878, loss is 0.0005924643483012915\n",
      "epoch: 5 step: 879, loss is 0.0077354987151920795\n",
      "epoch: 5 step: 880, loss is 0.009475099854171276\n",
      "epoch: 5 step: 881, loss is 0.007960373535752296\n",
      "epoch: 5 step: 882, loss is 0.013694394379854202\n",
      "epoch: 5 step: 883, loss is 0.004781235940754414\n",
      "epoch: 5 step: 884, loss is 0.01108272559940815\n",
      "epoch: 5 step: 885, loss is 0.0001440919004380703\n",
      "epoch: 5 step: 886, loss is 0.023641429841518402\n",
      "epoch: 5 step: 887, loss is 0.1535094529390335\n",
      "epoch: 5 step: 888, loss is 0.00197069370187819\n",
      "epoch: 5 step: 889, loss is 0.04036417976021767\n",
      "epoch: 5 step: 890, loss is 0.09676964581012726\n",
      "epoch: 5 step: 891, loss is 0.010714048519730568\n",
      "epoch: 5 step: 892, loss is 0.11986592411994934\n",
      "epoch: 5 step: 893, loss is 0.025533059611916542\n",
      "epoch: 5 step: 894, loss is 0.0005626926431432366\n",
      "epoch: 5 step: 895, loss is 0.004047039896249771\n",
      "epoch: 5 step: 896, loss is 0.0004340072046034038\n",
      "epoch: 5 step: 897, loss is 0.013666967861354351\n",
      "epoch: 5 step: 898, loss is 0.0009961130563169718\n",
      "epoch: 5 step: 899, loss is 0.015725111588835716\n",
      "epoch: 5 step: 900, loss is 0.0005928836180828512\n",
      "epoch: 5 step: 901, loss is 0.0075936163775622845\n",
      "epoch: 5 step: 902, loss is 0.03676140680909157\n",
      "epoch: 5 step: 903, loss is 0.000313051714329049\n",
      "epoch: 5 step: 904, loss is 0.029415784403681755\n",
      "epoch: 5 step: 905, loss is 0.08877906203269958\n",
      "epoch: 5 step: 906, loss is 0.11230557411909103\n",
      "epoch: 5 step: 907, loss is 0.02606441266834736\n",
      "epoch: 5 step: 908, loss is 0.008466093800961971\n",
      "epoch: 5 step: 909, loss is 0.017600758001208305\n",
      "epoch: 5 step: 910, loss is 0.005638433620333672\n",
      "epoch: 5 step: 911, loss is 0.12345686554908752\n",
      "epoch: 5 step: 912, loss is 0.06849126517772675\n",
      "epoch: 5 step: 913, loss is 0.14770321547985077\n",
      "epoch: 5 step: 914, loss is 0.12298529595136642\n",
      "epoch: 5 step: 915, loss is 0.003122241236269474\n",
      "epoch: 5 step: 916, loss is 0.0009322067489847541\n",
      "epoch: 5 step: 917, loss is 0.005314290057867765\n",
      "epoch: 5 step: 918, loss is 0.011524274945259094\n",
      "epoch: 5 step: 919, loss is 0.12073102593421936\n",
      "epoch: 5 step: 920, loss is 0.012284358032047749\n",
      "epoch: 5 step: 921, loss is 0.006675987970083952\n",
      "epoch: 5 step: 922, loss is 0.046316538006067276\n",
      "epoch: 5 step: 923, loss is 0.029991475865244865\n",
      "epoch: 5 step: 924, loss is 0.013645760715007782\n",
      "epoch: 5 step: 925, loss is 0.0007828787202015519\n",
      "epoch: 5 step: 926, loss is 0.02096676453948021\n",
      "epoch: 5 step: 927, loss is 0.003188442438840866\n",
      "epoch: 5 step: 928, loss is 0.0010302322916686535\n",
      "epoch: 5 step: 929, loss is 0.0011795664904639125\n",
      "epoch: 5 step: 930, loss is 0.002605814952403307\n",
      "epoch: 5 step: 931, loss is 0.006118660792708397\n",
      "epoch: 5 step: 932, loss is 0.0018619876354932785\n",
      "epoch: 5 step: 933, loss is 0.002146792830899358\n",
      "epoch: 5 step: 934, loss is 0.138202965259552\n",
      "epoch: 5 step: 935, loss is 9.093221160583198e-05\n",
      "epoch: 5 step: 936, loss is 0.00032357301097363234\n",
      "epoch: 5 step: 937, loss is 0.01775563322007656\n",
      "epoch: 5 step: 938, loss is 0.0020689121447503567\n",
      "epoch: 5 step: 939, loss is 0.029269058257341385\n",
      "epoch: 5 step: 940, loss is 0.0002970274072140455\n",
      "epoch: 5 step: 941, loss is 0.003819481935352087\n",
      "epoch: 5 step: 942, loss is 0.0002848588628694415\n",
      "epoch: 5 step: 943, loss is 0.01693887822329998\n",
      "epoch: 5 step: 944, loss is 0.001774654840119183\n",
      "epoch: 5 step: 945, loss is 0.007838195189833641\n",
      "epoch: 5 step: 946, loss is 0.00018347532022744417\n",
      "epoch: 5 step: 947, loss is 0.0035983372945338488\n",
      "epoch: 5 step: 948, loss is 0.015937263146042824\n",
      "epoch: 5 step: 949, loss is 0.0012134487042203546\n",
      "epoch: 5 step: 950, loss is 0.020124107599258423\n",
      "epoch: 5 step: 951, loss is 0.020764853805303574\n",
      "epoch: 5 step: 952, loss is 0.008110434748232365\n",
      "epoch: 5 step: 953, loss is 0.0005784049280919135\n",
      "epoch: 5 step: 954, loss is 0.0007359919254668057\n",
      "epoch: 5 step: 955, loss is 0.0019444490317255259\n",
      "epoch: 5 step: 956, loss is 0.027088187634944916\n",
      "epoch: 5 step: 957, loss is 0.0012180001940578222\n",
      "epoch: 5 step: 958, loss is 0.0013782531023025513\n",
      "epoch: 5 step: 959, loss is 0.005775679834187031\n",
      "epoch: 5 step: 960, loss is 6.917040445841849e-05\n",
      "epoch: 5 step: 961, loss is 0.0014521597186103463\n",
      "epoch: 5 step: 962, loss is 0.0014297501184046268\n",
      "epoch: 5 step: 963, loss is 0.010101634077727795\n",
      "epoch: 5 step: 964, loss is 0.016622481867671013\n",
      "epoch: 5 step: 965, loss is 0.0012930292868986726\n",
      "epoch: 5 step: 966, loss is 0.04498951509594917\n",
      "epoch: 5 step: 967, loss is 0.03212818130850792\n",
      "epoch: 5 step: 968, loss is 0.0016609353478997946\n",
      "epoch: 5 step: 969, loss is 0.00023307230731006712\n",
      "epoch: 5 step: 970, loss is 0.016907431185245514\n",
      "epoch: 5 step: 971, loss is 0.0006433814996853471\n",
      "epoch: 5 step: 972, loss is 0.14579682052135468\n",
      "epoch: 5 step: 973, loss is 0.054962094873189926\n",
      "epoch: 5 step: 974, loss is 0.00024650467094033957\n",
      "epoch: 5 step: 975, loss is 0.02450970932841301\n",
      "epoch: 5 step: 976, loss is 0.010569652542471886\n",
      "epoch: 5 step: 977, loss is 0.00940132886171341\n",
      "epoch: 5 step: 978, loss is 0.0006654405733570457\n",
      "epoch: 5 step: 979, loss is 0.13100166618824005\n",
      "epoch: 5 step: 980, loss is 0.00018471611838322133\n",
      "epoch: 5 step: 981, loss is 0.0009576745214872062\n",
      "epoch: 5 step: 982, loss is 0.0018988727824762464\n",
      "epoch: 5 step: 983, loss is 0.002194981789216399\n",
      "epoch: 5 step: 984, loss is 0.14867495000362396\n",
      "epoch: 5 step: 985, loss is 0.028147459030151367\n",
      "epoch: 5 step: 986, loss is 0.003652088111266494\n",
      "epoch: 5 step: 987, loss is 0.0007772237877361476\n",
      "epoch: 5 step: 988, loss is 0.0003548649838194251\n",
      "epoch: 5 step: 989, loss is 0.0001737144630169496\n",
      "epoch: 5 step: 990, loss is 0.002285025781020522\n",
      "epoch: 5 step: 991, loss is 0.21723081171512604\n",
      "epoch: 5 step: 992, loss is 0.004078706726431847\n",
      "epoch: 5 step: 993, loss is 0.00717795267701149\n",
      "epoch: 5 step: 994, loss is 0.00012823725410271436\n",
      "epoch: 5 step: 995, loss is 0.029127225279808044\n",
      "epoch: 5 step: 996, loss is 0.001758668920956552\n",
      "epoch: 5 step: 997, loss is 0.06136949360370636\n",
      "epoch: 5 step: 998, loss is 0.00041811488335952163\n",
      "epoch: 5 step: 999, loss is 0.0044981325045228004\n",
      "epoch: 5 step: 1000, loss is 0.003354630433022976\n",
      "epoch: 5 step: 1001, loss is 0.037761520594358444\n",
      "epoch: 5 step: 1002, loss is 4.8394722398370504e-05\n",
      "epoch: 5 step: 1003, loss is 0.012580644339323044\n",
      "epoch: 5 step: 1004, loss is 0.19221946597099304\n",
      "epoch: 5 step: 1005, loss is 0.05222954601049423\n",
      "epoch: 5 step: 1006, loss is 0.0025343631859868765\n",
      "epoch: 5 step: 1007, loss is 0.11868029832839966\n",
      "epoch: 5 step: 1008, loss is 0.0019246938172727823\n",
      "epoch: 5 step: 1009, loss is 0.0005460828542709351\n",
      "epoch: 5 step: 1010, loss is 0.08791481703519821\n",
      "epoch: 5 step: 1011, loss is 0.0007624533027410507\n",
      "epoch: 5 step: 1012, loss is 0.011069568805396557\n",
      "epoch: 5 step: 1013, loss is 0.0439130924642086\n",
      "epoch: 5 step: 1014, loss is 0.000439030205598101\n",
      "epoch: 5 step: 1015, loss is 0.0013613465707749128\n",
      "epoch: 5 step: 1016, loss is 0.013730029575526714\n",
      "epoch: 5 step: 1017, loss is 0.003705363254994154\n",
      "epoch: 5 step: 1018, loss is 0.009657981805503368\n",
      "epoch: 5 step: 1019, loss is 0.0047150421887636185\n",
      "epoch: 5 step: 1020, loss is 0.09255944192409515\n",
      "epoch: 5 step: 1021, loss is 0.09314991533756256\n",
      "epoch: 5 step: 1022, loss is 0.0022520131897181273\n",
      "epoch: 5 step: 1023, loss is 0.0029248283244669437\n",
      "epoch: 5 step: 1024, loss is 0.0015085999621078372\n",
      "epoch: 5 step: 1025, loss is 0.0015944784972816706\n",
      "epoch: 5 step: 1026, loss is 0.016554327681660652\n",
      "epoch: 5 step: 1027, loss is 0.003766155568882823\n",
      "epoch: 5 step: 1028, loss is 0.0030517415143549442\n",
      "epoch: 5 step: 1029, loss is 0.0024205404333770275\n",
      "epoch: 5 step: 1030, loss is 0.02596435323357582\n",
      "epoch: 5 step: 1031, loss is 0.005754172336310148\n",
      "epoch: 5 step: 1032, loss is 0.000542940862942487\n",
      "epoch: 5 step: 1033, loss is 0.004221356939524412\n",
      "epoch: 5 step: 1034, loss is 0.3565928339958191\n",
      "epoch: 5 step: 1035, loss is 0.017066525295376778\n",
      "epoch: 5 step: 1036, loss is 0.0006812005303800106\n",
      "epoch: 5 step: 1037, loss is 0.01432255282998085\n",
      "epoch: 5 step: 1038, loss is 0.013870630413293839\n",
      "epoch: 5 step: 1039, loss is 0.022776270285248756\n",
      "epoch: 5 step: 1040, loss is 0.026231391355395317\n",
      "epoch: 5 step: 1041, loss is 0.006612770725041628\n",
      "epoch: 5 step: 1042, loss is 0.0007390609825961292\n",
      "epoch: 5 step: 1043, loss is 0.10548757016658783\n",
      "epoch: 5 step: 1044, loss is 0.0006119512254372239\n",
      "epoch: 5 step: 1045, loss is 0.014614841900765896\n",
      "epoch: 5 step: 1046, loss is 0.0056490241549909115\n",
      "epoch: 5 step: 1047, loss is 0.09138526022434235\n",
      "epoch: 5 step: 1048, loss is 0.008592188358306885\n",
      "epoch: 5 step: 1049, loss is 0.0023535313084721565\n",
      "epoch: 5 step: 1050, loss is 0.010351997800171375\n",
      "epoch: 5 step: 1051, loss is 0.05261289328336716\n",
      "epoch: 5 step: 1052, loss is 0.010081907734274864\n",
      "epoch: 5 step: 1053, loss is 0.0004292487574275583\n",
      "epoch: 5 step: 1054, loss is 0.000712999957613647\n",
      "epoch: 5 step: 1055, loss is 0.021484892815351486\n",
      "epoch: 5 step: 1056, loss is 0.001963949529454112\n",
      "epoch: 5 step: 1057, loss is 0.06136324629187584\n",
      "epoch: 5 step: 1058, loss is 0.002671787515282631\n",
      "epoch: 5 step: 1059, loss is 0.0013368516229093075\n",
      "epoch: 5 step: 1060, loss is 0.001289702602662146\n",
      "epoch: 5 step: 1061, loss is 0.0030416466761380434\n",
      "epoch: 5 step: 1062, loss is 0.0032934024930000305\n",
      "epoch: 5 step: 1063, loss is 0.021078823134303093\n",
      "epoch: 5 step: 1064, loss is 0.0010601697722449899\n",
      "epoch: 5 step: 1065, loss is 0.0017682125326246023\n",
      "epoch: 5 step: 1066, loss is 0.002589302370324731\n",
      "epoch: 5 step: 1067, loss is 0.002154286252334714\n",
      "epoch: 5 step: 1068, loss is 0.05949126183986664\n",
      "epoch: 5 step: 1069, loss is 0.0021336337085813284\n",
      "epoch: 5 step: 1070, loss is 0.006468913517892361\n",
      "epoch: 5 step: 1071, loss is 0.002012004842981696\n",
      "epoch: 5 step: 1072, loss is 0.0010453998111188412\n",
      "epoch: 5 step: 1073, loss is 0.021882658824324608\n",
      "epoch: 5 step: 1074, loss is 0.0016475832089781761\n",
      "epoch: 5 step: 1075, loss is 0.0017118114046752453\n",
      "epoch: 5 step: 1076, loss is 0.0035332078114151955\n",
      "epoch: 5 step: 1077, loss is 0.022076737135648727\n",
      "epoch: 5 step: 1078, loss is 0.009074809961020947\n",
      "epoch: 5 step: 1079, loss is 0.002698118332773447\n",
      "epoch: 5 step: 1080, loss is 0.03588707745075226\n",
      "epoch: 5 step: 1081, loss is 0.0007066103862598538\n",
      "epoch: 5 step: 1082, loss is 0.12765952944755554\n",
      "epoch: 5 step: 1083, loss is 0.003680055495351553\n",
      "epoch: 5 step: 1084, loss is 0.005780325271189213\n",
      "epoch: 5 step: 1085, loss is 0.004502153489738703\n",
      "epoch: 5 step: 1086, loss is 0.0140232490375638\n",
      "epoch: 5 step: 1087, loss is 0.0006223336094990373\n",
      "epoch: 5 step: 1088, loss is 0.011620872654020786\n",
      "epoch: 5 step: 1089, loss is 0.018242254853248596\n",
      "epoch: 5 step: 1090, loss is 0.0028951221611350775\n",
      "epoch: 5 step: 1091, loss is 0.013909648172557354\n",
      "epoch: 5 step: 1092, loss is 0.03207162395119667\n",
      "epoch: 5 step: 1093, loss is 0.008960138075053692\n",
      "epoch: 5 step: 1094, loss is 0.053726643323898315\n",
      "epoch: 5 step: 1095, loss is 0.0005108130280859768\n",
      "epoch: 5 step: 1096, loss is 0.14093251526355743\n",
      "epoch: 5 step: 1097, loss is 0.0006442943122237921\n",
      "epoch: 5 step: 1098, loss is 0.0046809036284685135\n",
      "epoch: 5 step: 1099, loss is 0.0030396352522075176\n",
      "epoch: 5 step: 1100, loss is 0.003780780825763941\n",
      "epoch: 5 step: 1101, loss is 0.09102118015289307\n",
      "epoch: 5 step: 1102, loss is 0.0011292932322248816\n",
      "epoch: 5 step: 1103, loss is 6.407859473256394e-05\n",
      "epoch: 5 step: 1104, loss is 0.01606663316488266\n",
      "epoch: 5 step: 1105, loss is 0.11982176452875137\n",
      "epoch: 5 step: 1106, loss is 0.0029737548902630806\n",
      "epoch: 5 step: 1107, loss is 0.018596190959215164\n",
      "epoch: 5 step: 1108, loss is 0.003589903935790062\n",
      "epoch: 5 step: 1109, loss is 0.0002786351542454213\n",
      "epoch: 5 step: 1110, loss is 0.0010326285846531391\n",
      "epoch: 5 step: 1111, loss is 0.00028300407575443387\n",
      "epoch: 5 step: 1112, loss is 0.012580759823322296\n",
      "epoch: 5 step: 1113, loss is 0.020795924589037895\n",
      "epoch: 5 step: 1114, loss is 0.0006005752948112786\n",
      "epoch: 5 step: 1115, loss is 0.0001991303579416126\n",
      "epoch: 5 step: 1116, loss is 0.006048705894500017\n",
      "epoch: 5 step: 1117, loss is 0.0343281514942646\n",
      "epoch: 5 step: 1118, loss is 0.0034771403297781944\n",
      "epoch: 5 step: 1119, loss is 0.26115965843200684\n",
      "epoch: 5 step: 1120, loss is 0.02532896399497986\n",
      "epoch: 5 step: 1121, loss is 0.007083569187670946\n",
      "epoch: 5 step: 1122, loss is 0.00037533967406488955\n",
      "epoch: 5 step: 1123, loss is 0.014242534525692463\n",
      "epoch: 5 step: 1124, loss is 0.006200141739100218\n",
      "epoch: 5 step: 1125, loss is 0.07554195821285248\n",
      "epoch: 5 step: 1126, loss is 0.00300537864677608\n",
      "epoch: 5 step: 1127, loss is 0.0758657455444336\n",
      "epoch: 5 step: 1128, loss is 0.005719442386180162\n",
      "epoch: 5 step: 1129, loss is 0.0005777889164164662\n",
      "epoch: 5 step: 1130, loss is 0.0031555783934891224\n",
      "epoch: 5 step: 1131, loss is 0.04797825217247009\n",
      "epoch: 5 step: 1132, loss is 0.03410441428422928\n",
      "epoch: 5 step: 1133, loss is 0.004621283151209354\n",
      "epoch: 5 step: 1134, loss is 0.008750757202506065\n",
      "epoch: 5 step: 1135, loss is 0.12411930412054062\n",
      "epoch: 5 step: 1136, loss is 0.0037378817796707153\n",
      "epoch: 5 step: 1137, loss is 0.007942781783640385\n",
      "epoch: 5 step: 1138, loss is 0.0015210529090836644\n",
      "epoch: 5 step: 1139, loss is 0.007857325486838818\n",
      "epoch: 5 step: 1140, loss is 0.0019010198302567005\n",
      "epoch: 5 step: 1141, loss is 0.012900755740702152\n",
      "epoch: 5 step: 1142, loss is 0.0028435448184609413\n",
      "epoch: 5 step: 1143, loss is 0.0028768787160515785\n",
      "epoch: 5 step: 1144, loss is 0.01651117019355297\n",
      "epoch: 5 step: 1145, loss is 0.012686751782894135\n",
      "epoch: 5 step: 1146, loss is 0.13334132730960846\n",
      "epoch: 5 step: 1147, loss is 0.0020632764790207148\n",
      "epoch: 5 step: 1148, loss is 0.07879530638456345\n",
      "epoch: 5 step: 1149, loss is 0.023908214643597603\n",
      "epoch: 5 step: 1150, loss is 0.0550423189997673\n",
      "epoch: 5 step: 1151, loss is 0.0012692626332864165\n",
      "epoch: 5 step: 1152, loss is 0.008090278133749962\n",
      "epoch: 5 step: 1153, loss is 0.053927820175886154\n",
      "epoch: 5 step: 1154, loss is 0.0029490571469068527\n",
      "epoch: 5 step: 1155, loss is 0.18246127665042877\n",
      "epoch: 5 step: 1156, loss is 0.06076841428875923\n",
      "epoch: 5 step: 1157, loss is 0.0002245594805572182\n",
      "epoch: 5 step: 1158, loss is 0.07319764792919159\n",
      "epoch: 5 step: 1159, loss is 0.0030934337992221117\n",
      "epoch: 5 step: 1160, loss is 0.0024444223381578922\n",
      "epoch: 5 step: 1161, loss is 8.252720726886764e-05\n",
      "epoch: 5 step: 1162, loss is 0.010024859569966793\n",
      "epoch: 5 step: 1163, loss is 0.005105878226459026\n",
      "epoch: 5 step: 1164, loss is 0.0021150982938706875\n",
      "epoch: 5 step: 1165, loss is 0.026199370622634888\n",
      "epoch: 5 step: 1166, loss is 0.01888478733599186\n",
      "epoch: 5 step: 1167, loss is 0.03563552349805832\n",
      "epoch: 5 step: 1168, loss is 0.0009596923482604325\n",
      "epoch: 5 step: 1169, loss is 0.007397869136184454\n",
      "epoch: 5 step: 1170, loss is 0.021950071677565575\n",
      "epoch: 5 step: 1171, loss is 0.00025072175776585937\n",
      "epoch: 5 step: 1172, loss is 0.011728385463356972\n",
      "epoch: 5 step: 1173, loss is 0.01615261100232601\n",
      "epoch: 5 step: 1174, loss is 0.006426784209907055\n",
      "epoch: 5 step: 1175, loss is 0.0008562077418901026\n",
      "epoch: 5 step: 1176, loss is 0.04101286083459854\n",
      "epoch: 5 step: 1177, loss is 0.006062001921236515\n",
      "epoch: 5 step: 1178, loss is 0.00043917831499129534\n",
      "epoch: 5 step: 1179, loss is 0.0004675451200455427\n",
      "epoch: 5 step: 1180, loss is 0.006553647108376026\n",
      "epoch: 5 step: 1181, loss is 0.0006375834345817566\n",
      "epoch: 5 step: 1182, loss is 0.007009752560406923\n",
      "epoch: 5 step: 1183, loss is 0.07801896333694458\n",
      "epoch: 5 step: 1184, loss is 0.0002682922058738768\n",
      "epoch: 5 step: 1185, loss is 0.004271514713764191\n",
      "epoch: 5 step: 1186, loss is 0.011487098410725594\n",
      "epoch: 5 step: 1187, loss is 0.0011445090640336275\n",
      "epoch: 5 step: 1188, loss is 0.06642885506153107\n",
      "epoch: 5 step: 1189, loss is 0.04172349348664284\n",
      "epoch: 5 step: 1190, loss is 0.19740982353687286\n",
      "epoch: 5 step: 1191, loss is 0.017941789701581\n",
      "epoch: 5 step: 1192, loss is 0.000713485642336309\n",
      "epoch: 5 step: 1193, loss is 0.15298296511173248\n",
      "epoch: 5 step: 1194, loss is 0.005334928631782532\n",
      "epoch: 5 step: 1195, loss is 0.0015876756515353918\n",
      "epoch: 5 step: 1196, loss is 0.0017910784808918834\n",
      "epoch: 5 step: 1197, loss is 0.09269813448190689\n",
      "epoch: 5 step: 1198, loss is 0.001838055206462741\n",
      "epoch: 5 step: 1199, loss is 0.002595745027065277\n",
      "epoch: 5 step: 1200, loss is 0.003955272026360035\n",
      "epoch: 5 step: 1201, loss is 0.017220260575413704\n",
      "epoch: 5 step: 1202, loss is 0.0055349236354231834\n",
      "epoch: 5 step: 1203, loss is 0.019053857773542404\n",
      "epoch: 5 step: 1204, loss is 0.02851870283484459\n",
      "epoch: 5 step: 1205, loss is 0.0003432700177654624\n",
      "epoch: 5 step: 1206, loss is 0.05828944966197014\n",
      "epoch: 5 step: 1207, loss is 0.005681162234395742\n",
      "epoch: 5 step: 1208, loss is 0.0030875294469296932\n",
      "epoch: 5 step: 1209, loss is 0.00025167744024656713\n",
      "epoch: 5 step: 1210, loss is 0.006466474384069443\n",
      "epoch: 5 step: 1211, loss is 0.004751795437186956\n",
      "epoch: 5 step: 1212, loss is 0.0053334846161305904\n",
      "epoch: 5 step: 1213, loss is 0.016552725806832314\n",
      "epoch: 5 step: 1214, loss is 0.01680615171790123\n",
      "epoch: 5 step: 1215, loss is 0.10310377925634384\n",
      "epoch: 5 step: 1216, loss is 0.00023640361905563623\n",
      "epoch: 5 step: 1217, loss is 0.0014546766178682446\n",
      "epoch: 5 step: 1218, loss is 0.019296441227197647\n",
      "epoch: 5 step: 1219, loss is 0.007598164491355419\n",
      "epoch: 5 step: 1220, loss is 0.017470622435212135\n",
      "epoch: 5 step: 1221, loss is 0.01590798795223236\n",
      "epoch: 5 step: 1222, loss is 0.0020377198234200478\n",
      "epoch: 5 step: 1223, loss is 0.2242565155029297\n",
      "epoch: 5 step: 1224, loss is 0.024681709706783295\n",
      "epoch: 5 step: 1225, loss is 0.005781147163361311\n",
      "epoch: 5 step: 1226, loss is 0.14312006533145905\n",
      "epoch: 5 step: 1227, loss is 0.004375286400318146\n",
      "epoch: 5 step: 1228, loss is 0.030679743736982346\n",
      "epoch: 5 step: 1229, loss is 0.0011115565430372953\n",
      "epoch: 5 step: 1230, loss is 0.00585361709818244\n",
      "epoch: 5 step: 1231, loss is 0.01757308840751648\n",
      "epoch: 5 step: 1232, loss is 0.016934270039200783\n",
      "epoch: 5 step: 1233, loss is 0.006682919804006815\n",
      "epoch: 5 step: 1234, loss is 0.018629347905516624\n",
      "epoch: 5 step: 1235, loss is 0.0009889982175081968\n",
      "epoch: 5 step: 1236, loss is 0.00037405072362162173\n",
      "epoch: 5 step: 1237, loss is 0.0007367137586697936\n",
      "epoch: 5 step: 1238, loss is 0.0002336126344744116\n",
      "epoch: 5 step: 1239, loss is 0.00029694268596358597\n",
      "epoch: 5 step: 1240, loss is 0.035369277000427246\n",
      "epoch: 5 step: 1241, loss is 0.0011086531449109316\n",
      "epoch: 5 step: 1242, loss is 0.006351930554956198\n",
      "epoch: 5 step: 1243, loss is 0.006650072988122702\n",
      "epoch: 5 step: 1244, loss is 0.07599837332963943\n",
      "epoch: 5 step: 1245, loss is 0.018496651202440262\n",
      "epoch: 5 step: 1246, loss is 0.007292707916349173\n",
      "epoch: 5 step: 1247, loss is 0.0057981060817837715\n",
      "epoch: 5 step: 1248, loss is 0.012330923229455948\n",
      "epoch: 5 step: 1249, loss is 0.0018715402111411095\n",
      "epoch: 5 step: 1250, loss is 0.009741406887769699\n",
      "epoch: 5 step: 1251, loss is 0.00036431499756872654\n",
      "epoch: 5 step: 1252, loss is 0.0006266770651564002\n",
      "epoch: 5 step: 1253, loss is 0.00028049713000655174\n",
      "epoch: 5 step: 1254, loss is 0.04241243749856949\n",
      "epoch: 5 step: 1255, loss is 0.0002248457312816754\n",
      "epoch: 5 step: 1256, loss is 0.0018977572908625007\n",
      "epoch: 5 step: 1257, loss is 0.011987023986876011\n",
      "epoch: 5 step: 1258, loss is 0.0022956058382987976\n",
      "epoch: 5 step: 1259, loss is 0.003252616384997964\n",
      "epoch: 5 step: 1260, loss is 0.0027077319100499153\n",
      "epoch: 5 step: 1261, loss is 0.0005108256009407341\n",
      "epoch: 5 step: 1262, loss is 0.02453724667429924\n",
      "epoch: 5 step: 1263, loss is 0.011203822679817677\n",
      "epoch: 5 step: 1264, loss is 0.009541407227516174\n",
      "epoch: 5 step: 1265, loss is 0.007613303139805794\n",
      "epoch: 5 step: 1266, loss is 0.16801360249519348\n",
      "epoch: 5 step: 1267, loss is 0.0873926505446434\n",
      "epoch: 5 step: 1268, loss is 0.08512547612190247\n",
      "epoch: 5 step: 1269, loss is 0.0008071275078691542\n",
      "epoch: 5 step: 1270, loss is 0.0009619923075661063\n",
      "epoch: 5 step: 1271, loss is 0.0006041628657840192\n",
      "epoch: 5 step: 1272, loss is 0.00018157553859055042\n",
      "epoch: 5 step: 1273, loss is 0.027925709262490273\n",
      "epoch: 5 step: 1274, loss is 0.003447840455919504\n",
      "epoch: 5 step: 1275, loss is 0.0005646703066304326\n",
      "epoch: 5 step: 1276, loss is 0.005430759862065315\n",
      "epoch: 5 step: 1277, loss is 0.24793821573257446\n",
      "epoch: 5 step: 1278, loss is 0.0015625252854079008\n",
      "epoch: 5 step: 1279, loss is 0.03956328332424164\n",
      "epoch: 5 step: 1280, loss is 0.02349604293704033\n",
      "epoch: 5 step: 1281, loss is 0.01801810972392559\n",
      "epoch: 5 step: 1282, loss is 0.1399986445903778\n",
      "epoch: 5 step: 1283, loss is 0.00022944057127460837\n",
      "epoch: 5 step: 1284, loss is 0.022801116108894348\n",
      "epoch: 5 step: 1285, loss is 0.00029578505200333893\n",
      "epoch: 5 step: 1286, loss is 0.0017062779515981674\n",
      "epoch: 5 step: 1287, loss is 0.05208093672990799\n",
      "epoch: 5 step: 1288, loss is 0.19928216934204102\n",
      "epoch: 5 step: 1289, loss is 0.0008679757593199611\n",
      "epoch: 5 step: 1290, loss is 0.008368558250367641\n",
      "epoch: 5 step: 1291, loss is 0.006107522174715996\n",
      "epoch: 5 step: 1292, loss is 0.026381544768810272\n",
      "epoch: 5 step: 1293, loss is 0.019115250557661057\n",
      "epoch: 5 step: 1294, loss is 0.042151231318712234\n",
      "epoch: 5 step: 1295, loss is 0.014651501551270485\n",
      "epoch: 5 step: 1296, loss is 0.001592977554537356\n",
      "epoch: 5 step: 1297, loss is 0.0013895300216972828\n",
      "epoch: 5 step: 1298, loss is 0.0033703662920743227\n",
      "epoch: 5 step: 1299, loss is 0.009458180516958237\n",
      "epoch: 5 step: 1300, loss is 0.013667704537510872\n",
      "epoch: 5 step: 1301, loss is 0.09901456534862518\n",
      "epoch: 5 step: 1302, loss is 0.01432392094284296\n",
      "epoch: 5 step: 1303, loss is 0.0009008317720144987\n",
      "epoch: 5 step: 1304, loss is 0.0013685290468856692\n",
      "epoch: 5 step: 1305, loss is 0.0005797836929559708\n",
      "epoch: 5 step: 1306, loss is 0.007913678884506226\n",
      "epoch: 5 step: 1307, loss is 0.11641863733530045\n",
      "epoch: 5 step: 1308, loss is 0.057804930955171585\n",
      "epoch: 5 step: 1309, loss is 0.008510651998221874\n",
      "epoch: 5 step: 1310, loss is 0.004428749904036522\n",
      "epoch: 5 step: 1311, loss is 0.0003535775758791715\n",
      "epoch: 5 step: 1312, loss is 0.037384651601314545\n",
      "epoch: 5 step: 1313, loss is 0.0010744740720838308\n",
      "epoch: 5 step: 1314, loss is 0.01053081825375557\n",
      "epoch: 5 step: 1315, loss is 0.0006893952959217131\n",
      "epoch: 5 step: 1316, loss is 0.006236892193555832\n",
      "epoch: 5 step: 1317, loss is 0.005927088670432568\n",
      "epoch: 5 step: 1318, loss is 0.07209375500679016\n",
      "epoch: 5 step: 1319, loss is 0.0004280218272469938\n",
      "epoch: 5 step: 1320, loss is 0.007500494830310345\n",
      "epoch: 5 step: 1321, loss is 0.08390270918607712\n",
      "epoch: 5 step: 1322, loss is 0.06073608621954918\n",
      "epoch: 5 step: 1323, loss is 0.004285594914108515\n",
      "epoch: 5 step: 1324, loss is 0.0003207488334737718\n",
      "epoch: 5 step: 1325, loss is 0.012853945605456829\n",
      "epoch: 5 step: 1326, loss is 0.05162205919623375\n",
      "epoch: 5 step: 1327, loss is 0.02928313799202442\n",
      "epoch: 5 step: 1328, loss is 0.0642671287059784\n",
      "epoch: 5 step: 1329, loss is 0.00795921590179205\n",
      "epoch: 5 step: 1330, loss is 0.046890027821063995\n",
      "epoch: 5 step: 1331, loss is 0.01919005997478962\n",
      "epoch: 5 step: 1332, loss is 0.2571203410625458\n",
      "epoch: 5 step: 1333, loss is 0.017059069126844406\n",
      "epoch: 5 step: 1334, loss is 0.0002459229144733399\n",
      "epoch: 5 step: 1335, loss is 0.004240097012370825\n",
      "epoch: 5 step: 1336, loss is 0.0037889282684773207\n",
      "epoch: 5 step: 1337, loss is 0.0012467993656173348\n",
      "epoch: 5 step: 1338, loss is 0.0005635844427160919\n",
      "epoch: 5 step: 1339, loss is 0.10018502920866013\n",
      "epoch: 5 step: 1340, loss is 0.09883559495210648\n",
      "epoch: 5 step: 1341, loss is 0.0077560339123010635\n",
      "epoch: 5 step: 1342, loss is 0.0008026702562347054\n",
      "epoch: 5 step: 1343, loss is 0.1396070420742035\n",
      "epoch: 5 step: 1344, loss is 0.0027470141649246216\n",
      "epoch: 5 step: 1345, loss is 0.002552265301346779\n",
      "epoch: 5 step: 1346, loss is 0.029707640409469604\n",
      "epoch: 5 step: 1347, loss is 0.0010219891555607319\n",
      "epoch: 5 step: 1348, loss is 0.00018268724670633674\n",
      "epoch: 5 step: 1349, loss is 0.0010118925711140037\n",
      "epoch: 5 step: 1350, loss is 0.0036489316262304783\n",
      "epoch: 5 step: 1351, loss is 0.08754296600818634\n",
      "epoch: 5 step: 1352, loss is 0.003056985093280673\n",
      "epoch: 5 step: 1353, loss is 0.18113169074058533\n",
      "epoch: 5 step: 1354, loss is 0.0009061450255103409\n",
      "epoch: 5 step: 1355, loss is 0.0003214342868886888\n",
      "epoch: 5 step: 1356, loss is 0.01388690248131752\n",
      "epoch: 5 step: 1357, loss is 0.0021877973340451717\n",
      "epoch: 5 step: 1358, loss is 0.1578315943479538\n",
      "epoch: 5 step: 1359, loss is 0.09608478844165802\n",
      "epoch: 5 step: 1360, loss is 0.014829784631729126\n",
      "epoch: 5 step: 1361, loss is 0.0003302300756331533\n",
      "epoch: 5 step: 1362, loss is 0.003259761491790414\n",
      "epoch: 5 step: 1363, loss is 0.14185242354869843\n",
      "epoch: 5 step: 1364, loss is 0.019367102533578873\n",
      "epoch: 5 step: 1365, loss is 0.038936130702495575\n",
      "epoch: 5 step: 1366, loss is 0.011747163720428944\n",
      "epoch: 5 step: 1367, loss is 0.21205665171146393\n",
      "epoch: 5 step: 1368, loss is 0.029049444943666458\n",
      "epoch: 5 step: 1369, loss is 0.013846133835613728\n",
      "epoch: 5 step: 1370, loss is 0.5120350122451782\n",
      "epoch: 5 step: 1371, loss is 0.0021290977019816637\n",
      "epoch: 5 step: 1372, loss is 0.0027376674115657806\n",
      "epoch: 5 step: 1373, loss is 0.005733783822506666\n",
      "epoch: 5 step: 1374, loss is 0.09651880711317062\n",
      "epoch: 5 step: 1375, loss is 0.07101868838071823\n",
      "epoch: 5 step: 1376, loss is 0.004142109304666519\n",
      "epoch: 5 step: 1377, loss is 0.029324304312467575\n",
      "epoch: 5 step: 1378, loss is 0.005470478441566229\n",
      "epoch: 5 step: 1379, loss is 0.03312230482697487\n",
      "epoch: 5 step: 1380, loss is 0.03360594063997269\n",
      "epoch: 5 step: 1381, loss is 0.0031892976257950068\n",
      "epoch: 5 step: 1382, loss is 0.08792228996753693\n",
      "epoch: 5 step: 1383, loss is 0.03823404386639595\n",
      "epoch: 5 step: 1384, loss is 0.09288942813873291\n",
      "epoch: 5 step: 1385, loss is 0.07399952411651611\n",
      "epoch: 5 step: 1386, loss is 0.008228378370404243\n",
      "epoch: 5 step: 1387, loss is 0.22779063880443573\n",
      "epoch: 5 step: 1388, loss is 0.0038935451302677393\n",
      "epoch: 5 step: 1389, loss is 0.20482376217842102\n",
      "epoch: 5 step: 1390, loss is 0.07383973896503448\n",
      "epoch: 5 step: 1391, loss is 0.0030425679869949818\n",
      "epoch: 5 step: 1392, loss is 0.006417982745915651\n",
      "epoch: 5 step: 1393, loss is 0.009136182256042957\n",
      "epoch: 5 step: 1394, loss is 0.011843383312225342\n",
      "epoch: 5 step: 1395, loss is 0.014395151287317276\n",
      "epoch: 5 step: 1396, loss is 0.0028839039150625467\n",
      "epoch: 5 step: 1397, loss is 0.007904700934886932\n",
      "epoch: 5 step: 1398, loss is 0.002942510647699237\n",
      "epoch: 5 step: 1399, loss is 0.0013410444371402264\n",
      "epoch: 5 step: 1400, loss is 0.06643659621477127\n",
      "epoch: 5 step: 1401, loss is 0.013876027427613735\n",
      "epoch: 5 step: 1402, loss is 0.00031912815757095814\n",
      "epoch: 5 step: 1403, loss is 0.0021172240376472473\n",
      "epoch: 5 step: 1404, loss is 0.0012427506735548377\n",
      "epoch: 5 step: 1405, loss is 0.13984455168247223\n",
      "epoch: 5 step: 1406, loss is 0.0016346583142876625\n",
      "epoch: 5 step: 1407, loss is 0.0017094513168558478\n",
      "epoch: 5 step: 1408, loss is 0.10539291054010391\n",
      "epoch: 5 step: 1409, loss is 0.0037034773267805576\n",
      "epoch: 5 step: 1410, loss is 0.19479355216026306\n",
      "epoch: 5 step: 1411, loss is 0.0008577112457714975\n",
      "epoch: 5 step: 1412, loss is 0.0001010210980894044\n",
      "epoch: 5 step: 1413, loss is 0.0027784216217696667\n",
      "epoch: 5 step: 1414, loss is 0.005957558285444975\n",
      "epoch: 5 step: 1415, loss is 0.09055599570274353\n",
      "epoch: 5 step: 1416, loss is 0.000604413275141269\n",
      "epoch: 5 step: 1417, loss is 0.0015320641687139869\n",
      "epoch: 5 step: 1418, loss is 0.07493676245212555\n",
      "epoch: 5 step: 1419, loss is 0.0026363693177700043\n",
      "epoch: 5 step: 1420, loss is 0.09591342508792877\n",
      "epoch: 5 step: 1421, loss is 0.013674428686499596\n",
      "epoch: 5 step: 1422, loss is 0.012556049041450024\n",
      "epoch: 5 step: 1423, loss is 0.014133308082818985\n",
      "epoch: 5 step: 1424, loss is 0.0023910158779472113\n",
      "epoch: 5 step: 1425, loss is 0.0015910728834569454\n",
      "epoch: 5 step: 1426, loss is 0.004819978028535843\n",
      "epoch: 5 step: 1427, loss is 0.03183211013674736\n",
      "epoch: 5 step: 1428, loss is 0.04750765115022659\n",
      "epoch: 5 step: 1429, loss is 0.010387832298874855\n",
      "epoch: 5 step: 1430, loss is 0.04961010441184044\n",
      "epoch: 5 step: 1431, loss is 0.0023808926343917847\n",
      "epoch: 5 step: 1432, loss is 0.005795626901090145\n",
      "epoch: 5 step: 1433, loss is 0.002433081855997443\n",
      "epoch: 5 step: 1434, loss is 0.010091068223118782\n",
      "epoch: 5 step: 1435, loss is 0.012952664867043495\n",
      "epoch: 5 step: 1436, loss is 0.0010469735134392977\n",
      "epoch: 5 step: 1437, loss is 0.05253812298178673\n",
      "epoch: 5 step: 1438, loss is 0.026509050279855728\n",
      "epoch: 5 step: 1439, loss is 0.1732882559299469\n",
      "epoch: 5 step: 1440, loss is 0.00850330013781786\n",
      "epoch: 5 step: 1441, loss is 0.16715507209300995\n",
      "epoch: 5 step: 1442, loss is 0.184928297996521\n",
      "epoch: 5 step: 1443, loss is 0.0002308648981852457\n",
      "epoch: 5 step: 1444, loss is 0.017094293609261513\n",
      "epoch: 5 step: 1445, loss is 0.00021916181140113622\n",
      "epoch: 5 step: 1446, loss is 0.010687300004065037\n",
      "epoch: 5 step: 1447, loss is 0.012738118879497051\n",
      "epoch: 5 step: 1448, loss is 0.05833896994590759\n",
      "epoch: 5 step: 1449, loss is 0.007692492567002773\n",
      "epoch: 5 step: 1450, loss is 0.1315256804227829\n",
      "epoch: 5 step: 1451, loss is 0.003117083804681897\n",
      "epoch: 5 step: 1452, loss is 0.03159385919570923\n",
      "epoch: 5 step: 1453, loss is 0.00015650098794139922\n",
      "epoch: 5 step: 1454, loss is 0.0015869896160438657\n",
      "epoch: 5 step: 1455, loss is 0.013125957921147346\n",
      "epoch: 5 step: 1456, loss is 0.05413569509983063\n",
      "epoch: 5 step: 1457, loss is 0.0015364161226898432\n",
      "epoch: 5 step: 1458, loss is 0.0059287650510668755\n",
      "epoch: 5 step: 1459, loss is 0.12754052877426147\n",
      "epoch: 5 step: 1460, loss is 0.0010485447710379958\n",
      "epoch: 5 step: 1461, loss is 0.00033896780223585665\n",
      "epoch: 5 step: 1462, loss is 0.002422827761620283\n",
      "epoch: 5 step: 1463, loss is 0.005559114273637533\n",
      "epoch: 5 step: 1464, loss is 0.003688299097120762\n",
      "epoch: 5 step: 1465, loss is 0.034103840589523315\n",
      "epoch: 5 step: 1466, loss is 0.006620017811655998\n",
      "epoch: 5 step: 1467, loss is 0.0038430122658610344\n",
      "epoch: 5 step: 1468, loss is 0.040052443742752075\n",
      "epoch: 5 step: 1469, loss is 0.0017544302390888333\n",
      "epoch: 5 step: 1470, loss is 0.06704786419868469\n",
      "epoch: 5 step: 1471, loss is 0.0028757555410265923\n",
      "epoch: 5 step: 1472, loss is 0.055478427559137344\n",
      "epoch: 5 step: 1473, loss is 0.02245471253991127\n",
      "epoch: 5 step: 1474, loss is 0.007227130234241486\n",
      "epoch: 5 step: 1475, loss is 0.0035933346953243017\n",
      "epoch: 5 step: 1476, loss is 0.10438266396522522\n",
      "epoch: 5 step: 1477, loss is 0.03681938722729683\n",
      "epoch: 5 step: 1478, loss is 0.0031983996741473675\n",
      "epoch: 5 step: 1479, loss is 0.10855671763420105\n",
      "epoch: 5 step: 1480, loss is 0.23074854910373688\n",
      "epoch: 5 step: 1481, loss is 0.0005645804922096431\n",
      "epoch: 5 step: 1482, loss is 0.0033837587106972933\n",
      "epoch: 5 step: 1483, loss is 0.0008415105985477567\n",
      "epoch: 5 step: 1484, loss is 0.0006229457794688642\n",
      "epoch: 5 step: 1485, loss is 0.0003242340753786266\n",
      "epoch: 5 step: 1486, loss is 0.0021957363933324814\n",
      "epoch: 5 step: 1487, loss is 0.024226199835538864\n",
      "epoch: 5 step: 1488, loss is 0.004963810555636883\n",
      "epoch: 5 step: 1489, loss is 0.3467314541339874\n",
      "epoch: 5 step: 1490, loss is 0.0003264416882302612\n",
      "epoch: 5 step: 1491, loss is 0.01586814969778061\n",
      "epoch: 5 step: 1492, loss is 0.09286443889141083\n",
      "epoch: 5 step: 1493, loss is 0.002733921865001321\n",
      "epoch: 5 step: 1494, loss is 0.0009289653971791267\n",
      "epoch: 5 step: 1495, loss is 0.0011365070240572095\n",
      "epoch: 5 step: 1496, loss is 0.186218723654747\n",
      "epoch: 5 step: 1497, loss is 0.007051782216876745\n",
      "epoch: 5 step: 1498, loss is 0.007019432727247477\n",
      "epoch: 5 step: 1499, loss is 0.00082034693332389\n",
      "epoch: 5 step: 1500, loss is 0.07102256268262863\n",
      "epoch: 5 step: 1501, loss is 0.008128943853080273\n",
      "epoch: 5 step: 1502, loss is 0.010787144303321838\n",
      "epoch: 5 step: 1503, loss is 0.018099654465913773\n",
      "epoch: 5 step: 1504, loss is 0.00365253584459424\n",
      "epoch: 5 step: 1505, loss is 0.010938392952084541\n",
      "epoch: 5 step: 1506, loss is 0.007535822689533234\n",
      "epoch: 5 step: 1507, loss is 0.003597161965444684\n",
      "epoch: 5 step: 1508, loss is 0.07043715566396713\n",
      "epoch: 5 step: 1509, loss is 0.006891987752169371\n",
      "epoch: 5 step: 1510, loss is 0.0007040335331112146\n",
      "epoch: 5 step: 1511, loss is 0.19237981736660004\n",
      "epoch: 5 step: 1512, loss is 0.009966438636183739\n",
      "epoch: 5 step: 1513, loss is 0.0013827407965436578\n",
      "epoch: 5 step: 1514, loss is 0.03922528028488159\n",
      "epoch: 5 step: 1515, loss is 0.013010870665311813\n",
      "epoch: 5 step: 1516, loss is 0.004873123951256275\n",
      "epoch: 5 step: 1517, loss is 0.0010683001019060612\n",
      "epoch: 5 step: 1518, loss is 0.0007308446802198887\n",
      "epoch: 5 step: 1519, loss is 0.021797804161906242\n",
      "epoch: 5 step: 1520, loss is 0.0014462026301771402\n",
      "epoch: 5 step: 1521, loss is 0.011023616418242455\n",
      "epoch: 5 step: 1522, loss is 0.0018427313771098852\n",
      "epoch: 5 step: 1523, loss is 0.005597775336354971\n",
      "epoch: 5 step: 1524, loss is 0.008753075264394283\n",
      "epoch: 5 step: 1525, loss is 0.0008655121782794595\n",
      "epoch: 5 step: 1526, loss is 0.0014464686391875148\n",
      "epoch: 5 step: 1527, loss is 0.00254297093488276\n",
      "epoch: 5 step: 1528, loss is 0.0017743775388225913\n",
      "epoch: 5 step: 1529, loss is 0.0004109616274945438\n",
      "epoch: 5 step: 1530, loss is 0.0016556920018047094\n",
      "epoch: 5 step: 1531, loss is 0.0007816664292477071\n",
      "epoch: 5 step: 1532, loss is 0.06753594428300858\n",
      "epoch: 5 step: 1533, loss is 0.003886645194143057\n",
      "epoch: 5 step: 1534, loss is 0.0016806452767923474\n",
      "epoch: 5 step: 1535, loss is 0.09851893037557602\n",
      "epoch: 5 step: 1536, loss is 0.006192669738084078\n",
      "epoch: 5 step: 1537, loss is 0.0024825946893543005\n",
      "epoch: 5 step: 1538, loss is 0.004305039532482624\n",
      "epoch: 5 step: 1539, loss is 0.0034637628123164177\n",
      "epoch: 5 step: 1540, loss is 0.03562382608652115\n",
      "epoch: 5 step: 1541, loss is 0.002261933172121644\n",
      "epoch: 5 step: 1542, loss is 0.0019819866865873337\n",
      "epoch: 5 step: 1543, loss is 0.0030187603551894426\n",
      "epoch: 5 step: 1544, loss is 0.002802743576467037\n",
      "epoch: 5 step: 1545, loss is 0.0024703291710466146\n",
      "epoch: 5 step: 1546, loss is 0.0010126607958227396\n",
      "epoch: 5 step: 1547, loss is 0.014049413613975048\n",
      "epoch: 5 step: 1548, loss is 0.0015191002748906612\n",
      "epoch: 5 step: 1549, loss is 0.08519233018159866\n",
      "epoch: 5 step: 1550, loss is 0.025269119068980217\n",
      "epoch: 5 step: 1551, loss is 0.008197596296668053\n",
      "epoch: 5 step: 1552, loss is 0.03074091114103794\n",
      "epoch: 5 step: 1553, loss is 0.1332693248987198\n",
      "epoch: 5 step: 1554, loss is 0.004497720394283533\n",
      "epoch: 5 step: 1555, loss is 0.07208296656608582\n",
      "epoch: 5 step: 1556, loss is 0.01615256443619728\n",
      "epoch: 5 step: 1557, loss is 0.015149890445172787\n",
      "epoch: 5 step: 1558, loss is 0.0007266167085617781\n",
      "epoch: 5 step: 1559, loss is 0.0006348025635816157\n",
      "epoch: 5 step: 1560, loss is 0.01086872536689043\n",
      "epoch: 5 step: 1561, loss is 0.13080459833145142\n",
      "epoch: 5 step: 1562, loss is 0.00024023021978791803\n",
      "epoch: 5 step: 1563, loss is 0.006217001937329769\n",
      "epoch: 5 step: 1564, loss is 0.10072976350784302\n",
      "epoch: 5 step: 1565, loss is 0.0041174329817295074\n",
      "epoch: 5 step: 1566, loss is 0.060484498739242554\n",
      "epoch: 5 step: 1567, loss is 0.2294423133134842\n",
      "epoch: 5 step: 1568, loss is 0.014032097533345222\n",
      "epoch: 5 step: 1569, loss is 0.0024237260222434998\n",
      "epoch: 5 step: 1570, loss is 0.01129755936563015\n",
      "epoch: 5 step: 1571, loss is 0.00843135267496109\n",
      "epoch: 5 step: 1572, loss is 0.01009040791541338\n",
      "epoch: 5 step: 1573, loss is 0.0956929549574852\n",
      "epoch: 5 step: 1574, loss is 0.014869346283376217\n",
      "epoch: 5 step: 1575, loss is 0.010694661177694798\n",
      "epoch: 5 step: 1576, loss is 0.004889243748039007\n",
      "epoch: 5 step: 1577, loss is 0.02134058065712452\n",
      "epoch: 5 step: 1578, loss is 0.0018357235239818692\n",
      "epoch: 5 step: 1579, loss is 0.054340437054634094\n",
      "epoch: 5 step: 1580, loss is 0.05550817772746086\n",
      "epoch: 5 step: 1581, loss is 0.016958657652139664\n",
      "epoch: 5 step: 1582, loss is 0.022960852831602097\n",
      "epoch: 5 step: 1583, loss is 0.03079306147992611\n",
      "epoch: 5 step: 1584, loss is 0.11077369749546051\n",
      "epoch: 5 step: 1585, loss is 0.0012872933875769377\n",
      "epoch: 5 step: 1586, loss is 0.06269420683383942\n",
      "epoch: 5 step: 1587, loss is 0.0027475710958242416\n",
      "epoch: 5 step: 1588, loss is 0.010949400253593922\n",
      "epoch: 5 step: 1589, loss is 0.011604078114032745\n",
      "epoch: 5 step: 1590, loss is 0.0016051104757934809\n",
      "epoch: 5 step: 1591, loss is 0.003128236858174205\n",
      "epoch: 5 step: 1592, loss is 0.0078118108212947845\n",
      "epoch: 5 step: 1593, loss is 0.027905263006687164\n",
      "epoch: 5 step: 1594, loss is 5.659703674609773e-05\n",
      "epoch: 5 step: 1595, loss is 0.05174243822693825\n",
      "epoch: 5 step: 1596, loss is 0.0003042347962036729\n",
      "epoch: 5 step: 1597, loss is 0.000932540453504771\n",
      "epoch: 5 step: 1598, loss is 0.023035211488604546\n",
      "epoch: 5 step: 1599, loss is 0.0016749155474826694\n",
      "epoch: 5 step: 1600, loss is 0.18502020835876465\n",
      "epoch: 5 step: 1601, loss is 0.012447071261703968\n",
      "epoch: 5 step: 1602, loss is 0.001320811454206705\n",
      "epoch: 5 step: 1603, loss is 0.005363019648939371\n",
      "epoch: 5 step: 1604, loss is 0.007325408048927784\n",
      "epoch: 5 step: 1605, loss is 0.012721690349280834\n",
      "epoch: 5 step: 1606, loss is 0.054489944130182266\n",
      "epoch: 5 step: 1607, loss is 0.06629300862550735\n",
      "epoch: 5 step: 1608, loss is 0.0020946841686964035\n",
      "epoch: 5 step: 1609, loss is 0.002494037849828601\n",
      "epoch: 5 step: 1610, loss is 0.18483178317546844\n",
      "epoch: 5 step: 1611, loss is 0.0004877795581705868\n",
      "epoch: 5 step: 1612, loss is 0.1928407996892929\n",
      "epoch: 5 step: 1613, loss is 0.018513616174459457\n",
      "epoch: 5 step: 1614, loss is 0.014284474775195122\n",
      "epoch: 5 step: 1615, loss is 0.0009050334338098764\n",
      "epoch: 5 step: 1616, loss is 0.011926182545721531\n",
      "epoch: 5 step: 1617, loss is 0.004928387701511383\n",
      "epoch: 5 step: 1618, loss is 0.0024526366032660007\n",
      "epoch: 5 step: 1619, loss is 0.03720201551914215\n",
      "epoch: 5 step: 1620, loss is 0.104759082198143\n",
      "epoch: 5 step: 1621, loss is 0.0059969439171254635\n",
      "epoch: 5 step: 1622, loss is 0.00031137673067860305\n",
      "epoch: 5 step: 1623, loss is 0.0261610709130764\n",
      "epoch: 5 step: 1624, loss is 0.0015275590121746063\n",
      "epoch: 5 step: 1625, loss is 0.0004368219233583659\n",
      "epoch: 5 step: 1626, loss is 0.005121175199747086\n",
      "epoch: 5 step: 1627, loss is 0.0020262280013412237\n",
      "epoch: 5 step: 1628, loss is 0.001473243348300457\n",
      "epoch: 5 step: 1629, loss is 0.0008767935796640813\n",
      "epoch: 5 step: 1630, loss is 0.0005953441723249853\n",
      "epoch: 5 step: 1631, loss is 0.018192142248153687\n",
      "epoch: 5 step: 1632, loss is 0.0011950029293075204\n",
      "epoch: 5 step: 1633, loss is 0.00014592218212783337\n",
      "epoch: 5 step: 1634, loss is 0.02566860243678093\n",
      "epoch: 5 step: 1635, loss is 0.0018713753670454025\n",
      "epoch: 5 step: 1636, loss is 0.00032305685454048216\n",
      "epoch: 5 step: 1637, loss is 0.0007084655808284879\n",
      "epoch: 5 step: 1638, loss is 0.0009345225407741964\n",
      "epoch: 5 step: 1639, loss is 0.02333740144968033\n",
      "epoch: 5 step: 1640, loss is 0.008120536804199219\n",
      "epoch: 5 step: 1641, loss is 0.005787256173789501\n",
      "epoch: 5 step: 1642, loss is 0.0002523602161090821\n",
      "epoch: 5 step: 1643, loss is 0.023288365453481674\n",
      "epoch: 5 step: 1644, loss is 0.06845982372760773\n",
      "epoch: 5 step: 1645, loss is 0.014693884178996086\n",
      "epoch: 5 step: 1646, loss is 0.0054571498185396194\n",
      "epoch: 5 step: 1647, loss is 0.014575335197150707\n",
      "epoch: 5 step: 1648, loss is 0.00033392070326954126\n",
      "epoch: 5 step: 1649, loss is 0.00013941263023298234\n",
      "epoch: 5 step: 1650, loss is 0.001786199165508151\n",
      "epoch: 5 step: 1651, loss is 0.0012386760208755732\n",
      "epoch: 5 step: 1652, loss is 0.009613711386919022\n",
      "epoch: 5 step: 1653, loss is 0.004423529841005802\n",
      "epoch: 5 step: 1654, loss is 0.012398882769048214\n",
      "epoch: 5 step: 1655, loss is 0.012400561943650246\n",
      "epoch: 5 step: 1656, loss is 0.0021118049044162035\n",
      "epoch: 5 step: 1657, loss is 0.0010956316255033016\n",
      "epoch: 5 step: 1658, loss is 0.0007931706495583057\n",
      "epoch: 5 step: 1659, loss is 0.0015654026065021753\n",
      "epoch: 5 step: 1660, loss is 0.0030045844614505768\n",
      "epoch: 5 step: 1661, loss is 0.002553072990849614\n",
      "epoch: 5 step: 1662, loss is 0.061971552670001984\n",
      "epoch: 5 step: 1663, loss is 0.20003066956996918\n",
      "epoch: 5 step: 1664, loss is 0.009962305426597595\n",
      "epoch: 5 step: 1665, loss is 0.0006125025684013963\n",
      "epoch: 5 step: 1666, loss is 0.00017932486662175506\n",
      "epoch: 5 step: 1667, loss is 0.0034553506411612034\n",
      "epoch: 5 step: 1668, loss is 0.26192858815193176\n",
      "epoch: 5 step: 1669, loss is 0.0014583516167476773\n",
      "epoch: 5 step: 1670, loss is 0.0004993761540390551\n",
      "epoch: 5 step: 1671, loss is 0.000887720612809062\n",
      "epoch: 5 step: 1672, loss is 0.00045873867929913104\n",
      "epoch: 5 step: 1673, loss is 0.003468027338385582\n",
      "epoch: 5 step: 1674, loss is 0.02180253528058529\n",
      "epoch: 5 step: 1675, loss is 0.00038563317502848804\n",
      "epoch: 5 step: 1676, loss is 0.053712546825408936\n",
      "epoch: 5 step: 1677, loss is 0.002260846085846424\n",
      "epoch: 5 step: 1678, loss is 0.037404514849185944\n",
      "epoch: 5 step: 1679, loss is 0.02734535187482834\n",
      "epoch: 5 step: 1680, loss is 0.017226818948984146\n",
      "epoch: 5 step: 1681, loss is 0.0018070077057927847\n",
      "epoch: 5 step: 1682, loss is 0.015767859295010567\n",
      "epoch: 5 step: 1683, loss is 0.028980383649468422\n",
      "epoch: 5 step: 1684, loss is 0.0072891125455498695\n",
      "epoch: 5 step: 1685, loss is 0.005049818195402622\n",
      "epoch: 5 step: 1686, loss is 0.006354184355586767\n",
      "epoch: 5 step: 1687, loss is 0.07182178646326065\n",
      "epoch: 5 step: 1688, loss is 0.0011013008188456297\n",
      "epoch: 5 step: 1689, loss is 0.009194347076117992\n",
      "epoch: 5 step: 1690, loss is 0.0006202426739037037\n",
      "epoch: 5 step: 1691, loss is 0.0009815245866775513\n",
      "epoch: 5 step: 1692, loss is 0.05441389977931976\n",
      "epoch: 5 step: 1693, loss is 0.000380101177142933\n",
      "epoch: 5 step: 1694, loss is 6.339795072562993e-05\n",
      "epoch: 5 step: 1695, loss is 0.009753440506756306\n",
      "epoch: 5 step: 1696, loss is 0.0016235540388152003\n",
      "epoch: 5 step: 1697, loss is 0.03063093312084675\n",
      "epoch: 5 step: 1698, loss is 0.1464153379201889\n",
      "epoch: 5 step: 1699, loss is 0.017370978370308876\n",
      "epoch: 5 step: 1700, loss is 0.0008236895082518458\n",
      "epoch: 5 step: 1701, loss is 0.0029835663735866547\n",
      "epoch: 5 step: 1702, loss is 0.00010268726327922195\n",
      "epoch: 5 step: 1703, loss is 0.004840856418013573\n",
      "epoch: 5 step: 1704, loss is 0.18846121430397034\n",
      "epoch: 5 step: 1705, loss is 0.0002142554585589096\n",
      "epoch: 5 step: 1706, loss is 0.07280007749795914\n",
      "epoch: 5 step: 1707, loss is 0.00812405813485384\n",
      "epoch: 5 step: 1708, loss is 0.011668720282614231\n",
      "epoch: 5 step: 1709, loss is 0.0003566601953934878\n",
      "epoch: 5 step: 1710, loss is 0.007528273854404688\n",
      "epoch: 5 step: 1711, loss is 0.0006676777848042548\n",
      "epoch: 5 step: 1712, loss is 0.17920474708080292\n",
      "epoch: 5 step: 1713, loss is 0.00896612647920847\n",
      "epoch: 5 step: 1714, loss is 0.002275370294228196\n",
      "epoch: 5 step: 1715, loss is 0.005471041891723871\n",
      "epoch: 5 step: 1716, loss is 0.0037692522164434195\n",
      "epoch: 5 step: 1717, loss is 0.0037465498317033052\n",
      "epoch: 5 step: 1718, loss is 0.010301381349563599\n",
      "epoch: 5 step: 1719, loss is 0.0022123525850474834\n",
      "epoch: 5 step: 1720, loss is 0.04254554212093353\n",
      "epoch: 5 step: 1721, loss is 0.00287040788680315\n",
      "epoch: 5 step: 1722, loss is 0.0021708044223487377\n",
      "epoch: 5 step: 1723, loss is 0.003537185722962022\n",
      "epoch: 5 step: 1724, loss is 0.004962208215147257\n",
      "epoch: 5 step: 1725, loss is 0.008296988904476166\n",
      "epoch: 5 step: 1726, loss is 0.017250852659344673\n",
      "epoch: 5 step: 1727, loss is 0.01438071671873331\n",
      "epoch: 5 step: 1728, loss is 0.0028774470556527376\n",
      "epoch: 5 step: 1729, loss is 0.00033972947858273983\n",
      "epoch: 5 step: 1730, loss is 0.0026309576351195574\n",
      "epoch: 5 step: 1731, loss is 0.012196963652968407\n",
      "epoch: 5 step: 1732, loss is 0.0073379273526370525\n",
      "epoch: 5 step: 1733, loss is 0.04146429896354675\n",
      "epoch: 5 step: 1734, loss is 0.0017897295765578747\n",
      "epoch: 5 step: 1735, loss is 0.0030774129554629326\n",
      "epoch: 5 step: 1736, loss is 0.0026394985616207123\n",
      "epoch: 5 step: 1737, loss is 0.002912360243499279\n",
      "epoch: 5 step: 1738, loss is 0.0041733235120773315\n",
      "epoch: 5 step: 1739, loss is 0.0010414557764306664\n",
      "epoch: 5 step: 1740, loss is 0.0020451867021620274\n",
      "epoch: 5 step: 1741, loss is 7.735676626907662e-05\n",
      "epoch: 5 step: 1742, loss is 0.008383229374885559\n",
      "epoch: 5 step: 1743, loss is 0.0016014011343941092\n",
      "epoch: 5 step: 1744, loss is 0.0019658212549984455\n",
      "epoch: 5 step: 1745, loss is 0.01666436158120632\n",
      "epoch: 5 step: 1746, loss is 0.0005139201530255377\n",
      "epoch: 5 step: 1747, loss is 0.00047001271741464734\n",
      "epoch: 5 step: 1748, loss is 0.0035843756049871445\n",
      "epoch: 5 step: 1749, loss is 0.0007186525035649538\n",
      "epoch: 5 step: 1750, loss is 0.0144675113260746\n",
      "epoch: 5 step: 1751, loss is 0.0004137087962590158\n",
      "epoch: 5 step: 1752, loss is 0.06119228154420853\n",
      "epoch: 5 step: 1753, loss is 0.0005371838342398405\n",
      "epoch: 5 step: 1754, loss is 0.025641225278377533\n",
      "epoch: 5 step: 1755, loss is 0.0008280246984213591\n",
      "epoch: 5 step: 1756, loss is 0.006699688732624054\n",
      "epoch: 5 step: 1757, loss is 0.001975567312911153\n",
      "epoch: 5 step: 1758, loss is 0.0010733698727563024\n",
      "epoch: 5 step: 1759, loss is 6.420665886253119e-05\n",
      "epoch: 5 step: 1760, loss is 0.008570791222155094\n",
      "epoch: 5 step: 1761, loss is 0.0009247148409485817\n",
      "epoch: 5 step: 1762, loss is 0.0002888052840717137\n",
      "epoch: 5 step: 1763, loss is 0.27989593148231506\n",
      "epoch: 5 step: 1764, loss is 0.042335763573646545\n",
      "epoch: 5 step: 1765, loss is 0.002483249641954899\n",
      "epoch: 5 step: 1766, loss is 0.0006503731128759682\n",
      "epoch: 5 step: 1767, loss is 0.02416696958243847\n",
      "epoch: 5 step: 1768, loss is 0.0004240649868734181\n",
      "epoch: 5 step: 1769, loss is 0.038126103579998016\n",
      "epoch: 5 step: 1770, loss is 0.020303640514612198\n",
      "epoch: 5 step: 1771, loss is 0.00014623970491811633\n",
      "epoch: 5 step: 1772, loss is 0.1064969152212143\n",
      "epoch: 5 step: 1773, loss is 0.0052168951369822025\n",
      "epoch: 5 step: 1774, loss is 0.009505029767751694\n",
      "epoch: 5 step: 1775, loss is 0.012614337727427483\n",
      "epoch: 5 step: 1776, loss is 0.0057982709258794785\n",
      "epoch: 5 step: 1777, loss is 0.08710107952356339\n",
      "epoch: 5 step: 1778, loss is 0.004247462842613459\n",
      "epoch: 5 step: 1779, loss is 0.048055823892354965\n",
      "epoch: 5 step: 1780, loss is 0.005387229844927788\n",
      "epoch: 5 step: 1781, loss is 0.0014992286451160908\n",
      "epoch: 5 step: 1782, loss is 0.0017019723309203982\n",
      "epoch: 5 step: 1783, loss is 0.0009596546296961606\n",
      "epoch: 5 step: 1784, loss is 0.005152296740561724\n",
      "epoch: 5 step: 1785, loss is 0.000513714796397835\n",
      "epoch: 5 step: 1786, loss is 0.19499649107456207\n",
      "epoch: 5 step: 1787, loss is 0.0014347617980092764\n",
      "epoch: 5 step: 1788, loss is 0.09880257397890091\n",
      "epoch: 5 step: 1789, loss is 0.00033819550299085677\n",
      "epoch: 5 step: 1790, loss is 0.0010885641677305102\n",
      "epoch: 5 step: 1791, loss is 0.12526044249534607\n",
      "epoch: 5 step: 1792, loss is 0.011849469505250454\n",
      "epoch: 5 step: 1793, loss is 0.184127077460289\n",
      "epoch: 5 step: 1794, loss is 0.10070019215345383\n",
      "epoch: 5 step: 1795, loss is 0.00039549972279928625\n",
      "epoch: 5 step: 1796, loss is 0.0024588988162577152\n",
      "epoch: 5 step: 1797, loss is 0.0071092932485044\n",
      "epoch: 5 step: 1798, loss is 0.005908600054681301\n",
      "epoch: 5 step: 1799, loss is 0.0003771011542994529\n",
      "epoch: 5 step: 1800, loss is 0.2515530586242676\n",
      "epoch: 5 step: 1801, loss is 0.00302255991846323\n",
      "epoch: 5 step: 1802, loss is 0.0004625557048711926\n",
      "epoch: 5 step: 1803, loss is 0.11961252987384796\n",
      "epoch: 5 step: 1804, loss is 0.012364412657916546\n",
      "epoch: 5 step: 1805, loss is 0.0800967812538147\n",
      "epoch: 5 step: 1806, loss is 0.021260615438222885\n",
      "epoch: 5 step: 1807, loss is 0.0019030224066227674\n",
      "epoch: 5 step: 1808, loss is 0.0013360276352614164\n",
      "epoch: 5 step: 1809, loss is 0.028527334332466125\n",
      "epoch: 5 step: 1810, loss is 0.001816575531847775\n",
      "epoch: 5 step: 1811, loss is 0.09730080515146255\n",
      "epoch: 5 step: 1812, loss is 0.00252742157317698\n",
      "epoch: 5 step: 1813, loss is 0.0008834742475301027\n",
      "epoch: 5 step: 1814, loss is 0.0004347615467850119\n",
      "epoch: 5 step: 1815, loss is 0.021293511614203453\n",
      "epoch: 5 step: 1816, loss is 0.008978288620710373\n",
      "epoch: 5 step: 1817, loss is 0.02047652192413807\n",
      "epoch: 5 step: 1818, loss is 0.23114213347434998\n",
      "epoch: 5 step: 1819, loss is 0.050487618893384933\n",
      "epoch: 5 step: 1820, loss is 0.00030036226962693036\n",
      "epoch: 5 step: 1821, loss is 0.037103500217199326\n",
      "epoch: 5 step: 1822, loss is 0.018426591530442238\n",
      "epoch: 5 step: 1823, loss is 0.004137931391596794\n",
      "epoch: 5 step: 1824, loss is 0.006448641885071993\n",
      "epoch: 5 step: 1825, loss is 0.0008291778503917158\n",
      "epoch: 5 step: 1826, loss is 0.021209057420492172\n",
      "epoch: 5 step: 1827, loss is 0.0007711941143497825\n",
      "epoch: 5 step: 1828, loss is 0.14535678923130035\n",
      "epoch: 5 step: 1829, loss is 0.06621471047401428\n",
      "epoch: 5 step: 1830, loss is 0.0014935205690562725\n",
      "epoch: 5 step: 1831, loss is 0.03254470229148865\n",
      "epoch: 5 step: 1832, loss is 0.0005907206796109676\n",
      "epoch: 5 step: 1833, loss is 0.00043446841300465167\n",
      "epoch: 5 step: 1834, loss is 0.10401643812656403\n",
      "epoch: 5 step: 1835, loss is 0.008985655382275581\n",
      "epoch: 5 step: 1836, loss is 0.005228140857070684\n",
      "epoch: 5 step: 1837, loss is 0.014017404988408089\n",
      "epoch: 5 step: 1838, loss is 0.023663973435759544\n",
      "epoch: 5 step: 1839, loss is 7.24445708328858e-05\n",
      "epoch: 5 step: 1840, loss is 0.15191759169101715\n",
      "epoch: 5 step: 1841, loss is 0.018453538417816162\n",
      "epoch: 5 step: 1842, loss is 0.01155028771609068\n",
      "epoch: 5 step: 1843, loss is 0.00022991211153566837\n",
      "epoch: 5 step: 1844, loss is 0.014191843569278717\n",
      "epoch: 5 step: 1845, loss is 0.002620752900838852\n",
      "epoch: 5 step: 1846, loss is 0.007311366964131594\n",
      "epoch: 5 step: 1847, loss is 0.11969397217035294\n",
      "epoch: 5 step: 1848, loss is 0.0006619024206884205\n",
      "epoch: 5 step: 1849, loss is 0.0014946144074201584\n",
      "epoch: 5 step: 1850, loss is 0.05254954844713211\n",
      "epoch: 5 step: 1851, loss is 0.0005419249064289033\n",
      "epoch: 5 step: 1852, loss is 0.005267459433525801\n",
      "epoch: 5 step: 1853, loss is 0.0013075617607682943\n",
      "epoch: 5 step: 1854, loss is 0.05849117413163185\n",
      "epoch: 5 step: 1855, loss is 0.006724982988089323\n",
      "epoch: 5 step: 1856, loss is 0.1219397783279419\n",
      "epoch: 5 step: 1857, loss is 0.04173584282398224\n",
      "epoch: 5 step: 1858, loss is 0.007413266226649284\n",
      "epoch: 5 step: 1859, loss is 0.006259451620280743\n",
      "epoch: 5 step: 1860, loss is 0.009126395918428898\n",
      "epoch: 5 step: 1861, loss is 0.00030177863663993776\n",
      "epoch: 5 step: 1862, loss is 0.014844680204987526\n",
      "epoch: 5 step: 1863, loss is 0.008885759860277176\n",
      "epoch: 5 step: 1864, loss is 0.008152098394930363\n",
      "epoch: 5 step: 1865, loss is 0.015087221749126911\n",
      "epoch: 5 step: 1866, loss is 0.0008244356140494347\n",
      "epoch: 5 step: 1867, loss is 0.04874001070857048\n",
      "epoch: 5 step: 1868, loss is 0.0021860289853066206\n",
      "epoch: 5 step: 1869, loss is 0.0010631280019879341\n",
      "epoch: 5 step: 1870, loss is 0.0032970465254038572\n",
      "epoch: 5 step: 1871, loss is 0.010975474491715431\n",
      "epoch: 5 step: 1872, loss is 0.0278485007584095\n",
      "epoch: 5 step: 1873, loss is 0.015672732144594193\n",
      "epoch: 5 step: 1874, loss is 0.01568925939500332\n",
      "epoch: 5 step: 1875, loss is 0.0025121138896793127\n",
      "Train epoch time: 15252.964 ms, per step time: 8.135 ms\n",
      "epoch: 6 step: 1, loss is 0.02516118437051773\n",
      "epoch: 6 step: 2, loss is 0.000585045840125531\n",
      "epoch: 6 step: 3, loss is 0.0028509306721389294\n",
      "epoch: 6 step: 4, loss is 0.00010768610809464008\n",
      "epoch: 6 step: 5, loss is 0.08350810408592224\n",
      "epoch: 6 step: 6, loss is 0.013102268800139427\n",
      "epoch: 6 step: 7, loss is 0.3592905104160309\n",
      "epoch: 6 step: 8, loss is 0.008339197374880314\n",
      "epoch: 6 step: 9, loss is 0.000530183082446456\n",
      "epoch: 6 step: 10, loss is 0.006043013650923967\n",
      "epoch: 6 step: 11, loss is 0.0011398600181564689\n",
      "epoch: 6 step: 12, loss is 0.001265774597413838\n",
      "epoch: 6 step: 13, loss is 0.00023462473473045975\n",
      "epoch: 6 step: 14, loss is 0.00148725975304842\n",
      "epoch: 6 step: 15, loss is 0.004978773184120655\n",
      "epoch: 6 step: 16, loss is 0.002470958512276411\n",
      "epoch: 6 step: 17, loss is 0.0007293806411325932\n",
      "epoch: 6 step: 18, loss is 0.0007225578301586211\n",
      "epoch: 6 step: 19, loss is 0.0010930629214271903\n",
      "epoch: 6 step: 20, loss is 0.03407653793692589\n",
      "epoch: 6 step: 21, loss is 0.06976886838674545\n",
      "epoch: 6 step: 22, loss is 0.0016317394329234958\n",
      "epoch: 6 step: 23, loss is 0.05911407619714737\n",
      "epoch: 6 step: 24, loss is 0.02057000622153282\n",
      "epoch: 6 step: 25, loss is 0.029866104945540428\n",
      "epoch: 6 step: 26, loss is 0.0416303388774395\n",
      "epoch: 6 step: 27, loss is 0.0004360144666861743\n",
      "epoch: 6 step: 28, loss is 0.01962929219007492\n",
      "epoch: 6 step: 29, loss is 0.11487744748592377\n",
      "epoch: 6 step: 30, loss is 0.003133558202534914\n",
      "epoch: 6 step: 31, loss is 0.004001023713499308\n",
      "epoch: 6 step: 32, loss is 0.06522674113512039\n",
      "epoch: 6 step: 33, loss is 0.026774391531944275\n",
      "epoch: 6 step: 34, loss is 0.0002912281488534063\n",
      "epoch: 6 step: 35, loss is 0.011776100844144821\n",
      "epoch: 6 step: 36, loss is 0.0014403924578800797\n",
      "epoch: 6 step: 37, loss is 0.009989800862967968\n",
      "epoch: 6 step: 38, loss is 0.001443520886823535\n",
      "epoch: 6 step: 39, loss is 0.003923019859939814\n",
      "epoch: 6 step: 40, loss is 0.0009993271669372916\n",
      "epoch: 6 step: 41, loss is 0.004619718063622713\n",
      "epoch: 6 step: 42, loss is 0.013042154721915722\n",
      "epoch: 6 step: 43, loss is 0.057739757001399994\n",
      "epoch: 6 step: 44, loss is 0.003303257282823324\n",
      "epoch: 6 step: 45, loss is 0.008019787259399891\n",
      "epoch: 6 step: 46, loss is 0.012796251103281975\n",
      "epoch: 6 step: 47, loss is 0.0003009003703482449\n",
      "epoch: 6 step: 48, loss is 0.00013242624117992818\n",
      "epoch: 6 step: 49, loss is 0.1822197139263153\n",
      "epoch: 6 step: 50, loss is 0.03706175088882446\n",
      "epoch: 6 step: 51, loss is 0.0034419980365782976\n",
      "epoch: 6 step: 52, loss is 0.00045689044054597616\n",
      "epoch: 6 step: 53, loss is 0.057747676968574524\n",
      "epoch: 6 step: 54, loss is 0.0011216697748750448\n",
      "epoch: 6 step: 55, loss is 0.004196430090814829\n",
      "epoch: 6 step: 56, loss is 0.010473274625837803\n",
      "epoch: 6 step: 57, loss is 0.0002246614603791386\n",
      "epoch: 6 step: 58, loss is 0.004289872944355011\n",
      "epoch: 6 step: 59, loss is 0.0029317508451640606\n",
      "epoch: 6 step: 60, loss is 0.011351142078638077\n",
      "epoch: 6 step: 61, loss is 0.002725327154621482\n",
      "epoch: 6 step: 62, loss is 0.12072650343179703\n",
      "epoch: 6 step: 63, loss is 0.0049166190437972546\n",
      "epoch: 6 step: 64, loss is 0.07783377915620804\n",
      "epoch: 6 step: 65, loss is 0.002494770335033536\n",
      "epoch: 6 step: 66, loss is 0.0001768782240105793\n",
      "epoch: 6 step: 67, loss is 0.011433595791459084\n",
      "epoch: 6 step: 68, loss is 0.10679827630519867\n",
      "epoch: 6 step: 69, loss is 0.006763124372810125\n",
      "epoch: 6 step: 70, loss is 0.024912433698773384\n",
      "epoch: 6 step: 71, loss is 0.004436860326677561\n",
      "epoch: 6 step: 72, loss is 0.0010396265424787998\n",
      "epoch: 6 step: 73, loss is 0.004042882472276688\n",
      "epoch: 6 step: 74, loss is 0.002176869660615921\n",
      "epoch: 6 step: 75, loss is 0.0007055946625769138\n",
      "epoch: 6 step: 76, loss is 0.00411975709721446\n",
      "epoch: 6 step: 77, loss is 0.0007670445484109223\n",
      "epoch: 6 step: 78, loss is 0.004824511706829071\n",
      "epoch: 6 step: 79, loss is 0.0015363518614321947\n",
      "epoch: 6 step: 80, loss is 0.007329410407692194\n",
      "epoch: 6 step: 81, loss is 0.0001093303071684204\n",
      "epoch: 6 step: 82, loss is 0.0013801929308101535\n",
      "epoch: 6 step: 83, loss is 0.0025902842171490192\n",
      "epoch: 6 step: 84, loss is 0.0006539452588185668\n",
      "epoch: 6 step: 85, loss is 0.001641388051211834\n",
      "epoch: 6 step: 86, loss is 0.004220690578222275\n",
      "epoch: 6 step: 87, loss is 0.0015919493744149804\n",
      "epoch: 6 step: 88, loss is 0.0014644538750872016\n",
      "epoch: 6 step: 89, loss is 0.0004635643563233316\n",
      "epoch: 6 step: 90, loss is 0.002102685160934925\n",
      "epoch: 6 step: 91, loss is 0.006056746002286673\n",
      "epoch: 6 step: 92, loss is 0.004051106050610542\n",
      "epoch: 6 step: 93, loss is 0.008609569631516933\n",
      "epoch: 6 step: 94, loss is 0.002069129142910242\n",
      "epoch: 6 step: 95, loss is 0.02018756978213787\n",
      "epoch: 6 step: 96, loss is 0.005556507036089897\n",
      "epoch: 6 step: 97, loss is 0.0007071399595588446\n",
      "epoch: 6 step: 98, loss is 0.022776616737246513\n",
      "epoch: 6 step: 99, loss is 0.012771913781762123\n",
      "epoch: 6 step: 100, loss is 0.024266313761472702\n",
      "epoch: 6 step: 101, loss is 0.003173703560605645\n",
      "epoch: 6 step: 102, loss is 0.012606906704604626\n",
      "epoch: 6 step: 103, loss is 0.0018899360438808799\n",
      "epoch: 6 step: 104, loss is 0.011812543496489525\n",
      "epoch: 6 step: 105, loss is 0.00255302875302732\n",
      "epoch: 6 step: 106, loss is 0.0023417137563228607\n",
      "epoch: 6 step: 107, loss is 0.000283286499325186\n",
      "epoch: 6 step: 108, loss is 0.17201846837997437\n",
      "epoch: 6 step: 109, loss is 0.001581736491061747\n",
      "epoch: 6 step: 110, loss is 0.00139011035207659\n",
      "epoch: 6 step: 111, loss is 0.04744015634059906\n",
      "epoch: 6 step: 112, loss is 0.0003532639821060002\n",
      "epoch: 6 step: 113, loss is 0.00651603564620018\n",
      "epoch: 6 step: 114, loss is 7.979640940902755e-05\n",
      "epoch: 6 step: 115, loss is 0.003804498352110386\n",
      "epoch: 6 step: 116, loss is 0.03835050016641617\n",
      "epoch: 6 step: 117, loss is 0.0007514257449656725\n",
      "epoch: 6 step: 118, loss is 0.004960091784596443\n",
      "epoch: 6 step: 119, loss is 0.036365117877721786\n",
      "epoch: 6 step: 120, loss is 0.11412517726421356\n",
      "epoch: 6 step: 121, loss is 0.009054945781826973\n",
      "epoch: 6 step: 122, loss is 0.005486481357365847\n",
      "epoch: 6 step: 123, loss is 0.0003909409570042044\n",
      "epoch: 6 step: 124, loss is 0.0010539938230067492\n",
      "epoch: 6 step: 125, loss is 0.0014206552878022194\n",
      "epoch: 6 step: 126, loss is 0.03247779607772827\n",
      "epoch: 6 step: 127, loss is 0.0035997750237584114\n",
      "epoch: 6 step: 128, loss is 0.007074302062392235\n",
      "epoch: 6 step: 129, loss is 0.0010742656886577606\n",
      "epoch: 6 step: 130, loss is 0.010666326619684696\n",
      "epoch: 6 step: 131, loss is 0.003922071773558855\n",
      "epoch: 6 step: 132, loss is 0.03253420069813728\n",
      "epoch: 6 step: 133, loss is 0.0003705572453327477\n",
      "epoch: 6 step: 134, loss is 0.0008481978438794613\n",
      "epoch: 6 step: 135, loss is 0.0004362943291198462\n",
      "epoch: 6 step: 136, loss is 0.010569168254733086\n",
      "epoch: 6 step: 137, loss is 0.00010814256529556587\n",
      "epoch: 6 step: 138, loss is 0.011544745415449142\n",
      "epoch: 6 step: 139, loss is 0.07032743841409683\n",
      "epoch: 6 step: 140, loss is 0.002919913502410054\n",
      "epoch: 6 step: 141, loss is 0.0007909920532256365\n",
      "epoch: 6 step: 142, loss is 0.024902714416384697\n",
      "epoch: 6 step: 143, loss is 0.0009116196306422353\n",
      "epoch: 6 step: 144, loss is 0.0007178516825661063\n",
      "epoch: 6 step: 145, loss is 0.027162572368979454\n",
      "epoch: 6 step: 146, loss is 0.043503839522600174\n",
      "epoch: 6 step: 147, loss is 0.0005485951551236212\n",
      "epoch: 6 step: 148, loss is 0.022170165553689003\n",
      "epoch: 6 step: 149, loss is 0.0017537253443151712\n",
      "epoch: 6 step: 150, loss is 0.0036896069068461657\n",
      "epoch: 6 step: 151, loss is 0.00018874864326789975\n",
      "epoch: 6 step: 152, loss is 0.011440319009125233\n",
      "epoch: 6 step: 153, loss is 0.00026893350877799094\n",
      "epoch: 6 step: 154, loss is 0.0003449023643042892\n",
      "epoch: 6 step: 155, loss is 0.0016531990841031075\n",
      "epoch: 6 step: 156, loss is 0.0002682810591068119\n",
      "epoch: 6 step: 157, loss is 0.05034409463405609\n",
      "epoch: 6 step: 158, loss is 0.005185851361602545\n",
      "epoch: 6 step: 159, loss is 0.00031528089311905205\n",
      "epoch: 6 step: 160, loss is 0.0906774252653122\n",
      "epoch: 6 step: 161, loss is 0.004070853348821402\n",
      "epoch: 6 step: 162, loss is 0.007656266912817955\n",
      "epoch: 6 step: 163, loss is 0.008054015226662159\n",
      "epoch: 6 step: 164, loss is 0.002236713655292988\n",
      "epoch: 6 step: 165, loss is 0.013382112607359886\n",
      "epoch: 6 step: 166, loss is 0.012348727323114872\n",
      "epoch: 6 step: 167, loss is 0.0024712032172828913\n",
      "epoch: 6 step: 168, loss is 0.0015501306625083089\n",
      "epoch: 6 step: 169, loss is 0.003214921336621046\n",
      "epoch: 6 step: 170, loss is 0.015796953812241554\n",
      "epoch: 6 step: 171, loss is 0.0036754212342202663\n",
      "epoch: 6 step: 172, loss is 0.11317114531993866\n",
      "epoch: 6 step: 173, loss is 0.022673042491078377\n",
      "epoch: 6 step: 174, loss is 0.0015460680006071925\n",
      "epoch: 6 step: 175, loss is 0.0021588983945548534\n",
      "epoch: 6 step: 176, loss is 0.044737908989191055\n",
      "epoch: 6 step: 177, loss is 0.0018926790216937661\n",
      "epoch: 6 step: 178, loss is 0.0009422266739420593\n",
      "epoch: 6 step: 179, loss is 0.007674576714634895\n",
      "epoch: 6 step: 180, loss is 0.0505983904004097\n",
      "epoch: 6 step: 181, loss is 0.0003071617684327066\n",
      "epoch: 6 step: 182, loss is 0.002693243557587266\n",
      "epoch: 6 step: 183, loss is 0.0027941681910306215\n",
      "epoch: 6 step: 184, loss is 0.0010834101121872663\n",
      "epoch: 6 step: 185, loss is 0.05433597415685654\n",
      "epoch: 6 step: 186, loss is 0.0005810597795061767\n",
      "epoch: 6 step: 187, loss is 0.0008653212571516633\n",
      "epoch: 6 step: 188, loss is 0.0011331848800182343\n",
      "epoch: 6 step: 189, loss is 0.0007118894136510789\n",
      "epoch: 6 step: 190, loss is 0.033165767788887024\n",
      "epoch: 6 step: 191, loss is 0.042158499360084534\n",
      "epoch: 6 step: 192, loss is 0.00021564506459981203\n",
      "epoch: 6 step: 193, loss is 0.0005926855956204236\n",
      "epoch: 6 step: 194, loss is 0.0004309610521886498\n",
      "epoch: 6 step: 195, loss is 0.002985871396958828\n",
      "epoch: 6 step: 196, loss is 0.018287774175405502\n",
      "epoch: 6 step: 197, loss is 0.008531739935278893\n",
      "epoch: 6 step: 198, loss is 0.06445477157831192\n",
      "epoch: 6 step: 199, loss is 0.006312668789178133\n",
      "epoch: 6 step: 200, loss is 0.0005882868426851928\n",
      "epoch: 6 step: 201, loss is 0.0034664347767829895\n",
      "epoch: 6 step: 202, loss is 0.031213415786623955\n",
      "epoch: 6 step: 203, loss is 0.002380194840952754\n",
      "epoch: 6 step: 204, loss is 0.0020601486321538687\n",
      "epoch: 6 step: 205, loss is 3.997257590526715e-05\n",
      "epoch: 6 step: 206, loss is 0.02885724976658821\n",
      "epoch: 6 step: 207, loss is 0.021027278155088425\n",
      "epoch: 6 step: 208, loss is 0.001190506387501955\n",
      "epoch: 6 step: 209, loss is 0.003918678965419531\n",
      "epoch: 6 step: 210, loss is 0.0009283166145905852\n",
      "epoch: 6 step: 211, loss is 0.0037525733932852745\n",
      "epoch: 6 step: 212, loss is 0.0028213588520884514\n",
      "epoch: 6 step: 213, loss is 0.010746009647846222\n",
      "epoch: 6 step: 214, loss is 0.0007467127870768309\n",
      "epoch: 6 step: 215, loss is 0.00029298983281478286\n",
      "epoch: 6 step: 216, loss is 6.906978524057195e-05\n",
      "epoch: 6 step: 217, loss is 0.0013969572028145194\n",
      "epoch: 6 step: 218, loss is 0.001107335789129138\n",
      "epoch: 6 step: 219, loss is 0.007707502227276564\n",
      "epoch: 6 step: 220, loss is 0.0011788326082751155\n",
      "epoch: 6 step: 221, loss is 0.015359712764620781\n",
      "epoch: 6 step: 222, loss is 0.0038895071484148502\n",
      "epoch: 6 step: 223, loss is 0.0003786756133195013\n",
      "epoch: 6 step: 224, loss is 0.00015680721844546497\n",
      "epoch: 6 step: 225, loss is 0.0038300668820738792\n",
      "epoch: 6 step: 226, loss is 0.0008758257026784122\n",
      "epoch: 6 step: 227, loss is 0.0027218591421842575\n",
      "epoch: 6 step: 228, loss is 0.013705141842365265\n",
      "epoch: 6 step: 229, loss is 0.04772908613085747\n",
      "epoch: 6 step: 230, loss is 0.058048415929079056\n",
      "epoch: 6 step: 231, loss is 0.00011125341552542523\n",
      "epoch: 6 step: 232, loss is 0.016260959208011627\n",
      "epoch: 6 step: 233, loss is 0.00016168876027222723\n",
      "epoch: 6 step: 234, loss is 0.000394680566387251\n",
      "epoch: 6 step: 235, loss is 6.081608808017336e-05\n",
      "epoch: 6 step: 236, loss is 0.009861447848379612\n",
      "epoch: 6 step: 237, loss is 0.00046746083535254\n",
      "epoch: 6 step: 238, loss is 0.005566428881138563\n",
      "epoch: 6 step: 239, loss is 0.0005118997069075704\n",
      "epoch: 6 step: 240, loss is 0.025563567876815796\n",
      "epoch: 6 step: 241, loss is 0.0009657823829911649\n",
      "epoch: 6 step: 242, loss is 0.01336580142378807\n",
      "epoch: 6 step: 243, loss is 0.0038976462092250586\n",
      "epoch: 6 step: 244, loss is 0.008103074505925179\n",
      "epoch: 6 step: 245, loss is 0.1718076765537262\n",
      "epoch: 6 step: 246, loss is 0.003252681577578187\n",
      "epoch: 6 step: 247, loss is 0.004509817808866501\n",
      "epoch: 6 step: 248, loss is 0.002855548867955804\n",
      "epoch: 6 step: 249, loss is 0.018505627289414406\n",
      "epoch: 6 step: 250, loss is 0.004693198949098587\n",
      "epoch: 6 step: 251, loss is 0.0018860604614019394\n",
      "epoch: 6 step: 252, loss is 0.0004294918617233634\n",
      "epoch: 6 step: 253, loss is 0.015369261614978313\n",
      "epoch: 6 step: 254, loss is 0.01935325562953949\n",
      "epoch: 6 step: 255, loss is 0.00039625083445571363\n",
      "epoch: 6 step: 256, loss is 0.0015847086906433105\n",
      "epoch: 6 step: 257, loss is 0.014838576316833496\n",
      "epoch: 6 step: 258, loss is 5.6575576309114695e-05\n",
      "epoch: 6 step: 259, loss is 0.0003183286462444812\n",
      "epoch: 6 step: 260, loss is 0.0004217910463921726\n",
      "epoch: 6 step: 261, loss is 0.0019769826903939247\n",
      "epoch: 6 step: 262, loss is 0.02662634663283825\n",
      "epoch: 6 step: 263, loss is 0.0039154780097305775\n",
      "epoch: 6 step: 264, loss is 0.00012631670688278973\n",
      "epoch: 6 step: 265, loss is 0.001257176510989666\n",
      "epoch: 6 step: 266, loss is 0.013596703298389912\n",
      "epoch: 6 step: 267, loss is 0.0010613022604957223\n",
      "epoch: 6 step: 268, loss is 0.00018053864187095314\n",
      "epoch: 6 step: 269, loss is 0.0031001612078398466\n",
      "epoch: 6 step: 270, loss is 0.00011037850345019251\n",
      "epoch: 6 step: 271, loss is 0.00039923671283759177\n",
      "epoch: 6 step: 272, loss is 0.002414554823189974\n",
      "epoch: 6 step: 273, loss is 0.00018025866302195936\n",
      "epoch: 6 step: 274, loss is 0.0002697473391890526\n",
      "epoch: 6 step: 275, loss is 0.007037627510726452\n",
      "epoch: 6 step: 276, loss is 0.003606200683861971\n",
      "epoch: 6 step: 277, loss is 0.0007553742616437376\n",
      "epoch: 6 step: 278, loss is 0.08742351084947586\n",
      "epoch: 6 step: 279, loss is 0.0024751978926360607\n",
      "epoch: 6 step: 280, loss is 0.00016591933672316372\n",
      "epoch: 6 step: 281, loss is 0.0018655478488653898\n",
      "epoch: 6 step: 282, loss is 0.00010112812014995143\n",
      "epoch: 6 step: 283, loss is 0.00019317211990710348\n",
      "epoch: 6 step: 284, loss is 0.018553543835878372\n",
      "epoch: 6 step: 285, loss is 0.10934185236692429\n",
      "epoch: 6 step: 286, loss is 0.0026057539507746696\n",
      "epoch: 6 step: 287, loss is 0.009181595407426357\n",
      "epoch: 6 step: 288, loss is 0.11864516884088516\n",
      "epoch: 6 step: 289, loss is 0.002782894065603614\n",
      "epoch: 6 step: 290, loss is 0.00023808705736882985\n",
      "epoch: 6 step: 291, loss is 0.0005868215230293572\n",
      "epoch: 6 step: 292, loss is 0.0012576553272083402\n",
      "epoch: 6 step: 293, loss is 0.0011211467208340764\n",
      "epoch: 6 step: 294, loss is 8.560054266126826e-05\n",
      "epoch: 6 step: 295, loss is 0.00026646899641491473\n",
      "epoch: 6 step: 296, loss is 0.014258453622460365\n",
      "epoch: 6 step: 297, loss is 3.916188870789483e-05\n",
      "epoch: 6 step: 298, loss is 0.0010758924763649702\n",
      "epoch: 6 step: 299, loss is 0.00018987205112352967\n",
      "epoch: 6 step: 300, loss is 0.21579989790916443\n",
      "epoch: 6 step: 301, loss is 0.030779749155044556\n",
      "epoch: 6 step: 302, loss is 0.00014542305143550038\n",
      "epoch: 6 step: 303, loss is 0.012472705915570259\n",
      "epoch: 6 step: 304, loss is 0.0003228907589800656\n",
      "epoch: 6 step: 305, loss is 0.0039885640144348145\n",
      "epoch: 6 step: 306, loss is 0.025658313184976578\n",
      "epoch: 6 step: 307, loss is 0.10618127137422562\n",
      "epoch: 6 step: 308, loss is 8.83060711203143e-05\n",
      "epoch: 6 step: 309, loss is 0.005349033046513796\n",
      "epoch: 6 step: 310, loss is 0.0008063909481279552\n",
      "epoch: 6 step: 311, loss is 0.025094902142882347\n",
      "epoch: 6 step: 312, loss is 0.010585702024400234\n",
      "epoch: 6 step: 313, loss is 0.000897655903827399\n",
      "epoch: 6 step: 314, loss is 0.00041554984636604786\n",
      "epoch: 6 step: 315, loss is 0.0002987414482049644\n",
      "epoch: 6 step: 316, loss is 0.0016766423359513283\n",
      "epoch: 6 step: 317, loss is 0.0033613620325922966\n",
      "epoch: 6 step: 318, loss is 0.00013947198749519885\n",
      "epoch: 6 step: 319, loss is 0.0047015175223350525\n",
      "epoch: 6 step: 320, loss is 0.0009828219190239906\n",
      "epoch: 6 step: 321, loss is 0.0005248758825473487\n",
      "epoch: 6 step: 322, loss is 0.0001751059462549165\n",
      "epoch: 6 step: 323, loss is 0.0017434178153052926\n",
      "epoch: 6 step: 324, loss is 0.0038510416634380817\n",
      "epoch: 6 step: 325, loss is 0.008899716660380363\n",
      "epoch: 6 step: 326, loss is 0.004198530223220587\n",
      "epoch: 6 step: 327, loss is 0.004208851605653763\n",
      "epoch: 6 step: 328, loss is 0.007685478311032057\n",
      "epoch: 6 step: 329, loss is 0.0434475876390934\n",
      "epoch: 6 step: 330, loss is 0.0011744152288883924\n",
      "epoch: 6 step: 331, loss is 0.09991784393787384\n",
      "epoch: 6 step: 332, loss is 0.00010217988165095448\n",
      "epoch: 6 step: 333, loss is 0.008974113501608372\n",
      "epoch: 6 step: 334, loss is 0.0034176309127360582\n",
      "epoch: 6 step: 335, loss is 9.56770236371085e-05\n",
      "epoch: 6 step: 336, loss is 0.009134321473538876\n",
      "epoch: 6 step: 337, loss is 0.0012800170807167888\n",
      "epoch: 6 step: 338, loss is 0.07610232383012772\n",
      "epoch: 6 step: 339, loss is 0.03402547538280487\n",
      "epoch: 6 step: 340, loss is 0.0002456587099004537\n",
      "epoch: 6 step: 341, loss is 0.000836285122204572\n",
      "epoch: 6 step: 342, loss is 0.011524186469614506\n",
      "epoch: 6 step: 343, loss is 0.02113742008805275\n",
      "epoch: 6 step: 344, loss is 0.0008955444791354239\n",
      "epoch: 6 step: 345, loss is 0.022669166326522827\n",
      "epoch: 6 step: 346, loss is 0.0016399696469306946\n",
      "epoch: 6 step: 347, loss is 0.0002476236259099096\n",
      "epoch: 6 step: 348, loss is 0.003244479186832905\n",
      "epoch: 6 step: 349, loss is 0.20405633747577667\n",
      "epoch: 6 step: 350, loss is 0.01637515053153038\n",
      "epoch: 6 step: 351, loss is 0.007585018873214722\n",
      "epoch: 6 step: 352, loss is 2.195880369981751e-05\n",
      "epoch: 6 step: 353, loss is 4.6627974370494485e-05\n",
      "epoch: 6 step: 354, loss is 0.015539135783910751\n",
      "epoch: 6 step: 355, loss is 0.004074122756719589\n",
      "epoch: 6 step: 356, loss is 0.021251674741506577\n",
      "epoch: 6 step: 357, loss is 0.0016684932634234428\n",
      "epoch: 6 step: 358, loss is 0.000667426735162735\n",
      "epoch: 6 step: 359, loss is 0.08619072288274765\n",
      "epoch: 6 step: 360, loss is 0.01167746726423502\n",
      "epoch: 6 step: 361, loss is 0.00020871676679234952\n",
      "epoch: 6 step: 362, loss is 0.036550216376781464\n",
      "epoch: 6 step: 363, loss is 0.0061602359637618065\n",
      "epoch: 6 step: 364, loss is 0.03776697441935539\n",
      "epoch: 6 step: 365, loss is 0.04357299208641052\n",
      "epoch: 6 step: 366, loss is 0.012270577251911163\n",
      "epoch: 6 step: 367, loss is 0.02066398411989212\n",
      "epoch: 6 step: 368, loss is 0.0030480846762657166\n",
      "epoch: 6 step: 369, loss is 0.0014928431482985616\n",
      "epoch: 6 step: 370, loss is 0.0020788253750652075\n",
      "epoch: 6 step: 371, loss is 0.000332806259393692\n",
      "epoch: 6 step: 372, loss is 0.0012415936216711998\n",
      "epoch: 6 step: 373, loss is 0.14175525307655334\n",
      "epoch: 6 step: 374, loss is 0.07252209633588791\n",
      "epoch: 6 step: 375, loss is 0.0013937492622062564\n",
      "epoch: 6 step: 376, loss is 0.004015663638710976\n",
      "epoch: 6 step: 377, loss is 0.0004175978829152882\n",
      "epoch: 6 step: 378, loss is 0.0032681378070265055\n",
      "epoch: 6 step: 379, loss is 0.0002106258034473285\n",
      "epoch: 6 step: 380, loss is 0.0016988752176985145\n",
      "epoch: 6 step: 381, loss is 0.03810298070311546\n",
      "epoch: 6 step: 382, loss is 0.024322086945176125\n",
      "epoch: 6 step: 383, loss is 0.0001002498174784705\n",
      "epoch: 6 step: 384, loss is 0.009485332295298576\n",
      "epoch: 6 step: 385, loss is 0.0001502647646702826\n",
      "epoch: 6 step: 386, loss is 0.00032490500598214567\n",
      "epoch: 6 step: 387, loss is 0.008985603228211403\n",
      "epoch: 6 step: 388, loss is 7.486920367227867e-05\n",
      "epoch: 6 step: 389, loss is 0.019742798060178757\n",
      "epoch: 6 step: 390, loss is 0.0005661118775606155\n",
      "epoch: 6 step: 391, loss is 8.306891686515883e-05\n",
      "epoch: 6 step: 392, loss is 0.012646771036088467\n",
      "epoch: 6 step: 393, loss is 0.1548130065202713\n",
      "epoch: 6 step: 394, loss is 0.0005182731547392905\n",
      "epoch: 6 step: 395, loss is 0.00018076274136547\n",
      "epoch: 6 step: 396, loss is 0.01191415823996067\n",
      "epoch: 6 step: 397, loss is 0.004765653517097235\n",
      "epoch: 6 step: 398, loss is 0.008977035991847515\n",
      "epoch: 6 step: 399, loss is 0.29904717206954956\n",
      "epoch: 6 step: 400, loss is 0.060074374079704285\n",
      "epoch: 6 step: 401, loss is 0.02843434363603592\n",
      "epoch: 6 step: 402, loss is 0.07580804079771042\n",
      "epoch: 6 step: 403, loss is 0.02256733551621437\n",
      "epoch: 6 step: 404, loss is 0.0031998995691537857\n",
      "epoch: 6 step: 405, loss is 0.012890470214188099\n",
      "epoch: 6 step: 406, loss is 0.03697076067328453\n",
      "epoch: 6 step: 407, loss is 0.056589774787425995\n",
      "epoch: 6 step: 408, loss is 0.002706067403778434\n",
      "epoch: 6 step: 409, loss is 0.030792640522122383\n",
      "epoch: 6 step: 410, loss is 0.015331970527768135\n",
      "epoch: 6 step: 411, loss is 0.0008942400454543531\n",
      "epoch: 6 step: 412, loss is 0.0014576285611838102\n",
      "epoch: 6 step: 413, loss is 0.023908331990242004\n",
      "epoch: 6 step: 414, loss is 0.00015356541553046554\n",
      "epoch: 6 step: 415, loss is 0.013775655999779701\n",
      "epoch: 6 step: 416, loss is 0.0010877371532842517\n",
      "epoch: 6 step: 417, loss is 0.012674584053456783\n",
      "epoch: 6 step: 418, loss is 0.1580992341041565\n",
      "epoch: 6 step: 419, loss is 0.03917667269706726\n",
      "epoch: 6 step: 420, loss is 0.0010358178988099098\n",
      "epoch: 6 step: 421, loss is 0.015563451685011387\n",
      "epoch: 6 step: 422, loss is 0.005014232825487852\n",
      "epoch: 6 step: 423, loss is 0.00329052796587348\n",
      "epoch: 6 step: 424, loss is 0.006850020028650761\n",
      "epoch: 6 step: 425, loss is 0.1816883385181427\n",
      "epoch: 6 step: 426, loss is 0.00511198490858078\n",
      "epoch: 6 step: 427, loss is 0.0257128719240427\n",
      "epoch: 6 step: 428, loss is 0.00022007629740983248\n",
      "epoch: 6 step: 429, loss is 0.0004469514242373407\n",
      "epoch: 6 step: 430, loss is 0.00038560276152566075\n",
      "epoch: 6 step: 431, loss is 0.012066197581589222\n",
      "epoch: 6 step: 432, loss is 0.0039208997040987015\n",
      "epoch: 6 step: 433, loss is 0.0023624880705028772\n",
      "epoch: 6 step: 434, loss is 0.006302594672888517\n",
      "epoch: 6 step: 435, loss is 0.0021792827174067497\n",
      "epoch: 6 step: 436, loss is 0.000687246210873127\n",
      "epoch: 6 step: 437, loss is 0.0002157928392989561\n",
      "epoch: 6 step: 438, loss is 0.004595102742314339\n",
      "epoch: 6 step: 439, loss is 0.0006827290635555983\n",
      "epoch: 6 step: 440, loss is 0.07861188054084778\n",
      "epoch: 6 step: 441, loss is 0.005547303706407547\n",
      "epoch: 6 step: 442, loss is 0.017903685569763184\n",
      "epoch: 6 step: 443, loss is 0.0005932975327596068\n",
      "epoch: 6 step: 444, loss is 0.06550111621618271\n",
      "epoch: 6 step: 445, loss is 0.027339281514286995\n",
      "epoch: 6 step: 446, loss is 0.005239431280642748\n",
      "epoch: 6 step: 447, loss is 0.00684319855645299\n",
      "epoch: 6 step: 448, loss is 0.0008637751452624798\n",
      "epoch: 6 step: 449, loss is 0.00910715851932764\n",
      "epoch: 6 step: 450, loss is 0.0015695864567533135\n",
      "epoch: 6 step: 451, loss is 0.06018497049808502\n",
      "epoch: 6 step: 452, loss is 0.0005657203728333116\n",
      "epoch: 6 step: 453, loss is 0.0033491719514131546\n",
      "epoch: 6 step: 454, loss is 0.004163598176091909\n",
      "epoch: 6 step: 455, loss is 9.83000427368097e-05\n",
      "epoch: 6 step: 456, loss is 0.009303709492087364\n",
      "epoch: 6 step: 457, loss is 0.06295561790466309\n",
      "epoch: 6 step: 458, loss is 0.0009297504439018667\n",
      "epoch: 6 step: 459, loss is 0.015379915945231915\n",
      "epoch: 6 step: 460, loss is 0.036484476178884506\n",
      "epoch: 6 step: 461, loss is 0.10207456350326538\n",
      "epoch: 6 step: 462, loss is 4.132078538532369e-05\n",
      "epoch: 6 step: 463, loss is 0.11081910878419876\n",
      "epoch: 6 step: 464, loss is 0.0007924690726213157\n",
      "epoch: 6 step: 465, loss is 0.0018563360208645463\n",
      "epoch: 6 step: 466, loss is 0.1275126039981842\n",
      "epoch: 6 step: 467, loss is 0.00059344811597839\n",
      "epoch: 6 step: 468, loss is 0.04598750174045563\n",
      "epoch: 6 step: 469, loss is 0.26749885082244873\n",
      "epoch: 6 step: 470, loss is 0.025900624692440033\n",
      "epoch: 6 step: 471, loss is 2.479770228092093e-05\n",
      "epoch: 6 step: 472, loss is 0.007773526478558779\n",
      "epoch: 6 step: 473, loss is 0.019893191754817963\n",
      "epoch: 6 step: 474, loss is 0.0003753250930458307\n",
      "epoch: 6 step: 475, loss is 0.03240874037146568\n",
      "epoch: 6 step: 476, loss is 0.0007744086906313896\n",
      "epoch: 6 step: 477, loss is 0.003257390344515443\n",
      "epoch: 6 step: 478, loss is 0.011554704047739506\n",
      "epoch: 6 step: 479, loss is 0.00031608084100298584\n",
      "epoch: 6 step: 480, loss is 0.017892014235258102\n",
      "epoch: 6 step: 481, loss is 0.00434686616063118\n",
      "epoch: 6 step: 482, loss is 0.0007317936979234219\n",
      "epoch: 6 step: 483, loss is 0.0005008227890357375\n",
      "epoch: 6 step: 484, loss is 0.015151334926486015\n",
      "epoch: 6 step: 485, loss is 0.20034410059452057\n",
      "epoch: 6 step: 486, loss is 0.0066207218915224075\n",
      "epoch: 6 step: 487, loss is 0.035718753933906555\n",
      "epoch: 6 step: 488, loss is 0.0639764815568924\n",
      "epoch: 6 step: 489, loss is 0.13861121237277985\n",
      "epoch: 6 step: 490, loss is 0.007534288801252842\n",
      "epoch: 6 step: 491, loss is 0.005711363162845373\n",
      "epoch: 6 step: 492, loss is 0.007756406906992197\n",
      "epoch: 6 step: 493, loss is 0.00132920709438622\n",
      "epoch: 6 step: 494, loss is 0.0004927451955154538\n",
      "epoch: 6 step: 495, loss is 0.0001982644316740334\n",
      "epoch: 6 step: 496, loss is 0.0003079590678680688\n",
      "epoch: 6 step: 497, loss is 0.03562000021338463\n",
      "epoch: 6 step: 498, loss is 0.0003160675405524671\n",
      "epoch: 6 step: 499, loss is 0.012914729304611683\n",
      "epoch: 6 step: 500, loss is 0.0026107346639037132\n",
      "epoch: 6 step: 501, loss is 0.0008150391513481736\n",
      "epoch: 6 step: 502, loss is 0.02322695031762123\n",
      "epoch: 6 step: 503, loss is 0.22883588075637817\n",
      "epoch: 6 step: 504, loss is 0.002113546244800091\n",
      "epoch: 6 step: 505, loss is 0.002776512410491705\n",
      "epoch: 6 step: 506, loss is 0.0018960910383611917\n",
      "epoch: 6 step: 507, loss is 0.00675513781607151\n",
      "epoch: 6 step: 508, loss is 0.0013991906307637691\n",
      "epoch: 6 step: 509, loss is 0.042254552245140076\n",
      "epoch: 6 step: 510, loss is 0.05292726680636406\n",
      "epoch: 6 step: 511, loss is 0.007049439940601587\n",
      "epoch: 6 step: 512, loss is 0.004119180608540773\n",
      "epoch: 6 step: 513, loss is 0.00044058269122615457\n",
      "epoch: 6 step: 514, loss is 0.002047956921160221\n",
      "epoch: 6 step: 515, loss is 0.00045047642197459936\n",
      "epoch: 6 step: 516, loss is 0.0001407150994054973\n",
      "epoch: 6 step: 517, loss is 0.005114126950502396\n",
      "epoch: 6 step: 518, loss is 0.127933070063591\n",
      "epoch: 6 step: 519, loss is 0.15518781542778015\n",
      "epoch: 6 step: 520, loss is 0.08859345316886902\n",
      "epoch: 6 step: 521, loss is 0.03768894821405411\n",
      "epoch: 6 step: 522, loss is 0.013165870681405067\n",
      "epoch: 6 step: 523, loss is 0.001624277443625033\n",
      "epoch: 6 step: 524, loss is 0.0008530368213541806\n",
      "epoch: 6 step: 525, loss is 0.007483846042305231\n",
      "epoch: 6 step: 526, loss is 0.0009103242191486061\n",
      "epoch: 6 step: 527, loss is 0.00020821143698412925\n",
      "epoch: 6 step: 528, loss is 0.014537634328007698\n",
      "epoch: 6 step: 529, loss is 0.00019679406250361353\n",
      "epoch: 6 step: 530, loss is 0.0007339372532442212\n",
      "epoch: 6 step: 531, loss is 0.0031272750347852707\n",
      "epoch: 6 step: 532, loss is 0.0009441602160222828\n",
      "epoch: 6 step: 533, loss is 0.14750319719314575\n",
      "epoch: 6 step: 534, loss is 0.0004753305984195322\n",
      "epoch: 6 step: 535, loss is 0.0034055730793625116\n",
      "epoch: 6 step: 536, loss is 0.0002448284940328449\n",
      "epoch: 6 step: 537, loss is 0.022963501513004303\n",
      "epoch: 6 step: 538, loss is 0.002265540650114417\n",
      "epoch: 6 step: 539, loss is 0.0039286259561777115\n",
      "epoch: 6 step: 540, loss is 0.0007011682027950883\n",
      "epoch: 6 step: 541, loss is 0.017232336103916168\n",
      "epoch: 6 step: 542, loss is 0.012406833469867706\n",
      "epoch: 6 step: 543, loss is 0.02949119545519352\n",
      "epoch: 6 step: 544, loss is 0.11280132830142975\n",
      "epoch: 6 step: 545, loss is 0.022600451484322548\n",
      "epoch: 6 step: 546, loss is 0.0898386612534523\n",
      "epoch: 6 step: 547, loss is 0.1162448450922966\n",
      "epoch: 6 step: 548, loss is 0.06873609870672226\n",
      "epoch: 6 step: 549, loss is 0.0142770791426301\n",
      "epoch: 6 step: 550, loss is 0.0033992608077824116\n",
      "epoch: 6 step: 551, loss is 0.0003560568147804588\n",
      "epoch: 6 step: 552, loss is 0.010673352517187595\n",
      "epoch: 6 step: 553, loss is 0.019870173186063766\n",
      "epoch: 6 step: 554, loss is 0.020214736461639404\n",
      "epoch: 6 step: 555, loss is 0.04194151982665062\n",
      "epoch: 6 step: 556, loss is 0.008576753549277782\n",
      "epoch: 6 step: 557, loss is 0.0004954984178766608\n",
      "epoch: 6 step: 558, loss is 0.0025806662160903215\n",
      "epoch: 6 step: 559, loss is 0.010770863853394985\n",
      "epoch: 6 step: 560, loss is 0.005097555462270975\n",
      "epoch: 6 step: 561, loss is 0.027505790814757347\n",
      "epoch: 6 step: 562, loss is 0.007461078930646181\n",
      "epoch: 6 step: 563, loss is 0.0005248608067631721\n",
      "epoch: 6 step: 564, loss is 0.10373065620660782\n",
      "epoch: 6 step: 565, loss is 0.00023421824153047055\n",
      "epoch: 6 step: 566, loss is 0.0016966514522209764\n",
      "epoch: 6 step: 567, loss is 0.04255642741918564\n",
      "epoch: 6 step: 568, loss is 0.038643453270196915\n",
      "epoch: 6 step: 569, loss is 0.002550239209085703\n",
      "epoch: 6 step: 570, loss is 0.19358979165554047\n",
      "epoch: 6 step: 571, loss is 0.07084079831838608\n",
      "epoch: 6 step: 572, loss is 0.14242032170295715\n",
      "epoch: 6 step: 573, loss is 0.09343323856592178\n",
      "epoch: 6 step: 574, loss is 0.00027767641586251557\n",
      "epoch: 6 step: 575, loss is 0.008943273685872555\n",
      "epoch: 6 step: 576, loss is 0.01909039355814457\n",
      "epoch: 6 step: 577, loss is 0.013950591906905174\n",
      "epoch: 6 step: 578, loss is 0.00038785833748988807\n",
      "epoch: 6 step: 579, loss is 0.0024337531067430973\n",
      "epoch: 6 step: 580, loss is 0.017878428101539612\n",
      "epoch: 6 step: 581, loss is 0.0019936985336244106\n",
      "epoch: 6 step: 582, loss is 0.003127496922388673\n",
      "epoch: 6 step: 583, loss is 0.014205025508999825\n",
      "epoch: 6 step: 584, loss is 0.06668290495872498\n",
      "epoch: 6 step: 585, loss is 0.000641190679743886\n",
      "epoch: 6 step: 586, loss is 0.003467598930001259\n",
      "epoch: 6 step: 587, loss is 0.0019379581790417433\n",
      "epoch: 6 step: 588, loss is 0.0016911893617361784\n",
      "epoch: 6 step: 589, loss is 0.02566920593380928\n",
      "epoch: 6 step: 590, loss is 0.00019576730846893042\n",
      "epoch: 6 step: 591, loss is 0.11705700308084488\n",
      "epoch: 6 step: 592, loss is 0.07711189985275269\n",
      "epoch: 6 step: 593, loss is 0.012422044761478901\n",
      "epoch: 6 step: 594, loss is 0.0009103385964408517\n",
      "epoch: 6 step: 595, loss is 0.0596904531121254\n",
      "epoch: 6 step: 596, loss is 0.0005572220543399453\n",
      "epoch: 6 step: 597, loss is 0.0008523346623405814\n",
      "epoch: 6 step: 598, loss is 0.00036917379475198686\n",
      "epoch: 6 step: 599, loss is 0.044472794979810715\n",
      "epoch: 6 step: 600, loss is 0.019086090847849846\n",
      "epoch: 6 step: 601, loss is 0.09174352139234543\n",
      "epoch: 6 step: 602, loss is 0.008753884583711624\n",
      "epoch: 6 step: 603, loss is 0.0007709974888712168\n",
      "epoch: 6 step: 604, loss is 0.06422106176614761\n",
      "epoch: 6 step: 605, loss is 0.012674779631197453\n",
      "epoch: 6 step: 606, loss is 0.022218303754925728\n",
      "epoch: 6 step: 607, loss is 0.057877201586961746\n",
      "epoch: 6 step: 608, loss is 0.02032979391515255\n",
      "epoch: 6 step: 609, loss is 0.003920522518455982\n",
      "epoch: 6 step: 610, loss is 0.0026442143134772778\n",
      "epoch: 6 step: 611, loss is 0.043622054159641266\n",
      "epoch: 6 step: 612, loss is 0.026566889137029648\n",
      "epoch: 6 step: 613, loss is 0.004127602558583021\n",
      "epoch: 6 step: 614, loss is 0.03246309608221054\n",
      "epoch: 6 step: 615, loss is 0.10384432226419449\n",
      "epoch: 6 step: 616, loss is 0.23737846314907074\n",
      "epoch: 6 step: 617, loss is 0.0035895544569939375\n",
      "epoch: 6 step: 618, loss is 0.00413105171173811\n",
      "epoch: 6 step: 619, loss is 0.1207294687628746\n",
      "epoch: 6 step: 620, loss is 0.010038352571427822\n",
      "epoch: 6 step: 621, loss is 0.0018938479479402304\n",
      "epoch: 6 step: 622, loss is 0.004555747844278812\n",
      "epoch: 6 step: 623, loss is 0.13897979259490967\n",
      "epoch: 6 step: 624, loss is 0.014364763163030148\n",
      "epoch: 6 step: 625, loss is 0.09422378987073898\n",
      "epoch: 6 step: 626, loss is 0.04074005037546158\n",
      "epoch: 6 step: 627, loss is 0.006298633757978678\n",
      "epoch: 6 step: 628, loss is 0.0021549323573708534\n",
      "epoch: 6 step: 629, loss is 0.0005889305612072349\n",
      "epoch: 6 step: 630, loss is 0.006087238434702158\n",
      "epoch: 6 step: 631, loss is 0.0019166599959135056\n",
      "epoch: 6 step: 632, loss is 0.12225481867790222\n",
      "epoch: 6 step: 633, loss is 0.0014150673523545265\n",
      "epoch: 6 step: 634, loss is 0.12352221459150314\n",
      "epoch: 6 step: 635, loss is 0.008322012610733509\n",
      "epoch: 6 step: 636, loss is 0.0003453396784607321\n",
      "epoch: 6 step: 637, loss is 0.0002856120700016618\n",
      "epoch: 6 step: 638, loss is 0.03497909754514694\n",
      "epoch: 6 step: 639, loss is 0.09407038986682892\n",
      "epoch: 6 step: 640, loss is 0.003934949170798063\n",
      "epoch: 6 step: 641, loss is 0.00371667486615479\n",
      "epoch: 6 step: 642, loss is 0.029864219948649406\n",
      "epoch: 6 step: 643, loss is 0.03962577506899834\n",
      "epoch: 6 step: 644, loss is 0.007125128526240587\n",
      "epoch: 6 step: 645, loss is 0.011401214636862278\n",
      "epoch: 6 step: 646, loss is 0.0024481071159243584\n",
      "epoch: 6 step: 647, loss is 0.018280569463968277\n",
      "epoch: 6 step: 648, loss is 0.006222390569746494\n",
      "epoch: 6 step: 649, loss is 0.23801052570343018\n",
      "epoch: 6 step: 650, loss is 0.09761685132980347\n",
      "epoch: 6 step: 651, loss is 0.0034080459736287594\n",
      "epoch: 6 step: 652, loss is 0.0879516452550888\n",
      "epoch: 6 step: 653, loss is 0.023884063586592674\n",
      "epoch: 6 step: 654, loss is 0.06756915897130966\n",
      "epoch: 6 step: 655, loss is 0.0009548742673359811\n",
      "epoch: 6 step: 656, loss is 0.004457967355847359\n",
      "epoch: 6 step: 657, loss is 0.022452939301729202\n",
      "epoch: 6 step: 658, loss is 0.0006721632089465857\n",
      "epoch: 6 step: 659, loss is 0.003092898055911064\n",
      "epoch: 6 step: 660, loss is 0.013012265786528587\n",
      "epoch: 6 step: 661, loss is 0.0027211082633584738\n",
      "epoch: 6 step: 662, loss is 0.04210694134235382\n",
      "epoch: 6 step: 663, loss is 0.00042522780131548643\n",
      "epoch: 6 step: 664, loss is 0.0034583033993840218\n",
      "epoch: 6 step: 665, loss is 0.015456988476216793\n",
      "epoch: 6 step: 666, loss is 0.02595018967986107\n",
      "epoch: 6 step: 667, loss is 0.064752496778965\n",
      "epoch: 6 step: 668, loss is 0.0013081873767077923\n",
      "epoch: 6 step: 669, loss is 0.0006308415322564542\n",
      "epoch: 6 step: 670, loss is 0.0019344313768669963\n",
      "epoch: 6 step: 671, loss is 0.0003324825956951827\n",
      "epoch: 6 step: 672, loss is 0.0020551409106701612\n",
      "epoch: 6 step: 673, loss is 0.0007903323275968432\n",
      "epoch: 6 step: 674, loss is 0.011992158368229866\n",
      "epoch: 6 step: 675, loss is 0.20925375819206238\n",
      "epoch: 6 step: 676, loss is 0.0012646703980863094\n",
      "epoch: 6 step: 677, loss is 0.10406872630119324\n",
      "epoch: 6 step: 678, loss is 0.030360843986272812\n",
      "epoch: 6 step: 679, loss is 0.04085749015212059\n",
      "epoch: 6 step: 680, loss is 0.0052181147038936615\n",
      "epoch: 6 step: 681, loss is 0.1224994882941246\n",
      "epoch: 6 step: 682, loss is 0.012485572136938572\n",
      "epoch: 6 step: 683, loss is 0.14826981723308563\n",
      "epoch: 6 step: 684, loss is 0.10225225985050201\n",
      "epoch: 6 step: 685, loss is 0.0006573401624336839\n",
      "epoch: 6 step: 686, loss is 0.00011376268230378628\n",
      "epoch: 6 step: 687, loss is 0.006657587364315987\n",
      "epoch: 6 step: 688, loss is 0.00031911011319607496\n",
      "epoch: 6 step: 689, loss is 0.010110468603670597\n",
      "epoch: 6 step: 690, loss is 0.006810224149376154\n",
      "epoch: 6 step: 691, loss is 0.009346007369458675\n",
      "epoch: 6 step: 692, loss is 0.008949165232479572\n",
      "epoch: 6 step: 693, loss is 0.0008578984998166561\n",
      "epoch: 6 step: 694, loss is 0.0002812212915159762\n",
      "epoch: 6 step: 695, loss is 0.06714557856321335\n",
      "epoch: 6 step: 696, loss is 0.0004131629248149693\n",
      "epoch: 6 step: 697, loss is 0.010015128180384636\n",
      "epoch: 6 step: 698, loss is 0.16980013251304626\n",
      "epoch: 6 step: 699, loss is 0.0010239671682938933\n",
      "epoch: 6 step: 700, loss is 0.0024184128269553185\n",
      "epoch: 6 step: 701, loss is 0.010456415824592113\n",
      "epoch: 6 step: 702, loss is 0.05782679468393326\n",
      "epoch: 6 step: 703, loss is 0.012805204838514328\n",
      "epoch: 6 step: 704, loss is 0.022792328149080276\n",
      "epoch: 6 step: 705, loss is 0.10476282984018326\n",
      "epoch: 6 step: 706, loss is 0.001203893218189478\n",
      "epoch: 6 step: 707, loss is 0.0032085443381220102\n",
      "epoch: 6 step: 708, loss is 0.0018785004504024982\n",
      "epoch: 6 step: 709, loss is 0.026440991088747978\n",
      "epoch: 6 step: 710, loss is 0.0098233912140131\n",
      "epoch: 6 step: 711, loss is 0.0060678161680698395\n",
      "epoch: 6 step: 712, loss is 0.0019156776834279299\n",
      "epoch: 6 step: 713, loss is 0.0010570923332124949\n",
      "epoch: 6 step: 714, loss is 0.007559489458799362\n",
      "epoch: 6 step: 715, loss is 0.013284323737025261\n",
      "epoch: 6 step: 716, loss is 0.01207173615694046\n",
      "epoch: 6 step: 717, loss is 0.0006494419649243355\n",
      "epoch: 6 step: 718, loss is 0.0007820824394002557\n",
      "epoch: 6 step: 719, loss is 0.0002841929963324219\n",
      "epoch: 6 step: 720, loss is 0.004539217334240675\n",
      "epoch: 6 step: 721, loss is 7.606765575474128e-05\n",
      "epoch: 6 step: 722, loss is 0.0026460622902959585\n",
      "epoch: 6 step: 723, loss is 0.0015362587291747332\n",
      "epoch: 6 step: 724, loss is 0.07667013257741928\n",
      "epoch: 6 step: 725, loss is 0.00034509689430706203\n",
      "epoch: 6 step: 726, loss is 0.018416788429021835\n",
      "epoch: 6 step: 727, loss is 0.0019284457666799426\n",
      "epoch: 6 step: 728, loss is 0.0011386730475351214\n",
      "epoch: 6 step: 729, loss is 7.678111433051527e-05\n",
      "epoch: 6 step: 730, loss is 0.0002173201646655798\n",
      "epoch: 6 step: 731, loss is 0.035438619554042816\n",
      "epoch: 6 step: 732, loss is 0.0046484628692269325\n",
      "epoch: 6 step: 733, loss is 0.05039658397436142\n",
      "epoch: 6 step: 734, loss is 0.010221236385405064\n",
      "epoch: 6 step: 735, loss is 0.001726897549815476\n",
      "epoch: 6 step: 736, loss is 0.0002795160689856857\n",
      "epoch: 6 step: 737, loss is 0.0469822958111763\n",
      "epoch: 6 step: 738, loss is 0.00016194878844544291\n",
      "epoch: 6 step: 739, loss is 0.05259500816464424\n",
      "epoch: 6 step: 740, loss is 0.07140614837408066\n",
      "epoch: 6 step: 741, loss is 0.004373855423182249\n",
      "epoch: 6 step: 742, loss is 0.0007290943758562207\n",
      "epoch: 6 step: 743, loss is 0.0015525451162829995\n",
      "epoch: 6 step: 744, loss is 0.00024905113968998194\n",
      "epoch: 6 step: 745, loss is 0.017924193292856216\n",
      "epoch: 6 step: 746, loss is 0.0039055875968188047\n",
      "epoch: 6 step: 747, loss is 0.01626182720065117\n",
      "epoch: 6 step: 748, loss is 0.0002794007887132466\n",
      "epoch: 6 step: 749, loss is 0.005255597177892923\n",
      "epoch: 6 step: 750, loss is 0.06378254294395447\n",
      "epoch: 6 step: 751, loss is 0.002852688543498516\n",
      "epoch: 6 step: 752, loss is 0.0003298021911177784\n",
      "epoch: 6 step: 753, loss is 0.027055921033024788\n",
      "epoch: 6 step: 754, loss is 0.005973167717456818\n",
      "epoch: 6 step: 755, loss is 0.1650335192680359\n",
      "epoch: 6 step: 756, loss is 0.0008350071148015559\n",
      "epoch: 6 step: 757, loss is 0.0011204418260604143\n",
      "epoch: 6 step: 758, loss is 0.0011935961665585637\n",
      "epoch: 6 step: 759, loss is 0.0009850377682596445\n",
      "epoch: 6 step: 760, loss is 0.00182187813334167\n",
      "epoch: 6 step: 761, loss is 0.0007540112710557878\n",
      "epoch: 6 step: 762, loss is 0.003729102900251746\n",
      "epoch: 6 step: 763, loss is 0.03258517012000084\n",
      "epoch: 6 step: 764, loss is 0.12773235142230988\n",
      "epoch: 6 step: 765, loss is 0.0003308046143501997\n",
      "epoch: 6 step: 766, loss is 0.019233936443924904\n",
      "epoch: 6 step: 767, loss is 0.0005777793703600764\n",
      "epoch: 6 step: 768, loss is 0.006010417360812426\n",
      "epoch: 6 step: 769, loss is 0.013596548698842525\n",
      "epoch: 6 step: 770, loss is 0.0035318960435688496\n",
      "epoch: 6 step: 771, loss is 0.004897833336144686\n",
      "epoch: 6 step: 772, loss is 0.01691943034529686\n",
      "epoch: 6 step: 773, loss is 0.0649288073182106\n",
      "epoch: 6 step: 774, loss is 0.02217276394367218\n",
      "epoch: 6 step: 775, loss is 0.0010003806091845036\n",
      "epoch: 6 step: 776, loss is 0.18162067234516144\n",
      "epoch: 6 step: 777, loss is 0.0005424230475910008\n",
      "epoch: 6 step: 778, loss is 0.19423681497573853\n",
      "epoch: 6 step: 779, loss is 0.07088954001665115\n",
      "epoch: 6 step: 780, loss is 0.00030444114236161113\n",
      "epoch: 6 step: 781, loss is 0.02452022023499012\n",
      "epoch: 6 step: 782, loss is 0.058123935014009476\n",
      "epoch: 6 step: 783, loss is 0.06573157757520676\n",
      "epoch: 6 step: 784, loss is 0.001272480352781713\n",
      "epoch: 6 step: 785, loss is 0.0036043778527528048\n",
      "epoch: 6 step: 786, loss is 0.022125106304883957\n",
      "epoch: 6 step: 787, loss is 0.002481770468875766\n",
      "epoch: 6 step: 788, loss is 0.004186323378235102\n",
      "epoch: 6 step: 789, loss is 0.03404572233557701\n",
      "epoch: 6 step: 790, loss is 0.0017721366602927446\n",
      "epoch: 6 step: 791, loss is 0.03417842090129852\n",
      "epoch: 6 step: 792, loss is 0.0014145211316645145\n",
      "epoch: 6 step: 793, loss is 0.0018603875068947673\n",
      "epoch: 6 step: 794, loss is 0.0022320107091218233\n",
      "epoch: 6 step: 795, loss is 0.03925462067127228\n",
      "epoch: 6 step: 796, loss is 0.003617271315306425\n",
      "epoch: 6 step: 797, loss is 0.026919512078166008\n",
      "epoch: 6 step: 798, loss is 0.013078422285616398\n",
      "epoch: 6 step: 799, loss is 0.021850483492016792\n",
      "epoch: 6 step: 800, loss is 0.008958917111158371\n",
      "epoch: 6 step: 801, loss is 0.006621160078793764\n",
      "epoch: 6 step: 802, loss is 0.06982141733169556\n",
      "epoch: 6 step: 803, loss is 0.06553608179092407\n",
      "epoch: 6 step: 804, loss is 0.0003628381236921996\n",
      "epoch: 6 step: 805, loss is 0.0011086693266406655\n",
      "epoch: 6 step: 806, loss is 4.235438609612174e-05\n",
      "epoch: 6 step: 807, loss is 0.0006445811013691127\n",
      "epoch: 6 step: 808, loss is 0.00026439395151101053\n",
      "epoch: 6 step: 809, loss is 0.0008474581409245729\n",
      "epoch: 6 step: 810, loss is 2.1754036424681544e-05\n",
      "epoch: 6 step: 811, loss is 0.052521415054798126\n",
      "epoch: 6 step: 812, loss is 0.0011447089491412044\n",
      "epoch: 6 step: 813, loss is 0.005682986229658127\n",
      "epoch: 6 step: 814, loss is 0.0022354796528816223\n",
      "epoch: 6 step: 815, loss is 0.00021247936820145696\n",
      "epoch: 6 step: 816, loss is 0.0014001487288624048\n",
      "epoch: 6 step: 817, loss is 0.0015796927036717534\n",
      "epoch: 6 step: 818, loss is 0.025316715240478516\n",
      "epoch: 6 step: 819, loss is 0.00021651556016877294\n",
      "epoch: 6 step: 820, loss is 0.0002569934877101332\n",
      "epoch: 6 step: 821, loss is 0.0006485696649178863\n",
      "epoch: 6 step: 822, loss is 0.002293437020853162\n",
      "epoch: 6 step: 823, loss is 0.00034578979830257595\n",
      "epoch: 6 step: 824, loss is 0.0009225838584825397\n",
      "epoch: 6 step: 825, loss is 0.07618965208530426\n",
      "epoch: 6 step: 826, loss is 0.0018103077309206128\n",
      "epoch: 6 step: 827, loss is 0.06459186226129532\n",
      "epoch: 6 step: 828, loss is 0.002590257441624999\n",
      "epoch: 6 step: 829, loss is 0.0028773352969437838\n",
      "epoch: 6 step: 830, loss is 0.00893133133649826\n",
      "epoch: 6 step: 831, loss is 0.0010705917375162244\n",
      "epoch: 6 step: 832, loss is 0.0033424554858356714\n",
      "epoch: 6 step: 833, loss is 0.017996979877352715\n",
      "epoch: 6 step: 834, loss is 0.0006498818402178586\n",
      "epoch: 6 step: 835, loss is 0.1228930801153183\n",
      "epoch: 6 step: 836, loss is 0.00030902677099220455\n",
      "epoch: 6 step: 837, loss is 0.05208227038383484\n",
      "epoch: 6 step: 838, loss is 0.012501446530222893\n",
      "epoch: 6 step: 839, loss is 0.020299365743994713\n",
      "epoch: 6 step: 840, loss is 0.0001193723437609151\n",
      "epoch: 6 step: 841, loss is 0.0020624238532036543\n",
      "epoch: 6 step: 842, loss is 0.00234954128973186\n",
      "epoch: 6 step: 843, loss is 3.02166063192999e-05\n",
      "epoch: 6 step: 844, loss is 0.1230684369802475\n",
      "epoch: 6 step: 845, loss is 0.0006582484347745776\n",
      "epoch: 6 step: 846, loss is 0.0007455058512277901\n",
      "epoch: 6 step: 847, loss is 0.0439605675637722\n",
      "epoch: 6 step: 848, loss is 0.00024364484124816954\n",
      "epoch: 6 step: 849, loss is 0.005922900978475809\n",
      "epoch: 6 step: 850, loss is 6.355764344334602e-05\n",
      "epoch: 6 step: 851, loss is 0.028575606644153595\n",
      "epoch: 6 step: 852, loss is 0.07289039343595505\n",
      "epoch: 6 step: 853, loss is 0.005107194650918245\n",
      "epoch: 6 step: 854, loss is 0.009488115087151527\n",
      "epoch: 6 step: 855, loss is 0.009446099400520325\n",
      "epoch: 6 step: 856, loss is 0.0006788753089495003\n",
      "epoch: 6 step: 857, loss is 0.0005737683386541903\n",
      "epoch: 6 step: 858, loss is 0.0006488829967565835\n",
      "epoch: 6 step: 859, loss is 0.0005208199145272374\n",
      "epoch: 6 step: 860, loss is 0.0015252180164679885\n",
      "epoch: 6 step: 861, loss is 8.965838060248643e-05\n",
      "epoch: 6 step: 862, loss is 0.0005229728994891047\n",
      "epoch: 6 step: 863, loss is 0.0015508868964388967\n",
      "epoch: 6 step: 864, loss is 0.0032997550442814827\n",
      "epoch: 6 step: 865, loss is 0.16397911310195923\n",
      "epoch: 6 step: 866, loss is 0.003432106226682663\n",
      "epoch: 6 step: 867, loss is 0.002406549174338579\n",
      "epoch: 6 step: 868, loss is 0.003103457158431411\n",
      "epoch: 6 step: 869, loss is 0.1331401765346527\n",
      "epoch: 6 step: 870, loss is 0.0002655946009326726\n",
      "epoch: 6 step: 871, loss is 0.005490612238645554\n",
      "epoch: 6 step: 872, loss is 0.15474922955036163\n",
      "epoch: 6 step: 873, loss is 0.00013865527580492198\n",
      "epoch: 6 step: 874, loss is 0.0003801861312240362\n",
      "epoch: 6 step: 875, loss is 8.428336150245741e-05\n",
      "epoch: 6 step: 876, loss is 0.02097024768590927\n",
      "epoch: 6 step: 877, loss is 0.015755392611026764\n",
      "epoch: 6 step: 878, loss is 0.0022113558370620012\n",
      "epoch: 6 step: 879, loss is 0.03341520205140114\n",
      "epoch: 6 step: 880, loss is 0.01091779861599207\n",
      "epoch: 6 step: 881, loss is 0.0032015424221754074\n",
      "epoch: 6 step: 882, loss is 0.2543480694293976\n",
      "epoch: 6 step: 883, loss is 0.020737914368510246\n",
      "epoch: 6 step: 884, loss is 0.0017624705797061324\n",
      "epoch: 6 step: 885, loss is 0.0048739234916865826\n",
      "epoch: 6 step: 886, loss is 0.0014692601980641484\n",
      "epoch: 6 step: 887, loss is 0.0023712420370429754\n",
      "epoch: 6 step: 888, loss is 0.003964713308960199\n",
      "epoch: 6 step: 889, loss is 0.002058411715552211\n",
      "epoch: 6 step: 890, loss is 0.07793456315994263\n",
      "epoch: 6 step: 891, loss is 0.0014892980689182878\n",
      "epoch: 6 step: 892, loss is 0.13967503607273102\n",
      "epoch: 6 step: 893, loss is 0.21951797604560852\n",
      "epoch: 6 step: 894, loss is 0.03314494341611862\n",
      "epoch: 6 step: 895, loss is 0.0250228401273489\n",
      "epoch: 6 step: 896, loss is 0.00347128021530807\n",
      "epoch: 6 step: 897, loss is 0.0005339615163393319\n",
      "epoch: 6 step: 898, loss is 0.0003613501030486077\n",
      "epoch: 6 step: 899, loss is 0.07944300770759583\n",
      "epoch: 6 step: 900, loss is 0.006847280077636242\n",
      "epoch: 6 step: 901, loss is 0.021056899800896645\n",
      "epoch: 6 step: 902, loss is 0.023819155991077423\n",
      "epoch: 6 step: 903, loss is 0.0026324819773435593\n",
      "epoch: 6 step: 904, loss is 0.004167540464550257\n",
      "epoch: 6 step: 905, loss is 0.007705975789576769\n",
      "epoch: 6 step: 906, loss is 0.013787690550088882\n",
      "epoch: 6 step: 907, loss is 0.010008326731622219\n",
      "epoch: 6 step: 908, loss is 0.06807820498943329\n",
      "epoch: 6 step: 909, loss is 0.0018738157814368606\n",
      "epoch: 6 step: 910, loss is 0.025043386965990067\n",
      "epoch: 6 step: 911, loss is 0.001629114500246942\n",
      "epoch: 6 step: 912, loss is 0.0009571833652444184\n",
      "epoch: 6 step: 913, loss is 0.0010157827055081725\n",
      "epoch: 6 step: 914, loss is 0.014440891332924366\n",
      "epoch: 6 step: 915, loss is 0.007893625646829605\n",
      "epoch: 6 step: 916, loss is 0.0002729647094383836\n",
      "epoch: 6 step: 917, loss is 0.0182685274630785\n",
      "epoch: 6 step: 918, loss is 0.0008206465863622725\n",
      "epoch: 6 step: 919, loss is 0.1760830581188202\n",
      "epoch: 6 step: 920, loss is 0.008823733776807785\n",
      "epoch: 6 step: 921, loss is 0.013536364771425724\n",
      "epoch: 6 step: 922, loss is 0.0830947756767273\n",
      "epoch: 6 step: 923, loss is 0.002251917729154229\n",
      "epoch: 6 step: 924, loss is 0.007056136149913073\n",
      "epoch: 6 step: 925, loss is 0.0004548879514914006\n",
      "epoch: 6 step: 926, loss is 0.011506457813084126\n",
      "epoch: 6 step: 927, loss is 0.009573673829436302\n",
      "epoch: 6 step: 928, loss is 0.09022950381040573\n",
      "epoch: 6 step: 929, loss is 0.017930805683135986\n",
      "epoch: 6 step: 930, loss is 0.003461699467152357\n",
      "epoch: 6 step: 931, loss is 0.10420443117618561\n",
      "epoch: 6 step: 932, loss is 0.03585883975028992\n",
      "epoch: 6 step: 933, loss is 0.00425803754478693\n",
      "epoch: 6 step: 934, loss is 0.03987082466483116\n",
      "epoch: 6 step: 935, loss is 0.0025781390722841024\n",
      "epoch: 6 step: 936, loss is 0.07566021382808685\n",
      "epoch: 6 step: 937, loss is 0.00130524265114218\n",
      "epoch: 6 step: 938, loss is 0.0009238238562829792\n",
      "epoch: 6 step: 939, loss is 0.012796701863408089\n",
      "epoch: 6 step: 940, loss is 0.09749643504619598\n",
      "epoch: 6 step: 941, loss is 0.06875501573085785\n",
      "epoch: 6 step: 942, loss is 0.0012216520262882113\n",
      "epoch: 6 step: 943, loss is 0.0016015962464734912\n",
      "epoch: 6 step: 944, loss is 0.0004880180349573493\n",
      "epoch: 6 step: 945, loss is 0.09845568239688873\n",
      "epoch: 6 step: 946, loss is 8.911543409340084e-05\n",
      "epoch: 6 step: 947, loss is 0.0011598202399909496\n",
      "epoch: 6 step: 948, loss is 0.002252238802611828\n",
      "epoch: 6 step: 949, loss is 0.010109363123774529\n",
      "epoch: 6 step: 950, loss is 0.12357461452484131\n",
      "epoch: 6 step: 951, loss is 0.009179468266665936\n",
      "epoch: 6 step: 952, loss is 0.0021233430597931147\n",
      "epoch: 6 step: 953, loss is 0.0012611282290890813\n",
      "epoch: 6 step: 954, loss is 0.0001265746250282973\n",
      "epoch: 6 step: 955, loss is 0.005539140198379755\n",
      "epoch: 6 step: 956, loss is 0.0027345605194568634\n",
      "epoch: 6 step: 957, loss is 0.00016857973241712898\n",
      "epoch: 6 step: 958, loss is 0.037848878651857376\n",
      "epoch: 6 step: 959, loss is 0.19367268681526184\n",
      "epoch: 6 step: 960, loss is 0.00038557976949959993\n",
      "epoch: 6 step: 961, loss is 0.000980702112428844\n",
      "epoch: 6 step: 962, loss is 0.0005005942075513303\n",
      "epoch: 6 step: 963, loss is 0.012025462463498116\n",
      "epoch: 6 step: 964, loss is 0.0876399576663971\n",
      "epoch: 6 step: 965, loss is 0.018456537276506424\n",
      "epoch: 6 step: 966, loss is 0.05920039862394333\n",
      "epoch: 6 step: 967, loss is 0.09933321177959442\n",
      "epoch: 6 step: 968, loss is 0.08380764722824097\n",
      "epoch: 6 step: 969, loss is 0.002665453590452671\n",
      "epoch: 6 step: 970, loss is 0.0008912582998163998\n",
      "epoch: 6 step: 971, loss is 0.0003779576509259641\n",
      "epoch: 6 step: 972, loss is 0.11753267794847488\n",
      "epoch: 6 step: 973, loss is 0.0007164530106820166\n",
      "epoch: 6 step: 974, loss is 0.0008443898987025023\n",
      "epoch: 6 step: 975, loss is 0.010579175315797329\n",
      "epoch: 6 step: 976, loss is 0.00571422278881073\n",
      "epoch: 6 step: 977, loss is 0.012250598520040512\n",
      "epoch: 6 step: 978, loss is 0.01680537313222885\n",
      "epoch: 6 step: 979, loss is 0.0003085387870669365\n",
      "epoch: 6 step: 980, loss is 0.022356558591127396\n",
      "epoch: 6 step: 981, loss is 0.007201315835118294\n",
      "epoch: 6 step: 982, loss is 0.0033145237248390913\n",
      "epoch: 6 step: 983, loss is 0.00592318270355463\n",
      "epoch: 6 step: 984, loss is 0.0077728768810629845\n",
      "epoch: 6 step: 985, loss is 0.06297706067562103\n",
      "epoch: 6 step: 986, loss is 0.003664000891149044\n",
      "epoch: 6 step: 987, loss is 0.0024938848800957203\n",
      "epoch: 6 step: 988, loss is 0.005277466494590044\n",
      "epoch: 6 step: 989, loss is 0.09992039948701859\n",
      "epoch: 6 step: 990, loss is 0.000699653581250459\n",
      "epoch: 6 step: 991, loss is 0.0014654910191893578\n",
      "epoch: 6 step: 992, loss is 0.012503230944275856\n",
      "epoch: 6 step: 993, loss is 0.004485752899199724\n",
      "epoch: 6 step: 994, loss is 0.023656286299228668\n",
      "epoch: 6 step: 995, loss is 0.006356758996844292\n",
      "epoch: 6 step: 996, loss is 0.018239112570881844\n",
      "epoch: 6 step: 997, loss is 0.00936522800475359\n",
      "epoch: 6 step: 998, loss is 0.002875932725146413\n",
      "epoch: 6 step: 999, loss is 0.006379046477377415\n",
      "epoch: 6 step: 1000, loss is 0.10207030177116394\n",
      "epoch: 6 step: 1001, loss is 0.06571419537067413\n",
      "epoch: 6 step: 1002, loss is 0.012852394953370094\n",
      "epoch: 6 step: 1003, loss is 0.03804238140583038\n",
      "epoch: 6 step: 1004, loss is 0.01690688356757164\n",
      "epoch: 6 step: 1005, loss is 0.016675300896167755\n",
      "epoch: 6 step: 1006, loss is 0.17256887257099152\n",
      "epoch: 6 step: 1007, loss is 0.004448982886970043\n",
      "epoch: 6 step: 1008, loss is 0.021962203085422516\n",
      "epoch: 6 step: 1009, loss is 0.0041024950332939625\n",
      "epoch: 6 step: 1010, loss is 0.010114147327840328\n",
      "epoch: 6 step: 1011, loss is 0.006683520041406155\n",
      "epoch: 6 step: 1012, loss is 0.0009627799736335874\n",
      "epoch: 6 step: 1013, loss is 0.00038581524859182537\n",
      "epoch: 6 step: 1014, loss is 0.02881709858775139\n",
      "epoch: 6 step: 1015, loss is 0.00011693570559145883\n",
      "epoch: 6 step: 1016, loss is 0.0025715921074151993\n",
      "epoch: 6 step: 1017, loss is 0.0180999543517828\n",
      "epoch: 6 step: 1018, loss is 0.054331813007593155\n",
      "epoch: 6 step: 1019, loss is 0.004831420257687569\n",
      "epoch: 6 step: 1020, loss is 0.007456548511981964\n",
      "epoch: 6 step: 1021, loss is 0.013371210545301437\n",
      "epoch: 6 step: 1022, loss is 0.005540024489164352\n",
      "epoch: 6 step: 1023, loss is 0.0019958035554736853\n",
      "epoch: 6 step: 1024, loss is 0.0006357313832268119\n",
      "epoch: 6 step: 1025, loss is 0.0006306312861852348\n",
      "epoch: 6 step: 1026, loss is 0.0029764221981167793\n",
      "epoch: 6 step: 1027, loss is 0.004340415354818106\n",
      "epoch: 6 step: 1028, loss is 0.010326250456273556\n",
      "epoch: 6 step: 1029, loss is 0.0037298090755939484\n",
      "epoch: 6 step: 1030, loss is 0.0011045000283047557\n",
      "epoch: 6 step: 1031, loss is 0.0003954780986532569\n",
      "epoch: 6 step: 1032, loss is 0.01990397647023201\n",
      "epoch: 6 step: 1033, loss is 0.00671934150159359\n",
      "epoch: 6 step: 1034, loss is 0.009371490217745304\n",
      "epoch: 6 step: 1035, loss is 0.06152641028165817\n",
      "epoch: 6 step: 1036, loss is 0.0030702378135174513\n",
      "epoch: 6 step: 1037, loss is 0.02255810797214508\n",
      "epoch: 6 step: 1038, loss is 0.0030696105677634478\n",
      "epoch: 6 step: 1039, loss is 0.03229742869734764\n",
      "epoch: 6 step: 1040, loss is 0.15534858405590057\n",
      "epoch: 6 step: 1041, loss is 0.047610070556402206\n",
      "epoch: 6 step: 1042, loss is 0.05768606811761856\n",
      "epoch: 6 step: 1043, loss is 0.0020620219875127077\n",
      "epoch: 6 step: 1044, loss is 0.02377644181251526\n",
      "epoch: 6 step: 1045, loss is 0.007260046899318695\n",
      "epoch: 6 step: 1046, loss is 0.24847280979156494\n",
      "epoch: 6 step: 1047, loss is 0.012923368252813816\n",
      "epoch: 6 step: 1048, loss is 0.0001451082935091108\n",
      "epoch: 6 step: 1049, loss is 0.046667441725730896\n",
      "epoch: 6 step: 1050, loss is 0.04536657780408859\n",
      "epoch: 6 step: 1051, loss is 0.13328325748443604\n",
      "epoch: 6 step: 1052, loss is 0.014750473201274872\n",
      "epoch: 6 step: 1053, loss is 0.0023148374166339636\n",
      "epoch: 6 step: 1054, loss is 0.03819289803504944\n",
      "epoch: 6 step: 1055, loss is 0.0016881432384252548\n",
      "epoch: 6 step: 1056, loss is 0.011943847872316837\n",
      "epoch: 6 step: 1057, loss is 0.15779490768909454\n",
      "epoch: 6 step: 1058, loss is 0.08359155058860779\n",
      "epoch: 6 step: 1059, loss is 0.0003370242193341255\n",
      "epoch: 6 step: 1060, loss is 0.027384914457798004\n",
      "epoch: 6 step: 1061, loss is 0.0184185728430748\n",
      "epoch: 6 step: 1062, loss is 0.00024242325162049383\n",
      "epoch: 6 step: 1063, loss is 0.0005646712961606681\n",
      "epoch: 6 step: 1064, loss is 0.0022288504987955093\n",
      "epoch: 6 step: 1065, loss is 0.0018192388815805316\n",
      "epoch: 6 step: 1066, loss is 0.002180910436436534\n",
      "epoch: 6 step: 1067, loss is 0.013729535974562168\n",
      "epoch: 6 step: 1068, loss is 0.00034991654683835804\n",
      "epoch: 6 step: 1069, loss is 0.06747271865606308\n",
      "epoch: 6 step: 1070, loss is 0.026196110993623734\n",
      "epoch: 6 step: 1071, loss is 0.0006452703382819891\n",
      "epoch: 6 step: 1072, loss is 0.060372330248355865\n",
      "epoch: 6 step: 1073, loss is 0.0027958264108747244\n",
      "epoch: 6 step: 1074, loss is 0.004009818658232689\n",
      "epoch: 6 step: 1075, loss is 0.00040838922723196447\n",
      "epoch: 6 step: 1076, loss is 0.0003392795624677092\n",
      "epoch: 6 step: 1077, loss is 0.0029537903610616922\n",
      "epoch: 6 step: 1078, loss is 0.016642067581415176\n",
      "epoch: 6 step: 1079, loss is 0.014473940245807171\n",
      "epoch: 6 step: 1080, loss is 0.002674317453056574\n",
      "epoch: 6 step: 1081, loss is 0.00012740748934447765\n",
      "epoch: 6 step: 1082, loss is 0.0008806658443063498\n",
      "epoch: 6 step: 1083, loss is 0.03909529745578766\n",
      "epoch: 6 step: 1084, loss is 0.06958995759487152\n",
      "epoch: 6 step: 1085, loss is 0.019757265225052834\n",
      "epoch: 6 step: 1086, loss is 0.013803496956825256\n",
      "epoch: 6 step: 1087, loss is 0.0028268112801015377\n",
      "epoch: 6 step: 1088, loss is 0.014855110086500645\n",
      "epoch: 6 step: 1089, loss is 0.01316876895725727\n",
      "epoch: 6 step: 1090, loss is 0.036902472376823425\n",
      "epoch: 6 step: 1091, loss is 0.0012322826078161597\n",
      "epoch: 6 step: 1092, loss is 0.014504238031804562\n",
      "epoch: 6 step: 1093, loss is 0.0011090219486504793\n",
      "epoch: 6 step: 1094, loss is 0.0022556681651622057\n",
      "epoch: 6 step: 1095, loss is 5.369335485738702e-05\n",
      "epoch: 6 step: 1096, loss is 0.0313139371573925\n",
      "epoch: 6 step: 1097, loss is 0.01478648278862238\n",
      "epoch: 6 step: 1098, loss is 0.012535035610198975\n",
      "epoch: 6 step: 1099, loss is 0.00017505355936009437\n",
      "epoch: 6 step: 1100, loss is 0.00027782516553997993\n",
      "epoch: 6 step: 1101, loss is 0.0005191889358684421\n",
      "epoch: 6 step: 1102, loss is 0.019473683089017868\n",
      "epoch: 6 step: 1103, loss is 0.006407571025192738\n",
      "epoch: 6 step: 1104, loss is 0.00018652714788913727\n",
      "epoch: 6 step: 1105, loss is 0.020482489839196205\n",
      "epoch: 6 step: 1106, loss is 0.014094001613557339\n",
      "epoch: 6 step: 1107, loss is 0.01901501975953579\n",
      "epoch: 6 step: 1108, loss is 0.0031912915874272585\n",
      "epoch: 6 step: 1109, loss is 0.003547729691490531\n",
      "epoch: 6 step: 1110, loss is 0.0003480269806459546\n",
      "epoch: 6 step: 1111, loss is 0.013957400806248188\n",
      "epoch: 6 step: 1112, loss is 0.029044844210147858\n",
      "epoch: 6 step: 1113, loss is 0.05511297285556793\n",
      "epoch: 6 step: 1114, loss is 0.007610172964632511\n",
      "epoch: 6 step: 1115, loss is 0.0031396641861647367\n",
      "epoch: 6 step: 1116, loss is 0.002416827017441392\n",
      "epoch: 6 step: 1117, loss is 0.0010294957319274545\n",
      "epoch: 6 step: 1118, loss is 0.0015140698524191976\n",
      "epoch: 6 step: 1119, loss is 0.0036119490396231413\n",
      "epoch: 6 step: 1120, loss is 0.0010160526726394892\n",
      "epoch: 6 step: 1121, loss is 9.001507714856416e-05\n",
      "epoch: 6 step: 1122, loss is 0.0006097721634432673\n",
      "epoch: 6 step: 1123, loss is 0.05443258956074715\n",
      "epoch: 6 step: 1124, loss is 0.04533732309937477\n",
      "epoch: 6 step: 1125, loss is 0.08493946492671967\n",
      "epoch: 6 step: 1126, loss is 0.0017804087838158011\n",
      "epoch: 6 step: 1127, loss is 0.0016904713120311499\n",
      "epoch: 6 step: 1128, loss is 0.027221838012337685\n",
      "epoch: 6 step: 1129, loss is 0.40478578209877014\n",
      "epoch: 6 step: 1130, loss is 0.005034207366406918\n",
      "epoch: 6 step: 1131, loss is 0.008360081352293491\n",
      "epoch: 6 step: 1132, loss is 0.07435780018568039\n",
      "epoch: 6 step: 1133, loss is 0.006871949881315231\n",
      "epoch: 6 step: 1134, loss is 0.0016827565850690007\n",
      "epoch: 6 step: 1135, loss is 0.009167221374809742\n",
      "epoch: 6 step: 1136, loss is 0.020225917920470238\n",
      "epoch: 6 step: 1137, loss is 0.007706029340624809\n",
      "epoch: 6 step: 1138, loss is 0.002527685137465596\n",
      "epoch: 6 step: 1139, loss is 0.00858977809548378\n",
      "epoch: 6 step: 1140, loss is 0.013420105911791325\n",
      "epoch: 6 step: 1141, loss is 0.0447726845741272\n",
      "epoch: 6 step: 1142, loss is 6.56160045764409e-05\n",
      "epoch: 6 step: 1143, loss is 0.011581776663661003\n",
      "epoch: 6 step: 1144, loss is 0.0029414116870611906\n",
      "epoch: 6 step: 1145, loss is 0.004202930722385645\n",
      "epoch: 6 step: 1146, loss is 0.002291524549946189\n",
      "epoch: 6 step: 1147, loss is 0.0009151673875749111\n",
      "epoch: 6 step: 1148, loss is 0.0010729269124567509\n",
      "epoch: 6 step: 1149, loss is 0.12705019116401672\n",
      "epoch: 6 step: 1150, loss is 0.008058835752308369\n",
      "epoch: 6 step: 1151, loss is 0.1196032240986824\n",
      "epoch: 6 step: 1152, loss is 0.0009981832699850202\n",
      "epoch: 6 step: 1153, loss is 0.01297047920525074\n",
      "epoch: 6 step: 1154, loss is 0.0030426729936152697\n",
      "epoch: 6 step: 1155, loss is 0.09461664408445358\n",
      "epoch: 6 step: 1156, loss is 0.002620022976770997\n",
      "epoch: 6 step: 1157, loss is 0.08041532337665558\n",
      "epoch: 6 step: 1158, loss is 0.001899976865388453\n",
      "epoch: 6 step: 1159, loss is 9.54026763793081e-05\n",
      "epoch: 6 step: 1160, loss is 0.007167522795498371\n",
      "epoch: 6 step: 1161, loss is 0.0002677671436686069\n",
      "epoch: 6 step: 1162, loss is 0.000438010785728693\n",
      "epoch: 6 step: 1163, loss is 0.0024945272598415613\n",
      "epoch: 6 step: 1164, loss is 0.004586098715662956\n",
      "epoch: 6 step: 1165, loss is 0.0021970728412270546\n",
      "epoch: 6 step: 1166, loss is 0.03677166998386383\n",
      "epoch: 6 step: 1167, loss is 0.0017870618030428886\n",
      "epoch: 6 step: 1168, loss is 0.03898497298359871\n",
      "epoch: 6 step: 1169, loss is 0.006111183203756809\n",
      "epoch: 6 step: 1170, loss is 0.0031755331438034773\n",
      "epoch: 6 step: 1171, loss is 0.0022169924341142178\n",
      "epoch: 6 step: 1172, loss is 0.010295776650309563\n",
      "epoch: 6 step: 1173, loss is 0.03323988988995552\n",
      "epoch: 6 step: 1174, loss is 0.012030224315822124\n",
      "epoch: 6 step: 1175, loss is 0.015610835514962673\n",
      "epoch: 6 step: 1176, loss is 0.01053556613624096\n",
      "epoch: 6 step: 1177, loss is 0.006902868393808603\n",
      "epoch: 6 step: 1178, loss is 0.010838455520570278\n",
      "epoch: 6 step: 1179, loss is 0.002199456561356783\n",
      "epoch: 6 step: 1180, loss is 0.011077460832893848\n",
      "epoch: 6 step: 1181, loss is 0.0019511794671416283\n",
      "epoch: 6 step: 1182, loss is 0.0020865940023213625\n",
      "epoch: 6 step: 1183, loss is 0.0014969120966270566\n",
      "epoch: 6 step: 1184, loss is 0.005207127891480923\n",
      "epoch: 6 step: 1185, loss is 0.0021052900701761246\n",
      "epoch: 6 step: 1186, loss is 0.033765845000743866\n",
      "epoch: 6 step: 1187, loss is 0.00032708552316762507\n",
      "epoch: 6 step: 1188, loss is 0.010607454925775528\n",
      "epoch: 6 step: 1189, loss is 0.002089667832478881\n",
      "epoch: 6 step: 1190, loss is 0.12438379228115082\n",
      "epoch: 6 step: 1191, loss is 0.15000206232070923\n",
      "epoch: 6 step: 1192, loss is 0.0008603911846876144\n",
      "epoch: 6 step: 1193, loss is 0.0007650375482626259\n",
      "epoch: 6 step: 1194, loss is 0.0020584098529070616\n",
      "epoch: 6 step: 1195, loss is 0.0010544877732172608\n",
      "epoch: 6 step: 1196, loss is 0.2005201131105423\n",
      "epoch: 6 step: 1197, loss is 0.004298867657780647\n",
      "epoch: 6 step: 1198, loss is 0.005731571465730667\n",
      "epoch: 6 step: 1199, loss is 0.0008942005806602538\n",
      "epoch: 6 step: 1200, loss is 0.004013500176370144\n",
      "epoch: 6 step: 1201, loss is 0.013779322616755962\n",
      "epoch: 6 step: 1202, loss is 0.09545662999153137\n",
      "epoch: 6 step: 1203, loss is 0.004379842430353165\n",
      "epoch: 6 step: 1204, loss is 0.001906546880491078\n",
      "epoch: 6 step: 1205, loss is 0.0013436232693493366\n",
      "epoch: 6 step: 1206, loss is 0.06064319610595703\n",
      "epoch: 6 step: 1207, loss is 0.008162501268088818\n",
      "epoch: 6 step: 1208, loss is 0.0058782994747161865\n",
      "epoch: 6 step: 1209, loss is 0.0013461033813655376\n",
      "epoch: 6 step: 1210, loss is 0.0007214838988147676\n",
      "epoch: 6 step: 1211, loss is 0.010956522077322006\n",
      "epoch: 6 step: 1212, loss is 0.008910601027309895\n",
      "epoch: 6 step: 1213, loss is 0.014683309011161327\n",
      "epoch: 6 step: 1214, loss is 0.007309931796044111\n",
      "epoch: 6 step: 1215, loss is 0.0005284395883791149\n",
      "epoch: 6 step: 1216, loss is 0.000933467352297157\n",
      "epoch: 6 step: 1217, loss is 0.015201314352452755\n",
      "epoch: 6 step: 1218, loss is 0.009701802395284176\n",
      "epoch: 6 step: 1219, loss is 0.0024028306361287832\n",
      "epoch: 6 step: 1220, loss is 0.008139979094266891\n",
      "epoch: 6 step: 1221, loss is 0.002503378316760063\n",
      "epoch: 6 step: 1222, loss is 0.09559183567762375\n",
      "epoch: 6 step: 1223, loss is 0.01104126125574112\n",
      "epoch: 6 step: 1224, loss is 0.00022236513905227184\n",
      "epoch: 6 step: 1225, loss is 0.025689488276839256\n",
      "epoch: 6 step: 1226, loss is 0.11540322005748749\n",
      "epoch: 6 step: 1227, loss is 0.009193498641252518\n",
      "epoch: 6 step: 1228, loss is 0.0022473952267318964\n",
      "epoch: 6 step: 1229, loss is 0.0002935181255452335\n",
      "epoch: 6 step: 1230, loss is 0.000499618356116116\n",
      "epoch: 6 step: 1231, loss is 0.14934377372264862\n",
      "epoch: 6 step: 1232, loss is 0.005490398500114679\n",
      "epoch: 6 step: 1233, loss is 0.000954780145548284\n",
      "epoch: 6 step: 1234, loss is 0.00021044431196060032\n",
      "epoch: 6 step: 1235, loss is 0.01145629957318306\n",
      "epoch: 6 step: 1236, loss is 0.02127799578011036\n",
      "epoch: 6 step: 1237, loss is 0.03867178037762642\n",
      "epoch: 6 step: 1238, loss is 0.0013210847973823547\n",
      "epoch: 6 step: 1239, loss is 0.008055264130234718\n",
      "epoch: 6 step: 1240, loss is 0.0037859464064240456\n",
      "epoch: 6 step: 1241, loss is 0.0071777841076254845\n",
      "epoch: 6 step: 1242, loss is 0.003159821033477783\n",
      "epoch: 6 step: 1243, loss is 0.0015380793483927846\n",
      "epoch: 6 step: 1244, loss is 0.029037529602646828\n",
      "epoch: 6 step: 1245, loss is 0.1301855593919754\n",
      "epoch: 6 step: 1246, loss is 0.0013903842773288488\n",
      "epoch: 6 step: 1247, loss is 0.0003145442751701921\n",
      "epoch: 6 step: 1248, loss is 0.001200714148581028\n",
      "epoch: 6 step: 1249, loss is 0.000555044156499207\n",
      "epoch: 6 step: 1250, loss is 0.00023122223501559347\n",
      "epoch: 6 step: 1251, loss is 0.03776119276881218\n",
      "epoch: 6 step: 1252, loss is 0.0001383632334182039\n",
      "epoch: 6 step: 1253, loss is 0.0005912198103033006\n",
      "epoch: 6 step: 1254, loss is 0.08791212737560272\n",
      "epoch: 6 step: 1255, loss is 0.031316693872213364\n",
      "epoch: 6 step: 1256, loss is 0.028203193098306656\n",
      "epoch: 6 step: 1257, loss is 0.013869439251720905\n",
      "epoch: 6 step: 1258, loss is 0.012219156138598919\n",
      "epoch: 6 step: 1259, loss is 0.0300357136875391\n",
      "epoch: 6 step: 1260, loss is 0.0037385919131338596\n",
      "epoch: 6 step: 1261, loss is 0.0005231569521129131\n",
      "epoch: 6 step: 1262, loss is 0.07438702881336212\n",
      "epoch: 6 step: 1263, loss is 0.0013741463189944625\n",
      "epoch: 6 step: 1264, loss is 0.0008990076021291316\n",
      "epoch: 6 step: 1265, loss is 0.005714280530810356\n",
      "epoch: 6 step: 1266, loss is 0.08273080736398697\n",
      "epoch: 6 step: 1267, loss is 0.0006762417033314705\n",
      "epoch: 6 step: 1268, loss is 0.061223067343235016\n",
      "epoch: 6 step: 1269, loss is 0.011067078448832035\n",
      "epoch: 6 step: 1270, loss is 0.002125687198713422\n",
      "epoch: 6 step: 1271, loss is 0.003180427011102438\n",
      "epoch: 6 step: 1272, loss is 0.005200478248298168\n",
      "epoch: 6 step: 1273, loss is 0.001161869615316391\n",
      "epoch: 6 step: 1274, loss is 0.04298914968967438\n",
      "epoch: 6 step: 1275, loss is 0.16939504444599152\n",
      "epoch: 6 step: 1276, loss is 0.014737331308424473\n",
      "epoch: 6 step: 1277, loss is 0.08392797410488129\n",
      "epoch: 6 step: 1278, loss is 0.036073990166187286\n",
      "epoch: 6 step: 1279, loss is 0.016514724120497704\n",
      "epoch: 6 step: 1280, loss is 0.0038609602488577366\n",
      "epoch: 6 step: 1281, loss is 0.004338081926107407\n",
      "epoch: 6 step: 1282, loss is 0.0009490232914686203\n",
      "epoch: 6 step: 1283, loss is 0.024319719523191452\n",
      "epoch: 6 step: 1284, loss is 0.029132826253771782\n",
      "epoch: 6 step: 1285, loss is 0.002805474679917097\n",
      "epoch: 6 step: 1286, loss is 0.03645126521587372\n",
      "epoch: 6 step: 1287, loss is 5.80166197323706e-05\n",
      "epoch: 6 step: 1288, loss is 0.0028787273913621902\n",
      "epoch: 6 step: 1289, loss is 0.001673703663982451\n",
      "epoch: 6 step: 1290, loss is 0.03810349851846695\n",
      "epoch: 6 step: 1291, loss is 0.009212540462613106\n",
      "epoch: 6 step: 1292, loss is 0.0005543508450500667\n",
      "epoch: 6 step: 1293, loss is 0.0011892117327079177\n",
      "epoch: 6 step: 1294, loss is 0.013731002807617188\n",
      "epoch: 6 step: 1295, loss is 0.003283096244558692\n",
      "epoch: 6 step: 1296, loss is 0.006036016158759594\n",
      "epoch: 6 step: 1297, loss is 0.0005862480611540377\n",
      "epoch: 6 step: 1298, loss is 0.004270417150110006\n",
      "epoch: 6 step: 1299, loss is 3.180422936566174e-05\n",
      "epoch: 6 step: 1300, loss is 0.05888453498482704\n",
      "epoch: 6 step: 1301, loss is 0.03602617606520653\n",
      "epoch: 6 step: 1302, loss is 0.002994391368702054\n",
      "epoch: 6 step: 1303, loss is 0.014426332898437977\n",
      "epoch: 6 step: 1304, loss is 0.0006791581399738789\n",
      "epoch: 6 step: 1305, loss is 0.013817710801959038\n",
      "epoch: 6 step: 1306, loss is 0.0022163435351103544\n",
      "epoch: 6 step: 1307, loss is 0.05643105134367943\n",
      "epoch: 6 step: 1308, loss is 0.010141611099243164\n",
      "epoch: 6 step: 1309, loss is 0.002258429303765297\n",
      "epoch: 6 step: 1310, loss is 0.0012085943017154932\n",
      "epoch: 6 step: 1311, loss is 0.0002780937938950956\n",
      "epoch: 6 step: 1312, loss is 0.0003478990402072668\n",
      "epoch: 6 step: 1313, loss is 0.00041188212344422936\n",
      "epoch: 6 step: 1314, loss is 0.0030921809375286102\n",
      "epoch: 6 step: 1315, loss is 0.0027989870868623257\n",
      "epoch: 6 step: 1316, loss is 0.002405220875516534\n",
      "epoch: 6 step: 1317, loss is 0.08663289994001389\n",
      "epoch: 6 step: 1318, loss is 0.040656473487615585\n",
      "epoch: 6 step: 1319, loss is 0.00037409120704978704\n",
      "epoch: 6 step: 1320, loss is 0.0033288358245044947\n",
      "epoch: 6 step: 1321, loss is 0.0016994193429127336\n",
      "epoch: 6 step: 1322, loss is 0.0003184728557243943\n",
      "epoch: 6 step: 1323, loss is 0.00421161251142621\n",
      "epoch: 6 step: 1324, loss is 0.0021203160285949707\n",
      "epoch: 6 step: 1325, loss is 0.00046263515832833946\n",
      "epoch: 6 step: 1326, loss is 0.20024889707565308\n",
      "epoch: 6 step: 1327, loss is 0.02559845522046089\n",
      "epoch: 6 step: 1328, loss is 0.0038182891439646482\n",
      "epoch: 6 step: 1329, loss is 0.0015611047856509686\n",
      "epoch: 6 step: 1330, loss is 0.0063481866382062435\n",
      "epoch: 6 step: 1331, loss is 0.0030397484079003334\n",
      "epoch: 6 step: 1332, loss is 0.004136445000767708\n",
      "epoch: 6 step: 1333, loss is 0.00035005249083042145\n",
      "epoch: 6 step: 1334, loss is 0.0001482216757722199\n",
      "epoch: 6 step: 1335, loss is 0.006196058355271816\n",
      "epoch: 6 step: 1336, loss is 0.0314287468791008\n",
      "epoch: 6 step: 1337, loss is 0.10676749050617218\n",
      "epoch: 6 step: 1338, loss is 0.052110638469457626\n",
      "epoch: 6 step: 1339, loss is 5.521664934349246e-05\n",
      "epoch: 6 step: 1340, loss is 0.029689868912100792\n",
      "epoch: 6 step: 1341, loss is 9.286478598369285e-05\n",
      "epoch: 6 step: 1342, loss is 0.023017393425107002\n",
      "epoch: 6 step: 1343, loss is 0.0018955814884975553\n",
      "epoch: 6 step: 1344, loss is 0.0004906494868919253\n",
      "epoch: 6 step: 1345, loss is 0.0021615333389490843\n",
      "epoch: 6 step: 1346, loss is 0.09282616525888443\n",
      "epoch: 6 step: 1347, loss is 0.0807943195104599\n",
      "epoch: 6 step: 1348, loss is 0.0002884017303586006\n",
      "epoch: 6 step: 1349, loss is 0.01153162308037281\n",
      "epoch: 6 step: 1350, loss is 0.004843742586672306\n",
      "epoch: 6 step: 1351, loss is 0.0008904366986826062\n",
      "epoch: 6 step: 1352, loss is 0.00013865888467989862\n",
      "epoch: 6 step: 1353, loss is 0.09186973422765732\n",
      "epoch: 6 step: 1354, loss is 0.0001481606304878369\n",
      "epoch: 6 step: 1355, loss is 0.010079760104417801\n",
      "epoch: 6 step: 1356, loss is 0.006148299667984247\n",
      "epoch: 6 step: 1357, loss is 0.011310731060802937\n",
      "epoch: 6 step: 1358, loss is 0.0003481985768303275\n",
      "epoch: 6 step: 1359, loss is 0.015559569001197815\n",
      "epoch: 6 step: 1360, loss is 0.053320080041885376\n",
      "epoch: 6 step: 1361, loss is 0.0003263087710365653\n",
      "epoch: 6 step: 1362, loss is 0.0011966987513005733\n",
      "epoch: 6 step: 1363, loss is 0.0027963691391050816\n",
      "epoch: 6 step: 1364, loss is 0.02129211090505123\n",
      "epoch: 6 step: 1365, loss is 0.000625201384536922\n",
      "epoch: 6 step: 1366, loss is 0.0009467989439144731\n",
      "epoch: 6 step: 1367, loss is 0.0005961077404208481\n",
      "epoch: 6 step: 1368, loss is 0.0013528072740882635\n",
      "epoch: 6 step: 1369, loss is 0.014953479170799255\n",
      "epoch: 6 step: 1370, loss is 0.031756240874528885\n",
      "epoch: 6 step: 1371, loss is 0.13397952914237976\n",
      "epoch: 6 step: 1372, loss is 1.8720329535426572e-05\n",
      "epoch: 6 step: 1373, loss is 0.0013190701138228178\n",
      "epoch: 6 step: 1374, loss is 0.00041090077138505876\n",
      "epoch: 6 step: 1375, loss is 0.001368228462524712\n",
      "epoch: 6 step: 1376, loss is 0.0003769721370190382\n",
      "epoch: 6 step: 1377, loss is 0.18996915221214294\n",
      "epoch: 6 step: 1378, loss is 0.0005477165686897933\n",
      "epoch: 6 step: 1379, loss is 0.056034207344055176\n",
      "epoch: 6 step: 1380, loss is 0.0015317982761189342\n",
      "epoch: 6 step: 1381, loss is 0.00013621289690490812\n",
      "epoch: 6 step: 1382, loss is 0.00023718168085906655\n",
      "epoch: 6 step: 1383, loss is 0.0007826946093700826\n",
      "epoch: 6 step: 1384, loss is 0.01684354804456234\n",
      "epoch: 6 step: 1385, loss is 0.003391609061509371\n",
      "epoch: 6 step: 1386, loss is 0.011760500259697437\n",
      "epoch: 6 step: 1387, loss is 0.0011412554886192083\n",
      "epoch: 6 step: 1388, loss is 0.001790638081729412\n",
      "epoch: 6 step: 1389, loss is 0.014216200448572636\n",
      "epoch: 6 step: 1390, loss is 0.001703018555417657\n",
      "epoch: 6 step: 1391, loss is 0.0021197577007114887\n",
      "epoch: 6 step: 1392, loss is 0.01619376800954342\n",
      "epoch: 6 step: 1393, loss is 0.001122210524044931\n",
      "epoch: 6 step: 1394, loss is 0.10278875380754471\n",
      "epoch: 6 step: 1395, loss is 0.1642383635044098\n",
      "epoch: 6 step: 1396, loss is 0.009802740067243576\n",
      "epoch: 6 step: 1397, loss is 0.005190057680010796\n",
      "epoch: 6 step: 1398, loss is 0.10188829153776169\n",
      "epoch: 6 step: 1399, loss is 0.0026102198753505945\n",
      "epoch: 6 step: 1400, loss is 0.007463885936886072\n",
      "epoch: 6 step: 1401, loss is 0.004910729825496674\n",
      "epoch: 6 step: 1402, loss is 0.026603834703564644\n",
      "epoch: 6 step: 1403, loss is 0.05815265327692032\n",
      "epoch: 6 step: 1404, loss is 0.023731959983706474\n",
      "epoch: 6 step: 1405, loss is 0.04511289298534393\n",
      "epoch: 6 step: 1406, loss is 0.0011339581105858088\n",
      "epoch: 6 step: 1407, loss is 0.0010164680425077677\n",
      "epoch: 6 step: 1408, loss is 0.006550461053848267\n",
      "epoch: 6 step: 1409, loss is 0.0030822367407381535\n",
      "epoch: 6 step: 1410, loss is 0.006817917339503765\n",
      "epoch: 6 step: 1411, loss is 0.008005885407328606\n",
      "epoch: 6 step: 1412, loss is 0.06338503211736679\n",
      "epoch: 6 step: 1413, loss is 0.017780179157853127\n",
      "epoch: 6 step: 1414, loss is 0.08240318298339844\n",
      "epoch: 6 step: 1415, loss is 0.07826508581638336\n",
      "epoch: 6 step: 1416, loss is 0.0073714046739041805\n",
      "epoch: 6 step: 1417, loss is 0.008150924928486347\n",
      "epoch: 6 step: 1418, loss is 0.00024294565082527697\n",
      "epoch: 6 step: 1419, loss is 0.0022651534527540207\n",
      "epoch: 6 step: 1420, loss is 0.0009911356028169394\n",
      "epoch: 6 step: 1421, loss is 0.0018141703912988305\n",
      "epoch: 6 step: 1422, loss is 0.0002148646308341995\n",
      "epoch: 6 step: 1423, loss is 0.0178983211517334\n",
      "epoch: 6 step: 1424, loss is 0.0005855615600012243\n",
      "epoch: 6 step: 1425, loss is 0.00034345712629146874\n",
      "epoch: 6 step: 1426, loss is 0.0014982473803684115\n",
      "epoch: 6 step: 1427, loss is 0.000958064803853631\n",
      "epoch: 6 step: 1428, loss is 0.02352067641913891\n",
      "epoch: 6 step: 1429, loss is 0.09640984237194061\n",
      "epoch: 6 step: 1430, loss is 0.0005416370695456862\n",
      "epoch: 6 step: 1431, loss is 0.02417478896677494\n",
      "epoch: 6 step: 1432, loss is 0.005147245246917009\n",
      "epoch: 6 step: 1433, loss is 0.004888980649411678\n",
      "epoch: 6 step: 1434, loss is 0.003960121888667345\n",
      "epoch: 6 step: 1435, loss is 0.04697280377149582\n",
      "epoch: 6 step: 1436, loss is 0.002813753206282854\n",
      "epoch: 6 step: 1437, loss is 0.0021101224701851606\n",
      "epoch: 6 step: 1438, loss is 0.0005369315622374415\n",
      "epoch: 6 step: 1439, loss is 0.009687311016023159\n",
      "epoch: 6 step: 1440, loss is 0.01265828125178814\n",
      "epoch: 6 step: 1441, loss is 0.0885603204369545\n",
      "epoch: 6 step: 1442, loss is 0.0009534637210890651\n",
      "epoch: 6 step: 1443, loss is 0.000939109711907804\n",
      "epoch: 6 step: 1444, loss is 0.01592111960053444\n",
      "epoch: 6 step: 1445, loss is 0.0025516620371490717\n",
      "epoch: 6 step: 1446, loss is 0.006311529316008091\n",
      "epoch: 6 step: 1447, loss is 0.046462640166282654\n",
      "epoch: 6 step: 1448, loss is 0.0016354052349925041\n",
      "epoch: 6 step: 1449, loss is 0.0035257984418421984\n",
      "epoch: 6 step: 1450, loss is 0.004907670896500349\n",
      "epoch: 6 step: 1451, loss is 0.003459251718595624\n",
      "epoch: 6 step: 1452, loss is 0.000386845669709146\n",
      "epoch: 6 step: 1453, loss is 0.0007340149604715407\n",
      "epoch: 6 step: 1454, loss is 0.0009405813761986792\n",
      "epoch: 6 step: 1455, loss is 0.11275513470172882\n",
      "epoch: 6 step: 1456, loss is 0.0023952473420649767\n",
      "epoch: 6 step: 1457, loss is 0.13930006325244904\n",
      "epoch: 6 step: 1458, loss is 0.01344987004995346\n",
      "epoch: 6 step: 1459, loss is 0.0027511054649949074\n",
      "epoch: 6 step: 1460, loss is 0.0006435021059587598\n",
      "epoch: 6 step: 1461, loss is 0.012851092033088207\n",
      "epoch: 6 step: 1462, loss is 0.015674235299229622\n",
      "epoch: 6 step: 1463, loss is 0.0041132341139018536\n",
      "epoch: 6 step: 1464, loss is 0.00023813467123545706\n",
      "epoch: 6 step: 1465, loss is 0.0002769926213659346\n",
      "epoch: 6 step: 1466, loss is 0.04091764613986015\n",
      "epoch: 6 step: 1467, loss is 0.007355359382927418\n",
      "epoch: 6 step: 1468, loss is 0.06169530749320984\n",
      "epoch: 6 step: 1469, loss is 0.00794587004929781\n",
      "epoch: 6 step: 1470, loss is 0.09821265935897827\n",
      "epoch: 6 step: 1471, loss is 0.01674504391849041\n",
      "epoch: 6 step: 1472, loss is 0.0017027378780767322\n",
      "epoch: 6 step: 1473, loss is 0.04046386107802391\n",
      "epoch: 6 step: 1474, loss is 0.001302222954109311\n",
      "epoch: 6 step: 1475, loss is 0.005406687036156654\n",
      "epoch: 6 step: 1476, loss is 0.08660347014665604\n",
      "epoch: 6 step: 1477, loss is 0.009220083244144917\n",
      "epoch: 6 step: 1478, loss is 0.0018676463514566422\n",
      "epoch: 6 step: 1479, loss is 4.841643385589123e-05\n",
      "epoch: 6 step: 1480, loss is 0.001667415606789291\n",
      "epoch: 6 step: 1481, loss is 0.025199009105563164\n",
      "epoch: 6 step: 1482, loss is 0.010064715519547462\n",
      "epoch: 6 step: 1483, loss is 0.21175655722618103\n",
      "epoch: 6 step: 1484, loss is 0.0004561578098218888\n",
      "epoch: 6 step: 1485, loss is 0.004166311118751764\n",
      "epoch: 6 step: 1486, loss is 0.0005679390160366893\n",
      "epoch: 6 step: 1487, loss is 0.015223635360598564\n",
      "epoch: 6 step: 1488, loss is 0.0028247255831956863\n",
      "epoch: 6 step: 1489, loss is 0.020023975521326065\n",
      "epoch: 6 step: 1490, loss is 0.01612832583487034\n",
      "epoch: 6 step: 1491, loss is 0.0006983664934523404\n",
      "epoch: 6 step: 1492, loss is 0.00012362269626464695\n",
      "epoch: 6 step: 1493, loss is 0.04229990765452385\n",
      "epoch: 6 step: 1494, loss is 0.0003596345486585051\n",
      "epoch: 6 step: 1495, loss is 0.053814467042684555\n",
      "epoch: 6 step: 1496, loss is 0.036742325872182846\n",
      "epoch: 6 step: 1497, loss is 0.0058608707040548325\n",
      "epoch: 6 step: 1498, loss is 0.0034834209363907576\n",
      "epoch: 6 step: 1499, loss is 0.0008530353661626577\n",
      "epoch: 6 step: 1500, loss is 0.003266348270699382\n",
      "epoch: 6 step: 1501, loss is 0.013427935540676117\n",
      "epoch: 6 step: 1502, loss is 0.0006238765781745315\n",
      "epoch: 6 step: 1503, loss is 0.04580043628811836\n",
      "epoch: 6 step: 1504, loss is 0.007430872414261103\n",
      "epoch: 6 step: 1505, loss is 0.07746154814958572\n",
      "epoch: 6 step: 1506, loss is 0.13821473717689514\n",
      "epoch: 6 step: 1507, loss is 0.001854314235970378\n",
      "epoch: 6 step: 1508, loss is 0.01174281258136034\n",
      "epoch: 6 step: 1509, loss is 0.0011923655401915312\n",
      "epoch: 6 step: 1510, loss is 0.0015623951330780983\n",
      "epoch: 6 step: 1511, loss is 0.000589008501265198\n",
      "epoch: 6 step: 1512, loss is 0.019428353756666183\n",
      "epoch: 6 step: 1513, loss is 0.019341669976711273\n",
      "epoch: 6 step: 1514, loss is 0.01236386876553297\n",
      "epoch: 6 step: 1515, loss is 0.028704265132546425\n",
      "epoch: 6 step: 1516, loss is 0.0005554494564421475\n",
      "epoch: 6 step: 1517, loss is 0.0003938476147595793\n",
      "epoch: 6 step: 1518, loss is 0.00014216583804227412\n",
      "epoch: 6 step: 1519, loss is 0.010165183804929256\n",
      "epoch: 6 step: 1520, loss is 0.055337511003017426\n",
      "epoch: 6 step: 1521, loss is 0.001497698831371963\n",
      "epoch: 6 step: 1522, loss is 0.0005959592526778579\n",
      "epoch: 6 step: 1523, loss is 0.015117825008928776\n",
      "epoch: 6 step: 1524, loss is 0.008302048780024052\n",
      "epoch: 6 step: 1525, loss is 0.0003717773361131549\n",
      "epoch: 6 step: 1526, loss is 0.0015354846836999059\n",
      "epoch: 6 step: 1527, loss is 0.003601417876780033\n",
      "epoch: 6 step: 1528, loss is 0.00015976013673935086\n",
      "epoch: 6 step: 1529, loss is 0.0009424779564142227\n",
      "epoch: 6 step: 1530, loss is 0.00047623063437640667\n",
      "epoch: 6 step: 1531, loss is 0.0037831354420632124\n",
      "epoch: 6 step: 1532, loss is 4.559799708658829e-05\n",
      "epoch: 6 step: 1533, loss is 0.14096884429454803\n",
      "epoch: 6 step: 1534, loss is 0.00032469257712364197\n",
      "epoch: 6 step: 1535, loss is 0.015201627276837826\n",
      "epoch: 6 step: 1536, loss is 0.00509238988161087\n",
      "epoch: 6 step: 1537, loss is 0.000933015369810164\n",
      "epoch: 6 step: 1538, loss is 0.001074875588528812\n",
      "epoch: 6 step: 1539, loss is 0.001210294896736741\n",
      "epoch: 6 step: 1540, loss is 0.004134665243327618\n",
      "epoch: 6 step: 1541, loss is 0.11662735044956207\n",
      "epoch: 6 step: 1542, loss is 0.13235831260681152\n",
      "epoch: 6 step: 1543, loss is 0.019601132720708847\n",
      "epoch: 6 step: 1544, loss is 0.04463038221001625\n",
      "epoch: 6 step: 1545, loss is 0.03183000534772873\n",
      "epoch: 6 step: 1546, loss is 0.0019433660199865699\n",
      "epoch: 6 step: 1547, loss is 0.0024382877163589\n",
      "epoch: 6 step: 1548, loss is 0.021355953067541122\n",
      "epoch: 6 step: 1549, loss is 0.004674855619668961\n",
      "epoch: 6 step: 1550, loss is 0.00017561439017299563\n",
      "epoch: 6 step: 1551, loss is 0.0005099016707390547\n",
      "epoch: 6 step: 1552, loss is 0.0006977344746701419\n",
      "epoch: 6 step: 1553, loss is 0.00313891121186316\n",
      "epoch: 6 step: 1554, loss is 0.02756909839808941\n",
      "epoch: 6 step: 1555, loss is 0.003927187062799931\n",
      "epoch: 6 step: 1556, loss is 0.002489016391336918\n",
      "epoch: 6 step: 1557, loss is 0.015324559062719345\n",
      "epoch: 6 step: 1558, loss is 0.004316891543567181\n",
      "epoch: 6 step: 1559, loss is 0.0005244643543846905\n",
      "epoch: 6 step: 1560, loss is 0.001066017197445035\n",
      "epoch: 6 step: 1561, loss is 0.0015410393243655562\n",
      "epoch: 6 step: 1562, loss is 0.011666645295917988\n",
      "epoch: 6 step: 1563, loss is 0.018146948888897896\n",
      "epoch: 6 step: 1564, loss is 0.011169202625751495\n",
      "epoch: 6 step: 1565, loss is 0.04503582417964935\n",
      "epoch: 6 step: 1566, loss is 0.02444552630186081\n",
      "epoch: 6 step: 1567, loss is 0.0004887069226242602\n",
      "epoch: 6 step: 1568, loss is 0.02542441338300705\n",
      "epoch: 6 step: 1569, loss is 0.004749279003590345\n",
      "epoch: 6 step: 1570, loss is 0.04952903091907501\n",
      "epoch: 6 step: 1571, loss is 0.0077214716002345085\n",
      "epoch: 6 step: 1572, loss is 0.00019549544958863407\n",
      "epoch: 6 step: 1573, loss is 0.04228312894701958\n",
      "epoch: 6 step: 1574, loss is 8.576195250498131e-05\n",
      "epoch: 6 step: 1575, loss is 6.625062815146521e-05\n",
      "epoch: 6 step: 1576, loss is 0.006328250281512737\n",
      "epoch: 6 step: 1577, loss is 0.0013539912179112434\n",
      "epoch: 6 step: 1578, loss is 0.009454059414565563\n",
      "epoch: 6 step: 1579, loss is 0.0007508558337576687\n",
      "epoch: 6 step: 1580, loss is 0.00629087258130312\n",
      "epoch: 6 step: 1581, loss is 0.010200297459959984\n",
      "epoch: 6 step: 1582, loss is 0.0002605223562568426\n",
      "epoch: 6 step: 1583, loss is 0.005271646194159985\n",
      "epoch: 6 step: 1584, loss is 0.0038358503952622414\n",
      "epoch: 6 step: 1585, loss is 0.0032764894422143698\n",
      "epoch: 6 step: 1586, loss is 0.00012816302478313446\n",
      "epoch: 6 step: 1587, loss is 0.00013631200999952853\n",
      "epoch: 6 step: 1588, loss is 0.0022275231312960386\n",
      "epoch: 6 step: 1589, loss is 9.200641216011718e-05\n",
      "epoch: 6 step: 1590, loss is 0.31890177726745605\n",
      "epoch: 6 step: 1591, loss is 0.0026501542888581753\n",
      "epoch: 6 step: 1592, loss is 0.0033968682400882244\n",
      "epoch: 6 step: 1593, loss is 0.025345226749777794\n",
      "epoch: 6 step: 1594, loss is 6.915673293406144e-05\n",
      "epoch: 6 step: 1595, loss is 0.00019272250938229263\n",
      "epoch: 6 step: 1596, loss is 0.11312078684568405\n",
      "epoch: 6 step: 1597, loss is 0.00021848765027243644\n",
      "epoch: 6 step: 1598, loss is 0.00026439421344548464\n",
      "epoch: 6 step: 1599, loss is 0.00010386546637164429\n",
      "epoch: 6 step: 1600, loss is 0.03812417387962341\n",
      "epoch: 6 step: 1601, loss is 0.03932468220591545\n",
      "epoch: 6 step: 1602, loss is 0.0066151851788163185\n",
      "epoch: 6 step: 1603, loss is 0.014718553982675076\n",
      "epoch: 6 step: 1604, loss is 0.227044478058815\n",
      "epoch: 6 step: 1605, loss is 0.023050259798765182\n",
      "epoch: 6 step: 1606, loss is 0.01288284920156002\n",
      "epoch: 6 step: 1607, loss is 0.039874959737062454\n",
      "epoch: 6 step: 1608, loss is 0.0011465010466054082\n",
      "epoch: 6 step: 1609, loss is 0.005055117420852184\n",
      "epoch: 6 step: 1610, loss is 0.03306301310658455\n",
      "epoch: 6 step: 1611, loss is 0.027970386669039726\n",
      "epoch: 6 step: 1612, loss is 0.007033000700175762\n",
      "epoch: 6 step: 1613, loss is 0.0024350143503397703\n",
      "epoch: 6 step: 1614, loss is 0.20704592764377594\n",
      "epoch: 6 step: 1615, loss is 0.00397414481267333\n",
      "epoch: 6 step: 1616, loss is 0.005905328784137964\n",
      "epoch: 6 step: 1617, loss is 0.0002230453974334523\n",
      "epoch: 6 step: 1618, loss is 0.0010149449808523059\n",
      "epoch: 6 step: 1619, loss is 0.00030948466155678034\n",
      "epoch: 6 step: 1620, loss is 0.0004279404238332063\n",
      "epoch: 6 step: 1621, loss is 0.0036612635012716055\n",
      "epoch: 6 step: 1622, loss is 0.017342600971460342\n",
      "epoch: 6 step: 1623, loss is 0.06489778310060501\n",
      "epoch: 6 step: 1624, loss is 0.006139116827398539\n",
      "epoch: 6 step: 1625, loss is 0.0062541840597987175\n",
      "epoch: 6 step: 1626, loss is 0.0017207091441377997\n",
      "epoch: 6 step: 1627, loss is 0.00040294337668456137\n",
      "epoch: 6 step: 1628, loss is 0.0005052771302871406\n",
      "epoch: 6 step: 1629, loss is 0.000238884735153988\n",
      "epoch: 6 step: 1630, loss is 0.00947150494903326\n",
      "epoch: 6 step: 1631, loss is 0.009206843562424183\n",
      "epoch: 6 step: 1632, loss is 0.0022673383355140686\n",
      "epoch: 6 step: 1633, loss is 0.035411905497312546\n",
      "epoch: 6 step: 1634, loss is 0.00102764624170959\n",
      "epoch: 6 step: 1635, loss is 0.17444784939289093\n",
      "epoch: 6 step: 1636, loss is 0.0003244916442781687\n",
      "epoch: 6 step: 1637, loss is 0.00023874593898653984\n",
      "epoch: 6 step: 1638, loss is 0.014042523689568043\n",
      "epoch: 6 step: 1639, loss is 0.027640553191304207\n",
      "epoch: 6 step: 1640, loss is 0.042174044996500015\n",
      "epoch: 6 step: 1641, loss is 0.01016667764633894\n",
      "epoch: 6 step: 1642, loss is 0.00012973023694939911\n",
      "epoch: 6 step: 1643, loss is 0.0031827911734580994\n",
      "epoch: 6 step: 1644, loss is 0.000285651272861287\n",
      "epoch: 6 step: 1645, loss is 0.0010985902044922113\n",
      "epoch: 6 step: 1646, loss is 0.0010411001276224852\n",
      "epoch: 6 step: 1647, loss is 0.0008860965608619153\n",
      "epoch: 6 step: 1648, loss is 0.0009589825058355927\n",
      "epoch: 6 step: 1649, loss is 0.03098960407078266\n",
      "epoch: 6 step: 1650, loss is 0.012968616560101509\n",
      "epoch: 6 step: 1651, loss is 0.0039918734692037106\n",
      "epoch: 6 step: 1652, loss is 0.0009028766653500497\n",
      "epoch: 6 step: 1653, loss is 0.0007895508897490799\n",
      "epoch: 6 step: 1654, loss is 0.19290649890899658\n",
      "epoch: 6 step: 1655, loss is 0.005790402181446552\n",
      "epoch: 6 step: 1656, loss is 0.0013161698589101434\n",
      "epoch: 6 step: 1657, loss is 0.0276245828717947\n",
      "epoch: 6 step: 1658, loss is 0.0029195286333560944\n",
      "epoch: 6 step: 1659, loss is 0.05965322256088257\n",
      "epoch: 6 step: 1660, loss is 0.11083412915468216\n",
      "epoch: 6 step: 1661, loss is 0.019009854644536972\n",
      "epoch: 6 step: 1662, loss is 0.014605780132114887\n",
      "epoch: 6 step: 1663, loss is 0.0005730996490456164\n",
      "epoch: 6 step: 1664, loss is 0.007831974886357784\n",
      "epoch: 6 step: 1665, loss is 0.0003268001601099968\n",
      "epoch: 6 step: 1666, loss is 0.0017790028359740973\n",
      "epoch: 6 step: 1667, loss is 0.03650248050689697\n",
      "epoch: 6 step: 1668, loss is 0.0003185331297572702\n",
      "epoch: 6 step: 1669, loss is 0.10156001150608063\n",
      "epoch: 6 step: 1670, loss is 0.01559310033917427\n",
      "epoch: 6 step: 1671, loss is 0.0016956966137513518\n",
      "epoch: 6 step: 1672, loss is 0.011740332469344139\n",
      "epoch: 6 step: 1673, loss is 0.006124991923570633\n",
      "epoch: 6 step: 1674, loss is 0.00024468827177770436\n",
      "epoch: 6 step: 1675, loss is 0.04323958605527878\n",
      "epoch: 6 step: 1676, loss is 0.000490847451146692\n",
      "epoch: 6 step: 1677, loss is 0.3219089210033417\n",
      "epoch: 6 step: 1678, loss is 0.0001052079678629525\n",
      "epoch: 6 step: 1679, loss is 0.009687134996056557\n",
      "epoch: 6 step: 1680, loss is 0.0012512298999354243\n",
      "epoch: 6 step: 1681, loss is 0.0013101563090458512\n",
      "epoch: 6 step: 1682, loss is 0.006333053112030029\n",
      "epoch: 6 step: 1683, loss is 0.005543325562030077\n",
      "epoch: 6 step: 1684, loss is 0.003503635060042143\n",
      "epoch: 6 step: 1685, loss is 0.001592142740264535\n",
      "epoch: 6 step: 1686, loss is 0.060189686715602875\n",
      "epoch: 6 step: 1687, loss is 0.0016482891514897346\n",
      "epoch: 6 step: 1688, loss is 0.056814033538103104\n",
      "epoch: 6 step: 1689, loss is 0.026200100779533386\n",
      "epoch: 6 step: 1690, loss is 0.0030326328705996275\n",
      "epoch: 6 step: 1691, loss is 0.0012748492881655693\n",
      "epoch: 6 step: 1692, loss is 0.017548197880387306\n",
      "epoch: 6 step: 1693, loss is 0.046626508235931396\n",
      "epoch: 6 step: 1694, loss is 0.03534388914704323\n",
      "epoch: 6 step: 1695, loss is 0.03290094435214996\n",
      "epoch: 6 step: 1696, loss is 0.0009253156022168696\n",
      "epoch: 6 step: 1697, loss is 0.0025552716106176376\n",
      "epoch: 6 step: 1698, loss is 0.010811451822519302\n",
      "epoch: 6 step: 1699, loss is 0.0015616226010024548\n",
      "epoch: 6 step: 1700, loss is 0.10255812853574753\n",
      "epoch: 6 step: 1701, loss is 0.00012306963617447764\n",
      "epoch: 6 step: 1702, loss is 0.0017650574445724487\n",
      "epoch: 6 step: 1703, loss is 0.00027228842373006046\n",
      "epoch: 6 step: 1704, loss is 0.017492715269327164\n",
      "epoch: 6 step: 1705, loss is 0.00469554727897048\n",
      "epoch: 6 step: 1706, loss is 0.08740983158349991\n",
      "epoch: 6 step: 1707, loss is 0.015889396890997887\n",
      "epoch: 6 step: 1708, loss is 0.1249750629067421\n",
      "epoch: 6 step: 1709, loss is 0.023755304515361786\n",
      "epoch: 6 step: 1710, loss is 0.04544205963611603\n",
      "epoch: 6 step: 1711, loss is 0.21822421252727509\n",
      "epoch: 6 step: 1712, loss is 0.002576030557975173\n",
      "epoch: 6 step: 1713, loss is 0.0012262422824278474\n",
      "epoch: 6 step: 1714, loss is 0.02294984459877014\n",
      "epoch: 6 step: 1715, loss is 0.00257762405090034\n",
      "epoch: 6 step: 1716, loss is 0.0019574312027543783\n",
      "epoch: 6 step: 1717, loss is 0.0230849701911211\n",
      "epoch: 6 step: 1718, loss is 0.058643314987421036\n",
      "epoch: 6 step: 1719, loss is 0.0006108562811277807\n",
      "epoch: 6 step: 1720, loss is 0.01678035967051983\n",
      "epoch: 6 step: 1721, loss is 0.0017150285420939326\n",
      "epoch: 6 step: 1722, loss is 0.011500042863190174\n",
      "epoch: 6 step: 1723, loss is 0.0011583325685933232\n",
      "epoch: 6 step: 1724, loss is 0.00028431942337192595\n",
      "epoch: 6 step: 1725, loss is 0.003913055639714003\n",
      "epoch: 6 step: 1726, loss is 0.004356256686151028\n",
      "epoch: 6 step: 1727, loss is 0.00770304212346673\n",
      "epoch: 6 step: 1728, loss is 0.00259115407243371\n",
      "epoch: 6 step: 1729, loss is 0.0034654762130230665\n",
      "epoch: 6 step: 1730, loss is 0.007205416914075613\n",
      "epoch: 6 step: 1731, loss is 0.07452911883592606\n",
      "epoch: 6 step: 1732, loss is 0.00013695440429728478\n",
      "epoch: 6 step: 1733, loss is 0.0015428184997290373\n",
      "epoch: 6 step: 1734, loss is 0.016801638528704643\n",
      "epoch: 6 step: 1735, loss is 0.0021915107499808073\n",
      "epoch: 6 step: 1736, loss is 0.15199637413024902\n",
      "epoch: 6 step: 1737, loss is 0.028209948912262917\n",
      "epoch: 6 step: 1738, loss is 0.10996681451797485\n",
      "epoch: 6 step: 1739, loss is 0.13178081810474396\n",
      "epoch: 6 step: 1740, loss is 0.0012285548727959394\n",
      "epoch: 6 step: 1741, loss is 0.003975237254053354\n",
      "epoch: 6 step: 1742, loss is 0.1286165565252304\n",
      "epoch: 6 step: 1743, loss is 0.0019972999580204487\n",
      "epoch: 6 step: 1744, loss is 0.009926091879606247\n",
      "epoch: 6 step: 1745, loss is 0.08740033209323883\n",
      "epoch: 6 step: 1746, loss is 0.011189529672265053\n",
      "epoch: 6 step: 1747, loss is 0.06727078557014465\n",
      "epoch: 6 step: 1748, loss is 0.35860493779182434\n",
      "epoch: 6 step: 1749, loss is 0.003217263612896204\n",
      "epoch: 6 step: 1750, loss is 0.00029462765087373555\n",
      "epoch: 6 step: 1751, loss is 0.009397471323609352\n",
      "epoch: 6 step: 1752, loss is 0.000351099472027272\n",
      "epoch: 6 step: 1753, loss is 0.00031374706304632127\n",
      "epoch: 6 step: 1754, loss is 0.0002583937894087285\n",
      "epoch: 6 step: 1755, loss is 0.007430320605635643\n",
      "epoch: 6 step: 1756, loss is 0.0265154168009758\n",
      "epoch: 6 step: 1757, loss is 0.005758089944720268\n",
      "epoch: 6 step: 1758, loss is 0.009075898677110672\n",
      "epoch: 6 step: 1759, loss is 0.08492179960012436\n",
      "epoch: 6 step: 1760, loss is 0.0376705639064312\n",
      "epoch: 6 step: 1761, loss is 0.03315981477499008\n",
      "epoch: 6 step: 1762, loss is 0.0027895700186491013\n",
      "epoch: 6 step: 1763, loss is 0.009596443735063076\n",
      "epoch: 6 step: 1764, loss is 0.016510693356394768\n",
      "epoch: 6 step: 1765, loss is 0.006874385755509138\n",
      "epoch: 6 step: 1766, loss is 0.0021166496444493532\n",
      "epoch: 6 step: 1767, loss is 0.03679030016064644\n",
      "epoch: 6 step: 1768, loss is 0.027998823672533035\n",
      "epoch: 6 step: 1769, loss is 0.0036655685398727655\n",
      "epoch: 6 step: 1770, loss is 0.0025117918848991394\n",
      "epoch: 6 step: 1771, loss is 0.0017950081964954734\n",
      "epoch: 6 step: 1772, loss is 0.17561405897140503\n",
      "epoch: 6 step: 1773, loss is 0.04892251268029213\n",
      "epoch: 6 step: 1774, loss is 0.004623205866664648\n",
      "epoch: 6 step: 1775, loss is 0.0013284326996654272\n",
      "epoch: 6 step: 1776, loss is 0.0510152168571949\n",
      "epoch: 6 step: 1777, loss is 0.012713826261460781\n",
      "epoch: 6 step: 1778, loss is 0.13967037200927734\n",
      "epoch: 6 step: 1779, loss is 0.001559742260724306\n",
      "epoch: 6 step: 1780, loss is 6.470984226325527e-05\n",
      "epoch: 6 step: 1781, loss is 0.09174291789531708\n",
      "epoch: 6 step: 1782, loss is 0.0013269612099975348\n",
      "epoch: 6 step: 1783, loss is 0.09821340441703796\n",
      "epoch: 6 step: 1784, loss is 0.014630191028118134\n",
      "epoch: 6 step: 1785, loss is 0.00891138706356287\n",
      "epoch: 6 step: 1786, loss is 0.006485686171799898\n",
      "epoch: 6 step: 1787, loss is 0.0020186235196888447\n",
      "epoch: 6 step: 1788, loss is 0.14090193808078766\n",
      "epoch: 6 step: 1789, loss is 0.003614545799791813\n",
      "epoch: 6 step: 1790, loss is 0.0013920590281486511\n",
      "epoch: 6 step: 1791, loss is 0.0006934546981938183\n",
      "epoch: 6 step: 1792, loss is 0.00039740768261253834\n",
      "epoch: 6 step: 1793, loss is 0.004158253315836191\n",
      "epoch: 6 step: 1794, loss is 0.0007339378353208303\n",
      "epoch: 6 step: 1795, loss is 0.0022995497565716505\n",
      "epoch: 6 step: 1796, loss is 0.0008594890823587775\n",
      "epoch: 6 step: 1797, loss is 0.0037689043674618006\n",
      "epoch: 6 step: 1798, loss is 0.017712421715259552\n",
      "epoch: 6 step: 1799, loss is 0.0075295171700417995\n",
      "epoch: 6 step: 1800, loss is 0.006725260987877846\n",
      "epoch: 6 step: 1801, loss is 0.000704664911609143\n",
      "epoch: 6 step: 1802, loss is 0.006932602263987064\n",
      "epoch: 6 step: 1803, loss is 0.0006270703743211925\n",
      "epoch: 6 step: 1804, loss is 0.001953590428456664\n",
      "epoch: 6 step: 1805, loss is 0.004359201528131962\n",
      "epoch: 6 step: 1806, loss is 0.028763839974999428\n",
      "epoch: 6 step: 1807, loss is 0.045430783182382584\n",
      "epoch: 6 step: 1808, loss is 0.012671593576669693\n",
      "epoch: 6 step: 1809, loss is 0.01358584314584732\n",
      "epoch: 6 step: 1810, loss is 0.0022924812510609627\n",
      "epoch: 6 step: 1811, loss is 0.0037336796522140503\n",
      "epoch: 6 step: 1812, loss is 0.00663925101980567\n",
      "epoch: 6 step: 1813, loss is 0.0007665933226235211\n",
      "epoch: 6 step: 1814, loss is 0.03178403899073601\n",
      "epoch: 6 step: 1815, loss is 0.0013618802186101675\n",
      "epoch: 6 step: 1816, loss is 0.0007086388068273664\n",
      "epoch: 6 step: 1817, loss is 0.02202056162059307\n",
      "epoch: 6 step: 1818, loss is 0.013053864240646362\n",
      "epoch: 6 step: 1819, loss is 0.00108529778663069\n",
      "epoch: 6 step: 1820, loss is 0.018810471519827843\n",
      "epoch: 6 step: 1821, loss is 0.019291933625936508\n",
      "epoch: 6 step: 1822, loss is 0.0022885382641106844\n",
      "epoch: 6 step: 1823, loss is 0.0005472631310112774\n",
      "epoch: 6 step: 1824, loss is 0.0073351990431547165\n",
      "epoch: 6 step: 1825, loss is 0.04140004515647888\n",
      "epoch: 6 step: 1826, loss is 0.02127690054476261\n",
      "epoch: 6 step: 1827, loss is 0.0009641392389312387\n",
      "epoch: 6 step: 1828, loss is 0.09611392766237259\n",
      "epoch: 6 step: 1829, loss is 7.396652654279023e-05\n",
      "epoch: 6 step: 1830, loss is 0.006222354248166084\n",
      "epoch: 6 step: 1831, loss is 0.003888342995196581\n",
      "epoch: 6 step: 1832, loss is 0.00518827186897397\n",
      "epoch: 6 step: 1833, loss is 0.00035060610389336944\n",
      "epoch: 6 step: 1834, loss is 0.042290788143873215\n",
      "epoch: 6 step: 1835, loss is 0.0008477078517898917\n",
      "epoch: 6 step: 1836, loss is 0.0018185031367465854\n",
      "epoch: 6 step: 1837, loss is 0.0003356982197146863\n",
      "epoch: 6 step: 1838, loss is 0.0043631065636873245\n",
      "epoch: 6 step: 1839, loss is 0.00012849645281676203\n",
      "epoch: 6 step: 1840, loss is 0.017442509531974792\n",
      "epoch: 6 step: 1841, loss is 0.0017248217482119799\n",
      "epoch: 6 step: 1842, loss is 0.0714644119143486\n",
      "epoch: 6 step: 1843, loss is 7.30019310140051e-05\n",
      "epoch: 6 step: 1844, loss is 0.0005770384450443089\n",
      "epoch: 6 step: 1845, loss is 0.004135990981012583\n",
      "epoch: 6 step: 1846, loss is 0.02135380171239376\n",
      "epoch: 6 step: 1847, loss is 0.0035727478098124266\n",
      "epoch: 6 step: 1848, loss is 0.006231474224478006\n",
      "epoch: 6 step: 1849, loss is 0.0011004243278875947\n",
      "epoch: 6 step: 1850, loss is 0.03508223593235016\n",
      "epoch: 6 step: 1851, loss is 0.001177572412416339\n",
      "epoch: 6 step: 1852, loss is 0.013119937852025032\n",
      "epoch: 6 step: 1853, loss is 0.008077330887317657\n",
      "epoch: 6 step: 1854, loss is 0.0006196714821271598\n",
      "epoch: 6 step: 1855, loss is 6.46872867946513e-05\n",
      "epoch: 6 step: 1856, loss is 0.006154412403702736\n",
      "epoch: 6 step: 1857, loss is 0.0017504123970866203\n",
      "epoch: 6 step: 1858, loss is 0.0002101096761180088\n",
      "epoch: 6 step: 1859, loss is 0.007667915429919958\n",
      "epoch: 6 step: 1860, loss is 0.004871389828622341\n",
      "epoch: 6 step: 1861, loss is 0.0017717020818963647\n",
      "epoch: 6 step: 1862, loss is 0.002149889711290598\n",
      "epoch: 6 step: 1863, loss is 0.017452800646424294\n",
      "epoch: 6 step: 1864, loss is 8.662945765536278e-05\n",
      "epoch: 6 step: 1865, loss is 0.0013939369237050414\n",
      "epoch: 6 step: 1866, loss is 0.07557670772075653\n",
      "epoch: 6 step: 1867, loss is 0.009230735711753368\n",
      "epoch: 6 step: 1868, loss is 0.002259026514366269\n",
      "epoch: 6 step: 1869, loss is 0.0004639659309759736\n",
      "epoch: 6 step: 1870, loss is 0.3594665825366974\n",
      "epoch: 6 step: 1871, loss is 0.0017299901228398085\n",
      "epoch: 6 step: 1872, loss is 6.295795174082741e-06\n",
      "epoch: 6 step: 1873, loss is 0.0008948398754000664\n",
      "epoch: 6 step: 1874, loss is 0.00034438868169672787\n",
      "epoch: 6 step: 1875, loss is 0.03606366738677025\n",
      "Train epoch time: 15339.098 ms, per step time: 8.181 ms\n",
      "epoch: 7 step: 1, loss is 0.005751810967922211\n",
      "epoch: 7 step: 2, loss is 0.00015217604232020676\n",
      "epoch: 7 step: 3, loss is 0.08570582419633865\n",
      "epoch: 7 step: 4, loss is 0.00537217827513814\n",
      "epoch: 7 step: 5, loss is 0.0005364046082831919\n",
      "epoch: 7 step: 6, loss is 0.08560137450695038\n",
      "epoch: 7 step: 7, loss is 0.00023765681544318795\n",
      "epoch: 7 step: 8, loss is 0.00029344356153160334\n",
      "epoch: 7 step: 9, loss is 0.08286526054143906\n",
      "epoch: 7 step: 10, loss is 0.0008823678945191205\n",
      "epoch: 7 step: 11, loss is 0.0007879396434873343\n",
      "epoch: 7 step: 12, loss is 0.06471248716115952\n",
      "epoch: 7 step: 13, loss is 0.0012920753797516227\n",
      "epoch: 7 step: 14, loss is 0.00046985645894892514\n",
      "epoch: 7 step: 15, loss is 0.0040677315555512905\n",
      "epoch: 7 step: 16, loss is 0.0005583995953202248\n",
      "epoch: 7 step: 17, loss is 0.00046985631342977285\n",
      "epoch: 7 step: 18, loss is 0.002021385356783867\n",
      "epoch: 7 step: 19, loss is 0.0025367194321006536\n",
      "epoch: 7 step: 20, loss is 0.0071505168452858925\n",
      "epoch: 7 step: 21, loss is 0.0034904642961919308\n",
      "epoch: 7 step: 22, loss is 0.028893165290355682\n",
      "epoch: 7 step: 23, loss is 0.0011575032258406281\n",
      "epoch: 7 step: 24, loss is 0.06264232844114304\n",
      "epoch: 7 step: 25, loss is 0.04689552262425423\n",
      "epoch: 7 step: 26, loss is 0.0011982513824477792\n",
      "epoch: 7 step: 27, loss is 0.01610439643263817\n",
      "epoch: 7 step: 28, loss is 0.046207454055547714\n",
      "epoch: 7 step: 29, loss is 0.011090055108070374\n",
      "epoch: 7 step: 30, loss is 0.006159704178571701\n",
      "epoch: 7 step: 31, loss is 0.0003171099233441055\n",
      "epoch: 7 step: 32, loss is 0.0005789099959656596\n",
      "epoch: 7 step: 33, loss is 0.06391040235757828\n",
      "epoch: 7 step: 34, loss is 0.004198523238301277\n",
      "epoch: 7 step: 35, loss is 0.004230692517012358\n",
      "epoch: 7 step: 36, loss is 0.0036785947158932686\n",
      "epoch: 7 step: 37, loss is 0.001127677853219211\n",
      "epoch: 7 step: 38, loss is 0.0007887502433732152\n",
      "epoch: 7 step: 39, loss is 0.0003460606385488063\n",
      "epoch: 7 step: 40, loss is 0.0014614914543926716\n",
      "epoch: 7 step: 41, loss is 0.010933924466371536\n",
      "epoch: 7 step: 42, loss is 0.015124362893402576\n",
      "epoch: 7 step: 43, loss is 0.0017444862751290202\n",
      "epoch: 7 step: 44, loss is 4.049850394949317e-05\n",
      "epoch: 7 step: 45, loss is 0.02319410815834999\n",
      "epoch: 7 step: 46, loss is 0.05922474339604378\n",
      "epoch: 7 step: 47, loss is 0.027247117832303047\n",
      "epoch: 7 step: 48, loss is 9.971693361876532e-05\n",
      "epoch: 7 step: 49, loss is 0.0021328697912395\n",
      "epoch: 7 step: 50, loss is 0.016495322808623314\n",
      "epoch: 7 step: 51, loss is 0.010412520729005337\n",
      "epoch: 7 step: 52, loss is 0.2748993933200836\n",
      "epoch: 7 step: 53, loss is 0.002803352428600192\n",
      "epoch: 7 step: 54, loss is 0.0003571747220121324\n",
      "epoch: 7 step: 55, loss is 0.015567749738693237\n",
      "epoch: 7 step: 56, loss is 0.0006351692136377096\n",
      "epoch: 7 step: 57, loss is 0.032283272594213486\n",
      "epoch: 7 step: 58, loss is 0.07296592742204666\n",
      "epoch: 7 step: 59, loss is 0.0038575432263314724\n",
      "epoch: 7 step: 60, loss is 0.0021658861078321934\n",
      "epoch: 7 step: 61, loss is 0.05772151052951813\n",
      "epoch: 7 step: 62, loss is 0.011572240851819515\n",
      "epoch: 7 step: 63, loss is 0.0012158640893176198\n",
      "epoch: 7 step: 64, loss is 0.03607283532619476\n",
      "epoch: 7 step: 65, loss is 0.0009468167554587126\n",
      "epoch: 7 step: 66, loss is 0.04093262180685997\n",
      "epoch: 7 step: 67, loss is 0.0008531924104318023\n",
      "epoch: 7 step: 68, loss is 4.048429036629386e-05\n",
      "epoch: 7 step: 69, loss is 0.01798793114721775\n",
      "epoch: 7 step: 70, loss is 0.0023873706813901663\n",
      "epoch: 7 step: 71, loss is 0.015975207090377808\n",
      "epoch: 7 step: 72, loss is 4.0049362723948434e-05\n",
      "epoch: 7 step: 73, loss is 0.027112219482660294\n",
      "epoch: 7 step: 74, loss is 0.012227866798639297\n",
      "epoch: 7 step: 75, loss is 0.0010270095663145185\n",
      "epoch: 7 step: 76, loss is 0.004845389164984226\n",
      "epoch: 7 step: 77, loss is 0.009307656437158585\n",
      "epoch: 7 step: 78, loss is 0.007017955183982849\n",
      "epoch: 7 step: 79, loss is 0.1751725822687149\n",
      "epoch: 7 step: 80, loss is 0.0013258977560326457\n",
      "epoch: 7 step: 81, loss is 0.018547359853982925\n",
      "epoch: 7 step: 82, loss is 0.0011412559542804956\n",
      "epoch: 7 step: 83, loss is 0.008820752613246441\n",
      "epoch: 7 step: 84, loss is 0.0017034340417012572\n",
      "epoch: 7 step: 85, loss is 0.001495902892202139\n",
      "epoch: 7 step: 86, loss is 0.04651172459125519\n",
      "epoch: 7 step: 87, loss is 0.052859798073768616\n",
      "epoch: 7 step: 88, loss is 0.03646104410290718\n",
      "epoch: 7 step: 89, loss is 0.05483465641736984\n",
      "epoch: 7 step: 90, loss is 0.010842117480933666\n",
      "epoch: 7 step: 91, loss is 0.0017160847783088684\n",
      "epoch: 7 step: 92, loss is 0.011536741629242897\n",
      "epoch: 7 step: 93, loss is 0.0005076497327536345\n",
      "epoch: 7 step: 94, loss is 0.05161984637379646\n",
      "epoch: 7 step: 95, loss is 0.0049422006122767925\n",
      "epoch: 7 step: 96, loss is 0.09501726180315018\n",
      "epoch: 7 step: 97, loss is 0.0007131091551855206\n",
      "epoch: 7 step: 98, loss is 0.005697229877114296\n",
      "epoch: 7 step: 99, loss is 0.029631126672029495\n",
      "epoch: 7 step: 100, loss is 0.00040912971599027514\n",
      "epoch: 7 step: 101, loss is 0.016067832708358765\n",
      "epoch: 7 step: 102, loss is 0.009498034603893757\n",
      "epoch: 7 step: 103, loss is 0.04480966553092003\n",
      "epoch: 7 step: 104, loss is 0.009483315981924534\n",
      "epoch: 7 step: 105, loss is 0.012202833779156208\n",
      "epoch: 7 step: 106, loss is 0.02295779623091221\n",
      "epoch: 7 step: 107, loss is 0.015400119125843048\n",
      "epoch: 7 step: 108, loss is 0.026079881936311722\n",
      "epoch: 7 step: 109, loss is 0.004100629594177008\n",
      "epoch: 7 step: 110, loss is 0.0025455052964389324\n",
      "epoch: 7 step: 111, loss is 0.018870780244469643\n",
      "epoch: 7 step: 112, loss is 0.013071785680949688\n",
      "epoch: 7 step: 113, loss is 0.0038217606488615274\n",
      "epoch: 7 step: 114, loss is 0.09793372452259064\n",
      "epoch: 7 step: 115, loss is 0.05140281468629837\n",
      "epoch: 7 step: 116, loss is 0.0008799560600891709\n",
      "epoch: 7 step: 117, loss is 0.0208639707416296\n",
      "epoch: 7 step: 118, loss is 0.0012011093785986304\n",
      "epoch: 7 step: 119, loss is 0.0005591303925029933\n",
      "epoch: 7 step: 120, loss is 0.00293933623470366\n",
      "epoch: 7 step: 121, loss is 0.0019995092879980803\n",
      "epoch: 7 step: 122, loss is 0.01651180349290371\n",
      "epoch: 7 step: 123, loss is 0.0031468034721910954\n",
      "epoch: 7 step: 124, loss is 0.001739382278174162\n",
      "epoch: 7 step: 125, loss is 0.034397292882204056\n",
      "epoch: 7 step: 126, loss is 0.13258987665176392\n",
      "epoch: 7 step: 127, loss is 0.00017610880604479462\n",
      "epoch: 7 step: 128, loss is 0.00456896610558033\n",
      "epoch: 7 step: 129, loss is 0.0005745927919633687\n",
      "epoch: 7 step: 130, loss is 0.0012599118053913116\n",
      "epoch: 7 step: 131, loss is 0.0003866290207952261\n",
      "epoch: 7 step: 132, loss is 0.05190642178058624\n",
      "epoch: 7 step: 133, loss is 0.02593952976167202\n",
      "epoch: 7 step: 134, loss is 0.04001833125948906\n",
      "epoch: 7 step: 135, loss is 0.00040897546568885446\n",
      "epoch: 7 step: 136, loss is 0.05313264578580856\n",
      "epoch: 7 step: 137, loss is 0.022941866889595985\n",
      "epoch: 7 step: 138, loss is 0.0032223903108388186\n",
      "epoch: 7 step: 139, loss is 0.00026635490939952433\n",
      "epoch: 7 step: 140, loss is 0.023209892213344574\n",
      "epoch: 7 step: 141, loss is 0.0030189899262040854\n",
      "epoch: 7 step: 142, loss is 0.0003561786434147507\n",
      "epoch: 7 step: 143, loss is 0.008459844626486301\n",
      "epoch: 7 step: 144, loss is 0.008985351771116257\n",
      "epoch: 7 step: 145, loss is 3.891106462106109e-05\n",
      "epoch: 7 step: 146, loss is 0.00047438190085813403\n",
      "epoch: 7 step: 147, loss is 0.003806547960266471\n",
      "epoch: 7 step: 148, loss is 0.03255969658493996\n",
      "epoch: 7 step: 149, loss is 0.00710753770545125\n",
      "epoch: 7 step: 150, loss is 0.0015996088040992618\n",
      "epoch: 7 step: 151, loss is 0.004319066181778908\n",
      "epoch: 7 step: 152, loss is 0.0009438317501917481\n",
      "epoch: 7 step: 153, loss is 0.00626903073862195\n",
      "epoch: 7 step: 154, loss is 0.0012139916652813554\n",
      "epoch: 7 step: 155, loss is 0.00016010126273613423\n",
      "epoch: 7 step: 156, loss is 0.01672997698187828\n",
      "epoch: 7 step: 157, loss is 0.0008576372638344765\n",
      "epoch: 7 step: 158, loss is 0.05024798586964607\n",
      "epoch: 7 step: 159, loss is 0.008637655526399612\n",
      "epoch: 7 step: 160, loss is 0.0014702901244163513\n",
      "epoch: 7 step: 161, loss is 0.001788599998690188\n",
      "epoch: 7 step: 162, loss is 0.08723165094852448\n",
      "epoch: 7 step: 163, loss is 0.0005066796438768506\n",
      "epoch: 7 step: 164, loss is 0.0015192906139418483\n",
      "epoch: 7 step: 165, loss is 0.00017590713105164468\n",
      "epoch: 7 step: 166, loss is 0.0004867360694333911\n",
      "epoch: 7 step: 167, loss is 0.001823398401029408\n",
      "epoch: 7 step: 168, loss is 0.0007721791043877602\n",
      "epoch: 7 step: 169, loss is 0.0060409922152757645\n",
      "epoch: 7 step: 170, loss is 0.0008502327837049961\n",
      "epoch: 7 step: 171, loss is 0.002406357554718852\n",
      "epoch: 7 step: 172, loss is 0.00038015798781998456\n",
      "epoch: 7 step: 173, loss is 0.06023111194372177\n",
      "epoch: 7 step: 174, loss is 0.13680103421211243\n",
      "epoch: 7 step: 175, loss is 0.001454624580219388\n",
      "epoch: 7 step: 176, loss is 0.016823308542370796\n",
      "epoch: 7 step: 177, loss is 0.01190814096480608\n",
      "epoch: 7 step: 178, loss is 0.0018228928092867136\n",
      "epoch: 7 step: 179, loss is 0.17811107635498047\n",
      "epoch: 7 step: 180, loss is 0.0010349518852308393\n",
      "epoch: 7 step: 181, loss is 0.017834670841693878\n",
      "epoch: 7 step: 182, loss is 0.0025911133270710707\n",
      "epoch: 7 step: 183, loss is 0.002000731648877263\n",
      "epoch: 7 step: 184, loss is 0.0009390914929099381\n",
      "epoch: 7 step: 185, loss is 0.0054038166999816895\n",
      "epoch: 7 step: 186, loss is 0.005345124751329422\n",
      "epoch: 7 step: 187, loss is 0.021906239911913872\n",
      "epoch: 7 step: 188, loss is 0.003237185999751091\n",
      "epoch: 7 step: 189, loss is 0.05680195987224579\n",
      "epoch: 7 step: 190, loss is 0.004883217625319958\n",
      "epoch: 7 step: 191, loss is 0.0004470801504794508\n",
      "epoch: 7 step: 192, loss is 0.11925332993268967\n",
      "epoch: 7 step: 193, loss is 0.002831802936270833\n",
      "epoch: 7 step: 194, loss is 0.0012203101068735123\n",
      "epoch: 7 step: 195, loss is 0.000212580751394853\n",
      "epoch: 7 step: 196, loss is 0.054291434586048126\n",
      "epoch: 7 step: 197, loss is 0.02487216889858246\n",
      "epoch: 7 step: 198, loss is 0.0011916100047528744\n",
      "epoch: 7 step: 199, loss is 0.0007232140051200986\n",
      "epoch: 7 step: 200, loss is 0.00583741394802928\n",
      "epoch: 7 step: 201, loss is 0.008577516302466393\n",
      "epoch: 7 step: 202, loss is 0.004029796924442053\n",
      "epoch: 7 step: 203, loss is 0.14750099182128906\n",
      "epoch: 7 step: 204, loss is 0.021540429443120956\n",
      "epoch: 7 step: 205, loss is 0.005356556735932827\n",
      "epoch: 7 step: 206, loss is 0.08243289589881897\n",
      "epoch: 7 step: 207, loss is 0.0007066904217936099\n",
      "epoch: 7 step: 208, loss is 0.0006498589646071196\n",
      "epoch: 7 step: 209, loss is 0.0003977090527769178\n",
      "epoch: 7 step: 210, loss is 0.020586034283041954\n",
      "epoch: 7 step: 211, loss is 0.001662424416281283\n",
      "epoch: 7 step: 212, loss is 0.0004399924073368311\n",
      "epoch: 7 step: 213, loss is 0.0017509901663288474\n",
      "epoch: 7 step: 214, loss is 0.0025019189342856407\n",
      "epoch: 7 step: 215, loss is 0.011055628769099712\n",
      "epoch: 7 step: 216, loss is 0.11779901385307312\n",
      "epoch: 7 step: 217, loss is 0.0008891028119251132\n",
      "epoch: 7 step: 218, loss is 0.00015831612108740956\n",
      "epoch: 7 step: 219, loss is 0.0639171451330185\n",
      "epoch: 7 step: 220, loss is 0.005471024662256241\n",
      "epoch: 7 step: 221, loss is 0.0007663283031433821\n",
      "epoch: 7 step: 222, loss is 0.2941247820854187\n",
      "epoch: 7 step: 223, loss is 0.004361164756119251\n",
      "epoch: 7 step: 224, loss is 0.014777938835322857\n",
      "epoch: 7 step: 225, loss is 0.0012327292934060097\n",
      "epoch: 7 step: 226, loss is 0.00687676016241312\n",
      "epoch: 7 step: 227, loss is 0.005973325110971928\n",
      "epoch: 7 step: 228, loss is 0.11897989362478256\n",
      "epoch: 7 step: 229, loss is 0.004024962894618511\n",
      "epoch: 7 step: 230, loss is 0.0005466268630698323\n",
      "epoch: 7 step: 231, loss is 0.011657397262752056\n",
      "epoch: 7 step: 232, loss is 0.007542657200247049\n",
      "epoch: 7 step: 233, loss is 0.0038765964563935995\n",
      "epoch: 7 step: 234, loss is 0.004224137868732214\n",
      "epoch: 7 step: 235, loss is 0.011368071660399437\n",
      "epoch: 7 step: 236, loss is 0.000676801020745188\n",
      "epoch: 7 step: 237, loss is 0.0017641558079048991\n",
      "epoch: 7 step: 238, loss is 0.0026947984006255865\n",
      "epoch: 7 step: 239, loss is 0.000678747019264847\n",
      "epoch: 7 step: 240, loss is 0.0009805462323129177\n",
      "epoch: 7 step: 241, loss is 0.00030774608603678644\n",
      "epoch: 7 step: 242, loss is 0.0014313093852251768\n",
      "epoch: 7 step: 243, loss is 0.0001613170898053795\n",
      "epoch: 7 step: 244, loss is 0.020401140674948692\n",
      "epoch: 7 step: 245, loss is 0.06714040040969849\n",
      "epoch: 7 step: 246, loss is 0.0005257123848423362\n",
      "epoch: 7 step: 247, loss is 0.0007472913130186498\n",
      "epoch: 7 step: 248, loss is 0.0005369241698645055\n",
      "epoch: 7 step: 249, loss is 0.0018054478568956256\n",
      "epoch: 7 step: 250, loss is 0.002517866203561425\n",
      "epoch: 7 step: 251, loss is 0.04196612536907196\n",
      "epoch: 7 step: 252, loss is 0.005033780355006456\n",
      "epoch: 7 step: 253, loss is 0.00066693767439574\n",
      "epoch: 7 step: 254, loss is 0.0005090546910651028\n",
      "epoch: 7 step: 255, loss is 0.00048394600162282586\n",
      "epoch: 7 step: 256, loss is 0.0016388226067647338\n",
      "epoch: 7 step: 257, loss is 0.0008496150257997215\n",
      "epoch: 7 step: 258, loss is 0.011499582789838314\n",
      "epoch: 7 step: 259, loss is 0.010875297710299492\n",
      "epoch: 7 step: 260, loss is 0.00020990832126699388\n",
      "epoch: 7 step: 261, loss is 0.00019992425222881138\n",
      "epoch: 7 step: 262, loss is 0.00775790074840188\n",
      "epoch: 7 step: 263, loss is 0.051325492560863495\n",
      "epoch: 7 step: 264, loss is 0.0054467846639454365\n",
      "epoch: 7 step: 265, loss is 0.0004219959955662489\n",
      "epoch: 7 step: 266, loss is 0.005691039375960827\n",
      "epoch: 7 step: 267, loss is 0.00031610674341209233\n",
      "epoch: 7 step: 268, loss is 0.0012759009841829538\n",
      "epoch: 7 step: 269, loss is 0.0008724847575649619\n",
      "epoch: 7 step: 270, loss is 0.0005199337610974908\n",
      "epoch: 7 step: 271, loss is 0.0012479581637308002\n",
      "epoch: 7 step: 272, loss is 0.015367136336863041\n",
      "epoch: 7 step: 273, loss is 0.0007149866432882845\n",
      "epoch: 7 step: 274, loss is 0.0035339780151844025\n",
      "epoch: 7 step: 275, loss is 0.0011230745585635304\n",
      "epoch: 7 step: 276, loss is 0.000318706501275301\n",
      "epoch: 7 step: 277, loss is 0.003999256528913975\n",
      "epoch: 7 step: 278, loss is 0.006772402673959732\n",
      "epoch: 7 step: 279, loss is 0.0017340144841000438\n",
      "epoch: 7 step: 280, loss is 0.0005416019703261554\n",
      "epoch: 7 step: 281, loss is 0.00020947751181665808\n",
      "epoch: 7 step: 282, loss is 0.0009049733635038137\n",
      "epoch: 7 step: 283, loss is 0.0038115493953227997\n",
      "epoch: 7 step: 284, loss is 0.008515936322510242\n",
      "epoch: 7 step: 285, loss is 0.000596956058871001\n",
      "epoch: 7 step: 286, loss is 0.019471535459160805\n",
      "epoch: 7 step: 287, loss is 0.1175651028752327\n",
      "epoch: 7 step: 288, loss is 0.0009589357068762183\n",
      "epoch: 7 step: 289, loss is 0.0008651543757878244\n",
      "epoch: 7 step: 290, loss is 0.004416303243488073\n",
      "epoch: 7 step: 291, loss is 0.001219928148202598\n",
      "epoch: 7 step: 292, loss is 0.07269237190485\n",
      "epoch: 7 step: 293, loss is 0.053010184317827225\n",
      "epoch: 7 step: 294, loss is 0.001360671129077673\n",
      "epoch: 7 step: 295, loss is 0.0014314441941678524\n",
      "epoch: 7 step: 296, loss is 0.0008886092691682279\n",
      "epoch: 7 step: 297, loss is 0.006443379912525415\n",
      "epoch: 7 step: 298, loss is 0.017513368278741837\n",
      "epoch: 7 step: 299, loss is 0.0009895932162180543\n",
      "epoch: 7 step: 300, loss is 0.00120557623449713\n",
      "epoch: 7 step: 301, loss is 0.00042877555824816227\n",
      "epoch: 7 step: 302, loss is 0.00513386121019721\n",
      "epoch: 7 step: 303, loss is 0.07374799251556396\n",
      "epoch: 7 step: 304, loss is 0.0004873752186540514\n",
      "epoch: 7 step: 305, loss is 0.18806925415992737\n",
      "epoch: 7 step: 306, loss is 0.0002810258010867983\n",
      "epoch: 7 step: 307, loss is 0.0005637124413624406\n",
      "epoch: 7 step: 308, loss is 0.00023343924840446562\n",
      "epoch: 7 step: 309, loss is 0.0030372522305697203\n",
      "epoch: 7 step: 310, loss is 0.013360527344048023\n",
      "epoch: 7 step: 311, loss is 0.0043385145254433155\n",
      "epoch: 7 step: 312, loss is 0.0006982663762755692\n",
      "epoch: 7 step: 313, loss is 0.0016990189906209707\n",
      "epoch: 7 step: 314, loss is 0.00046449981164187193\n",
      "epoch: 7 step: 315, loss is 0.004232133273035288\n",
      "epoch: 7 step: 316, loss is 0.002157330047339201\n",
      "epoch: 7 step: 317, loss is 0.009237928315997124\n",
      "epoch: 7 step: 318, loss is 0.0004907757975161076\n",
      "epoch: 7 step: 319, loss is 0.0036817649379372597\n",
      "epoch: 7 step: 320, loss is 0.007289453875273466\n",
      "epoch: 7 step: 321, loss is 0.015554998070001602\n",
      "epoch: 7 step: 322, loss is 0.0007325956830754876\n",
      "epoch: 7 step: 323, loss is 0.004843556322157383\n",
      "epoch: 7 step: 324, loss is 0.0003318952221889049\n",
      "epoch: 7 step: 325, loss is 0.0006064495537430048\n",
      "epoch: 7 step: 326, loss is 0.04630517214536667\n",
      "epoch: 7 step: 327, loss is 0.00032093864865601063\n",
      "epoch: 7 step: 328, loss is 0.0026300896424800158\n",
      "epoch: 7 step: 329, loss is 0.015254157595336437\n",
      "epoch: 7 step: 330, loss is 0.0015180377522483468\n",
      "epoch: 7 step: 331, loss is 0.09701011329889297\n",
      "epoch: 7 step: 332, loss is 0.0005018346128053963\n",
      "epoch: 7 step: 333, loss is 0.0007152253529056907\n",
      "epoch: 7 step: 334, loss is 0.010337846353650093\n",
      "epoch: 7 step: 335, loss is 8.964203880168498e-05\n",
      "epoch: 7 step: 336, loss is 0.0008230770472437143\n",
      "epoch: 7 step: 337, loss is 0.003188574453815818\n",
      "epoch: 7 step: 338, loss is 0.013254976831376553\n",
      "epoch: 7 step: 339, loss is 0.01463552936911583\n",
      "epoch: 7 step: 340, loss is 0.005432545207440853\n",
      "epoch: 7 step: 341, loss is 0.028604969382286072\n",
      "epoch: 7 step: 342, loss is 0.0038265318144112825\n",
      "epoch: 7 step: 343, loss is 0.0006785139557905495\n",
      "epoch: 7 step: 344, loss is 0.00012835476081818342\n",
      "epoch: 7 step: 345, loss is 0.006056535057723522\n",
      "epoch: 7 step: 346, loss is 0.0025415404234081507\n",
      "epoch: 7 step: 347, loss is 0.00022614085173700005\n",
      "epoch: 7 step: 348, loss is 9.871652582660317e-05\n",
      "epoch: 7 step: 349, loss is 0.0008509535691700876\n",
      "epoch: 7 step: 350, loss is 0.012858445756137371\n",
      "epoch: 7 step: 351, loss is 0.0004290509968996048\n",
      "epoch: 7 step: 352, loss is 3.048761391255539e-05\n",
      "epoch: 7 step: 353, loss is 0.0015107817016541958\n",
      "epoch: 7 step: 354, loss is 0.00041451278957538307\n",
      "epoch: 7 step: 355, loss is 0.006603596732020378\n",
      "epoch: 7 step: 356, loss is 0.11221208423376083\n",
      "epoch: 7 step: 357, loss is 0.0011117036920040846\n",
      "epoch: 7 step: 358, loss is 0.00449725054204464\n",
      "epoch: 7 step: 359, loss is 0.0018158515449613333\n",
      "epoch: 7 step: 360, loss is 0.028627784922719002\n",
      "epoch: 7 step: 361, loss is 0.02608497068285942\n",
      "epoch: 7 step: 362, loss is 0.0013662743149325252\n",
      "epoch: 7 step: 363, loss is 0.000345707027008757\n",
      "epoch: 7 step: 364, loss is 0.03699716553092003\n",
      "epoch: 7 step: 365, loss is 3.12581323669292e-05\n",
      "epoch: 7 step: 366, loss is 0.012579425238072872\n",
      "epoch: 7 step: 367, loss is 1.2277560017537326e-05\n",
      "epoch: 7 step: 368, loss is 4.628302122000605e-05\n",
      "epoch: 7 step: 369, loss is 3.14099743263796e-05\n",
      "epoch: 7 step: 370, loss is 0.000581573520321399\n",
      "epoch: 7 step: 371, loss is 0.00012331608741078526\n",
      "epoch: 7 step: 372, loss is 0.0001432506542187184\n",
      "epoch: 7 step: 373, loss is 0.004687754437327385\n",
      "epoch: 7 step: 374, loss is 0.054493822157382965\n",
      "epoch: 7 step: 375, loss is 0.0008727137465029955\n",
      "epoch: 7 step: 376, loss is 0.000143538840347901\n",
      "epoch: 7 step: 377, loss is 0.00011023322440451011\n",
      "epoch: 7 step: 378, loss is 0.00016638307715766132\n",
      "epoch: 7 step: 379, loss is 0.0003539459139574319\n",
      "epoch: 7 step: 380, loss is 0.16675354540348053\n",
      "epoch: 7 step: 381, loss is 0.015442883595824242\n",
      "epoch: 7 step: 382, loss is 0.006368900183588266\n",
      "epoch: 7 step: 383, loss is 0.009462657384574413\n",
      "epoch: 7 step: 384, loss is 0.00029391286079771817\n",
      "epoch: 7 step: 385, loss is 0.00023140170378610492\n",
      "epoch: 7 step: 386, loss is 0.001958640059456229\n",
      "epoch: 7 step: 387, loss is 0.016256630420684814\n",
      "epoch: 7 step: 388, loss is 0.005605244543403387\n",
      "epoch: 7 step: 389, loss is 0.004565129522234201\n",
      "epoch: 7 step: 390, loss is 0.0026206495240330696\n",
      "epoch: 7 step: 391, loss is 7.841923797968775e-05\n",
      "epoch: 7 step: 392, loss is 0.1001526340842247\n",
      "epoch: 7 step: 393, loss is 0.0006879284628666937\n",
      "epoch: 7 step: 394, loss is 5.315074304235168e-05\n",
      "epoch: 7 step: 395, loss is 0.000501060567330569\n",
      "epoch: 7 step: 396, loss is 0.00012396127567626536\n",
      "epoch: 7 step: 397, loss is 0.059873200953006744\n",
      "epoch: 7 step: 398, loss is 0.0030459410045295954\n",
      "epoch: 7 step: 399, loss is 0.005000494420528412\n",
      "epoch: 7 step: 400, loss is 0.014944112859666348\n",
      "epoch: 7 step: 401, loss is 0.0004298370913602412\n",
      "epoch: 7 step: 402, loss is 0.00022822532628197223\n",
      "epoch: 7 step: 403, loss is 9.090326784644276e-05\n",
      "epoch: 7 step: 404, loss is 0.02317728102207184\n",
      "epoch: 7 step: 405, loss is 0.0005905876751057804\n",
      "epoch: 7 step: 406, loss is 1.3566736015491188e-05\n",
      "epoch: 7 step: 407, loss is 0.0012029623612761497\n",
      "epoch: 7 step: 408, loss is 0.001517664291895926\n",
      "epoch: 7 step: 409, loss is 0.00017898590886034071\n",
      "epoch: 7 step: 410, loss is 0.001408722368068993\n",
      "epoch: 7 step: 411, loss is 0.014082755893468857\n",
      "epoch: 7 step: 412, loss is 0.0031881192699074745\n",
      "epoch: 7 step: 413, loss is 5.870335007784888e-05\n",
      "epoch: 7 step: 414, loss is 0.004673967603594065\n",
      "epoch: 7 step: 415, loss is 0.0014720808248966932\n",
      "epoch: 7 step: 416, loss is 0.03753338381648064\n",
      "epoch: 7 step: 417, loss is 0.00023753303685225546\n",
      "epoch: 7 step: 418, loss is 0.0012985053472220898\n",
      "epoch: 7 step: 419, loss is 0.00022637800429947674\n",
      "epoch: 7 step: 420, loss is 6.922685133758932e-05\n",
      "epoch: 7 step: 421, loss is 0.09335294365882874\n",
      "epoch: 7 step: 422, loss is 0.003845157567411661\n",
      "epoch: 7 step: 423, loss is 0.0004818247107323259\n",
      "epoch: 7 step: 424, loss is 0.019851919263601303\n",
      "epoch: 7 step: 425, loss is 0.00022035920119378716\n",
      "epoch: 7 step: 426, loss is 0.0017189349746331573\n",
      "epoch: 7 step: 427, loss is 0.004521318711340427\n",
      "epoch: 7 step: 428, loss is 8.813243766780943e-05\n",
      "epoch: 7 step: 429, loss is 0.06696316599845886\n",
      "epoch: 7 step: 430, loss is 0.0016262002754956484\n",
      "epoch: 7 step: 431, loss is 0.0012772275367751718\n",
      "epoch: 7 step: 432, loss is 0.02165815606713295\n",
      "epoch: 7 step: 433, loss is 0.03401246666908264\n",
      "epoch: 7 step: 434, loss is 0.0004421908233780414\n",
      "epoch: 7 step: 435, loss is 0.00023848269484005868\n",
      "epoch: 7 step: 436, loss is 0.042580392211675644\n",
      "epoch: 7 step: 437, loss is 0.00026643602177500725\n",
      "epoch: 7 step: 438, loss is 0.004915621597319841\n",
      "epoch: 7 step: 439, loss is 0.0006222750525921583\n",
      "epoch: 7 step: 440, loss is 0.01680169813334942\n",
      "epoch: 7 step: 441, loss is 0.006914450321346521\n",
      "epoch: 7 step: 442, loss is 0.000621443148702383\n",
      "epoch: 7 step: 443, loss is 0.0002500488772056997\n",
      "epoch: 7 step: 444, loss is 0.10588410496711731\n",
      "epoch: 7 step: 445, loss is 0.0008205781923606992\n",
      "epoch: 7 step: 446, loss is 0.0014197404962033033\n",
      "epoch: 7 step: 447, loss is 0.003117097308859229\n",
      "epoch: 7 step: 448, loss is 0.0007130928570404649\n",
      "epoch: 7 step: 449, loss is 0.00010266279423376545\n",
      "epoch: 7 step: 450, loss is 0.0015412307111546397\n",
      "epoch: 7 step: 451, loss is 7.793877011863515e-05\n",
      "epoch: 7 step: 452, loss is 0.014829408377408981\n",
      "epoch: 7 step: 453, loss is 0.003351050429046154\n",
      "epoch: 7 step: 454, loss is 0.002151306951418519\n",
      "epoch: 7 step: 455, loss is 0.00042557454435154796\n",
      "epoch: 7 step: 456, loss is 0.018142612650990486\n",
      "epoch: 7 step: 457, loss is 0.04185523837804794\n",
      "epoch: 7 step: 458, loss is 0.019837694242596626\n",
      "epoch: 7 step: 459, loss is 0.0005697393789887428\n",
      "epoch: 7 step: 460, loss is 0.00014591577928513288\n",
      "epoch: 7 step: 461, loss is 0.0068464563228189945\n",
      "epoch: 7 step: 462, loss is 0.0004932226147502661\n",
      "epoch: 7 step: 463, loss is 0.00011563937732717022\n",
      "epoch: 7 step: 464, loss is 0.06997424364089966\n",
      "epoch: 7 step: 465, loss is 0.01001841202378273\n",
      "epoch: 7 step: 466, loss is 0.014915603213012218\n",
      "epoch: 7 step: 467, loss is 6.556120933964849e-05\n",
      "epoch: 7 step: 468, loss is 0.04629088193178177\n",
      "epoch: 7 step: 469, loss is 0.004223655443638563\n",
      "epoch: 7 step: 470, loss is 0.00010949881834676489\n",
      "epoch: 7 step: 471, loss is 0.0016254722140729427\n",
      "epoch: 7 step: 472, loss is 0.008504938334226608\n",
      "epoch: 7 step: 473, loss is 0.016970505937933922\n",
      "epoch: 7 step: 474, loss is 0.001624577329494059\n",
      "epoch: 7 step: 475, loss is 0.002229355275630951\n",
      "epoch: 7 step: 476, loss is 0.0008635322446934879\n",
      "epoch: 7 step: 477, loss is 0.0006351922638714314\n",
      "epoch: 7 step: 478, loss is 0.001946790493093431\n",
      "epoch: 7 step: 479, loss is 0.005168173462152481\n",
      "epoch: 7 step: 480, loss is 0.007865929044783115\n",
      "epoch: 7 step: 481, loss is 0.020066969096660614\n",
      "epoch: 7 step: 482, loss is 0.0009079568553715944\n",
      "epoch: 7 step: 483, loss is 0.00028868502704426646\n",
      "epoch: 7 step: 484, loss is 0.00014268516679294407\n",
      "epoch: 7 step: 485, loss is 0.0001758406579028815\n",
      "epoch: 7 step: 486, loss is 0.456920325756073\n",
      "epoch: 7 step: 487, loss is 0.009287957102060318\n",
      "epoch: 7 step: 488, loss is 0.0014119301922619343\n",
      "epoch: 7 step: 489, loss is 0.001742783933877945\n",
      "epoch: 7 step: 490, loss is 0.005219286773353815\n",
      "epoch: 7 step: 491, loss is 0.09745573252439499\n",
      "epoch: 7 step: 492, loss is 0.01922912336885929\n",
      "epoch: 7 step: 493, loss is 0.010148952715098858\n",
      "epoch: 7 step: 494, loss is 0.00260447827167809\n",
      "epoch: 7 step: 495, loss is 0.00012216990580782294\n",
      "epoch: 7 step: 496, loss is 0.002944108098745346\n",
      "epoch: 7 step: 497, loss is 0.0009262461680918932\n",
      "epoch: 7 step: 498, loss is 0.02003144845366478\n",
      "epoch: 7 step: 499, loss is 0.000886597263161093\n",
      "epoch: 7 step: 500, loss is 0.0002733853179961443\n",
      "epoch: 7 step: 501, loss is 0.0068948110565543175\n",
      "epoch: 7 step: 502, loss is 0.0022214073687791824\n",
      "epoch: 7 step: 503, loss is 0.00012690937728621066\n",
      "epoch: 7 step: 504, loss is 0.0006157379830256104\n",
      "epoch: 7 step: 505, loss is 0.03241816163063049\n",
      "epoch: 7 step: 506, loss is 0.002036608988419175\n",
      "epoch: 7 step: 507, loss is 0.004410982131958008\n",
      "epoch: 7 step: 508, loss is 0.0020607425831258297\n",
      "epoch: 7 step: 509, loss is 0.02289893664419651\n",
      "epoch: 7 step: 510, loss is 0.000605038832873106\n",
      "epoch: 7 step: 511, loss is 0.0023093312047421932\n",
      "epoch: 7 step: 512, loss is 8.50936266942881e-05\n",
      "epoch: 7 step: 513, loss is 0.000774131971411407\n",
      "epoch: 7 step: 514, loss is 0.005508607719093561\n",
      "epoch: 7 step: 515, loss is 0.0038081675302237272\n",
      "epoch: 7 step: 516, loss is 0.1131400540471077\n",
      "epoch: 7 step: 517, loss is 0.09630545228719711\n",
      "epoch: 7 step: 518, loss is 0.02346816286444664\n",
      "epoch: 7 step: 519, loss is 0.0009211795986630023\n",
      "epoch: 7 step: 520, loss is 0.01266896165907383\n",
      "epoch: 7 step: 521, loss is 0.005868022795766592\n",
      "epoch: 7 step: 522, loss is 0.012118751183152199\n",
      "epoch: 7 step: 523, loss is 0.00017108491738326848\n",
      "epoch: 7 step: 524, loss is 0.00045305449748411775\n",
      "epoch: 7 step: 525, loss is 0.009156285785138607\n",
      "epoch: 7 step: 526, loss is 0.0007524529937654734\n",
      "epoch: 7 step: 527, loss is 0.00043638801435008645\n",
      "epoch: 7 step: 528, loss is 0.0005291203851811588\n",
      "epoch: 7 step: 529, loss is 0.00036152309621684253\n",
      "epoch: 7 step: 530, loss is 0.0074721937999129295\n",
      "epoch: 7 step: 531, loss is 0.00046745582949370146\n",
      "epoch: 7 step: 532, loss is 0.00039415876381099224\n",
      "epoch: 7 step: 533, loss is 0.004467119928449392\n",
      "epoch: 7 step: 534, loss is 0.0528055876493454\n",
      "epoch: 7 step: 535, loss is 0.0409066379070282\n",
      "epoch: 7 step: 536, loss is 0.0613909587264061\n",
      "epoch: 7 step: 537, loss is 0.0008322476060129702\n",
      "epoch: 7 step: 538, loss is 5.7631747040431947e-05\n",
      "epoch: 7 step: 539, loss is 0.026301931589841843\n",
      "epoch: 7 step: 540, loss is 0.010749666951596737\n",
      "epoch: 7 step: 541, loss is 0.017505722120404243\n",
      "epoch: 7 step: 542, loss is 0.012963156215846539\n",
      "epoch: 7 step: 543, loss is 0.08684779703617096\n",
      "epoch: 7 step: 544, loss is 0.006447278894484043\n",
      "epoch: 7 step: 545, loss is 0.0031857159920036793\n",
      "epoch: 7 step: 546, loss is 0.002418247051537037\n",
      "epoch: 7 step: 547, loss is 0.013456784188747406\n",
      "epoch: 7 step: 548, loss is 0.00012584614160005003\n",
      "epoch: 7 step: 549, loss is 5.687923476216383e-05\n",
      "epoch: 7 step: 550, loss is 0.000996053102426231\n",
      "epoch: 7 step: 551, loss is 0.08097956329584122\n",
      "epoch: 7 step: 552, loss is 0.010858112946152687\n",
      "epoch: 7 step: 553, loss is 0.005988899152725935\n",
      "epoch: 7 step: 554, loss is 8.201197488233447e-05\n",
      "epoch: 7 step: 555, loss is 0.0007423344650305808\n",
      "epoch: 7 step: 556, loss is 0.05686189979314804\n",
      "epoch: 7 step: 557, loss is 0.08206913620233536\n",
      "epoch: 7 step: 558, loss is 0.010662511922419071\n",
      "epoch: 7 step: 559, loss is 0.0034288817550987005\n",
      "epoch: 7 step: 560, loss is 0.005713558755815029\n",
      "epoch: 7 step: 561, loss is 0.11119168251752853\n",
      "epoch: 7 step: 562, loss is 0.0016535589238628745\n",
      "epoch: 7 step: 563, loss is 0.00028752669459208846\n",
      "epoch: 7 step: 564, loss is 0.0011988936457782984\n",
      "epoch: 7 step: 565, loss is 0.0025641373358666897\n",
      "epoch: 7 step: 566, loss is 0.0018635664600878954\n",
      "epoch: 7 step: 567, loss is 0.0008477206574752927\n",
      "epoch: 7 step: 568, loss is 0.0025660074315965176\n",
      "epoch: 7 step: 569, loss is 0.0014351975405588746\n",
      "epoch: 7 step: 570, loss is 0.0009342331322841346\n",
      "epoch: 7 step: 571, loss is 0.02410234324634075\n",
      "epoch: 7 step: 572, loss is 0.004733111243695021\n",
      "epoch: 7 step: 573, loss is 0.00032442991505376995\n",
      "epoch: 7 step: 574, loss is 0.0016939992783591151\n",
      "epoch: 7 step: 575, loss is 0.012911176308989525\n",
      "epoch: 7 step: 576, loss is 0.0004304481844883412\n",
      "epoch: 7 step: 577, loss is 0.0015718336217105389\n",
      "epoch: 7 step: 578, loss is 0.0004986274288967252\n",
      "epoch: 7 step: 579, loss is 0.010707284323871136\n",
      "epoch: 7 step: 580, loss is 0.02330046519637108\n",
      "epoch: 7 step: 581, loss is 0.02404286153614521\n",
      "epoch: 7 step: 582, loss is 0.0002945832966361195\n",
      "epoch: 7 step: 583, loss is 0.00013201759429648519\n",
      "epoch: 7 step: 584, loss is 0.000161661664606072\n",
      "epoch: 7 step: 585, loss is 0.0009047831408679485\n",
      "epoch: 7 step: 586, loss is 0.010385646484792233\n",
      "epoch: 7 step: 587, loss is 0.0006659962236881256\n",
      "epoch: 7 step: 588, loss is 0.002174164168536663\n",
      "epoch: 7 step: 589, loss is 0.00597624434158206\n",
      "epoch: 7 step: 590, loss is 0.020085062831640244\n",
      "epoch: 7 step: 591, loss is 0.05699430778622627\n",
      "epoch: 7 step: 592, loss is 0.00020425484399311244\n",
      "epoch: 7 step: 593, loss is 0.005522315390408039\n",
      "epoch: 7 step: 594, loss is 0.0018620467744767666\n",
      "epoch: 7 step: 595, loss is 0.00020872113236691803\n",
      "epoch: 7 step: 596, loss is 0.0006850383360870183\n",
      "epoch: 7 step: 597, loss is 0.0007386363577097654\n",
      "epoch: 7 step: 598, loss is 0.0005502557614818215\n",
      "epoch: 7 step: 599, loss is 0.008233640342950821\n",
      "epoch: 7 step: 600, loss is 0.005869703367352486\n",
      "epoch: 7 step: 601, loss is 3.877760536852293e-05\n",
      "epoch: 7 step: 602, loss is 0.0010644730646163225\n",
      "epoch: 7 step: 603, loss is 0.005670416168868542\n",
      "epoch: 7 step: 604, loss is 0.001424709102138877\n",
      "epoch: 7 step: 605, loss is 0.012619337067008018\n",
      "epoch: 7 step: 606, loss is 0.0011884020641446114\n",
      "epoch: 7 step: 607, loss is 0.018008094280958176\n",
      "epoch: 7 step: 608, loss is 0.027733460068702698\n",
      "epoch: 7 step: 609, loss is 0.00025515264132991433\n",
      "epoch: 7 step: 610, loss is 0.04114386811852455\n",
      "epoch: 7 step: 611, loss is 0.001872821245342493\n",
      "epoch: 7 step: 612, loss is 0.0020555630326271057\n",
      "epoch: 7 step: 613, loss is 6.916817801538855e-05\n",
      "epoch: 7 step: 614, loss is 0.0028732905630022287\n",
      "epoch: 7 step: 615, loss is 2.21508635149803e-05\n",
      "epoch: 7 step: 616, loss is 0.010571232996881008\n",
      "epoch: 7 step: 617, loss is 0.00023582270659971982\n",
      "epoch: 7 step: 618, loss is 0.02750513143837452\n",
      "epoch: 7 step: 619, loss is 0.00842277705669403\n",
      "epoch: 7 step: 620, loss is 0.00014455478230956942\n",
      "epoch: 7 step: 621, loss is 0.0008160884026437998\n",
      "epoch: 7 step: 622, loss is 0.01134449988603592\n",
      "epoch: 7 step: 623, loss is 0.0007526069530285895\n",
      "epoch: 7 step: 624, loss is 0.013272578828036785\n",
      "epoch: 7 step: 625, loss is 0.010643230751156807\n",
      "epoch: 7 step: 626, loss is 0.00018938619177788496\n",
      "epoch: 7 step: 627, loss is 0.004589840769767761\n",
      "epoch: 7 step: 628, loss is 0.0019600428640842438\n",
      "epoch: 7 step: 629, loss is 0.01558206882327795\n",
      "epoch: 7 step: 630, loss is 0.0008382578380405903\n",
      "epoch: 7 step: 631, loss is 0.00018665852257981896\n",
      "epoch: 7 step: 632, loss is 0.0008351116557605565\n",
      "epoch: 7 step: 633, loss is 0.004250970669090748\n",
      "epoch: 7 step: 634, loss is 0.004410151392221451\n",
      "epoch: 7 step: 635, loss is 0.0003540455945767462\n",
      "epoch: 7 step: 636, loss is 0.0013726814649999142\n",
      "epoch: 7 step: 637, loss is 0.012160662561655045\n",
      "epoch: 7 step: 638, loss is 0.04526175931096077\n",
      "epoch: 7 step: 639, loss is 0.0055199177004396915\n",
      "epoch: 7 step: 640, loss is 0.0005741321365348995\n",
      "epoch: 7 step: 641, loss is 0.05899708718061447\n",
      "epoch: 7 step: 642, loss is 0.0006690250593237579\n",
      "epoch: 7 step: 643, loss is 0.03643648698925972\n",
      "epoch: 7 step: 644, loss is 0.00013656645023729652\n",
      "epoch: 7 step: 645, loss is 0.003637945745140314\n",
      "epoch: 7 step: 646, loss is 0.00016601236711721867\n",
      "epoch: 7 step: 647, loss is 0.0012375557562336326\n",
      "epoch: 7 step: 648, loss is 0.0002801379014272243\n",
      "epoch: 7 step: 649, loss is 6.649657007073984e-05\n",
      "epoch: 7 step: 650, loss is 0.0006604085210710764\n",
      "epoch: 7 step: 651, loss is 0.007924611680209637\n",
      "epoch: 7 step: 652, loss is 0.0027948394417762756\n",
      "epoch: 7 step: 653, loss is 0.0017057269578799605\n",
      "epoch: 7 step: 654, loss is 0.000570181873627007\n",
      "epoch: 7 step: 655, loss is 0.0019501405768096447\n",
      "epoch: 7 step: 656, loss is 0.004749507177621126\n",
      "epoch: 7 step: 657, loss is 0.005494988989084959\n",
      "epoch: 7 step: 658, loss is 0.002331578405573964\n",
      "epoch: 7 step: 659, loss is 0.03959912434220314\n",
      "epoch: 7 step: 660, loss is 0.0006540510803461075\n",
      "epoch: 7 step: 661, loss is 0.004163612611591816\n",
      "epoch: 7 step: 662, loss is 9.975435204978567e-06\n",
      "epoch: 7 step: 663, loss is 0.0005525370361283422\n",
      "epoch: 7 step: 664, loss is 0.007860501296818256\n",
      "epoch: 7 step: 665, loss is 0.0018470590002834797\n",
      "epoch: 7 step: 666, loss is 0.0033501973375678062\n",
      "epoch: 7 step: 667, loss is 0.0013047060929238796\n",
      "epoch: 7 step: 668, loss is 0.0022509910631924868\n",
      "epoch: 7 step: 669, loss is 0.0154524315148592\n",
      "epoch: 7 step: 670, loss is 0.00018759693193715066\n",
      "epoch: 7 step: 671, loss is 0.034064844250679016\n",
      "epoch: 7 step: 672, loss is 0.002717798575758934\n",
      "epoch: 7 step: 673, loss is 0.00036807075957767665\n",
      "epoch: 7 step: 674, loss is 0.000573253259062767\n",
      "epoch: 7 step: 675, loss is 0.00046842696610838175\n",
      "epoch: 7 step: 676, loss is 0.0017096896190196276\n",
      "epoch: 7 step: 677, loss is 0.0005436502397060394\n",
      "epoch: 7 step: 678, loss is 0.08536015450954437\n",
      "epoch: 7 step: 679, loss is 8.878900553099811e-05\n",
      "epoch: 7 step: 680, loss is 0.015057641081511974\n",
      "epoch: 7 step: 681, loss is 3.983964052167721e-05\n",
      "epoch: 7 step: 682, loss is 9.2443180619739e-05\n",
      "epoch: 7 step: 683, loss is 0.00036361051024869084\n",
      "epoch: 7 step: 684, loss is 0.0017555038211867213\n",
      "epoch: 7 step: 685, loss is 0.0009786987211555243\n",
      "epoch: 7 step: 686, loss is 0.005094452761113644\n",
      "epoch: 7 step: 687, loss is 0.00018201800412498415\n",
      "epoch: 7 step: 688, loss is 0.002683269791305065\n",
      "epoch: 7 step: 689, loss is 0.004149656742811203\n",
      "epoch: 7 step: 690, loss is 0.050877299159765244\n",
      "epoch: 7 step: 691, loss is 1.397283085680101e-05\n",
      "epoch: 7 step: 692, loss is 0.002189001999795437\n",
      "epoch: 7 step: 693, loss is 0.0010217434028163552\n",
      "epoch: 7 step: 694, loss is 0.05192599445581436\n",
      "epoch: 7 step: 695, loss is 0.00364652369171381\n",
      "epoch: 7 step: 696, loss is 0.0005815515760332346\n",
      "epoch: 7 step: 697, loss is 0.017536908388137817\n",
      "epoch: 7 step: 698, loss is 0.0412345752120018\n",
      "epoch: 7 step: 699, loss is 0.0008576854597777128\n",
      "epoch: 7 step: 700, loss is 0.0004515387408901006\n",
      "epoch: 7 step: 701, loss is 2.5997138436650857e-05\n",
      "epoch: 7 step: 702, loss is 0.0012704010587185621\n",
      "epoch: 7 step: 703, loss is 0.0022283883299678564\n",
      "epoch: 7 step: 704, loss is 0.0037951346021145582\n",
      "epoch: 7 step: 705, loss is 0.02643788605928421\n",
      "epoch: 7 step: 706, loss is 0.0004280806751921773\n",
      "epoch: 7 step: 707, loss is 5.889935437153326e-06\n",
      "epoch: 7 step: 708, loss is 0.023422643542289734\n",
      "epoch: 7 step: 709, loss is 0.0009543881169520319\n",
      "epoch: 7 step: 710, loss is 0.003643939970061183\n",
      "epoch: 7 step: 711, loss is 0.11601226776838303\n",
      "epoch: 7 step: 712, loss is 0.00328678241930902\n",
      "epoch: 7 step: 713, loss is 0.016960671171545982\n",
      "epoch: 7 step: 714, loss is 6.70118533889763e-05\n",
      "epoch: 7 step: 715, loss is 0.06466720998287201\n",
      "epoch: 7 step: 716, loss is 0.0029694426339119673\n",
      "epoch: 7 step: 717, loss is 0.0006500670569948852\n",
      "epoch: 7 step: 718, loss is 0.00044183508725836873\n",
      "epoch: 7 step: 719, loss is 0.031892016530036926\n",
      "epoch: 7 step: 720, loss is 0.19713374972343445\n",
      "epoch: 7 step: 721, loss is 5.254902134765871e-05\n",
      "epoch: 7 step: 722, loss is 4.029520641779527e-05\n",
      "epoch: 7 step: 723, loss is 0.00033839812385849655\n",
      "epoch: 7 step: 724, loss is 0.007321610581129789\n",
      "epoch: 7 step: 725, loss is 0.00015641974459867924\n",
      "epoch: 7 step: 726, loss is 0.00015794832143001258\n",
      "epoch: 7 step: 727, loss is 0.0018965875497087836\n",
      "epoch: 7 step: 728, loss is 0.005067961290478706\n",
      "epoch: 7 step: 729, loss is 0.00018341992108616978\n",
      "epoch: 7 step: 730, loss is 0.0004987308639101684\n",
      "epoch: 7 step: 731, loss is 0.05435260385274887\n",
      "epoch: 7 step: 732, loss is 0.35500937700271606\n",
      "epoch: 7 step: 733, loss is 0.0008563234587199986\n",
      "epoch: 7 step: 734, loss is 0.00027152863913215697\n",
      "epoch: 7 step: 735, loss is 0.0008590843062847853\n",
      "epoch: 7 step: 736, loss is 0.01054195687174797\n",
      "epoch: 7 step: 737, loss is 0.008693568408489227\n",
      "epoch: 7 step: 738, loss is 0.03578401356935501\n",
      "epoch: 7 step: 739, loss is 0.00012226149556227028\n",
      "epoch: 7 step: 740, loss is 0.004993687383830547\n",
      "epoch: 7 step: 741, loss is 0.0002194032713305205\n",
      "epoch: 7 step: 742, loss is 0.010904019698500633\n",
      "epoch: 7 step: 743, loss is 0.015049312263727188\n",
      "epoch: 7 step: 744, loss is 0.11853141337633133\n",
      "epoch: 7 step: 745, loss is 0.19978231191635132\n",
      "epoch: 7 step: 746, loss is 0.0001897003094200045\n",
      "epoch: 7 step: 747, loss is 0.03619770333170891\n",
      "epoch: 7 step: 748, loss is 0.0006640417268499732\n",
      "epoch: 7 step: 749, loss is 0.0002433506160741672\n",
      "epoch: 7 step: 750, loss is 4.396909935167059e-05\n",
      "epoch: 7 step: 751, loss is 0.0005068829632364213\n",
      "epoch: 7 step: 752, loss is 5.2479688747553155e-05\n",
      "epoch: 7 step: 753, loss is 0.002936974633485079\n",
      "epoch: 7 step: 754, loss is 0.0004712391528300941\n",
      "epoch: 7 step: 755, loss is 0.06765352189540863\n",
      "epoch: 7 step: 756, loss is 0.019832169637084007\n",
      "epoch: 7 step: 757, loss is 0.0012959801824763417\n",
      "epoch: 7 step: 758, loss is 0.028634078800678253\n",
      "epoch: 7 step: 759, loss is 8.905798313207924e-05\n",
      "epoch: 7 step: 760, loss is 0.045047957450151443\n",
      "epoch: 7 step: 761, loss is 0.13132324814796448\n",
      "epoch: 7 step: 762, loss is 0.00041860193596221507\n",
      "epoch: 7 step: 763, loss is 0.04941441863775253\n",
      "epoch: 7 step: 764, loss is 0.007945291697978973\n",
      "epoch: 7 step: 765, loss is 0.01917652040719986\n",
      "epoch: 7 step: 766, loss is 0.0011189908254891634\n",
      "epoch: 7 step: 767, loss is 0.016455527395009995\n",
      "epoch: 7 step: 768, loss is 0.0017621845472604036\n",
      "epoch: 7 step: 769, loss is 0.05204682797193527\n",
      "epoch: 7 step: 770, loss is 0.025102859362959862\n",
      "epoch: 7 step: 771, loss is 0.017796406522393227\n",
      "epoch: 7 step: 772, loss is 0.050350338220596313\n",
      "epoch: 7 step: 773, loss is 0.0014149443013593554\n",
      "epoch: 7 step: 774, loss is 0.012554123997688293\n",
      "epoch: 7 step: 775, loss is 0.012760509736835957\n",
      "epoch: 7 step: 776, loss is 0.0069310045801103115\n",
      "epoch: 7 step: 777, loss is 0.00045438448432832956\n",
      "epoch: 7 step: 778, loss is 0.0010458165779709816\n",
      "epoch: 7 step: 779, loss is 0.01328959595412016\n",
      "epoch: 7 step: 780, loss is 0.006736774928867817\n",
      "epoch: 7 step: 781, loss is 0.0003688177093863487\n",
      "epoch: 7 step: 782, loss is 0.0020490814931690693\n",
      "epoch: 7 step: 783, loss is 0.00010540729272179306\n",
      "epoch: 7 step: 784, loss is 0.021636608988046646\n",
      "epoch: 7 step: 785, loss is 0.10792247205972672\n",
      "epoch: 7 step: 786, loss is 0.002013181336224079\n",
      "epoch: 7 step: 787, loss is 0.000590565730817616\n",
      "epoch: 7 step: 788, loss is 0.007353021763265133\n",
      "epoch: 7 step: 789, loss is 0.000812231854069978\n",
      "epoch: 7 step: 790, loss is 0.00016928001423366368\n",
      "epoch: 7 step: 791, loss is 0.00031975097954273224\n",
      "epoch: 7 step: 792, loss is 0.0016313472297042608\n",
      "epoch: 7 step: 793, loss is 0.05252008140087128\n",
      "epoch: 7 step: 794, loss is 0.0076632252894341946\n",
      "epoch: 7 step: 795, loss is 0.10031787306070328\n",
      "epoch: 7 step: 796, loss is 0.1168898344039917\n",
      "epoch: 7 step: 797, loss is 0.006012022495269775\n",
      "epoch: 7 step: 798, loss is 0.0001411678531439975\n",
      "epoch: 7 step: 799, loss is 0.0009376411326229572\n",
      "epoch: 7 step: 800, loss is 0.002457279711961746\n",
      "epoch: 7 step: 801, loss is 0.037533052265644073\n",
      "epoch: 7 step: 802, loss is 0.0006496169371530414\n",
      "epoch: 7 step: 803, loss is 0.010485468432307243\n",
      "epoch: 7 step: 804, loss is 0.0002616124111227691\n",
      "epoch: 7 step: 805, loss is 0.00021728906722273678\n",
      "epoch: 7 step: 806, loss is 0.011461627669632435\n",
      "epoch: 7 step: 807, loss is 0.000101933088444639\n",
      "epoch: 7 step: 808, loss is 0.0015952589455991983\n",
      "epoch: 7 step: 809, loss is 0.0012713250471279025\n",
      "epoch: 7 step: 810, loss is 7.665604789508507e-05\n",
      "epoch: 7 step: 811, loss is 0.009753595106303692\n",
      "epoch: 7 step: 812, loss is 0.04362038895487785\n",
      "epoch: 7 step: 813, loss is 8.684174827067181e-05\n",
      "epoch: 7 step: 814, loss is 0.0009458095883019269\n",
      "epoch: 7 step: 815, loss is 0.00040344876470044255\n",
      "epoch: 7 step: 816, loss is 0.0018726568669080734\n",
      "epoch: 7 step: 817, loss is 0.0021683594677597284\n",
      "epoch: 7 step: 818, loss is 0.0018129176460206509\n",
      "epoch: 7 step: 819, loss is 0.00021640531485900283\n",
      "epoch: 7 step: 820, loss is 0.00449892645701766\n",
      "epoch: 7 step: 821, loss is 0.0014607389457523823\n",
      "epoch: 7 step: 822, loss is 0.00393131235614419\n",
      "epoch: 7 step: 823, loss is 0.06305242329835892\n",
      "epoch: 7 step: 824, loss is 0.0001037955516949296\n",
      "epoch: 7 step: 825, loss is 0.00011119716509710997\n",
      "epoch: 7 step: 826, loss is 0.0006601095083169639\n",
      "epoch: 7 step: 827, loss is 6.647712871199474e-05\n",
      "epoch: 7 step: 828, loss is 0.003094516694545746\n",
      "epoch: 7 step: 829, loss is 0.04118005558848381\n",
      "epoch: 7 step: 830, loss is 0.002502991119399667\n",
      "epoch: 7 step: 831, loss is 0.07659335434436798\n",
      "epoch: 7 step: 832, loss is 0.008909007534384727\n",
      "epoch: 7 step: 833, loss is 0.0005837857606820762\n",
      "epoch: 7 step: 834, loss is 0.00015835431986488402\n",
      "epoch: 7 step: 835, loss is 0.028942763805389404\n",
      "epoch: 7 step: 836, loss is 0.0023731994442641735\n",
      "epoch: 7 step: 837, loss is 0.006264058873057365\n",
      "epoch: 7 step: 838, loss is 0.0007233524811454117\n",
      "epoch: 7 step: 839, loss is 0.0010437114397063851\n",
      "epoch: 7 step: 840, loss is 2.4470877178828232e-05\n",
      "epoch: 7 step: 841, loss is 0.00152954354416579\n",
      "epoch: 7 step: 842, loss is 0.012243423610925674\n",
      "epoch: 7 step: 843, loss is 4.480133065953851e-05\n",
      "epoch: 7 step: 844, loss is 0.0009154500439763069\n",
      "epoch: 7 step: 845, loss is 0.0015429217601194978\n",
      "epoch: 7 step: 846, loss is 0.019153116270899773\n",
      "epoch: 7 step: 847, loss is 0.000533896207343787\n",
      "epoch: 7 step: 848, loss is 0.0037772187497466803\n",
      "epoch: 7 step: 849, loss is 0.005594365298748016\n",
      "epoch: 7 step: 850, loss is 0.0054507507011294365\n",
      "epoch: 7 step: 851, loss is 0.005517877172678709\n",
      "epoch: 7 step: 852, loss is 0.015492891892790794\n",
      "epoch: 7 step: 853, loss is 0.0002599712461233139\n",
      "epoch: 7 step: 854, loss is 0.0002453638007864356\n",
      "epoch: 7 step: 855, loss is 0.0004966703709214926\n",
      "epoch: 7 step: 856, loss is 0.0063164192251861095\n",
      "epoch: 7 step: 857, loss is 0.007483907975256443\n",
      "epoch: 7 step: 858, loss is 0.00030382879776880145\n",
      "epoch: 7 step: 859, loss is 0.0004132700269110501\n",
      "epoch: 7 step: 860, loss is 0.0010204357095062733\n",
      "epoch: 7 step: 861, loss is 0.00022039350005798042\n",
      "epoch: 7 step: 862, loss is 0.04260857775807381\n",
      "epoch: 7 step: 863, loss is 0.046603865921497345\n",
      "epoch: 7 step: 864, loss is 0.0007764844922348857\n",
      "epoch: 7 step: 865, loss is 4.947976049152203e-05\n",
      "epoch: 7 step: 866, loss is 0.0006059225415810943\n",
      "epoch: 7 step: 867, loss is 0.0002688256208784878\n",
      "epoch: 7 step: 868, loss is 0.2045270800590515\n",
      "epoch: 7 step: 869, loss is 0.0028317421674728394\n",
      "epoch: 7 step: 870, loss is 0.0006482235621660948\n",
      "epoch: 7 step: 871, loss is 8.929448813432828e-05\n",
      "epoch: 7 step: 872, loss is 0.014724483713507652\n",
      "epoch: 7 step: 873, loss is 0.0006686751148663461\n",
      "epoch: 7 step: 874, loss is 0.0004913833108730614\n",
      "epoch: 7 step: 875, loss is 0.00018490049114916474\n",
      "epoch: 7 step: 876, loss is 0.0011887972941622138\n",
      "epoch: 7 step: 877, loss is 0.000248901778832078\n",
      "epoch: 7 step: 878, loss is 0.051211170852184296\n",
      "epoch: 7 step: 879, loss is 0.06179520860314369\n",
      "epoch: 7 step: 880, loss is 0.0005185487680137157\n",
      "epoch: 7 step: 881, loss is 6.810449849581346e-05\n",
      "epoch: 7 step: 882, loss is 0.000525240320712328\n",
      "epoch: 7 step: 883, loss is 8.74082325026393e-05\n",
      "epoch: 7 step: 884, loss is 0.03605842962861061\n",
      "epoch: 7 step: 885, loss is 1.784115556802135e-05\n",
      "epoch: 7 step: 886, loss is 0.0035394812002778053\n",
      "epoch: 7 step: 887, loss is 0.028884897008538246\n",
      "epoch: 7 step: 888, loss is 0.14504481852054596\n",
      "epoch: 7 step: 889, loss is 6.877601117594168e-05\n",
      "epoch: 7 step: 890, loss is 0.0006551684928126633\n",
      "epoch: 7 step: 891, loss is 0.006401124410331249\n",
      "epoch: 7 step: 892, loss is 0.016052229329943657\n",
      "epoch: 7 step: 893, loss is 0.0016846898943185806\n",
      "epoch: 7 step: 894, loss is 0.009602569043636322\n",
      "epoch: 7 step: 895, loss is 0.00033651950070634484\n",
      "epoch: 7 step: 896, loss is 0.06253405660390854\n",
      "epoch: 7 step: 897, loss is 0.014681835658848286\n",
      "epoch: 7 step: 898, loss is 0.003684696275740862\n",
      "epoch: 7 step: 899, loss is 0.007473415695130825\n",
      "epoch: 7 step: 900, loss is 0.0008822052623145282\n",
      "epoch: 7 step: 901, loss is 0.005582381039857864\n",
      "epoch: 7 step: 902, loss is 0.0176310483366251\n",
      "epoch: 7 step: 903, loss is 0.00025395071133971214\n",
      "epoch: 7 step: 904, loss is 0.02616591937839985\n",
      "epoch: 7 step: 905, loss is 0.0011282481718808413\n",
      "epoch: 7 step: 906, loss is 0.03250119835138321\n",
      "epoch: 7 step: 907, loss is 0.0030091870576143265\n",
      "epoch: 7 step: 908, loss is 0.0015316917560994625\n",
      "epoch: 7 step: 909, loss is 0.0013069476699456573\n",
      "epoch: 7 step: 910, loss is 0.00481285247951746\n",
      "epoch: 7 step: 911, loss is 0.00010536205081734806\n",
      "epoch: 7 step: 912, loss is 0.00016582573880441487\n",
      "epoch: 7 step: 913, loss is 0.0005138200940564275\n",
      "epoch: 7 step: 914, loss is 0.009334659203886986\n",
      "epoch: 7 step: 915, loss is 0.0908302590250969\n",
      "epoch: 7 step: 916, loss is 0.0012779380194842815\n",
      "epoch: 7 step: 917, loss is 0.000635076139587909\n",
      "epoch: 7 step: 918, loss is 0.000298433646094054\n",
      "epoch: 7 step: 919, loss is 0.004558325745165348\n",
      "epoch: 7 step: 920, loss is 8.074272045632824e-05\n",
      "epoch: 7 step: 921, loss is 0.00038383627543225884\n",
      "epoch: 7 step: 922, loss is 0.0001786532229743898\n",
      "epoch: 7 step: 923, loss is 0.002450160449370742\n",
      "epoch: 7 step: 924, loss is 0.00011861178791150451\n",
      "epoch: 7 step: 925, loss is 0.0064246258698403835\n",
      "epoch: 7 step: 926, loss is 0.0002004820416914299\n",
      "epoch: 7 step: 927, loss is 0.002387559972703457\n",
      "epoch: 7 step: 928, loss is 0.00024645900703035295\n",
      "epoch: 7 step: 929, loss is 0.0002448434242978692\n",
      "epoch: 7 step: 930, loss is 0.0001734563848003745\n",
      "epoch: 7 step: 931, loss is 0.01821228489279747\n",
      "epoch: 7 step: 932, loss is 0.14033310115337372\n",
      "epoch: 7 step: 933, loss is 0.00857443269342184\n",
      "epoch: 7 step: 934, loss is 0.00029771457775495946\n",
      "epoch: 7 step: 935, loss is 0.0027248773258179426\n",
      "epoch: 7 step: 936, loss is 0.001500142039731145\n",
      "epoch: 7 step: 937, loss is 0.011664895340800285\n",
      "epoch: 7 step: 938, loss is 0.027008919045329094\n",
      "epoch: 7 step: 939, loss is 0.0012514969566836953\n",
      "epoch: 7 step: 940, loss is 0.10771158337593079\n",
      "epoch: 7 step: 941, loss is 0.00047449636622332036\n",
      "epoch: 7 step: 942, loss is 0.0017534641083329916\n",
      "epoch: 7 step: 943, loss is 0.02326030656695366\n",
      "epoch: 7 step: 944, loss is 0.000531399215105921\n",
      "epoch: 7 step: 945, loss is 0.013359268195927143\n",
      "epoch: 7 step: 946, loss is 0.0016517892945557833\n",
      "epoch: 7 step: 947, loss is 0.0003817591932602227\n",
      "epoch: 7 step: 948, loss is 0.1013929471373558\n",
      "epoch: 7 step: 949, loss is 0.0003905994235537946\n",
      "epoch: 7 step: 950, loss is 0.0004993032198399305\n",
      "epoch: 7 step: 951, loss is 0.015679584816098213\n",
      "epoch: 7 step: 952, loss is 0.0036365645937621593\n",
      "epoch: 7 step: 953, loss is 0.002316966187208891\n",
      "epoch: 7 step: 954, loss is 0.0026935862842947245\n",
      "epoch: 7 step: 955, loss is 0.000318053673254326\n",
      "epoch: 7 step: 956, loss is 0.0007286814507097006\n",
      "epoch: 7 step: 957, loss is 0.0013920306228101254\n",
      "epoch: 7 step: 958, loss is 0.0006953791598789394\n",
      "epoch: 7 step: 959, loss is 0.00015219466877169907\n",
      "epoch: 7 step: 960, loss is 3.9341888623312116e-05\n",
      "epoch: 7 step: 961, loss is 0.00010831079271156341\n",
      "epoch: 7 step: 962, loss is 0.0004429348336998373\n",
      "epoch: 7 step: 963, loss is 0.0022883655037730932\n",
      "epoch: 7 step: 964, loss is 0.000307582231471315\n",
      "epoch: 7 step: 965, loss is 0.007715706713497639\n",
      "epoch: 7 step: 966, loss is 0.00019031038391403854\n",
      "epoch: 7 step: 967, loss is 0.0024595195427536964\n",
      "epoch: 7 step: 968, loss is 0.009724587202072144\n",
      "epoch: 7 step: 969, loss is 0.011553487740457058\n",
      "epoch: 7 step: 970, loss is 0.0673246830701828\n",
      "epoch: 7 step: 971, loss is 0.00033188980887643993\n",
      "epoch: 7 step: 972, loss is 0.00010598543303785846\n",
      "epoch: 7 step: 973, loss is 0.0002319965569768101\n",
      "epoch: 7 step: 974, loss is 0.00040874857222661376\n",
      "epoch: 7 step: 975, loss is 0.04471341148018837\n",
      "epoch: 7 step: 976, loss is 0.003517230972647667\n",
      "epoch: 7 step: 977, loss is 0.021093670278787613\n",
      "epoch: 7 step: 978, loss is 0.0028799523133784533\n",
      "epoch: 7 step: 979, loss is 0.00016110479191411287\n",
      "epoch: 7 step: 980, loss is 0.11830303072929382\n",
      "epoch: 7 step: 981, loss is 0.0012823124416172504\n",
      "epoch: 7 step: 982, loss is 0.2741038203239441\n",
      "epoch: 7 step: 983, loss is 0.0022787749767303467\n",
      "epoch: 7 step: 984, loss is 9.267425048165023e-05\n",
      "epoch: 7 step: 985, loss is 0.0026812623254954815\n",
      "epoch: 7 step: 986, loss is 0.038567956537008286\n",
      "epoch: 7 step: 987, loss is 0.0015144320204854012\n",
      "epoch: 7 step: 988, loss is 0.053460411727428436\n",
      "epoch: 7 step: 989, loss is 0.0008336724131368101\n",
      "epoch: 7 step: 990, loss is 0.012418846599757671\n",
      "epoch: 7 step: 991, loss is 0.04884539917111397\n",
      "epoch: 7 step: 992, loss is 0.0006383819854818285\n",
      "epoch: 7 step: 993, loss is 9.3812559498474e-05\n",
      "epoch: 7 step: 994, loss is 0.022122403606772423\n",
      "epoch: 7 step: 995, loss is 0.3252721130847931\n",
      "epoch: 7 step: 996, loss is 0.00010384842607891187\n",
      "epoch: 7 step: 997, loss is 0.00028311446658335626\n",
      "epoch: 7 step: 998, loss is 0.0015982158947736025\n",
      "epoch: 7 step: 999, loss is 0.0031889129895716906\n",
      "epoch: 7 step: 1000, loss is 0.12081517279148102\n",
      "epoch: 7 step: 1001, loss is 0.004348049871623516\n",
      "epoch: 7 step: 1002, loss is 0.0007931732689030468\n",
      "epoch: 7 step: 1003, loss is 0.010815353132784367\n",
      "epoch: 7 step: 1004, loss is 0.005778904538601637\n",
      "epoch: 7 step: 1005, loss is 0.13288074731826782\n",
      "epoch: 7 step: 1006, loss is 0.00024478789418935776\n",
      "epoch: 7 step: 1007, loss is 0.02595839649438858\n",
      "epoch: 7 step: 1008, loss is 0.02799260802567005\n",
      "epoch: 7 step: 1009, loss is 0.018136486411094666\n",
      "epoch: 7 step: 1010, loss is 6.633676821365952e-05\n",
      "epoch: 7 step: 1011, loss is 0.00036383618135005236\n",
      "epoch: 7 step: 1012, loss is 0.0021535856649279594\n",
      "epoch: 7 step: 1013, loss is 0.0010042338399216533\n",
      "epoch: 7 step: 1014, loss is 0.04693334549665451\n",
      "epoch: 7 step: 1015, loss is 0.0015861265128478408\n",
      "epoch: 7 step: 1016, loss is 0.11292735487222672\n",
      "epoch: 7 step: 1017, loss is 0.020571814849972725\n",
      "epoch: 7 step: 1018, loss is 0.001263006473891437\n",
      "epoch: 7 step: 1019, loss is 0.023998601362109184\n",
      "epoch: 7 step: 1020, loss is 0.0002829630975611508\n",
      "epoch: 7 step: 1021, loss is 0.07310779392719269\n",
      "epoch: 7 step: 1022, loss is 0.021358419209718704\n",
      "epoch: 7 step: 1023, loss is 0.20407634973526\n",
      "epoch: 7 step: 1024, loss is 0.0004765429184772074\n",
      "epoch: 7 step: 1025, loss is 0.0007483396329917014\n",
      "epoch: 7 step: 1026, loss is 0.014267589896917343\n",
      "epoch: 7 step: 1027, loss is 0.0015086838975548744\n",
      "epoch: 7 step: 1028, loss is 0.0008455701172351837\n",
      "epoch: 7 step: 1029, loss is 0.03443460166454315\n",
      "epoch: 7 step: 1030, loss is 0.0058058868162333965\n",
      "epoch: 7 step: 1031, loss is 0.016961725428700447\n",
      "epoch: 7 step: 1032, loss is 0.005691073834896088\n",
      "epoch: 7 step: 1033, loss is 0.007166796363890171\n",
      "epoch: 7 step: 1034, loss is 0.02212553657591343\n",
      "epoch: 7 step: 1035, loss is 0.00475486321374774\n",
      "epoch: 7 step: 1036, loss is 0.05294480547308922\n",
      "epoch: 7 step: 1037, loss is 0.0022752322256565094\n",
      "epoch: 7 step: 1038, loss is 0.003842908190563321\n",
      "epoch: 7 step: 1039, loss is 0.14540988206863403\n",
      "epoch: 7 step: 1040, loss is 0.1312551498413086\n",
      "epoch: 7 step: 1041, loss is 0.057664211839437485\n",
      "epoch: 7 step: 1042, loss is 0.00023514557688031346\n",
      "epoch: 7 step: 1043, loss is 0.018257029354572296\n",
      "epoch: 7 step: 1044, loss is 0.020715098828077316\n",
      "epoch: 7 step: 1045, loss is 0.004307254683226347\n",
      "epoch: 7 step: 1046, loss is 0.14846323430538177\n",
      "epoch: 7 step: 1047, loss is 0.000757131609134376\n",
      "epoch: 7 step: 1048, loss is 0.029480677098035812\n",
      "epoch: 7 step: 1049, loss is 0.013085792772471905\n",
      "epoch: 7 step: 1050, loss is 0.010864091105759144\n",
      "epoch: 7 step: 1051, loss is 0.042141616344451904\n",
      "epoch: 7 step: 1052, loss is 0.014793612994253635\n",
      "epoch: 7 step: 1053, loss is 0.010090985335409641\n",
      "epoch: 7 step: 1054, loss is 0.0036921428982168436\n",
      "epoch: 7 step: 1055, loss is 0.0020297986920922995\n",
      "epoch: 7 step: 1056, loss is 0.1946357637643814\n",
      "epoch: 7 step: 1057, loss is 0.006985465995967388\n",
      "epoch: 7 step: 1058, loss is 0.038734085857868195\n",
      "epoch: 7 step: 1059, loss is 0.024348469451069832\n",
      "epoch: 7 step: 1060, loss is 0.0040775020606815815\n",
      "epoch: 7 step: 1061, loss is 0.008870445191860199\n",
      "epoch: 7 step: 1062, loss is 9.66080988291651e-05\n",
      "epoch: 7 step: 1063, loss is 0.12526436150074005\n",
      "epoch: 7 step: 1064, loss is 0.00689678406342864\n",
      "epoch: 7 step: 1065, loss is 0.06467034667730331\n",
      "epoch: 7 step: 1066, loss is 0.06324917823076248\n",
      "epoch: 7 step: 1067, loss is 0.049471836537122726\n",
      "epoch: 7 step: 1068, loss is 0.044073011726140976\n",
      "epoch: 7 step: 1069, loss is 0.0022092314902693033\n",
      "epoch: 7 step: 1070, loss is 0.0005128420307300985\n",
      "epoch: 7 step: 1071, loss is 0.06445848196744919\n",
      "epoch: 7 step: 1072, loss is 0.2052651047706604\n",
      "epoch: 7 step: 1073, loss is 0.0042665088549256325\n",
      "epoch: 7 step: 1074, loss is 0.062991201877594\n",
      "epoch: 7 step: 1075, loss is 0.0028253907803446054\n",
      "epoch: 7 step: 1076, loss is 0.0003194214659743011\n",
      "epoch: 7 step: 1077, loss is 0.0027285381220281124\n",
      "epoch: 7 step: 1078, loss is 0.0005632262327708304\n",
      "epoch: 7 step: 1079, loss is 0.024827536195516586\n",
      "epoch: 7 step: 1080, loss is 0.0011289301328361034\n",
      "epoch: 7 step: 1081, loss is 0.003967815078794956\n",
      "epoch: 7 step: 1082, loss is 0.00029822878423146904\n",
      "epoch: 7 step: 1083, loss is 0.011302364990115166\n",
      "epoch: 7 step: 1084, loss is 0.015782834962010384\n",
      "epoch: 7 step: 1085, loss is 0.0007924106903374195\n",
      "epoch: 7 step: 1086, loss is 0.004020886030048132\n",
      "epoch: 7 step: 1087, loss is 0.0037410883232951164\n",
      "epoch: 7 step: 1088, loss is 0.0033643993083387613\n",
      "epoch: 7 step: 1089, loss is 0.00196468411013484\n",
      "epoch: 7 step: 1090, loss is 0.00022115654428489506\n",
      "epoch: 7 step: 1091, loss is 0.0013919431949034333\n",
      "epoch: 7 step: 1092, loss is 0.01977720856666565\n",
      "epoch: 7 step: 1093, loss is 0.00015542654728051275\n",
      "epoch: 7 step: 1094, loss is 0.03782736882567406\n",
      "epoch: 7 step: 1095, loss is 0.0015003065345808864\n",
      "epoch: 7 step: 1096, loss is 0.009026563726365566\n",
      "epoch: 7 step: 1097, loss is 0.05620446428656578\n",
      "epoch: 7 step: 1098, loss is 0.007888682186603546\n",
      "epoch: 7 step: 1099, loss is 0.0018269839929416776\n",
      "epoch: 7 step: 1100, loss is 0.0025347888004034758\n",
      "epoch: 7 step: 1101, loss is 0.0014793180162087083\n",
      "epoch: 7 step: 1102, loss is 9.478053834754974e-05\n",
      "epoch: 7 step: 1103, loss is 0.0028568755369633436\n",
      "epoch: 7 step: 1104, loss is 0.00032150838524103165\n",
      "epoch: 7 step: 1105, loss is 0.0002507025492377579\n",
      "epoch: 7 step: 1106, loss is 0.002327915746718645\n",
      "epoch: 7 step: 1107, loss is 0.054910965263843536\n",
      "epoch: 7 step: 1108, loss is 0.007939819246530533\n",
      "epoch: 7 step: 1109, loss is 0.05055096372961998\n",
      "epoch: 7 step: 1110, loss is 0.007212086580693722\n",
      "epoch: 7 step: 1111, loss is 0.0001587618316989392\n",
      "epoch: 7 step: 1112, loss is 0.0008224947960115969\n",
      "epoch: 7 step: 1113, loss is 0.0028951598796993494\n",
      "epoch: 7 step: 1114, loss is 0.00906582921743393\n",
      "epoch: 7 step: 1115, loss is 0.0005105432937853038\n",
      "epoch: 7 step: 1116, loss is 0.0010085692629218102\n",
      "epoch: 7 step: 1117, loss is 0.069544218480587\n",
      "epoch: 7 step: 1118, loss is 0.0007111673476174474\n",
      "epoch: 7 step: 1119, loss is 0.0005530915805138648\n",
      "epoch: 7 step: 1120, loss is 0.007759039290249348\n",
      "epoch: 7 step: 1121, loss is 0.007982084527611732\n",
      "epoch: 7 step: 1122, loss is 0.011165706440806389\n",
      "epoch: 7 step: 1123, loss is 0.03268926590681076\n",
      "epoch: 7 step: 1124, loss is 0.00025711380294524133\n",
      "epoch: 7 step: 1125, loss is 0.0015626699896529317\n",
      "epoch: 7 step: 1126, loss is 4.998371878173202e-05\n",
      "epoch: 7 step: 1127, loss is 0.027479249984025955\n",
      "epoch: 7 step: 1128, loss is 0.0015889140777289867\n",
      "epoch: 7 step: 1129, loss is 0.044405072927474976\n",
      "epoch: 7 step: 1130, loss is 0.010508879087865353\n",
      "epoch: 7 step: 1131, loss is 0.00043294799979776144\n",
      "epoch: 7 step: 1132, loss is 0.03940450772643089\n",
      "epoch: 7 step: 1133, loss is 0.010171828791499138\n",
      "epoch: 7 step: 1134, loss is 0.00819151010364294\n",
      "epoch: 7 step: 1135, loss is 0.005220836028456688\n",
      "epoch: 7 step: 1136, loss is 0.0005305886152200401\n",
      "epoch: 7 step: 1137, loss is 0.044992681592702866\n",
      "epoch: 7 step: 1138, loss is 6.706896238029003e-05\n",
      "epoch: 7 step: 1139, loss is 0.0024211034178733826\n",
      "epoch: 7 step: 1140, loss is 0.000745496479794383\n",
      "epoch: 7 step: 1141, loss is 0.00043226417619735\n",
      "epoch: 7 step: 1142, loss is 0.00018198657198809087\n",
      "epoch: 7 step: 1143, loss is 0.0003291823959443718\n",
      "epoch: 7 step: 1144, loss is 0.0002481638512108475\n",
      "epoch: 7 step: 1145, loss is 0.017095256596803665\n",
      "epoch: 7 step: 1146, loss is 0.0015787295997142792\n",
      "epoch: 7 step: 1147, loss is 0.0015680189244449139\n",
      "epoch: 7 step: 1148, loss is 0.0011111010098829865\n",
      "epoch: 7 step: 1149, loss is 0.0009265856351703405\n",
      "epoch: 7 step: 1150, loss is 9.555324504617602e-05\n",
      "epoch: 7 step: 1151, loss is 0.0009418469853699207\n",
      "epoch: 7 step: 1152, loss is 0.0001976089843083173\n",
      "epoch: 7 step: 1153, loss is 0.015938693657517433\n",
      "epoch: 7 step: 1154, loss is 0.00024413321807514876\n",
      "epoch: 7 step: 1155, loss is 0.0001242148136952892\n",
      "epoch: 7 step: 1156, loss is 0.0002949922054540366\n",
      "epoch: 7 step: 1157, loss is 0.0035183837171643972\n",
      "epoch: 7 step: 1158, loss is 0.0421866849064827\n",
      "epoch: 7 step: 1159, loss is 0.0011916892835870385\n",
      "epoch: 7 step: 1160, loss is 0.008455712348222733\n",
      "epoch: 7 step: 1161, loss is 0.09930157661437988\n",
      "epoch: 7 step: 1162, loss is 0.0017196002881973982\n",
      "epoch: 7 step: 1163, loss is 0.00420633889734745\n",
      "epoch: 7 step: 1164, loss is 0.04249977320432663\n",
      "epoch: 7 step: 1165, loss is 0.0002582291199360043\n",
      "epoch: 7 step: 1166, loss is 9.258777572540566e-05\n",
      "epoch: 7 step: 1167, loss is 0.00041104876436293125\n",
      "epoch: 7 step: 1168, loss is 0.00018985422502737492\n",
      "epoch: 7 step: 1169, loss is 0.00014280939649324864\n",
      "epoch: 7 step: 1170, loss is 0.06868254393339157\n",
      "epoch: 7 step: 1171, loss is 0.002292988356202841\n",
      "epoch: 7 step: 1172, loss is 0.001438306411728263\n",
      "epoch: 7 step: 1173, loss is 0.07226934283971786\n",
      "epoch: 7 step: 1174, loss is 0.0001639207621337846\n",
      "epoch: 7 step: 1175, loss is 0.0008285075309686363\n",
      "epoch: 7 step: 1176, loss is 4.983689359505661e-05\n",
      "epoch: 7 step: 1177, loss is 0.0025126526597887278\n",
      "epoch: 7 step: 1178, loss is 0.09069371968507767\n",
      "epoch: 7 step: 1179, loss is 0.013675626367330551\n",
      "epoch: 7 step: 1180, loss is 0.05226115882396698\n",
      "epoch: 7 step: 1181, loss is 0.007666335441172123\n",
      "epoch: 7 step: 1182, loss is 0.0009521549800410867\n",
      "epoch: 7 step: 1183, loss is 0.08342444896697998\n",
      "epoch: 7 step: 1184, loss is 0.0005233243573457003\n",
      "epoch: 7 step: 1185, loss is 0.0008778399205766618\n",
      "epoch: 7 step: 1186, loss is 7.814934360794723e-05\n",
      "epoch: 7 step: 1187, loss is 0.004563546273857355\n",
      "epoch: 7 step: 1188, loss is 0.003974440507590771\n",
      "epoch: 7 step: 1189, loss is 0.00040765342419035733\n",
      "epoch: 7 step: 1190, loss is 0.0009398498223163188\n",
      "epoch: 7 step: 1191, loss is 0.00029238962451927364\n",
      "epoch: 7 step: 1192, loss is 0.25404638051986694\n",
      "epoch: 7 step: 1193, loss is 0.05368441715836525\n",
      "epoch: 7 step: 1194, loss is 0.0331030897796154\n",
      "epoch: 7 step: 1195, loss is 0.11863724887371063\n",
      "epoch: 7 step: 1196, loss is 0.006993011571466923\n",
      "epoch: 7 step: 1197, loss is 0.000202219482162036\n",
      "epoch: 7 step: 1198, loss is 0.0013014463474974036\n",
      "epoch: 7 step: 1199, loss is 8.45086105982773e-05\n",
      "epoch: 7 step: 1200, loss is 0.039499666541814804\n",
      "epoch: 7 step: 1201, loss is 0.014780891127884388\n",
      "epoch: 7 step: 1202, loss is 0.0004549842269625515\n",
      "epoch: 7 step: 1203, loss is 0.03120122291147709\n",
      "epoch: 7 step: 1204, loss is 0.0017463411204516888\n",
      "epoch: 7 step: 1205, loss is 0.0005705492221750319\n",
      "epoch: 7 step: 1206, loss is 0.0018642882350832224\n",
      "epoch: 7 step: 1207, loss is 0.0022013664711266756\n",
      "epoch: 7 step: 1208, loss is 0.001927197678014636\n",
      "epoch: 7 step: 1209, loss is 0.00022747743059881032\n",
      "epoch: 7 step: 1210, loss is 0.008116907440125942\n",
      "epoch: 7 step: 1211, loss is 0.017610060051083565\n",
      "epoch: 7 step: 1212, loss is 0.07770563662052155\n",
      "epoch: 7 step: 1213, loss is 0.0966491848230362\n",
      "epoch: 7 step: 1214, loss is 0.006723546423017979\n",
      "epoch: 7 step: 1215, loss is 0.004730342421680689\n",
      "epoch: 7 step: 1216, loss is 0.0009716032072901726\n",
      "epoch: 7 step: 1217, loss is 0.00017337733879685402\n",
      "epoch: 7 step: 1218, loss is 0.0003787124587688595\n",
      "epoch: 7 step: 1219, loss is 0.05510924011468887\n",
      "epoch: 7 step: 1220, loss is 0.001595119247213006\n",
      "epoch: 7 step: 1221, loss is 0.07694623619318008\n",
      "epoch: 7 step: 1222, loss is 0.00027894476079382\n",
      "epoch: 7 step: 1223, loss is 0.042309898883104324\n",
      "epoch: 7 step: 1224, loss is 0.0002684746286831796\n",
      "epoch: 7 step: 1225, loss is 0.00245490251109004\n",
      "epoch: 7 step: 1226, loss is 0.00023655808763578534\n",
      "epoch: 7 step: 1227, loss is 0.2685413360595703\n",
      "epoch: 7 step: 1228, loss is 0.009613033384084702\n",
      "epoch: 7 step: 1229, loss is 0.16719560325145721\n",
      "epoch: 7 step: 1230, loss is 0.031246041879057884\n",
      "epoch: 7 step: 1231, loss is 0.005138086155056953\n",
      "epoch: 7 step: 1232, loss is 0.050314467400312424\n",
      "epoch: 7 step: 1233, loss is 0.019766319543123245\n",
      "epoch: 7 step: 1234, loss is 0.107364721596241\n",
      "epoch: 7 step: 1235, loss is 0.03904257342219353\n",
      "epoch: 7 step: 1236, loss is 0.0005152054945938289\n",
      "epoch: 7 step: 1237, loss is 0.0006714439368806779\n",
      "epoch: 7 step: 1238, loss is 0.0896519124507904\n",
      "epoch: 7 step: 1239, loss is 0.00190664641559124\n",
      "epoch: 7 step: 1240, loss is 0.02418808452785015\n",
      "epoch: 7 step: 1241, loss is 0.002441984601318836\n",
      "epoch: 7 step: 1242, loss is 0.00013308927009347826\n",
      "epoch: 7 step: 1243, loss is 0.0016934582963585854\n",
      "epoch: 7 step: 1244, loss is 0.018775980919599533\n",
      "epoch: 7 step: 1245, loss is 0.04023540019989014\n",
      "epoch: 7 step: 1246, loss is 0.016010833904147148\n",
      "epoch: 7 step: 1247, loss is 0.0012240661308169365\n",
      "epoch: 7 step: 1248, loss is 0.0006162988720461726\n",
      "epoch: 7 step: 1249, loss is 0.008265549317002296\n",
      "epoch: 7 step: 1250, loss is 0.002371336566284299\n",
      "epoch: 7 step: 1251, loss is 0.0008613452082499862\n",
      "epoch: 7 step: 1252, loss is 0.004011356271803379\n",
      "epoch: 7 step: 1253, loss is 0.02261284925043583\n",
      "epoch: 7 step: 1254, loss is 0.007463192101567984\n",
      "epoch: 7 step: 1255, loss is 0.01625099778175354\n",
      "epoch: 7 step: 1256, loss is 0.0012674907920882106\n",
      "epoch: 7 step: 1257, loss is 0.0001271765650017187\n",
      "epoch: 7 step: 1258, loss is 0.0032936204224824905\n",
      "epoch: 7 step: 1259, loss is 0.0018499922007322311\n",
      "epoch: 7 step: 1260, loss is 0.10135773569345474\n",
      "epoch: 7 step: 1261, loss is 0.00032649285276420414\n",
      "epoch: 7 step: 1262, loss is 0.0013203069102019072\n",
      "epoch: 7 step: 1263, loss is 0.0017868680879473686\n",
      "epoch: 7 step: 1264, loss is 1.3966559890832286e-05\n",
      "epoch: 7 step: 1265, loss is 0.008322875015437603\n",
      "epoch: 7 step: 1266, loss is 0.0013081914512440562\n",
      "epoch: 7 step: 1267, loss is 0.026475965976715088\n",
      "epoch: 7 step: 1268, loss is 0.19385658204555511\n",
      "epoch: 7 step: 1269, loss is 0.002470445353537798\n",
      "epoch: 7 step: 1270, loss is 0.0007096703047864139\n",
      "epoch: 7 step: 1271, loss is 0.00010713050869526342\n",
      "epoch: 7 step: 1272, loss is 0.00017400160140823573\n",
      "epoch: 7 step: 1273, loss is 0.0010933703742921352\n",
      "epoch: 7 step: 1274, loss is 0.029122207313776016\n",
      "epoch: 7 step: 1275, loss is 0.0003886858175974339\n",
      "epoch: 7 step: 1276, loss is 0.019195592030882835\n",
      "epoch: 7 step: 1277, loss is 0.002644133288413286\n",
      "epoch: 7 step: 1278, loss is 0.0011812950251623988\n",
      "epoch: 7 step: 1279, loss is 0.15304656326770782\n",
      "epoch: 7 step: 1280, loss is 0.0021203109063208103\n",
      "epoch: 7 step: 1281, loss is 0.0005816296907141805\n",
      "epoch: 7 step: 1282, loss is 0.05978997051715851\n",
      "epoch: 7 step: 1283, loss is 0.000261490058619529\n",
      "epoch: 7 step: 1284, loss is 0.005182298365980387\n",
      "epoch: 7 step: 1285, loss is 0.006499177776277065\n",
      "epoch: 7 step: 1286, loss is 0.001372998347505927\n",
      "epoch: 7 step: 1287, loss is 3.409005148569122e-05\n",
      "epoch: 7 step: 1288, loss is 0.0016982594970613718\n",
      "epoch: 7 step: 1289, loss is 6.806242254242534e-06\n",
      "epoch: 7 step: 1290, loss is 0.01597645878791809\n",
      "epoch: 7 step: 1291, loss is 0.00017387556727044284\n",
      "epoch: 7 step: 1292, loss is 0.00010919915803242475\n",
      "epoch: 7 step: 1293, loss is 0.0003760645631700754\n",
      "epoch: 7 step: 1294, loss is 3.6731147702084854e-05\n",
      "epoch: 7 step: 1295, loss is 0.004017219878733158\n",
      "epoch: 7 step: 1296, loss is 0.022979246452450752\n",
      "epoch: 7 step: 1297, loss is 0.009612426161766052\n",
      "epoch: 7 step: 1298, loss is 0.048391446471214294\n",
      "epoch: 7 step: 1299, loss is 0.03498383238911629\n",
      "epoch: 7 step: 1300, loss is 0.012363404966890812\n",
      "epoch: 7 step: 1301, loss is 0.12477447837591171\n",
      "epoch: 7 step: 1302, loss is 0.09682752937078476\n",
      "epoch: 7 step: 1303, loss is 0.0034851261880248785\n",
      "epoch: 7 step: 1304, loss is 0.003929391037672758\n",
      "epoch: 7 step: 1305, loss is 0.00027630929253064096\n",
      "epoch: 7 step: 1306, loss is 0.009075148031115532\n",
      "epoch: 7 step: 1307, loss is 0.002711491659283638\n",
      "epoch: 7 step: 1308, loss is 0.0002517746761441231\n",
      "epoch: 7 step: 1309, loss is 0.002985607832670212\n",
      "epoch: 7 step: 1310, loss is 0.24169939756393433\n",
      "epoch: 7 step: 1311, loss is 0.0002379055949859321\n",
      "epoch: 7 step: 1312, loss is 0.1227567195892334\n",
      "epoch: 7 step: 1313, loss is 0.00042180283344350755\n",
      "epoch: 7 step: 1314, loss is 0.007307679392397404\n",
      "epoch: 7 step: 1315, loss is 0.0035568312741816044\n",
      "epoch: 7 step: 1316, loss is 0.008828346617519855\n",
      "epoch: 7 step: 1317, loss is 0.0006026795599609613\n",
      "epoch: 7 step: 1318, loss is 0.014966907911002636\n",
      "epoch: 7 step: 1319, loss is 0.0037420867010951042\n",
      "epoch: 7 step: 1320, loss is 0.008497534319758415\n",
      "epoch: 7 step: 1321, loss is 0.0013327794149518013\n",
      "epoch: 7 step: 1322, loss is 0.07842638343572617\n",
      "epoch: 7 step: 1323, loss is 0.001928586047142744\n",
      "epoch: 7 step: 1324, loss is 0.051913946866989136\n",
      "epoch: 7 step: 1325, loss is 0.006933826953172684\n",
      "epoch: 7 step: 1326, loss is 0.0012219900963827968\n",
      "epoch: 7 step: 1327, loss is 0.0009977292502298951\n",
      "epoch: 7 step: 1328, loss is 0.009491157718002796\n",
      "epoch: 7 step: 1329, loss is 0.004626123700290918\n",
      "epoch: 7 step: 1330, loss is 0.0014352109283208847\n",
      "epoch: 7 step: 1331, loss is 0.0016046279342845082\n",
      "epoch: 7 step: 1332, loss is 0.00010684443986974657\n",
      "epoch: 7 step: 1333, loss is 0.04033800587058067\n",
      "epoch: 7 step: 1334, loss is 0.0006706635467708111\n",
      "epoch: 7 step: 1335, loss is 0.005586777813732624\n",
      "epoch: 7 step: 1336, loss is 0.013124128803610802\n",
      "epoch: 7 step: 1337, loss is 0.00261304690502584\n",
      "epoch: 7 step: 1338, loss is 0.004583973437547684\n",
      "epoch: 7 step: 1339, loss is 0.024891231209039688\n",
      "epoch: 7 step: 1340, loss is 0.0011228416115045547\n",
      "epoch: 7 step: 1341, loss is 0.005264602601528168\n",
      "epoch: 7 step: 1342, loss is 0.01111653447151184\n",
      "epoch: 7 step: 1343, loss is 0.009941695258021355\n",
      "epoch: 7 step: 1344, loss is 0.10321337729692459\n",
      "epoch: 7 step: 1345, loss is 0.1312229037284851\n",
      "epoch: 7 step: 1346, loss is 0.0019716278184205294\n",
      "epoch: 7 step: 1347, loss is 0.002556213177740574\n",
      "epoch: 7 step: 1348, loss is 0.002630901290103793\n",
      "epoch: 7 step: 1349, loss is 0.01298705954104662\n",
      "epoch: 7 step: 1350, loss is 0.014499960467219353\n",
      "epoch: 7 step: 1351, loss is 0.009031351655721664\n",
      "epoch: 7 step: 1352, loss is 0.012802176177501678\n",
      "epoch: 7 step: 1353, loss is 0.001061974442563951\n",
      "epoch: 7 step: 1354, loss is 0.00811734888702631\n",
      "epoch: 7 step: 1355, loss is 0.004448838997632265\n",
      "epoch: 7 step: 1356, loss is 0.0005966112948954105\n",
      "epoch: 7 step: 1357, loss is 0.0004667417670134455\n",
      "epoch: 7 step: 1358, loss is 0.0023309001699090004\n",
      "epoch: 7 step: 1359, loss is 0.1306789219379425\n",
      "epoch: 7 step: 1360, loss is 0.0014413052704185247\n",
      "epoch: 7 step: 1361, loss is 0.12070103734731674\n",
      "epoch: 7 step: 1362, loss is 0.009406095370650291\n",
      "epoch: 7 step: 1363, loss is 0.0011606923071667552\n",
      "epoch: 7 step: 1364, loss is 6.825126183684915e-05\n",
      "epoch: 7 step: 1365, loss is 0.11996214091777802\n",
      "epoch: 7 step: 1366, loss is 3.746958827832714e-05\n",
      "epoch: 7 step: 1367, loss is 0.002838325221091509\n",
      "epoch: 7 step: 1368, loss is 0.0002711772976908833\n",
      "epoch: 7 step: 1369, loss is 8.017457730602473e-05\n",
      "epoch: 7 step: 1370, loss is 0.024819523096084595\n",
      "epoch: 7 step: 1371, loss is 0.002747656311839819\n",
      "epoch: 7 step: 1372, loss is 0.027109984308481216\n",
      "epoch: 7 step: 1373, loss is 0.0002592408563941717\n",
      "epoch: 7 step: 1374, loss is 0.0005436851643025875\n",
      "epoch: 7 step: 1375, loss is 0.01588485948741436\n",
      "epoch: 7 step: 1376, loss is 0.0604228675365448\n",
      "epoch: 7 step: 1377, loss is 0.07255624234676361\n",
      "epoch: 7 step: 1378, loss is 0.00025271583581343293\n",
      "epoch: 7 step: 1379, loss is 0.05426758527755737\n",
      "epoch: 7 step: 1380, loss is 0.001758152386173606\n",
      "epoch: 7 step: 1381, loss is 0.001020179595798254\n",
      "epoch: 7 step: 1382, loss is 0.0020628145430237055\n",
      "epoch: 7 step: 1383, loss is 0.0007360551971942186\n",
      "epoch: 7 step: 1384, loss is 0.009277342818677425\n",
      "epoch: 7 step: 1385, loss is 0.060031525790691376\n",
      "epoch: 7 step: 1386, loss is 0.013326339423656464\n",
      "epoch: 7 step: 1387, loss is 0.004745516926050186\n",
      "epoch: 7 step: 1388, loss is 0.0028227472212165594\n",
      "epoch: 7 step: 1389, loss is 0.00013103966193739325\n",
      "epoch: 7 step: 1390, loss is 0.0004098513745702803\n",
      "epoch: 7 step: 1391, loss is 0.001461719279177487\n",
      "epoch: 7 step: 1392, loss is 0.007641382981091738\n",
      "epoch: 7 step: 1393, loss is 0.0005753868608735502\n",
      "epoch: 7 step: 1394, loss is 0.01467806939035654\n",
      "epoch: 7 step: 1395, loss is 0.0011059199459850788\n",
      "epoch: 7 step: 1396, loss is 0.0009047263883985579\n",
      "epoch: 7 step: 1397, loss is 0.11573175340890884\n",
      "epoch: 7 step: 1398, loss is 0.0002503266150597483\n",
      "epoch: 7 step: 1399, loss is 0.032986439764499664\n",
      "epoch: 7 step: 1400, loss is 0.003982178401201963\n",
      "epoch: 7 step: 1401, loss is 0.0024450919590890408\n",
      "epoch: 7 step: 1402, loss is 0.0003238015342503786\n",
      "epoch: 7 step: 1403, loss is 0.00022014569549355656\n",
      "epoch: 7 step: 1404, loss is 0.0004852964775636792\n",
      "epoch: 7 step: 1405, loss is 0.0007638743845745921\n",
      "epoch: 7 step: 1406, loss is 0.0007488280534744263\n",
      "epoch: 7 step: 1407, loss is 5.166447226656601e-05\n",
      "epoch: 7 step: 1408, loss is 0.0021769970189779997\n",
      "epoch: 7 step: 1409, loss is 0.0006757396622560918\n",
      "epoch: 7 step: 1410, loss is 0.0001417149615008384\n",
      "epoch: 7 step: 1411, loss is 0.0007397405570372939\n",
      "epoch: 7 step: 1412, loss is 0.006578175351023674\n",
      "epoch: 7 step: 1413, loss is 0.005037982016801834\n",
      "epoch: 7 step: 1414, loss is 0.00017670303350314498\n",
      "epoch: 7 step: 1415, loss is 0.08833274245262146\n",
      "epoch: 7 step: 1416, loss is 0.0030532849486917257\n",
      "epoch: 7 step: 1417, loss is 0.13935412466526031\n",
      "epoch: 7 step: 1418, loss is 0.15219396352767944\n",
      "epoch: 7 step: 1419, loss is 0.001122577814385295\n",
      "epoch: 7 step: 1420, loss is 0.026799093931913376\n",
      "epoch: 7 step: 1421, loss is 0.0004834218416363001\n",
      "epoch: 7 step: 1422, loss is 0.0427679643034935\n",
      "epoch: 7 step: 1423, loss is 0.0015731097664684057\n",
      "epoch: 7 step: 1424, loss is 0.00332012795843184\n",
      "epoch: 7 step: 1425, loss is 0.0010626402217894793\n",
      "epoch: 7 step: 1426, loss is 0.0005873428890481591\n",
      "epoch: 7 step: 1427, loss is 0.0006243554526008666\n",
      "epoch: 7 step: 1428, loss is 0.056293562054634094\n",
      "epoch: 7 step: 1429, loss is 0.0800224244594574\n",
      "epoch: 7 step: 1430, loss is 0.0006495948182418942\n",
      "epoch: 7 step: 1431, loss is 0.00980634056031704\n",
      "epoch: 7 step: 1432, loss is 0.021165013313293457\n",
      "epoch: 7 step: 1433, loss is 0.0777362585067749\n",
      "epoch: 7 step: 1434, loss is 0.04246290773153305\n",
      "epoch: 7 step: 1435, loss is 0.0018838689429685473\n",
      "epoch: 7 step: 1436, loss is 0.020315034314990044\n",
      "epoch: 7 step: 1437, loss is 0.1103413999080658\n",
      "epoch: 7 step: 1438, loss is 0.11520776897668839\n",
      "epoch: 7 step: 1439, loss is 0.024266917258501053\n",
      "epoch: 7 step: 1440, loss is 0.09039826691150665\n",
      "epoch: 7 step: 1441, loss is 0.0051336949691176414\n",
      "epoch: 7 step: 1442, loss is 0.0029578376561403275\n",
      "epoch: 7 step: 1443, loss is 0.0018098000437021255\n",
      "epoch: 7 step: 1444, loss is 0.00974979903548956\n",
      "epoch: 7 step: 1445, loss is 0.0010969379218295217\n",
      "epoch: 7 step: 1446, loss is 0.006067190784960985\n",
      "epoch: 7 step: 1447, loss is 0.0031663987319916487\n",
      "epoch: 7 step: 1448, loss is 0.0006987354136072099\n",
      "epoch: 7 step: 1449, loss is 0.05922578647732735\n",
      "epoch: 7 step: 1450, loss is 0.001787336659617722\n",
      "epoch: 7 step: 1451, loss is 0.0030534479301422834\n",
      "epoch: 7 step: 1452, loss is 0.0002544553717598319\n",
      "epoch: 7 step: 1453, loss is 0.0006616360624320805\n",
      "epoch: 7 step: 1454, loss is 0.002474580891430378\n",
      "epoch: 7 step: 1455, loss is 0.0022900733165442944\n",
      "epoch: 7 step: 1456, loss is 0.015078798867762089\n",
      "epoch: 7 step: 1457, loss is 0.07839368283748627\n",
      "epoch: 7 step: 1458, loss is 0.020070206373929977\n",
      "epoch: 7 step: 1459, loss is 0.003588963532820344\n",
      "epoch: 7 step: 1460, loss is 0.0068110572174191475\n",
      "epoch: 7 step: 1461, loss is 0.14620652794837952\n",
      "epoch: 7 step: 1462, loss is 0.11423696577548981\n",
      "epoch: 7 step: 1463, loss is 0.20632807910442352\n",
      "epoch: 7 step: 1464, loss is 0.012516926042735577\n",
      "epoch: 7 step: 1465, loss is 0.0001776369899744168\n",
      "epoch: 7 step: 1466, loss is 0.04658732935786247\n",
      "epoch: 7 step: 1467, loss is 0.00033204234205186367\n",
      "epoch: 7 step: 1468, loss is 0.0016583007527515292\n",
      "epoch: 7 step: 1469, loss is 0.0052911872044205666\n",
      "epoch: 7 step: 1470, loss is 0.0029201358556747437\n",
      "epoch: 7 step: 1471, loss is 0.006228162441402674\n",
      "epoch: 7 step: 1472, loss is 0.010371264070272446\n",
      "epoch: 7 step: 1473, loss is 0.0008320932392962277\n",
      "epoch: 7 step: 1474, loss is 0.0049444446340203285\n",
      "epoch: 7 step: 1475, loss is 0.12057065218687057\n",
      "epoch: 7 step: 1476, loss is 0.051880255341529846\n",
      "epoch: 7 step: 1477, loss is 0.00031252577900886536\n",
      "epoch: 7 step: 1478, loss is 0.0349762849509716\n",
      "epoch: 7 step: 1479, loss is 0.009425308555364609\n",
      "epoch: 7 step: 1480, loss is 0.0005239963065832853\n",
      "epoch: 7 step: 1481, loss is 0.03019033558666706\n",
      "epoch: 7 step: 1482, loss is 0.03651031479239464\n",
      "epoch: 7 step: 1483, loss is 0.0024862599093466997\n",
      "epoch: 7 step: 1484, loss is 0.0028551595751196146\n",
      "epoch: 7 step: 1485, loss is 0.006648634560406208\n",
      "epoch: 7 step: 1486, loss is 0.005664569791406393\n",
      "epoch: 7 step: 1487, loss is 0.00012780028919223696\n",
      "epoch: 7 step: 1488, loss is 0.02956402488052845\n",
      "epoch: 7 step: 1489, loss is 0.001086249016225338\n",
      "epoch: 7 step: 1490, loss is 0.0023972224444150925\n",
      "epoch: 7 step: 1491, loss is 0.00036994245601817966\n",
      "epoch: 7 step: 1492, loss is 0.006775504443794489\n",
      "epoch: 7 step: 1493, loss is 0.00044066394912078977\n",
      "epoch: 7 step: 1494, loss is 0.0007022028439678252\n",
      "epoch: 7 step: 1495, loss is 0.0011080519761890173\n",
      "epoch: 7 step: 1496, loss is 0.0011664764024317265\n",
      "epoch: 7 step: 1497, loss is 0.019816840067505836\n",
      "epoch: 7 step: 1498, loss is 0.018361734226346016\n",
      "epoch: 7 step: 1499, loss is 0.000833444413729012\n",
      "epoch: 7 step: 1500, loss is 0.0016697304090484977\n",
      "epoch: 7 step: 1501, loss is 0.030906446278095245\n",
      "epoch: 7 step: 1502, loss is 0.0005052043707109988\n",
      "epoch: 7 step: 1503, loss is 0.010207531973719597\n",
      "epoch: 7 step: 1504, loss is 0.0035050739534199238\n",
      "epoch: 7 step: 1505, loss is 0.0016169205773621798\n",
      "epoch: 7 step: 1506, loss is 0.0007490900461561978\n",
      "epoch: 7 step: 1507, loss is 0.06383892893791199\n",
      "epoch: 7 step: 1508, loss is 0.0004979650839231908\n",
      "epoch: 7 step: 1509, loss is 0.059012271463871\n",
      "epoch: 7 step: 1510, loss is 0.001185775501653552\n",
      "epoch: 7 step: 1511, loss is 0.0006689854781143367\n",
      "epoch: 7 step: 1512, loss is 0.01043795794248581\n",
      "epoch: 7 step: 1513, loss is 0.004425217863172293\n",
      "epoch: 7 step: 1514, loss is 0.00020153765217401087\n",
      "epoch: 7 step: 1515, loss is 0.0016719258856028318\n",
      "epoch: 7 step: 1516, loss is 0.0017872325843200088\n",
      "epoch: 7 step: 1517, loss is 0.054832860827445984\n",
      "epoch: 7 step: 1518, loss is 0.0001586629805387929\n",
      "epoch: 7 step: 1519, loss is 0.00015066663036122918\n",
      "epoch: 7 step: 1520, loss is 0.002174633089452982\n",
      "epoch: 7 step: 1521, loss is 0.0008259139722213149\n",
      "epoch: 7 step: 1522, loss is 5.831968519487418e-05\n",
      "epoch: 7 step: 1523, loss is 0.0374922975897789\n",
      "epoch: 7 step: 1524, loss is 4.28955681854859e-05\n",
      "epoch: 7 step: 1525, loss is 0.0009855767711997032\n",
      "epoch: 7 step: 1526, loss is 0.005888114217668772\n",
      "epoch: 7 step: 1527, loss is 0.008093921467661858\n",
      "epoch: 7 step: 1528, loss is 0.004490946419537067\n",
      "epoch: 7 step: 1529, loss is 0.00943578127771616\n",
      "epoch: 7 step: 1530, loss is 0.08634093403816223\n",
      "epoch: 7 step: 1531, loss is 4.817121953237802e-05\n",
      "epoch: 7 step: 1532, loss is 0.0008813730673864484\n",
      "epoch: 7 step: 1533, loss is 0.0034319718834012747\n",
      "epoch: 7 step: 1534, loss is 0.003908631391823292\n",
      "epoch: 7 step: 1535, loss is 0.07488511502742767\n",
      "epoch: 7 step: 1536, loss is 0.0006613028235733509\n",
      "epoch: 7 step: 1537, loss is 0.0014823682140558958\n",
      "epoch: 7 step: 1538, loss is 0.008674032054841518\n",
      "epoch: 7 step: 1539, loss is 0.006219804286956787\n",
      "epoch: 7 step: 1540, loss is 4.979204823030159e-05\n",
      "epoch: 7 step: 1541, loss is 0.015556246973574162\n",
      "epoch: 7 step: 1542, loss is 0.10937268286943436\n",
      "epoch: 7 step: 1543, loss is 0.0002473752829246223\n",
      "epoch: 7 step: 1544, loss is 0.0140457758679986\n",
      "epoch: 7 step: 1545, loss is 0.00016142787353601307\n",
      "epoch: 7 step: 1546, loss is 0.0778738260269165\n",
      "epoch: 7 step: 1547, loss is 0.03668610751628876\n",
      "epoch: 7 step: 1548, loss is 0.0008105240995064378\n",
      "epoch: 7 step: 1549, loss is 0.002228330820798874\n",
      "epoch: 7 step: 1550, loss is 0.01299324631690979\n",
      "epoch: 7 step: 1551, loss is 0.00012979500752408057\n",
      "epoch: 7 step: 1552, loss is 0.001693077734671533\n",
      "epoch: 7 step: 1553, loss is 0.05311468988656998\n",
      "epoch: 7 step: 1554, loss is 0.01277469377964735\n",
      "epoch: 7 step: 1555, loss is 0.0014933821512386203\n",
      "epoch: 7 step: 1556, loss is 0.007924378849565983\n",
      "epoch: 7 step: 1557, loss is 0.0025625715497881174\n",
      "epoch: 7 step: 1558, loss is 0.0011627520434558392\n",
      "epoch: 7 step: 1559, loss is 0.000490702164825052\n",
      "epoch: 7 step: 1560, loss is 0.0026827258989214897\n",
      "epoch: 7 step: 1561, loss is 0.001687079668045044\n",
      "epoch: 7 step: 1562, loss is 0.003944850992411375\n",
      "epoch: 7 step: 1563, loss is 0.20144657790660858\n",
      "epoch: 7 step: 1564, loss is 0.012561831623315811\n",
      "epoch: 7 step: 1565, loss is 3.192792792106047e-05\n",
      "epoch: 7 step: 1566, loss is 0.004013485740870237\n",
      "epoch: 7 step: 1567, loss is 0.06229820474982262\n",
      "epoch: 7 step: 1568, loss is 0.013704294338822365\n",
      "epoch: 7 step: 1569, loss is 0.001968873431906104\n",
      "epoch: 7 step: 1570, loss is 0.052217837423086166\n",
      "epoch: 7 step: 1571, loss is 0.002369372174143791\n",
      "epoch: 7 step: 1572, loss is 0.00013374016270972788\n",
      "epoch: 7 step: 1573, loss is 0.0017434940673410892\n",
      "epoch: 7 step: 1574, loss is 0.012363390065729618\n",
      "epoch: 7 step: 1575, loss is 0.00033147374051623046\n",
      "epoch: 7 step: 1576, loss is 0.0014173436211422086\n",
      "epoch: 7 step: 1577, loss is 0.01014507096260786\n",
      "epoch: 7 step: 1578, loss is 4.904858360532671e-05\n",
      "epoch: 7 step: 1579, loss is 0.11458753049373627\n",
      "epoch: 7 step: 1580, loss is 0.00253588636405766\n",
      "epoch: 7 step: 1581, loss is 0.004166493192315102\n",
      "epoch: 7 step: 1582, loss is 4.498903945204802e-05\n",
      "epoch: 7 step: 1583, loss is 0.0004235688829794526\n",
      "epoch: 7 step: 1584, loss is 0.19590264558792114\n",
      "epoch: 7 step: 1585, loss is 0.0007862973143346608\n",
      "epoch: 7 step: 1586, loss is 0.0038137740921229124\n",
      "epoch: 7 step: 1587, loss is 0.00027585611678659916\n",
      "epoch: 7 step: 1588, loss is 0.0001296953414566815\n",
      "epoch: 7 step: 1589, loss is 0.007420091889798641\n",
      "epoch: 7 step: 1590, loss is 0.007506345398724079\n",
      "epoch: 7 step: 1591, loss is 0.0015155751025304198\n",
      "epoch: 7 step: 1592, loss is 0.04847995191812515\n",
      "epoch: 7 step: 1593, loss is 0.0034320754930377007\n",
      "epoch: 7 step: 1594, loss is 0.0005119951092638075\n",
      "epoch: 7 step: 1595, loss is 0.0006786886369809508\n",
      "epoch: 7 step: 1596, loss is 0.11617115885019302\n",
      "epoch: 7 step: 1597, loss is 0.002980699995532632\n",
      "epoch: 7 step: 1598, loss is 1.7443575416109525e-05\n",
      "epoch: 7 step: 1599, loss is 0.0003715609200298786\n",
      "epoch: 7 step: 1600, loss is 0.018231390044093132\n",
      "epoch: 7 step: 1601, loss is 0.0030366447754204273\n",
      "epoch: 7 step: 1602, loss is 0.013127973303198814\n",
      "epoch: 7 step: 1603, loss is 0.0006137333111837506\n",
      "epoch: 7 step: 1604, loss is 0.0004242177528794855\n",
      "epoch: 7 step: 1605, loss is 6.019222200848162e-05\n",
      "epoch: 7 step: 1606, loss is 0.007042750716209412\n",
      "epoch: 7 step: 1607, loss is 0.0006339105311781168\n",
      "epoch: 7 step: 1608, loss is 0.140793114900589\n",
      "epoch: 7 step: 1609, loss is 0.00045415712520480156\n",
      "epoch: 7 step: 1610, loss is 0.0008120412821881473\n",
      "epoch: 7 step: 1611, loss is 0.005959407426416874\n",
      "epoch: 7 step: 1612, loss is 0.00047124645789153874\n",
      "epoch: 7 step: 1613, loss is 0.00046697899233549833\n",
      "epoch: 7 step: 1614, loss is 0.06158531829714775\n",
      "epoch: 7 step: 1615, loss is 0.04901320859789848\n",
      "epoch: 7 step: 1616, loss is 0.06067921966314316\n",
      "epoch: 7 step: 1617, loss is 0.006388204637914896\n",
      "epoch: 7 step: 1618, loss is 0.0015348009765148163\n",
      "epoch: 7 step: 1619, loss is 0.07663610577583313\n",
      "epoch: 7 step: 1620, loss is 0.0028375210240483284\n",
      "epoch: 7 step: 1621, loss is 6.04479864705354e-05\n",
      "epoch: 7 step: 1622, loss is 0.001756887650117278\n",
      "epoch: 7 step: 1623, loss is 0.004699761979281902\n",
      "epoch: 7 step: 1624, loss is 0.0006439852295443416\n",
      "epoch: 7 step: 1625, loss is 0.0003125436487607658\n",
      "epoch: 7 step: 1626, loss is 0.05215214192867279\n",
      "epoch: 7 step: 1627, loss is 0.010657098144292831\n",
      "epoch: 7 step: 1628, loss is 0.01816040650010109\n",
      "epoch: 7 step: 1629, loss is 0.000597578939050436\n",
      "epoch: 7 step: 1630, loss is 0.0010221002157777548\n",
      "epoch: 7 step: 1631, loss is 0.00039862660923972726\n",
      "epoch: 7 step: 1632, loss is 0.0010525730904191732\n",
      "epoch: 7 step: 1633, loss is 0.0004581795947160572\n",
      "epoch: 7 step: 1634, loss is 0.002940342528745532\n",
      "epoch: 7 step: 1635, loss is 0.0035058162175118923\n",
      "epoch: 7 step: 1636, loss is 0.12816251814365387\n",
      "epoch: 7 step: 1637, loss is 0.043854016810655594\n",
      "epoch: 7 step: 1638, loss is 0.00034388594212941825\n",
      "epoch: 7 step: 1639, loss is 0.0087464964017272\n",
      "epoch: 7 step: 1640, loss is 0.01314895786345005\n",
      "epoch: 7 step: 1641, loss is 0.00032241048756986856\n",
      "epoch: 7 step: 1642, loss is 6.225646939128637e-05\n",
      "epoch: 7 step: 1643, loss is 0.016276786103844643\n",
      "epoch: 7 step: 1644, loss is 0.0005615433328785002\n",
      "epoch: 7 step: 1645, loss is 0.0005179567378945649\n",
      "epoch: 7 step: 1646, loss is 0.004590403754264116\n",
      "epoch: 7 step: 1647, loss is 0.0010722919832915068\n",
      "epoch: 7 step: 1648, loss is 0.00042437604861333966\n",
      "epoch: 7 step: 1649, loss is 0.0002807173295877874\n",
      "epoch: 7 step: 1650, loss is 0.000600398750975728\n",
      "epoch: 7 step: 1651, loss is 0.00044967286521568894\n",
      "epoch: 7 step: 1652, loss is 0.014096657745540142\n",
      "epoch: 7 step: 1653, loss is 0.0004979512304998934\n",
      "epoch: 7 step: 1654, loss is 0.00010941352229565382\n",
      "epoch: 7 step: 1655, loss is 0.000484576536109671\n",
      "epoch: 7 step: 1656, loss is 0.011779270134866238\n",
      "epoch: 7 step: 1657, loss is 0.013652185909450054\n",
      "epoch: 7 step: 1658, loss is 0.03314237296581268\n",
      "epoch: 7 step: 1659, loss is 0.0009019705466926098\n",
      "epoch: 7 step: 1660, loss is 0.018605848774313927\n",
      "epoch: 7 step: 1661, loss is 0.00030498357955366373\n",
      "epoch: 7 step: 1662, loss is 0.0009765948634594679\n",
      "epoch: 7 step: 1663, loss is 0.011339752934873104\n",
      "epoch: 7 step: 1664, loss is 0.0008558117551729083\n",
      "epoch: 7 step: 1665, loss is 0.0013975979527458549\n",
      "epoch: 7 step: 1666, loss is 0.002059012884274125\n",
      "epoch: 7 step: 1667, loss is 0.0005020262324251235\n",
      "epoch: 7 step: 1668, loss is 0.0006552545237354934\n",
      "epoch: 7 step: 1669, loss is 0.22070635855197906\n",
      "epoch: 7 step: 1670, loss is 0.0002896427467931062\n",
      "epoch: 7 step: 1671, loss is 0.010985191911458969\n",
      "epoch: 7 step: 1672, loss is 0.0004237111425027251\n",
      "epoch: 7 step: 1673, loss is 0.0042561134323477745\n",
      "epoch: 7 step: 1674, loss is 0.008140641264617443\n",
      "epoch: 7 step: 1675, loss is 0.007377083878964186\n",
      "epoch: 7 step: 1676, loss is 0.019731109961867332\n",
      "epoch: 7 step: 1677, loss is 0.00031592586310580373\n",
      "epoch: 7 step: 1678, loss is 0.0014241291210055351\n",
      "epoch: 7 step: 1679, loss is 0.0067013707011938095\n",
      "epoch: 7 step: 1680, loss is 0.032100602984428406\n",
      "epoch: 7 step: 1681, loss is 0.00018806476145982742\n",
      "epoch: 7 step: 1682, loss is 0.008854108862578869\n",
      "epoch: 7 step: 1683, loss is 0.0011555198580026627\n",
      "epoch: 7 step: 1684, loss is 0.017482440918684006\n",
      "epoch: 7 step: 1685, loss is 0.14381536841392517\n",
      "epoch: 7 step: 1686, loss is 0.003961310256272554\n",
      "epoch: 7 step: 1687, loss is 6.061256044631591e-06\n",
      "epoch: 7 step: 1688, loss is 0.00010023301001638174\n",
      "epoch: 7 step: 1689, loss is 0.06113359332084656\n",
      "epoch: 7 step: 1690, loss is 5.488920214702375e-05\n",
      "epoch: 7 step: 1691, loss is 0.09098687022924423\n",
      "epoch: 7 step: 1692, loss is 0.011690277606248856\n",
      "epoch: 7 step: 1693, loss is 0.00019074925512541085\n",
      "epoch: 7 step: 1694, loss is 5.7629047660157084e-05\n",
      "epoch: 7 step: 1695, loss is 0.0006563562783412635\n",
      "epoch: 7 step: 1696, loss is 0.00036409051972441375\n",
      "epoch: 7 step: 1697, loss is 0.0007650813204236329\n",
      "epoch: 7 step: 1698, loss is 0.0005821961676701903\n",
      "epoch: 7 step: 1699, loss is 0.050951261073350906\n",
      "epoch: 7 step: 1700, loss is 0.0009780306136235595\n",
      "epoch: 7 step: 1701, loss is 0.0029714913107454777\n",
      "epoch: 7 step: 1702, loss is 4.615491343429312e-05\n",
      "epoch: 7 step: 1703, loss is 0.007501590996980667\n",
      "epoch: 7 step: 1704, loss is 0.006738484371453524\n",
      "epoch: 7 step: 1705, loss is 0.0018400625558570027\n",
      "epoch: 7 step: 1706, loss is 0.004416747484356165\n",
      "epoch: 7 step: 1707, loss is 0.026457257568836212\n",
      "epoch: 7 step: 1708, loss is 0.00017767563986126333\n",
      "epoch: 7 step: 1709, loss is 0.0012773664202541113\n",
      "epoch: 7 step: 1710, loss is 0.049177881330251694\n",
      "epoch: 7 step: 1711, loss is 0.0065454598516225815\n",
      "epoch: 7 step: 1712, loss is 0.001324428478255868\n",
      "epoch: 7 step: 1713, loss is 0.01162111759185791\n",
      "epoch: 7 step: 1714, loss is 0.0007117348141036928\n",
      "epoch: 7 step: 1715, loss is 0.0015332621987909079\n",
      "epoch: 7 step: 1716, loss is 0.0003779065445996821\n",
      "epoch: 7 step: 1717, loss is 0.016337376087903976\n",
      "epoch: 7 step: 1718, loss is 0.007473209407180548\n",
      "epoch: 7 step: 1719, loss is 0.088619165122509\n",
      "epoch: 7 step: 1720, loss is 0.03408772125840187\n",
      "epoch: 7 step: 1721, loss is 0.003009572857990861\n",
      "epoch: 7 step: 1722, loss is 0.0021328867878764868\n",
      "epoch: 7 step: 1723, loss is 0.0016435112338513136\n",
      "epoch: 7 step: 1724, loss is 0.00010162952094106004\n",
      "epoch: 7 step: 1725, loss is 0.000924662163015455\n",
      "epoch: 7 step: 1726, loss is 0.0009927465580403805\n",
      "epoch: 7 step: 1727, loss is 0.002940879203379154\n",
      "epoch: 7 step: 1728, loss is 0.0001317600253969431\n",
      "epoch: 7 step: 1729, loss is 0.002431084867566824\n",
      "epoch: 7 step: 1730, loss is 0.0004286240437068045\n",
      "epoch: 7 step: 1731, loss is 0.005852188915014267\n",
      "epoch: 7 step: 1732, loss is 0.003948167897760868\n",
      "epoch: 7 step: 1733, loss is 0.002720537129789591\n",
      "epoch: 7 step: 1734, loss is 0.0018582192715257406\n",
      "epoch: 7 step: 1735, loss is 3.146105882478878e-05\n",
      "epoch: 7 step: 1736, loss is 0.023079877719283104\n",
      "epoch: 7 step: 1737, loss is 0.0212047528475523\n",
      "epoch: 7 step: 1738, loss is 0.0017380157951265574\n",
      "epoch: 7 step: 1739, loss is 0.0012405235320329666\n",
      "epoch: 7 step: 1740, loss is 0.004681913182139397\n",
      "epoch: 7 step: 1741, loss is 0.005795086268335581\n",
      "epoch: 7 step: 1742, loss is 0.0028982486110180616\n",
      "epoch: 7 step: 1743, loss is 0.00018930502119474113\n",
      "epoch: 7 step: 1744, loss is 0.037719257175922394\n",
      "epoch: 7 step: 1745, loss is 0.00024390564067289233\n",
      "epoch: 7 step: 1746, loss is 0.0005652284598909318\n",
      "epoch: 7 step: 1747, loss is 0.054651062935590744\n",
      "epoch: 7 step: 1748, loss is 0.00830506719648838\n",
      "epoch: 7 step: 1749, loss is 0.000417650502640754\n",
      "epoch: 7 step: 1750, loss is 0.022131282836198807\n",
      "epoch: 7 step: 1751, loss is 1.1370285392331425e-05\n",
      "epoch: 7 step: 1752, loss is 0.0013414117274805903\n",
      "epoch: 7 step: 1753, loss is 0.001325069461017847\n",
      "epoch: 7 step: 1754, loss is 0.02089296467602253\n",
      "epoch: 7 step: 1755, loss is 0.0014511971967294812\n",
      "epoch: 7 step: 1756, loss is 0.0018680836074054241\n",
      "epoch: 7 step: 1757, loss is 0.04542408883571625\n",
      "epoch: 7 step: 1758, loss is 0.0017213975079357624\n",
      "epoch: 7 step: 1759, loss is 0.0014586581382900476\n",
      "epoch: 7 step: 1760, loss is 0.0001802752522053197\n",
      "epoch: 7 step: 1761, loss is 0.0011291047558188438\n",
      "epoch: 7 step: 1762, loss is 0.0011044084094464779\n",
      "epoch: 7 step: 1763, loss is 0.008966988883912563\n",
      "epoch: 7 step: 1764, loss is 0.011565636843442917\n",
      "epoch: 7 step: 1765, loss is 0.0003367838216945529\n",
      "epoch: 7 step: 1766, loss is 0.002300780499354005\n",
      "epoch: 7 step: 1767, loss is 0.001684434013441205\n",
      "epoch: 7 step: 1768, loss is 0.0001096043415600434\n",
      "epoch: 7 step: 1769, loss is 6.244250107556581e-05\n",
      "epoch: 7 step: 1770, loss is 0.011371228843927383\n",
      "epoch: 7 step: 1771, loss is 0.007219182327389717\n",
      "epoch: 7 step: 1772, loss is 0.03441855311393738\n",
      "epoch: 7 step: 1773, loss is 0.00585633609443903\n",
      "epoch: 7 step: 1774, loss is 0.017835231497883797\n",
      "epoch: 7 step: 1775, loss is 0.0004968513967469335\n",
      "epoch: 7 step: 1776, loss is 5.828237772220746e-05\n",
      "epoch: 7 step: 1777, loss is 0.0010335835395380855\n",
      "epoch: 7 step: 1778, loss is 0.004440641030669212\n",
      "epoch: 7 step: 1779, loss is 0.0009919368894770741\n",
      "epoch: 7 step: 1780, loss is 0.004129898268729448\n",
      "epoch: 7 step: 1781, loss is 0.00899918656796217\n",
      "epoch: 7 step: 1782, loss is 0.02740861102938652\n",
      "epoch: 7 step: 1783, loss is 0.004227152094244957\n",
      "epoch: 7 step: 1784, loss is 0.00604979507625103\n",
      "epoch: 7 step: 1785, loss is 0.0008979171398095787\n",
      "epoch: 7 step: 1786, loss is 0.0010622089030221105\n",
      "epoch: 7 step: 1787, loss is 0.003783048363402486\n",
      "epoch: 7 step: 1788, loss is 0.0510786734521389\n",
      "epoch: 7 step: 1789, loss is 0.0002650795504450798\n",
      "epoch: 7 step: 1790, loss is 0.0025738007389009\n",
      "epoch: 7 step: 1791, loss is 0.00033165840432047844\n",
      "epoch: 7 step: 1792, loss is 0.006275664083659649\n",
      "epoch: 7 step: 1793, loss is 0.00035346316872164607\n",
      "epoch: 7 step: 1794, loss is 0.00042797959758900106\n",
      "epoch: 7 step: 1795, loss is 9.034210052050184e-06\n",
      "epoch: 7 step: 1796, loss is 9.131614933721721e-05\n",
      "epoch: 7 step: 1797, loss is 0.006265793461352587\n",
      "epoch: 7 step: 1798, loss is 0.0009217481128871441\n",
      "epoch: 7 step: 1799, loss is 1.2523763871286064e-05\n",
      "epoch: 7 step: 1800, loss is 0.001167499809525907\n",
      "epoch: 7 step: 1801, loss is 1.5761379472678527e-05\n",
      "epoch: 7 step: 1802, loss is 0.011649011634290218\n",
      "epoch: 7 step: 1803, loss is 0.04539087042212486\n",
      "epoch: 7 step: 1804, loss is 0.0022439744789153337\n",
      "epoch: 7 step: 1805, loss is 0.00022147074923850596\n",
      "epoch: 7 step: 1806, loss is 0.0023890782613307238\n",
      "epoch: 7 step: 1807, loss is 0.08283247798681259\n",
      "epoch: 7 step: 1808, loss is 0.0013793391408398747\n",
      "epoch: 7 step: 1809, loss is 0.008789870887994766\n",
      "epoch: 7 step: 1810, loss is 0.0006013059755787253\n",
      "epoch: 7 step: 1811, loss is 0.0004983879043720663\n",
      "epoch: 7 step: 1812, loss is 0.0003335413057357073\n",
      "epoch: 7 step: 1813, loss is 0.004098683129996061\n",
      "epoch: 7 step: 1814, loss is 0.013235747814178467\n",
      "epoch: 7 step: 1815, loss is 0.009236265905201435\n",
      "epoch: 7 step: 1816, loss is 0.03836536779999733\n",
      "epoch: 7 step: 1817, loss is 0.04231233522295952\n",
      "epoch: 7 step: 1818, loss is 0.029796935617923737\n",
      "epoch: 7 step: 1819, loss is 0.0038699640426784754\n",
      "epoch: 7 step: 1820, loss is 0.04948021098971367\n",
      "epoch: 7 step: 1821, loss is 0.14727400243282318\n",
      "epoch: 7 step: 1822, loss is 0.0504315011203289\n",
      "epoch: 7 step: 1823, loss is 0.040403109043836594\n",
      "epoch: 7 step: 1824, loss is 0.001699601300060749\n",
      "epoch: 7 step: 1825, loss is 0.007517012767493725\n",
      "epoch: 7 step: 1826, loss is 0.0004953916650265455\n",
      "epoch: 7 step: 1827, loss is 0.00925611611455679\n",
      "epoch: 7 step: 1828, loss is 0.002126665785908699\n",
      "epoch: 7 step: 1829, loss is 0.0037655241321772337\n",
      "epoch: 7 step: 1830, loss is 0.11619219928979874\n",
      "epoch: 7 step: 1831, loss is 0.004256044514477253\n",
      "epoch: 7 step: 1832, loss is 0.10141363739967346\n",
      "epoch: 7 step: 1833, loss is 0.09305550903081894\n",
      "epoch: 7 step: 1834, loss is 0.013617533259093761\n",
      "epoch: 7 step: 1835, loss is 0.004288573283702135\n",
      "epoch: 7 step: 1836, loss is 0.002909373724833131\n",
      "epoch: 7 step: 1837, loss is 0.0504840612411499\n",
      "epoch: 7 step: 1838, loss is 0.006358406972140074\n",
      "epoch: 7 step: 1839, loss is 0.00039871977060101926\n",
      "epoch: 7 step: 1840, loss is 5.720692206523381e-05\n",
      "epoch: 7 step: 1841, loss is 0.0002836668281815946\n",
      "epoch: 7 step: 1842, loss is 0.005847154650837183\n",
      "epoch: 7 step: 1843, loss is 0.00022817414719611406\n",
      "epoch: 7 step: 1844, loss is 0.002981955651193857\n",
      "epoch: 7 step: 1845, loss is 0.22955092787742615\n",
      "epoch: 7 step: 1846, loss is 0.1630505919456482\n",
      "epoch: 7 step: 1847, loss is 0.026381125673651695\n",
      "epoch: 7 step: 1848, loss is 0.003241727827116847\n",
      "epoch: 7 step: 1849, loss is 0.00980157870799303\n",
      "epoch: 7 step: 1850, loss is 3.4464450436644256e-05\n",
      "epoch: 7 step: 1851, loss is 0.0015107220970094204\n",
      "epoch: 7 step: 1852, loss is 0.0029383364599198103\n",
      "epoch: 7 step: 1853, loss is 0.000461781834019348\n",
      "epoch: 7 step: 1854, loss is 0.035759709775447845\n",
      "epoch: 7 step: 1855, loss is 0.0009748092270456254\n",
      "epoch: 7 step: 1856, loss is 0.00024900477728806436\n",
      "epoch: 7 step: 1857, loss is 0.0035613311920315027\n",
      "epoch: 7 step: 1858, loss is 4.716293460660381e-06\n",
      "epoch: 7 step: 1859, loss is 0.00038393615977838635\n",
      "epoch: 7 step: 1860, loss is 0.06654541939496994\n",
      "epoch: 7 step: 1861, loss is 0.00042540067806839943\n",
      "epoch: 7 step: 1862, loss is 0.0003964933566749096\n",
      "epoch: 7 step: 1863, loss is 0.007282172329723835\n",
      "epoch: 7 step: 1864, loss is 0.0004587019793689251\n",
      "epoch: 7 step: 1865, loss is 3.8313071854645386e-05\n",
      "epoch: 7 step: 1866, loss is 0.1519114077091217\n",
      "epoch: 7 step: 1867, loss is 0.00026345360674895346\n",
      "epoch: 7 step: 1868, loss is 0.015458289533853531\n",
      "epoch: 7 step: 1869, loss is 0.00996282510459423\n",
      "epoch: 7 step: 1870, loss is 8.389344293391332e-05\n",
      "epoch: 7 step: 1871, loss is 0.00028889504028484225\n",
      "epoch: 7 step: 1872, loss is 9.686966222943738e-05\n",
      "epoch: 7 step: 1873, loss is 0.0022782920859754086\n",
      "epoch: 7 step: 1874, loss is 6.457033305196092e-05\n",
      "epoch: 7 step: 1875, loss is 0.00021630582341458648\n",
      "Train epoch time: 16232.134 ms, per step time: 8.657 ms\n",
      "epoch: 8 step: 1, loss is 0.0002901180414482951\n",
      "epoch: 8 step: 2, loss is 0.0412236712872982\n",
      "epoch: 8 step: 3, loss is 0.002499817870557308\n",
      "epoch: 8 step: 4, loss is 0.0018033181549981236\n",
      "epoch: 8 step: 5, loss is 0.0036368381697684526\n",
      "epoch: 8 step: 6, loss is 0.04801082983613014\n",
      "epoch: 8 step: 7, loss is 0.00015058048302307725\n",
      "epoch: 8 step: 8, loss is 0.0002352513838559389\n",
      "epoch: 8 step: 9, loss is 0.00823481846600771\n",
      "epoch: 8 step: 10, loss is 0.00146420334931463\n",
      "epoch: 8 step: 11, loss is 0.0024206708185374737\n",
      "epoch: 8 step: 12, loss is 0.027602065354585648\n",
      "epoch: 8 step: 13, loss is 0.008539111353456974\n",
      "epoch: 8 step: 14, loss is 0.0002896560472436249\n",
      "epoch: 8 step: 15, loss is 8.679322490934283e-05\n",
      "epoch: 8 step: 16, loss is 0.01327830832451582\n",
      "epoch: 8 step: 17, loss is 0.0001594204513821751\n",
      "epoch: 8 step: 18, loss is 0.00015344710845965892\n",
      "epoch: 8 step: 19, loss is 0.00019226627773605287\n",
      "epoch: 8 step: 20, loss is 0.0004987103748135269\n",
      "epoch: 8 step: 21, loss is 0.00041198337567038834\n",
      "epoch: 8 step: 22, loss is 0.018290236592292786\n",
      "epoch: 8 step: 23, loss is 0.00012748930021189153\n",
      "epoch: 8 step: 24, loss is 0.00042890323675237596\n",
      "epoch: 8 step: 25, loss is 4.2504318116698414e-05\n",
      "epoch: 8 step: 26, loss is 0.00033863805583678186\n",
      "epoch: 8 step: 27, loss is 1.98761626961641e-05\n",
      "epoch: 8 step: 28, loss is 9.706365381134674e-05\n",
      "epoch: 8 step: 29, loss is 0.0314120389521122\n",
      "epoch: 8 step: 30, loss is 0.0003784208674915135\n",
      "epoch: 8 step: 31, loss is 0.022325310856103897\n",
      "epoch: 8 step: 32, loss is 0.00014602263399865478\n",
      "epoch: 8 step: 33, loss is 0.0007073793094605207\n",
      "epoch: 8 step: 34, loss is 0.00018335985078010708\n",
      "epoch: 8 step: 35, loss is 0.0002810392761602998\n",
      "epoch: 8 step: 36, loss is 0.010263810865581036\n",
      "epoch: 8 step: 37, loss is 0.030210744589567184\n",
      "epoch: 8 step: 38, loss is 0.00031814153771847486\n",
      "epoch: 8 step: 39, loss is 0.004394880961626768\n",
      "epoch: 8 step: 40, loss is 0.0016587702557444572\n",
      "epoch: 8 step: 41, loss is 0.0003715344937518239\n",
      "epoch: 8 step: 42, loss is 0.00029473614995367825\n",
      "epoch: 8 step: 43, loss is 3.419307904550806e-05\n",
      "epoch: 8 step: 44, loss is 3.344404467497952e-05\n",
      "epoch: 8 step: 45, loss is 0.00031215519993565977\n",
      "epoch: 8 step: 46, loss is 0.0008943472639657557\n",
      "epoch: 8 step: 47, loss is 0.007587995380163193\n",
      "epoch: 8 step: 48, loss is 0.0011415553744882345\n",
      "epoch: 8 step: 49, loss is 0.008665311150252819\n",
      "epoch: 8 step: 50, loss is 0.0008767714607529342\n",
      "epoch: 8 step: 51, loss is 0.03551945835351944\n",
      "epoch: 8 step: 52, loss is 0.010806594975292683\n",
      "epoch: 8 step: 53, loss is 0.035018082708120346\n",
      "epoch: 8 step: 54, loss is 0.00047361390898004174\n",
      "epoch: 8 step: 55, loss is 0.000521629408467561\n",
      "epoch: 8 step: 56, loss is 0.0002878605737350881\n",
      "epoch: 8 step: 57, loss is 0.011116256937384605\n",
      "epoch: 8 step: 58, loss is 7.325186743400991e-05\n",
      "epoch: 8 step: 59, loss is 6.123483035480604e-05\n",
      "epoch: 8 step: 60, loss is 0.0022602875251322985\n",
      "epoch: 8 step: 61, loss is 0.0016826156061142683\n",
      "epoch: 8 step: 62, loss is 0.0009204400121234357\n",
      "epoch: 8 step: 63, loss is 0.04064555466175079\n",
      "epoch: 8 step: 64, loss is 0.00028773752273991704\n",
      "epoch: 8 step: 65, loss is 0.0003274715563748032\n",
      "epoch: 8 step: 66, loss is 0.0002811663434840739\n",
      "epoch: 8 step: 67, loss is 8.61685475683771e-06\n",
      "epoch: 8 step: 68, loss is 0.00045386303099803627\n",
      "epoch: 8 step: 69, loss is 0.0029096913058310747\n",
      "epoch: 8 step: 70, loss is 0.00014908344019204378\n",
      "epoch: 8 step: 71, loss is 0.00025919044855982065\n",
      "epoch: 8 step: 72, loss is 0.08575723320245743\n",
      "epoch: 8 step: 73, loss is 1.2117821825086139e-05\n",
      "epoch: 8 step: 74, loss is 0.00029622967122122645\n",
      "epoch: 8 step: 75, loss is 0.007627499755471945\n",
      "epoch: 8 step: 76, loss is 0.004017614759504795\n",
      "epoch: 8 step: 77, loss is 0.0002400684606982395\n",
      "epoch: 8 step: 78, loss is 0.0011625667102634907\n",
      "epoch: 8 step: 79, loss is 0.008650044910609722\n",
      "epoch: 8 step: 80, loss is 0.000384418701287359\n",
      "epoch: 8 step: 81, loss is 2.901380321418401e-05\n",
      "epoch: 8 step: 82, loss is 9.458025306230411e-05\n",
      "epoch: 8 step: 83, loss is 0.00600246898829937\n",
      "epoch: 8 step: 84, loss is 5.902130214963108e-05\n",
      "epoch: 8 step: 85, loss is 0.00037938059540465474\n",
      "epoch: 8 step: 86, loss is 0.08290434628725052\n",
      "epoch: 8 step: 87, loss is 0.16064347326755524\n",
      "epoch: 8 step: 88, loss is 0.007846737280488014\n",
      "epoch: 8 step: 89, loss is 0.10230442881584167\n",
      "epoch: 8 step: 90, loss is 0.00578578794375062\n",
      "epoch: 8 step: 91, loss is 0.0002750625426415354\n",
      "epoch: 8 step: 92, loss is 0.06862238049507141\n",
      "epoch: 8 step: 93, loss is 0.00015594399883411825\n",
      "epoch: 8 step: 94, loss is 0.0021688423585146666\n",
      "epoch: 8 step: 95, loss is 0.0005280398763716221\n",
      "epoch: 8 step: 96, loss is 0.16885869204998016\n",
      "epoch: 8 step: 97, loss is 0.00278687197715044\n",
      "epoch: 8 step: 98, loss is 0.00017775245942175388\n",
      "epoch: 8 step: 99, loss is 0.0017728526145219803\n",
      "epoch: 8 step: 100, loss is 0.0001509781723143533\n",
      "epoch: 8 step: 101, loss is 0.0027289330027997494\n",
      "epoch: 8 step: 102, loss is 0.0015543922781944275\n",
      "epoch: 8 step: 103, loss is 0.00309389759786427\n",
      "epoch: 8 step: 104, loss is 0.0005048338207416236\n",
      "epoch: 8 step: 105, loss is 0.001043237280100584\n",
      "epoch: 8 step: 106, loss is 0.01007309090346098\n",
      "epoch: 8 step: 107, loss is 0.00012669384886976331\n",
      "epoch: 8 step: 108, loss is 0.0005410794401541352\n",
      "epoch: 8 step: 109, loss is 4.0787730540614575e-05\n",
      "epoch: 8 step: 110, loss is 0.033476993441581726\n",
      "epoch: 8 step: 111, loss is 0.014017883688211441\n",
      "epoch: 8 step: 112, loss is 0.0028652164619416\n",
      "epoch: 8 step: 113, loss is 0.005656668916344643\n",
      "epoch: 8 step: 114, loss is 5.6822002079570666e-05\n",
      "epoch: 8 step: 115, loss is 0.0020859744399785995\n",
      "epoch: 8 step: 116, loss is 0.00012398188118822873\n",
      "epoch: 8 step: 117, loss is 0.07452758401632309\n",
      "epoch: 8 step: 118, loss is 0.0013374040136113763\n",
      "epoch: 8 step: 119, loss is 0.19831272959709167\n",
      "epoch: 8 step: 120, loss is 0.08844880014657974\n",
      "epoch: 8 step: 121, loss is 2.636256795085501e-05\n",
      "epoch: 8 step: 122, loss is 0.07927271723747253\n",
      "epoch: 8 step: 123, loss is 0.007033440284430981\n",
      "epoch: 8 step: 124, loss is 0.00041752864490263164\n",
      "epoch: 8 step: 125, loss is 0.0006186510436236858\n",
      "epoch: 8 step: 126, loss is 0.00020494799537118524\n",
      "epoch: 8 step: 127, loss is 0.008991495706140995\n",
      "epoch: 8 step: 128, loss is 0.04632626846432686\n",
      "epoch: 8 step: 129, loss is 0.0002166850317735225\n",
      "epoch: 8 step: 130, loss is 0.0003767399466596544\n",
      "epoch: 8 step: 131, loss is 0.0016657277010381222\n",
      "epoch: 8 step: 132, loss is 0.051250606775283813\n",
      "epoch: 8 step: 133, loss is 0.002120617078617215\n",
      "epoch: 8 step: 134, loss is 0.0033187787048518658\n",
      "epoch: 8 step: 135, loss is 0.00041301402961835265\n",
      "epoch: 8 step: 136, loss is 0.0006738348165526986\n",
      "epoch: 8 step: 137, loss is 0.00011703248310368508\n",
      "epoch: 8 step: 138, loss is 0.0010402360931038857\n",
      "epoch: 8 step: 139, loss is 0.013784637674689293\n",
      "epoch: 8 step: 140, loss is 0.0032317477744072676\n",
      "epoch: 8 step: 141, loss is 0.0011542817810550332\n",
      "epoch: 8 step: 142, loss is 0.0038378932513296604\n",
      "epoch: 8 step: 143, loss is 0.00024293336900882423\n",
      "epoch: 8 step: 144, loss is 0.00493207760155201\n",
      "epoch: 8 step: 145, loss is 0.0005570120410993695\n",
      "epoch: 8 step: 146, loss is 0.0004752504755742848\n",
      "epoch: 8 step: 147, loss is 0.0008850138401612639\n",
      "epoch: 8 step: 148, loss is 0.0003725844726432115\n",
      "epoch: 8 step: 149, loss is 0.0017723699565976858\n",
      "epoch: 8 step: 150, loss is 0.0015735047636553645\n",
      "epoch: 8 step: 151, loss is 0.0008408884750679135\n",
      "epoch: 8 step: 152, loss is 0.005152768921107054\n",
      "epoch: 8 step: 153, loss is 0.0011840815423056483\n",
      "epoch: 8 step: 154, loss is 0.00018881329742725939\n",
      "epoch: 8 step: 155, loss is 0.0011732301209121943\n",
      "epoch: 8 step: 156, loss is 0.005308520048856735\n",
      "epoch: 8 step: 157, loss is 0.00011379616626072675\n",
      "epoch: 8 step: 158, loss is 0.004181720316410065\n",
      "epoch: 8 step: 159, loss is 0.004326538648456335\n",
      "epoch: 8 step: 160, loss is 0.025251111015677452\n",
      "epoch: 8 step: 161, loss is 0.011423748917877674\n",
      "epoch: 8 step: 162, loss is 0.006276766303926706\n",
      "epoch: 8 step: 163, loss is 0.0012174315052106977\n",
      "epoch: 8 step: 164, loss is 0.00041553794289939106\n",
      "epoch: 8 step: 165, loss is 0.006998686585575342\n",
      "epoch: 8 step: 166, loss is 0.0003019061987288296\n",
      "epoch: 8 step: 167, loss is 0.0041257161647081375\n",
      "epoch: 8 step: 168, loss is 0.007498227991163731\n",
      "epoch: 8 step: 169, loss is 0.0007179409149102867\n",
      "epoch: 8 step: 170, loss is 0.0021759900264441967\n",
      "epoch: 8 step: 171, loss is 0.0168756153434515\n",
      "epoch: 8 step: 172, loss is 0.02639019303023815\n",
      "epoch: 8 step: 173, loss is 0.00029762490885332227\n",
      "epoch: 8 step: 174, loss is 3.05557441606652e-05\n",
      "epoch: 8 step: 175, loss is 0.015801280736923218\n",
      "epoch: 8 step: 176, loss is 0.00022998232452664524\n",
      "epoch: 8 step: 177, loss is 4.025975431432016e-05\n",
      "epoch: 8 step: 178, loss is 0.00024140419554896653\n",
      "epoch: 8 step: 179, loss is 0.0004353919648565352\n",
      "epoch: 8 step: 180, loss is 0.06307366490364075\n",
      "epoch: 8 step: 181, loss is 0.00019226926087867469\n",
      "epoch: 8 step: 182, loss is 0.03824025020003319\n",
      "epoch: 8 step: 183, loss is 0.02030416578054428\n",
      "epoch: 8 step: 184, loss is 0.0004245666495990008\n",
      "epoch: 8 step: 185, loss is 0.0008986486354842782\n",
      "epoch: 8 step: 186, loss is 0.01549767330288887\n",
      "epoch: 8 step: 187, loss is 0.00301448255777359\n",
      "epoch: 8 step: 188, loss is 7.488705159630626e-05\n",
      "epoch: 8 step: 189, loss is 0.06676241010427475\n",
      "epoch: 8 step: 190, loss is 2.7083290206064703e-06\n",
      "epoch: 8 step: 191, loss is 0.08675622195005417\n",
      "epoch: 8 step: 192, loss is 0.004556918982416391\n",
      "epoch: 8 step: 193, loss is 0.03630080446600914\n",
      "epoch: 8 step: 194, loss is 0.008980883285403252\n",
      "epoch: 8 step: 195, loss is 0.002248935168609023\n",
      "epoch: 8 step: 196, loss is 0.0003930082602892071\n",
      "epoch: 8 step: 197, loss is 0.0022171803284436464\n",
      "epoch: 8 step: 198, loss is 0.03463342785835266\n",
      "epoch: 8 step: 199, loss is 0.036121439188718796\n",
      "epoch: 8 step: 200, loss is 0.0003538945165928453\n",
      "epoch: 8 step: 201, loss is 0.00018318546062801033\n",
      "epoch: 8 step: 202, loss is 0.0014385632239282131\n",
      "epoch: 8 step: 203, loss is 0.0014633063692599535\n",
      "epoch: 8 step: 204, loss is 0.026124095544219017\n",
      "epoch: 8 step: 205, loss is 0.025950264185667038\n",
      "epoch: 8 step: 206, loss is 0.00966948363929987\n",
      "epoch: 8 step: 207, loss is 0.0013810639502480626\n",
      "epoch: 8 step: 208, loss is 0.0007008489337749779\n",
      "epoch: 8 step: 209, loss is 0.10241767764091492\n",
      "epoch: 8 step: 210, loss is 7.256499520735815e-05\n",
      "epoch: 8 step: 211, loss is 0.1379012018442154\n",
      "epoch: 8 step: 212, loss is 0.0018902993761003017\n",
      "epoch: 8 step: 213, loss is 0.00011626700143096969\n",
      "epoch: 8 step: 214, loss is 0.024029642343521118\n",
      "epoch: 8 step: 215, loss is 0.0019018553430214524\n",
      "epoch: 8 step: 216, loss is 0.0006616331520490348\n",
      "epoch: 8 step: 217, loss is 0.015297260135412216\n",
      "epoch: 8 step: 218, loss is 0.0687166228890419\n",
      "epoch: 8 step: 219, loss is 0.00021355258650146425\n",
      "epoch: 8 step: 220, loss is 0.00027180954930372536\n",
      "epoch: 8 step: 221, loss is 0.011481636203825474\n",
      "epoch: 8 step: 222, loss is 0.027015944942831993\n",
      "epoch: 8 step: 223, loss is 0.00021747256687376648\n",
      "epoch: 8 step: 224, loss is 0.00036598494625650346\n",
      "epoch: 8 step: 225, loss is 8.571741636842489e-05\n",
      "epoch: 8 step: 226, loss is 2.818132816173602e-05\n",
      "epoch: 8 step: 227, loss is 0.0023765310179442167\n",
      "epoch: 8 step: 228, loss is 0.0001618018141016364\n",
      "epoch: 8 step: 229, loss is 0.007482779677957296\n",
      "epoch: 8 step: 230, loss is 0.00024508184287697077\n",
      "epoch: 8 step: 231, loss is 0.03440970182418823\n",
      "epoch: 8 step: 232, loss is 0.005030537024140358\n",
      "epoch: 8 step: 233, loss is 0.21443305909633636\n",
      "epoch: 8 step: 234, loss is 0.0022699807304888964\n",
      "epoch: 8 step: 235, loss is 0.006588658317923546\n",
      "epoch: 8 step: 236, loss is 3.3757591154426336e-05\n",
      "epoch: 8 step: 237, loss is 0.0038987642619758844\n",
      "epoch: 8 step: 238, loss is 0.09621600806713104\n",
      "epoch: 8 step: 239, loss is 0.00013622688129544258\n",
      "epoch: 8 step: 240, loss is 0.0016452983254566789\n",
      "epoch: 8 step: 241, loss is 0.045471128076314926\n",
      "epoch: 8 step: 242, loss is 0.0015491563826799393\n",
      "epoch: 8 step: 243, loss is 0.0005880487151443958\n",
      "epoch: 8 step: 244, loss is 0.0010797453578561544\n",
      "epoch: 8 step: 245, loss is 0.003309318097308278\n",
      "epoch: 8 step: 246, loss is 0.006236613262444735\n",
      "epoch: 8 step: 247, loss is 0.00034829528885893524\n",
      "epoch: 8 step: 248, loss is 0.010987432673573494\n",
      "epoch: 8 step: 249, loss is 0.0013183238916099072\n",
      "epoch: 8 step: 250, loss is 0.02619822882115841\n",
      "epoch: 8 step: 251, loss is 0.16157275438308716\n",
      "epoch: 8 step: 252, loss is 0.00931244995445013\n",
      "epoch: 8 step: 253, loss is 0.0022941415663808584\n",
      "epoch: 8 step: 254, loss is 0.016002459451556206\n",
      "epoch: 8 step: 255, loss is 0.024028975516557693\n",
      "epoch: 8 step: 256, loss is 0.00016581162344664335\n",
      "epoch: 8 step: 257, loss is 0.010280313901603222\n",
      "epoch: 8 step: 258, loss is 0.00010032104910351336\n",
      "epoch: 8 step: 259, loss is 0.0009201600914821029\n",
      "epoch: 8 step: 260, loss is 3.9392351027345285e-05\n",
      "epoch: 8 step: 261, loss is 0.00020461170061025769\n",
      "epoch: 8 step: 262, loss is 0.0005147600313648582\n",
      "epoch: 8 step: 263, loss is 0.01277010329067707\n",
      "epoch: 8 step: 264, loss is 0.004966539330780506\n",
      "epoch: 8 step: 265, loss is 0.00018774249474518\n",
      "epoch: 8 step: 266, loss is 0.06475845724344254\n",
      "epoch: 8 step: 267, loss is 0.004999249707907438\n",
      "epoch: 8 step: 268, loss is 0.004781876225024462\n",
      "epoch: 8 step: 269, loss is 0.0017390843713656068\n",
      "epoch: 8 step: 270, loss is 0.12350505590438843\n",
      "epoch: 8 step: 271, loss is 0.0005076085799373686\n",
      "epoch: 8 step: 272, loss is 0.002997269853949547\n",
      "epoch: 8 step: 273, loss is 0.0006125564686954021\n",
      "epoch: 8 step: 274, loss is 0.0016283787554129958\n",
      "epoch: 8 step: 275, loss is 0.005567528773099184\n",
      "epoch: 8 step: 276, loss is 0.0006106886430643499\n",
      "epoch: 8 step: 277, loss is 0.023188816383481026\n",
      "epoch: 8 step: 278, loss is 0.002057751640677452\n",
      "epoch: 8 step: 279, loss is 0.0005550935748033226\n",
      "epoch: 8 step: 280, loss is 0.008051969110965729\n",
      "epoch: 8 step: 281, loss is 0.0009286017739214003\n",
      "epoch: 8 step: 282, loss is 0.005171322263777256\n",
      "epoch: 8 step: 283, loss is 0.0005918487440794706\n",
      "epoch: 8 step: 284, loss is 0.0036969883367419243\n",
      "epoch: 8 step: 285, loss is 0.0008611889206804335\n",
      "epoch: 8 step: 286, loss is 0.11077338457107544\n",
      "epoch: 8 step: 287, loss is 4.67388381366618e-05\n",
      "epoch: 8 step: 288, loss is 0.03483019396662712\n",
      "epoch: 8 step: 289, loss is 0.0018047604244202375\n",
      "epoch: 8 step: 290, loss is 0.2281009703874588\n",
      "epoch: 8 step: 291, loss is 0.0032501500099897385\n",
      "epoch: 8 step: 292, loss is 0.01737319491803646\n",
      "epoch: 8 step: 293, loss is 0.07451793551445007\n",
      "epoch: 8 step: 294, loss is 0.0015926611376926303\n",
      "epoch: 8 step: 295, loss is 0.0027411957271397114\n",
      "epoch: 8 step: 296, loss is 0.006082636769860983\n",
      "epoch: 8 step: 297, loss is 0.007144581060856581\n",
      "epoch: 8 step: 298, loss is 0.0011898843804374337\n",
      "epoch: 8 step: 299, loss is 0.009210862219333649\n",
      "epoch: 8 step: 300, loss is 0.07918975502252579\n",
      "epoch: 8 step: 301, loss is 0.07727473974227905\n",
      "epoch: 8 step: 302, loss is 0.0018410516204312444\n",
      "epoch: 8 step: 303, loss is 0.0014163469895720482\n",
      "epoch: 8 step: 304, loss is 0.024187546223402023\n",
      "epoch: 8 step: 305, loss is 0.1405552178621292\n",
      "epoch: 8 step: 306, loss is 0.019689517095685005\n",
      "epoch: 8 step: 307, loss is 0.05007702112197876\n",
      "epoch: 8 step: 308, loss is 0.030023211613297462\n",
      "epoch: 8 step: 309, loss is 0.11729449033737183\n",
      "epoch: 8 step: 310, loss is 0.0037084321957081556\n",
      "epoch: 8 step: 311, loss is 0.0038810325786471367\n",
      "epoch: 8 step: 312, loss is 0.0008903087000362575\n",
      "epoch: 8 step: 313, loss is 0.0013917291071265936\n",
      "epoch: 8 step: 314, loss is 0.0026750683318823576\n",
      "epoch: 8 step: 315, loss is 0.0036404405254870653\n",
      "epoch: 8 step: 316, loss is 0.10343962162733078\n",
      "epoch: 8 step: 317, loss is 0.004503950010985136\n",
      "epoch: 8 step: 318, loss is 0.00044893851736560464\n",
      "epoch: 8 step: 319, loss is 0.001426620758138597\n",
      "epoch: 8 step: 320, loss is 0.052794285118579865\n",
      "epoch: 8 step: 321, loss is 0.005779115483164787\n",
      "epoch: 8 step: 322, loss is 0.10078560560941696\n",
      "epoch: 8 step: 323, loss is 0.0002328350965399295\n",
      "epoch: 8 step: 324, loss is 0.084446020424366\n",
      "epoch: 8 step: 325, loss is 0.07313338667154312\n",
      "epoch: 8 step: 326, loss is 0.0032092020846903324\n",
      "epoch: 8 step: 327, loss is 0.0003156863385811448\n",
      "epoch: 8 step: 328, loss is 0.005040675401687622\n",
      "epoch: 8 step: 329, loss is 0.001174151198938489\n",
      "epoch: 8 step: 330, loss is 0.0021921745501458645\n",
      "epoch: 8 step: 331, loss is 0.00022836972493678331\n",
      "epoch: 8 step: 332, loss is 0.000844804453663528\n",
      "epoch: 8 step: 333, loss is 0.003984504844993353\n",
      "epoch: 8 step: 334, loss is 0.04650571942329407\n",
      "epoch: 8 step: 335, loss is 0.00605152128264308\n",
      "epoch: 8 step: 336, loss is 0.0005689269746653736\n",
      "epoch: 8 step: 337, loss is 0.0012597369495779276\n",
      "epoch: 8 step: 338, loss is 0.014422438107430935\n",
      "epoch: 8 step: 339, loss is 0.0012615327723324299\n",
      "epoch: 8 step: 340, loss is 0.04473156854510307\n",
      "epoch: 8 step: 341, loss is 0.00028635445050895214\n",
      "epoch: 8 step: 342, loss is 0.008964475244283676\n",
      "epoch: 8 step: 343, loss is 0.010777091607451439\n",
      "epoch: 8 step: 344, loss is 0.013141890987753868\n",
      "epoch: 8 step: 345, loss is 0.0032185937743633986\n",
      "epoch: 8 step: 346, loss is 0.015406124293804169\n",
      "epoch: 8 step: 347, loss is 0.0002875226200558245\n",
      "epoch: 8 step: 348, loss is 0.00997979287058115\n",
      "epoch: 8 step: 349, loss is 0.023136025294661522\n",
      "epoch: 8 step: 350, loss is 0.031158002093434334\n",
      "epoch: 8 step: 351, loss is 0.039160214364528656\n",
      "epoch: 8 step: 352, loss is 0.002122224075719714\n",
      "epoch: 8 step: 353, loss is 0.0011704466305673122\n",
      "epoch: 8 step: 354, loss is 0.001118222251534462\n",
      "epoch: 8 step: 355, loss is 0.010627749375998974\n",
      "epoch: 8 step: 356, loss is 0.006816317792981863\n",
      "epoch: 8 step: 357, loss is 0.13824546337127686\n",
      "epoch: 8 step: 358, loss is 0.0004347699286881834\n",
      "epoch: 8 step: 359, loss is 0.01116130780428648\n",
      "epoch: 8 step: 360, loss is 0.001955219078809023\n",
      "epoch: 8 step: 361, loss is 0.0005492757773026824\n",
      "epoch: 8 step: 362, loss is 0.04824737459421158\n",
      "epoch: 8 step: 363, loss is 0.0022748562041670084\n",
      "epoch: 8 step: 364, loss is 0.029469352215528488\n",
      "epoch: 8 step: 365, loss is 0.021049125120043755\n",
      "epoch: 8 step: 366, loss is 0.000260994303971529\n",
      "epoch: 8 step: 367, loss is 0.03518346697092056\n",
      "epoch: 8 step: 368, loss is 0.0008138742996379733\n",
      "epoch: 8 step: 369, loss is 0.0030309082940220833\n",
      "epoch: 8 step: 370, loss is 0.00033609673846513033\n",
      "epoch: 8 step: 371, loss is 0.00022838493168819696\n",
      "epoch: 8 step: 372, loss is 0.0008348539122380316\n",
      "epoch: 8 step: 373, loss is 0.009332760237157345\n",
      "epoch: 8 step: 374, loss is 0.00898580253124237\n",
      "epoch: 8 step: 375, loss is 0.0021058586426079273\n",
      "epoch: 8 step: 376, loss is 0.0006111361435614526\n",
      "epoch: 8 step: 377, loss is 0.003583508776500821\n",
      "epoch: 8 step: 378, loss is 0.0473671518266201\n",
      "epoch: 8 step: 379, loss is 0.015628108754754066\n",
      "epoch: 8 step: 380, loss is 0.22529830038547516\n",
      "epoch: 8 step: 381, loss is 0.012886703945696354\n",
      "epoch: 8 step: 382, loss is 0.0014402993256226182\n",
      "epoch: 8 step: 383, loss is 0.0016736132092773914\n",
      "epoch: 8 step: 384, loss is 0.024627389386296272\n",
      "epoch: 8 step: 385, loss is 0.057314082980155945\n",
      "epoch: 8 step: 386, loss is 0.0005031803739257157\n",
      "epoch: 8 step: 387, loss is 0.0008348284754902124\n",
      "epoch: 8 step: 388, loss is 0.12266033887863159\n",
      "epoch: 8 step: 389, loss is 0.00020181685977149755\n",
      "epoch: 8 step: 390, loss is 0.011333336122334003\n",
      "epoch: 8 step: 391, loss is 0.0002493910724297166\n",
      "epoch: 8 step: 392, loss is 0.0004982661921530962\n",
      "epoch: 8 step: 393, loss is 0.0010519857751205564\n",
      "epoch: 8 step: 394, loss is 0.028614148497581482\n",
      "epoch: 8 step: 395, loss is 0.00022226286819204688\n",
      "epoch: 8 step: 396, loss is 0.0001725349429761991\n",
      "epoch: 8 step: 397, loss is 0.00524707930162549\n",
      "epoch: 8 step: 398, loss is 0.004897243343293667\n",
      "epoch: 8 step: 399, loss is 0.0011398675851523876\n",
      "epoch: 8 step: 400, loss is 0.0004361176397651434\n",
      "epoch: 8 step: 401, loss is 4.6725581341888756e-05\n",
      "epoch: 8 step: 402, loss is 0.0010771640809252858\n",
      "epoch: 8 step: 403, loss is 0.004224987234920263\n",
      "epoch: 8 step: 404, loss is 0.01181739754974842\n",
      "epoch: 8 step: 405, loss is 0.0006354963988997042\n",
      "epoch: 8 step: 406, loss is 0.000876031699590385\n",
      "epoch: 8 step: 407, loss is 0.0009206446120515466\n",
      "epoch: 8 step: 408, loss is 0.03121442347764969\n",
      "epoch: 8 step: 409, loss is 0.007601067423820496\n",
      "epoch: 8 step: 410, loss is 0.008245382457971573\n",
      "epoch: 8 step: 411, loss is 0.00022602306853514165\n",
      "epoch: 8 step: 412, loss is 0.014923875220119953\n",
      "epoch: 8 step: 413, loss is 0.01940685510635376\n",
      "epoch: 8 step: 414, loss is 0.00202574348077178\n",
      "epoch: 8 step: 415, loss is 0.03783809393644333\n",
      "epoch: 8 step: 416, loss is 0.0006662697996944189\n",
      "epoch: 8 step: 417, loss is 0.00018299494695384055\n",
      "epoch: 8 step: 418, loss is 0.053372204303741455\n",
      "epoch: 8 step: 419, loss is 0.00017784158990252763\n",
      "epoch: 8 step: 420, loss is 0.010678296908736229\n",
      "epoch: 8 step: 421, loss is 0.02322513610124588\n",
      "epoch: 8 step: 422, loss is 1.7006517737172544e-05\n",
      "epoch: 8 step: 423, loss is 0.1796557456254959\n",
      "epoch: 8 step: 424, loss is 0.0011157762492075562\n",
      "epoch: 8 step: 425, loss is 0.007644904311746359\n",
      "epoch: 8 step: 426, loss is 0.00019509968115016818\n",
      "epoch: 8 step: 427, loss is 0.00044761309982277453\n",
      "epoch: 8 step: 428, loss is 0.006590933073312044\n",
      "epoch: 8 step: 429, loss is 0.00116827548481524\n",
      "epoch: 8 step: 430, loss is 0.0026838716585189104\n",
      "epoch: 8 step: 431, loss is 0.012310926802456379\n",
      "epoch: 8 step: 432, loss is 0.011225642636418343\n",
      "epoch: 8 step: 433, loss is 0.0007389289094135165\n",
      "epoch: 8 step: 434, loss is 0.024417227134108543\n",
      "epoch: 8 step: 435, loss is 0.0013865381479263306\n",
      "epoch: 8 step: 436, loss is 0.00018718869250733405\n",
      "epoch: 8 step: 437, loss is 0.0014899240341037512\n",
      "epoch: 8 step: 438, loss is 0.00044480510405264795\n",
      "epoch: 8 step: 439, loss is 0.07677073031663895\n",
      "epoch: 8 step: 440, loss is 0.007359917741268873\n",
      "epoch: 8 step: 441, loss is 0.0343044176697731\n",
      "epoch: 8 step: 442, loss is 0.0011175373801961541\n",
      "epoch: 8 step: 443, loss is 0.001452288357540965\n",
      "epoch: 8 step: 444, loss is 0.0005786410765722394\n",
      "epoch: 8 step: 445, loss is 0.0006418252014555037\n",
      "epoch: 8 step: 446, loss is 0.008816475979983807\n",
      "epoch: 8 step: 447, loss is 0.00023521154071204364\n",
      "epoch: 8 step: 448, loss is 0.0004999527591280639\n",
      "epoch: 8 step: 449, loss is 0.12406966090202332\n",
      "epoch: 8 step: 450, loss is 0.003132747719064355\n",
      "epoch: 8 step: 451, loss is 0.00020322449563536793\n",
      "epoch: 8 step: 452, loss is 0.0022776674013584852\n",
      "epoch: 8 step: 453, loss is 7.623045530635864e-05\n",
      "epoch: 8 step: 454, loss is 0.014112342149019241\n",
      "epoch: 8 step: 455, loss is 0.11042783409357071\n",
      "epoch: 8 step: 456, loss is 0.000770891027059406\n",
      "epoch: 8 step: 457, loss is 0.001521961996331811\n",
      "epoch: 8 step: 458, loss is 0.014307845383882523\n",
      "epoch: 8 step: 459, loss is 0.0003550232504494488\n",
      "epoch: 8 step: 460, loss is 0.002336958423256874\n",
      "epoch: 8 step: 461, loss is 0.016461152583360672\n",
      "epoch: 8 step: 462, loss is 0.0036244462244212627\n",
      "epoch: 8 step: 463, loss is 0.000307671376504004\n",
      "epoch: 8 step: 464, loss is 0.007541615515947342\n",
      "epoch: 8 step: 465, loss is 0.00037922224146313965\n",
      "epoch: 8 step: 466, loss is 0.001918755704537034\n",
      "epoch: 8 step: 467, loss is 0.00997592881321907\n",
      "epoch: 8 step: 468, loss is 0.06896649301052094\n",
      "epoch: 8 step: 469, loss is 0.009509852156043053\n",
      "epoch: 8 step: 470, loss is 0.0012739336816594005\n",
      "epoch: 8 step: 471, loss is 0.09497006982564926\n",
      "epoch: 8 step: 472, loss is 0.0016810972010716796\n",
      "epoch: 8 step: 473, loss is 0.00029324364732019603\n",
      "epoch: 8 step: 474, loss is 0.0022071460261940956\n",
      "epoch: 8 step: 475, loss is 0.002786122728139162\n",
      "epoch: 8 step: 476, loss is 0.006569855846464634\n",
      "epoch: 8 step: 477, loss is 0.003033078508451581\n",
      "epoch: 8 step: 478, loss is 0.00018088528304360807\n",
      "epoch: 8 step: 479, loss is 0.016008319333195686\n",
      "epoch: 8 step: 480, loss is 0.005183327477425337\n",
      "epoch: 8 step: 481, loss is 0.0005698288441635668\n",
      "epoch: 8 step: 482, loss is 0.007688199169933796\n",
      "epoch: 8 step: 483, loss is 0.00034468347439542413\n",
      "epoch: 8 step: 484, loss is 0.003853886155411601\n",
      "epoch: 8 step: 485, loss is 0.00648552505299449\n",
      "epoch: 8 step: 486, loss is 0.0007443840149790049\n",
      "epoch: 8 step: 487, loss is 0.003049774095416069\n",
      "epoch: 8 step: 488, loss is 0.00039483566069975495\n",
      "epoch: 8 step: 489, loss is 0.00028968669357709587\n",
      "epoch: 8 step: 490, loss is 0.000859590305481106\n",
      "epoch: 8 step: 491, loss is 0.0001296861155424267\n",
      "epoch: 8 step: 492, loss is 0.0045137060806155205\n",
      "epoch: 8 step: 493, loss is 0.009272427298128605\n",
      "epoch: 8 step: 494, loss is 0.005391301121562719\n",
      "epoch: 8 step: 495, loss is 0.005539970938116312\n",
      "epoch: 8 step: 496, loss is 0.002526639262214303\n",
      "epoch: 8 step: 497, loss is 0.0008379193604923785\n",
      "epoch: 8 step: 498, loss is 0.0006303017144091427\n",
      "epoch: 8 step: 499, loss is 0.0002656868891790509\n",
      "epoch: 8 step: 500, loss is 0.007159380707889795\n",
      "epoch: 8 step: 501, loss is 0.06591000407934189\n",
      "epoch: 8 step: 502, loss is 0.017729438841342926\n",
      "epoch: 8 step: 503, loss is 0.06306958198547363\n",
      "epoch: 8 step: 504, loss is 0.0038363200146704912\n",
      "epoch: 8 step: 505, loss is 0.0008682497427798808\n",
      "epoch: 8 step: 506, loss is 0.0003715065831784159\n",
      "epoch: 8 step: 507, loss is 0.02239948697388172\n",
      "epoch: 8 step: 508, loss is 0.027057960629463196\n",
      "epoch: 8 step: 509, loss is 0.0007394269923679531\n",
      "epoch: 8 step: 510, loss is 0.008365745656192303\n",
      "epoch: 8 step: 511, loss is 0.032010819762945175\n",
      "epoch: 8 step: 512, loss is 0.0005491110496222973\n",
      "epoch: 8 step: 513, loss is 0.007151826284825802\n",
      "epoch: 8 step: 514, loss is 0.0010858983732759953\n",
      "epoch: 8 step: 515, loss is 0.0006351264892145991\n",
      "epoch: 8 step: 516, loss is 4.464819357963279e-05\n",
      "epoch: 8 step: 517, loss is 0.014355185441672802\n",
      "epoch: 8 step: 518, loss is 2.442845106997993e-05\n",
      "epoch: 8 step: 519, loss is 0.0007570518064312637\n",
      "epoch: 8 step: 520, loss is 0.0036234252620488405\n",
      "epoch: 8 step: 521, loss is 0.0011187175987288356\n",
      "epoch: 8 step: 522, loss is 0.006637744605541229\n",
      "epoch: 8 step: 523, loss is 0.0007623917772434652\n",
      "epoch: 8 step: 524, loss is 0.00012389021867420524\n",
      "epoch: 8 step: 525, loss is 0.0008839327492751181\n",
      "epoch: 8 step: 526, loss is 0.005247653927654028\n",
      "epoch: 8 step: 527, loss is 4.606136280926876e-05\n",
      "epoch: 8 step: 528, loss is 0.011936288326978683\n",
      "epoch: 8 step: 529, loss is 0.15537919104099274\n",
      "epoch: 8 step: 530, loss is 0.0007582950056530535\n",
      "epoch: 8 step: 531, loss is 0.07594458758831024\n",
      "epoch: 8 step: 532, loss is 0.005526977591216564\n",
      "epoch: 8 step: 533, loss is 0.0005567413754761219\n",
      "epoch: 8 step: 534, loss is 0.0007722211885266006\n",
      "epoch: 8 step: 535, loss is 0.007644196506589651\n",
      "epoch: 8 step: 536, loss is 0.002058539306744933\n",
      "epoch: 8 step: 537, loss is 0.01073817815631628\n",
      "epoch: 8 step: 538, loss is 0.10079456120729446\n",
      "epoch: 8 step: 539, loss is 0.000558186904527247\n",
      "epoch: 8 step: 540, loss is 9.334000060334802e-05\n",
      "epoch: 8 step: 541, loss is 0.000730311032384634\n",
      "epoch: 8 step: 542, loss is 0.0007856861338950694\n",
      "epoch: 8 step: 543, loss is 0.00045332720037549734\n",
      "epoch: 8 step: 544, loss is 0.0025912486016750336\n",
      "epoch: 8 step: 545, loss is 0.0001320986484643072\n",
      "epoch: 8 step: 546, loss is 0.000903898268006742\n",
      "epoch: 8 step: 547, loss is 0.00019980917568318546\n",
      "epoch: 8 step: 548, loss is 0.004015902988612652\n",
      "epoch: 8 step: 549, loss is 0.000255542661761865\n",
      "epoch: 8 step: 550, loss is 4.254286614013836e-05\n",
      "epoch: 8 step: 551, loss is 9.49082532315515e-05\n",
      "epoch: 8 step: 552, loss is 0.0010199124226346612\n",
      "epoch: 8 step: 553, loss is 0.050805896520614624\n",
      "epoch: 8 step: 554, loss is 0.06343017518520355\n",
      "epoch: 8 step: 555, loss is 0.004970037378370762\n",
      "epoch: 8 step: 556, loss is 0.004037558566778898\n",
      "epoch: 8 step: 557, loss is 0.0308424960821867\n",
      "epoch: 8 step: 558, loss is 0.010192913934588432\n",
      "epoch: 8 step: 559, loss is 0.04412580281496048\n",
      "epoch: 8 step: 560, loss is 0.00021106234635226429\n",
      "epoch: 8 step: 561, loss is 0.05007149651646614\n",
      "epoch: 8 step: 562, loss is 0.00695436168462038\n",
      "epoch: 8 step: 563, loss is 0.0009783681016415358\n",
      "epoch: 8 step: 564, loss is 0.010232369415462017\n",
      "epoch: 8 step: 565, loss is 0.002390109933912754\n",
      "epoch: 8 step: 566, loss is 0.02436339296400547\n",
      "epoch: 8 step: 567, loss is 0.011920412071049213\n",
      "epoch: 8 step: 568, loss is 0.0020444714464247227\n",
      "epoch: 8 step: 569, loss is 0.000285726273432374\n",
      "epoch: 8 step: 570, loss is 0.0009366617305204272\n",
      "epoch: 8 step: 571, loss is 0.00296869408339262\n",
      "epoch: 8 step: 572, loss is 0.004254562314599752\n",
      "epoch: 8 step: 573, loss is 0.008602594025433064\n",
      "epoch: 8 step: 574, loss is 0.010094559751451015\n",
      "epoch: 8 step: 575, loss is 0.012241341173648834\n",
      "epoch: 8 step: 576, loss is 0.06026960164308548\n",
      "epoch: 8 step: 577, loss is 0.00048548512859269977\n",
      "epoch: 8 step: 578, loss is 0.01641508936882019\n",
      "epoch: 8 step: 579, loss is 9.107305231736973e-05\n",
      "epoch: 8 step: 580, loss is 0.05000969022512436\n",
      "epoch: 8 step: 581, loss is 0.014279406517744064\n",
      "epoch: 8 step: 582, loss is 0.012986474670469761\n",
      "epoch: 8 step: 583, loss is 0.02485593594610691\n",
      "epoch: 8 step: 584, loss is 0.0019466368248686194\n",
      "epoch: 8 step: 585, loss is 0.024955429136753082\n",
      "epoch: 8 step: 586, loss is 0.031155021861195564\n",
      "epoch: 8 step: 587, loss is 0.007636022288352251\n",
      "epoch: 8 step: 588, loss is 0.006071942858397961\n",
      "epoch: 8 step: 589, loss is 0.00011902957339771092\n",
      "epoch: 8 step: 590, loss is 0.07297709584236145\n",
      "epoch: 8 step: 591, loss is 0.012805529870092869\n",
      "epoch: 8 step: 592, loss is 0.08133771270513535\n",
      "epoch: 8 step: 593, loss is 0.002587943570688367\n",
      "epoch: 8 step: 594, loss is 0.00019567704293876886\n",
      "epoch: 8 step: 595, loss is 0.0001749688235577196\n",
      "epoch: 8 step: 596, loss is 0.0007090293802320957\n",
      "epoch: 8 step: 597, loss is 0.0025417092256247997\n",
      "epoch: 8 step: 598, loss is 0.01411349605768919\n",
      "epoch: 8 step: 599, loss is 0.014336173422634602\n",
      "epoch: 8 step: 600, loss is 0.0013631118927150965\n",
      "epoch: 8 step: 601, loss is 0.0025028069503605366\n",
      "epoch: 8 step: 602, loss is 0.0007974328473210335\n",
      "epoch: 8 step: 603, loss is 0.0056079295463860035\n",
      "epoch: 8 step: 604, loss is 0.0061364322900772095\n",
      "epoch: 8 step: 605, loss is 0.0002761955838650465\n",
      "epoch: 8 step: 606, loss is 0.0034382513258606195\n",
      "epoch: 8 step: 607, loss is 2.8536191166494973e-05\n",
      "epoch: 8 step: 608, loss is 2.0612078515114263e-05\n",
      "epoch: 8 step: 609, loss is 0.04081886634230614\n",
      "epoch: 8 step: 610, loss is 6.96176866767928e-05\n",
      "epoch: 8 step: 611, loss is 0.0036752563901245594\n",
      "epoch: 8 step: 612, loss is 0.0635485053062439\n",
      "epoch: 8 step: 613, loss is 0.07581647485494614\n",
      "epoch: 8 step: 614, loss is 0.05532589182257652\n",
      "epoch: 8 step: 615, loss is 0.0018973269034177065\n",
      "epoch: 8 step: 616, loss is 4.084674583282322e-05\n",
      "epoch: 8 step: 617, loss is 0.0795002430677414\n",
      "epoch: 8 step: 618, loss is 0.0003926873905584216\n",
      "epoch: 8 step: 619, loss is 0.0032344036735594273\n",
      "epoch: 8 step: 620, loss is 0.00015612847346346825\n",
      "epoch: 8 step: 621, loss is 0.002112140180543065\n",
      "epoch: 8 step: 622, loss is 0.0021174922585487366\n",
      "epoch: 8 step: 623, loss is 0.012149550020694733\n",
      "epoch: 8 step: 624, loss is 0.004088718444108963\n",
      "epoch: 8 step: 625, loss is 0.00018781465769279748\n",
      "epoch: 8 step: 626, loss is 2.860291897377465e-05\n",
      "epoch: 8 step: 627, loss is 2.9955266654724255e-05\n",
      "epoch: 8 step: 628, loss is 5.063950811745599e-05\n",
      "epoch: 8 step: 629, loss is 8.224467455875129e-05\n",
      "epoch: 8 step: 630, loss is 0.00013579920050688088\n",
      "epoch: 8 step: 631, loss is 0.00499081052839756\n",
      "epoch: 8 step: 632, loss is 0.0002374914038227871\n",
      "epoch: 8 step: 633, loss is 3.995971565018408e-05\n",
      "epoch: 8 step: 634, loss is 0.03824138641357422\n",
      "epoch: 8 step: 635, loss is 0.0024927055928856134\n",
      "epoch: 8 step: 636, loss is 0.0280479546636343\n",
      "epoch: 8 step: 637, loss is 0.000750526029150933\n",
      "epoch: 8 step: 638, loss is 0.001225099666044116\n",
      "epoch: 8 step: 639, loss is 9.89253749139607e-05\n",
      "epoch: 8 step: 640, loss is 0.0003386487951502204\n",
      "epoch: 8 step: 641, loss is 1.0541803021624219e-05\n",
      "epoch: 8 step: 642, loss is 4.4565349526237696e-05\n",
      "epoch: 8 step: 643, loss is 0.0028450703248381615\n",
      "epoch: 8 step: 644, loss is 0.00020327842503320426\n",
      "epoch: 8 step: 645, loss is 0.004951216280460358\n",
      "epoch: 8 step: 646, loss is 0.017892934381961823\n",
      "epoch: 8 step: 647, loss is 0.0011450249003246427\n",
      "epoch: 8 step: 648, loss is 3.796005330514163e-05\n",
      "epoch: 8 step: 649, loss is 0.0002382225648034364\n",
      "epoch: 8 step: 650, loss is 0.010225827805697918\n",
      "epoch: 8 step: 651, loss is 0.000386991974664852\n",
      "epoch: 8 step: 652, loss is 0.0005439943051896989\n",
      "epoch: 8 step: 653, loss is 4.6582328650401905e-05\n",
      "epoch: 8 step: 654, loss is 0.0037827217020094395\n",
      "epoch: 8 step: 655, loss is 0.02808820828795433\n",
      "epoch: 8 step: 656, loss is 0.0015908963978290558\n",
      "epoch: 8 step: 657, loss is 0.0002612710522953421\n",
      "epoch: 8 step: 658, loss is 0.0001631901686778292\n",
      "epoch: 8 step: 659, loss is 6.363069587678183e-06\n",
      "epoch: 8 step: 660, loss is 0.04267179220914841\n",
      "epoch: 8 step: 661, loss is 0.03845195472240448\n",
      "epoch: 8 step: 662, loss is 0.00011021604586858302\n",
      "epoch: 8 step: 663, loss is 0.00037594273453578353\n",
      "epoch: 8 step: 664, loss is 0.0012613715371116996\n",
      "epoch: 8 step: 665, loss is 0.00019692940986715257\n",
      "epoch: 8 step: 666, loss is 0.012290767394006252\n",
      "epoch: 8 step: 667, loss is 0.0023714685812592506\n",
      "epoch: 8 step: 668, loss is 4.2678668251028284e-05\n",
      "epoch: 8 step: 669, loss is 0.0013910374836996198\n",
      "epoch: 8 step: 670, loss is 0.004078962374478579\n",
      "epoch: 8 step: 671, loss is 0.0011222894536331296\n",
      "epoch: 8 step: 672, loss is 0.00033108150819316506\n",
      "epoch: 8 step: 673, loss is 1.2877919289167039e-05\n",
      "epoch: 8 step: 674, loss is 0.0003013674868270755\n",
      "epoch: 8 step: 675, loss is 0.006762998644262552\n",
      "epoch: 8 step: 676, loss is 0.00031424782355315983\n",
      "epoch: 8 step: 677, loss is 0.002536191139370203\n",
      "epoch: 8 step: 678, loss is 2.5700850528664887e-05\n",
      "epoch: 8 step: 679, loss is 0.03214612975716591\n",
      "epoch: 8 step: 680, loss is 0.002086039399728179\n",
      "epoch: 8 step: 681, loss is 0.0020451101008802652\n",
      "epoch: 8 step: 682, loss is 0.00022197696671355516\n",
      "epoch: 8 step: 683, loss is 0.023649901151657104\n",
      "epoch: 8 step: 684, loss is 0.0013145118718966842\n",
      "epoch: 8 step: 685, loss is 0.0001936489570653066\n",
      "epoch: 8 step: 686, loss is 0.0008811490260995924\n",
      "epoch: 8 step: 687, loss is 0.0003561988705769181\n",
      "epoch: 8 step: 688, loss is 0.00012468008208088577\n",
      "epoch: 8 step: 689, loss is 0.00022961260401643813\n",
      "epoch: 8 step: 690, loss is 0.0001468880072934553\n",
      "epoch: 8 step: 691, loss is 0.021792380139231682\n",
      "epoch: 8 step: 692, loss is 4.21515796915628e-05\n",
      "epoch: 8 step: 693, loss is 0.0011149862548336387\n",
      "epoch: 8 step: 694, loss is 0.0006370969349518418\n",
      "epoch: 8 step: 695, loss is 0.003491288749501109\n",
      "epoch: 8 step: 696, loss is 0.00016620373935438693\n",
      "epoch: 8 step: 697, loss is 0.01041501946747303\n",
      "epoch: 8 step: 698, loss is 0.0032515444327145815\n",
      "epoch: 8 step: 699, loss is 0.16746175289154053\n",
      "epoch: 8 step: 700, loss is 4.092969174962491e-05\n",
      "epoch: 8 step: 701, loss is 0.0013008163077756763\n",
      "epoch: 8 step: 702, loss is 0.01850469782948494\n",
      "epoch: 8 step: 703, loss is 0.006874605547636747\n",
      "epoch: 8 step: 704, loss is 0.0002011308679357171\n",
      "epoch: 8 step: 705, loss is 3.740749161806889e-05\n",
      "epoch: 8 step: 706, loss is 0.00027270507416687906\n",
      "epoch: 8 step: 707, loss is 0.0014331127749755979\n",
      "epoch: 8 step: 708, loss is 0.0009589886758476496\n",
      "epoch: 8 step: 709, loss is 3.20672697853297e-05\n",
      "epoch: 8 step: 710, loss is 0.0012525842757895589\n",
      "epoch: 8 step: 711, loss is 0.00024068915809039026\n",
      "epoch: 8 step: 712, loss is 0.09848766028881073\n",
      "epoch: 8 step: 713, loss is 6.524795026052743e-05\n",
      "epoch: 8 step: 714, loss is 0.22261635959148407\n",
      "epoch: 8 step: 715, loss is 0.0008758689509704709\n",
      "epoch: 8 step: 716, loss is 0.005302081350237131\n",
      "epoch: 8 step: 717, loss is 0.0024329363368451595\n",
      "epoch: 8 step: 718, loss is 0.0006316137732937932\n",
      "epoch: 8 step: 719, loss is 0.002064360538497567\n",
      "epoch: 8 step: 720, loss is 0.002999903168529272\n",
      "epoch: 8 step: 721, loss is 0.0008871397585608065\n",
      "epoch: 8 step: 722, loss is 0.0002309079864062369\n",
      "epoch: 8 step: 723, loss is 0.001731742057017982\n",
      "epoch: 8 step: 724, loss is 0.010911226272583008\n",
      "epoch: 8 step: 725, loss is 0.00041110883466899395\n",
      "epoch: 8 step: 726, loss is 0.00021680952340830117\n",
      "epoch: 8 step: 727, loss is 0.03357589244842529\n",
      "epoch: 8 step: 728, loss is 0.0017334714066237211\n",
      "epoch: 8 step: 729, loss is 0.005349450744688511\n",
      "epoch: 8 step: 730, loss is 0.0006947763031348586\n",
      "epoch: 8 step: 731, loss is 0.000832324440125376\n",
      "epoch: 8 step: 732, loss is 0.0427638404071331\n",
      "epoch: 8 step: 733, loss is 0.0007782134925946593\n",
      "epoch: 8 step: 734, loss is 0.00017196487169712782\n",
      "epoch: 8 step: 735, loss is 0.001523692044429481\n",
      "epoch: 8 step: 736, loss is 0.001299639930948615\n",
      "epoch: 8 step: 737, loss is 0.0016091529978439212\n",
      "epoch: 8 step: 738, loss is 0.012747466564178467\n",
      "epoch: 8 step: 739, loss is 0.0006377766840159893\n",
      "epoch: 8 step: 740, loss is 0.02858549915254116\n",
      "epoch: 8 step: 741, loss is 0.002274218248203397\n",
      "epoch: 8 step: 742, loss is 0.001528072403743863\n",
      "epoch: 8 step: 743, loss is 0.00046626670518890023\n",
      "epoch: 8 step: 744, loss is 1.3808669791615102e-05\n",
      "epoch: 8 step: 745, loss is 8.055011130636558e-05\n",
      "epoch: 8 step: 746, loss is 0.0001361742033623159\n",
      "epoch: 8 step: 747, loss is 0.004517956171184778\n",
      "epoch: 8 step: 748, loss is 0.001998010789975524\n",
      "epoch: 8 step: 749, loss is 0.0001936821499839425\n",
      "epoch: 8 step: 750, loss is 0.0006144631770439446\n",
      "epoch: 8 step: 751, loss is 0.0049230437725782394\n",
      "epoch: 8 step: 752, loss is 0.0010093596065416932\n",
      "epoch: 8 step: 753, loss is 0.002213096246123314\n",
      "epoch: 8 step: 754, loss is 8.693403651705012e-05\n",
      "epoch: 8 step: 755, loss is 0.006868960801512003\n",
      "epoch: 8 step: 756, loss is 0.0002457421796862036\n",
      "epoch: 8 step: 757, loss is 0.03365817666053772\n",
      "epoch: 8 step: 758, loss is 0.007785289082676172\n",
      "epoch: 8 step: 759, loss is 0.0003476259589660913\n",
      "epoch: 8 step: 760, loss is 0.0001784167398000136\n",
      "epoch: 8 step: 761, loss is 0.0022775910329073668\n",
      "epoch: 8 step: 762, loss is 0.00013603377738036215\n",
      "epoch: 8 step: 763, loss is 1.0144286534341518e-05\n",
      "epoch: 8 step: 764, loss is 6.230841245269403e-05\n",
      "epoch: 8 step: 765, loss is 0.0006937042344361544\n",
      "epoch: 8 step: 766, loss is 0.0008437978103756905\n",
      "epoch: 8 step: 767, loss is 0.0008590124198235571\n",
      "epoch: 8 step: 768, loss is 3.984928844147362e-05\n",
      "epoch: 8 step: 769, loss is 0.0006076016579754651\n",
      "epoch: 8 step: 770, loss is 0.0004980918020009995\n",
      "epoch: 8 step: 771, loss is 1.8617860405356623e-05\n",
      "epoch: 8 step: 772, loss is 0.02369561418890953\n",
      "epoch: 8 step: 773, loss is 3.259346340200864e-05\n",
      "epoch: 8 step: 774, loss is 0.0005814841133542359\n",
      "epoch: 8 step: 775, loss is 0.00010515096073504537\n",
      "epoch: 8 step: 776, loss is 0.015472257509827614\n",
      "epoch: 8 step: 777, loss is 0.006363095715641975\n",
      "epoch: 8 step: 778, loss is 0.00023540871916338801\n",
      "epoch: 8 step: 779, loss is 0.0009397996473126113\n",
      "epoch: 8 step: 780, loss is 0.005314396228641272\n",
      "epoch: 8 step: 781, loss is 1.4387955161510035e-05\n",
      "epoch: 8 step: 782, loss is 0.003354941960424185\n",
      "epoch: 8 step: 783, loss is 0.0012548139784485102\n",
      "epoch: 8 step: 784, loss is 0.006376217119395733\n",
      "epoch: 8 step: 785, loss is 0.003030323889106512\n",
      "epoch: 8 step: 786, loss is 0.004229391925036907\n",
      "epoch: 8 step: 787, loss is 0.0001528186840005219\n",
      "epoch: 8 step: 788, loss is 0.013877041637897491\n",
      "epoch: 8 step: 789, loss is 0.0006397037068381906\n",
      "epoch: 8 step: 790, loss is 0.00034346917527727783\n",
      "epoch: 8 step: 791, loss is 0.0033946328330785036\n",
      "epoch: 8 step: 792, loss is 0.0015802434645593166\n",
      "epoch: 8 step: 793, loss is 0.007528435438871384\n",
      "epoch: 8 step: 794, loss is 0.005672363098710775\n",
      "epoch: 8 step: 795, loss is 5.701405825675465e-05\n",
      "epoch: 8 step: 796, loss is 0.0051228804513812065\n",
      "epoch: 8 step: 797, loss is 0.00010510637366678566\n",
      "epoch: 8 step: 798, loss is 3.580094471544726e-06\n",
      "epoch: 8 step: 799, loss is 0.0001265698520001024\n",
      "epoch: 8 step: 800, loss is 0.00017040174861904234\n",
      "epoch: 8 step: 801, loss is 0.015560382045805454\n",
      "epoch: 8 step: 802, loss is 0.00010301135625923052\n",
      "epoch: 8 step: 803, loss is 3.341332921991125e-05\n",
      "epoch: 8 step: 804, loss is 0.00015450856881216168\n",
      "epoch: 8 step: 805, loss is 2.0682711692643352e-05\n",
      "epoch: 8 step: 806, loss is 1.4385282156581525e-05\n",
      "epoch: 8 step: 807, loss is 0.00265085743740201\n",
      "epoch: 8 step: 808, loss is 2.238777778984513e-05\n",
      "epoch: 8 step: 809, loss is 5.3635547374142334e-05\n",
      "epoch: 8 step: 810, loss is 0.00020234579278621823\n",
      "epoch: 8 step: 811, loss is 0.0008360025240108371\n",
      "epoch: 8 step: 812, loss is 0.0024152109399437904\n",
      "epoch: 8 step: 813, loss is 0.0011823569657281041\n",
      "epoch: 8 step: 814, loss is 0.0031439720187336206\n",
      "epoch: 8 step: 815, loss is 0.028972353786230087\n",
      "epoch: 8 step: 816, loss is 3.822191501967609e-06\n",
      "epoch: 8 step: 817, loss is 4.026330861961469e-05\n",
      "epoch: 8 step: 818, loss is 0.0024879721459001303\n",
      "epoch: 8 step: 819, loss is 0.003059177193790674\n",
      "epoch: 8 step: 820, loss is 0.00013209904136601835\n",
      "epoch: 8 step: 821, loss is 0.00020885020785499364\n",
      "epoch: 8 step: 822, loss is 8.075659570749849e-05\n",
      "epoch: 8 step: 823, loss is 0.007247098721563816\n",
      "epoch: 8 step: 824, loss is 0.0004226831952109933\n",
      "epoch: 8 step: 825, loss is 0.06785186380147934\n",
      "epoch: 8 step: 826, loss is 0.02691376581788063\n",
      "epoch: 8 step: 827, loss is 0.17996945977210999\n",
      "epoch: 8 step: 828, loss is 0.003524760715663433\n",
      "epoch: 8 step: 829, loss is 0.0013489284319803119\n",
      "epoch: 8 step: 830, loss is 0.00041911471635103226\n",
      "epoch: 8 step: 831, loss is 0.002112759044393897\n",
      "epoch: 8 step: 832, loss is 6.485389167210087e-05\n",
      "epoch: 8 step: 833, loss is 0.0020436865743249655\n",
      "epoch: 8 step: 834, loss is 0.18019673228263855\n",
      "epoch: 8 step: 835, loss is 0.00011492546036606655\n",
      "epoch: 8 step: 836, loss is 5.206714558880776e-05\n",
      "epoch: 8 step: 837, loss is 1.4814590031164698e-05\n",
      "epoch: 8 step: 838, loss is 0.0034558805637061596\n",
      "epoch: 8 step: 839, loss is 0.004458143375813961\n",
      "epoch: 8 step: 840, loss is 0.0029690854717046022\n",
      "epoch: 8 step: 841, loss is 0.00010629527969285846\n",
      "epoch: 8 step: 842, loss is 0.004529071040451527\n",
      "epoch: 8 step: 843, loss is 0.005806128028780222\n",
      "epoch: 8 step: 844, loss is 0.00028399063739925623\n",
      "epoch: 8 step: 845, loss is 0.006548651028424501\n",
      "epoch: 8 step: 846, loss is 1.9177246940671466e-05\n",
      "epoch: 8 step: 847, loss is 0.0027141193859279156\n",
      "epoch: 8 step: 848, loss is 0.0020058767404407263\n",
      "epoch: 8 step: 849, loss is 6.037674756953493e-05\n",
      "epoch: 8 step: 850, loss is 0.014536810107529163\n",
      "epoch: 8 step: 851, loss is 0.00017006215057335794\n",
      "epoch: 8 step: 852, loss is 0.0003300371172372252\n",
      "epoch: 8 step: 853, loss is 0.017549052834510803\n",
      "epoch: 8 step: 854, loss is 1.489693568146322e-05\n",
      "epoch: 8 step: 855, loss is 0.00043362233554944396\n",
      "epoch: 8 step: 856, loss is 2.904897883126978e-05\n",
      "epoch: 8 step: 857, loss is 0.0045905569568276405\n",
      "epoch: 8 step: 858, loss is 0.000824265880510211\n",
      "epoch: 8 step: 859, loss is 0.0043816836550831795\n",
      "epoch: 8 step: 860, loss is 0.008308825083076954\n",
      "epoch: 8 step: 861, loss is 0.0012138880556449294\n",
      "epoch: 8 step: 862, loss is 0.007390948943793774\n",
      "epoch: 8 step: 863, loss is 0.04339556396007538\n",
      "epoch: 8 step: 864, loss is 0.00016107180272229016\n",
      "epoch: 8 step: 865, loss is 0.01328847836703062\n",
      "epoch: 8 step: 866, loss is 0.0006599324988201261\n",
      "epoch: 8 step: 867, loss is 0.046199262142181396\n",
      "epoch: 8 step: 868, loss is 1.440841515432112e-05\n",
      "epoch: 8 step: 869, loss is 0.2040003389120102\n",
      "epoch: 8 step: 870, loss is 0.0001298676652368158\n",
      "epoch: 8 step: 871, loss is 0.0006284630508162081\n",
      "epoch: 8 step: 872, loss is 0.045049332082271576\n",
      "epoch: 8 step: 873, loss is 0.10233695060014725\n",
      "epoch: 8 step: 874, loss is 0.00043656426714733243\n",
      "epoch: 8 step: 875, loss is 0.00019096140749752522\n",
      "epoch: 8 step: 876, loss is 7.097108209563885e-06\n",
      "epoch: 8 step: 877, loss is 3.2647192711010575e-05\n",
      "epoch: 8 step: 878, loss is 0.001197297591716051\n",
      "epoch: 8 step: 879, loss is 0.0001944048417499289\n",
      "epoch: 8 step: 880, loss is 0.048583757132291794\n",
      "epoch: 8 step: 881, loss is 0.001326545374467969\n",
      "epoch: 8 step: 882, loss is 0.0027946913614869118\n",
      "epoch: 8 step: 883, loss is 0.011810669675469398\n",
      "epoch: 8 step: 884, loss is 0.0005102977738715708\n",
      "epoch: 8 step: 885, loss is 0.0004353010153863579\n",
      "epoch: 8 step: 886, loss is 0.10857348889112473\n",
      "epoch: 8 step: 887, loss is 0.0008728921529836953\n",
      "epoch: 8 step: 888, loss is 0.003955350257456303\n",
      "epoch: 8 step: 889, loss is 0.004330526106059551\n",
      "epoch: 8 step: 890, loss is 0.002117728115990758\n",
      "epoch: 8 step: 891, loss is 0.02265489660203457\n",
      "epoch: 8 step: 892, loss is 2.59727930824738e-05\n",
      "epoch: 8 step: 893, loss is 0.017625411972403526\n",
      "epoch: 8 step: 894, loss is 0.0009971914114430547\n",
      "epoch: 8 step: 895, loss is 0.00021763346740044653\n",
      "epoch: 8 step: 896, loss is 0.0005031178006902337\n",
      "epoch: 8 step: 897, loss is 0.01931888796389103\n",
      "epoch: 8 step: 898, loss is 2.352311639697291e-05\n",
      "epoch: 8 step: 899, loss is 0.17178860306739807\n",
      "epoch: 8 step: 900, loss is 0.09427071362733841\n",
      "epoch: 8 step: 901, loss is 0.02284873276948929\n",
      "epoch: 8 step: 902, loss is 0.0016019503818824887\n",
      "epoch: 8 step: 903, loss is 0.0014604771276935935\n",
      "epoch: 8 step: 904, loss is 0.00035336779546923935\n",
      "epoch: 8 step: 905, loss is 0.00019110215362161398\n",
      "epoch: 8 step: 906, loss is 0.0028884452767670155\n",
      "epoch: 8 step: 907, loss is 0.000137723793159239\n",
      "epoch: 8 step: 908, loss is 0.003766977693885565\n",
      "epoch: 8 step: 909, loss is 0.04600553214550018\n",
      "epoch: 8 step: 910, loss is 0.00116340524982661\n",
      "epoch: 8 step: 911, loss is 0.0011593515519052744\n",
      "epoch: 8 step: 912, loss is 0.0006755879148840904\n",
      "epoch: 8 step: 913, loss is 0.10331513732671738\n",
      "epoch: 8 step: 914, loss is 0.00042579389992170036\n",
      "epoch: 8 step: 915, loss is 0.15222160518169403\n",
      "epoch: 8 step: 916, loss is 0.0003412911610212177\n",
      "epoch: 8 step: 917, loss is 0.0004932839656248689\n",
      "epoch: 8 step: 918, loss is 0.0016189226880669594\n",
      "epoch: 8 step: 919, loss is 0.010647375136613846\n",
      "epoch: 8 step: 920, loss is 0.004157217685133219\n",
      "epoch: 8 step: 921, loss is 0.0007051982684060931\n",
      "epoch: 8 step: 922, loss is 0.01784767210483551\n",
      "epoch: 8 step: 923, loss is 0.0006616314640268683\n",
      "epoch: 8 step: 924, loss is 0.0015113481786102057\n",
      "epoch: 8 step: 925, loss is 0.0023997491225600243\n",
      "epoch: 8 step: 926, loss is 0.020801067352294922\n",
      "epoch: 8 step: 927, loss is 0.0030080853030085564\n",
      "epoch: 8 step: 928, loss is 0.00022457593877334148\n",
      "epoch: 8 step: 929, loss is 0.007847186177968979\n",
      "epoch: 8 step: 930, loss is 0.007095906417816877\n",
      "epoch: 8 step: 931, loss is 0.005298208445310593\n",
      "epoch: 8 step: 932, loss is 0.012049759738147259\n",
      "epoch: 8 step: 933, loss is 0.034791309386491776\n",
      "epoch: 8 step: 934, loss is 0.0006804807344451547\n",
      "epoch: 8 step: 935, loss is 0.09154386073350906\n",
      "epoch: 8 step: 936, loss is 0.015351874753832817\n",
      "epoch: 8 step: 937, loss is 2.7035972379962914e-05\n",
      "epoch: 8 step: 938, loss is 0.0008641710155643523\n",
      "epoch: 8 step: 939, loss is 0.0010947799310088158\n",
      "epoch: 8 step: 940, loss is 0.0067790113389492035\n",
      "epoch: 8 step: 941, loss is 0.00038892438169568777\n",
      "epoch: 8 step: 942, loss is 0.0010391641408205032\n",
      "epoch: 8 step: 943, loss is 0.0027260002680122852\n",
      "epoch: 8 step: 944, loss is 0.007656322792172432\n",
      "epoch: 8 step: 945, loss is 0.00011057099618483335\n",
      "epoch: 8 step: 946, loss is 0.003855039831250906\n",
      "epoch: 8 step: 947, loss is 0.0001091689191525802\n",
      "epoch: 8 step: 948, loss is 0.003968058153986931\n",
      "epoch: 8 step: 949, loss is 0.0371820330619812\n",
      "epoch: 8 step: 950, loss is 0.07281919568777084\n",
      "epoch: 8 step: 951, loss is 0.00043756779632531106\n",
      "epoch: 8 step: 952, loss is 0.061190567910671234\n",
      "epoch: 8 step: 953, loss is 0.009442941285669804\n",
      "epoch: 8 step: 954, loss is 0.00010347356146667153\n",
      "epoch: 8 step: 955, loss is 0.00023380194033961743\n",
      "epoch: 8 step: 956, loss is 0.018755413591861725\n",
      "epoch: 8 step: 957, loss is 0.008941041305661201\n",
      "epoch: 8 step: 958, loss is 0.004638342186808586\n",
      "epoch: 8 step: 959, loss is 0.0003983160713687539\n",
      "epoch: 8 step: 960, loss is 0.00015457069093827158\n",
      "epoch: 8 step: 961, loss is 6.746796134393662e-05\n",
      "epoch: 8 step: 962, loss is 0.0016003267373889685\n",
      "epoch: 8 step: 963, loss is 0.011613900773227215\n",
      "epoch: 8 step: 964, loss is 0.005796070210635662\n",
      "epoch: 8 step: 965, loss is 0.011614011600613594\n",
      "epoch: 8 step: 966, loss is 0.0007038968615233898\n",
      "epoch: 8 step: 967, loss is 0.003757365047931671\n",
      "epoch: 8 step: 968, loss is 0.0004057156329508871\n",
      "epoch: 8 step: 969, loss is 0.022219659760594368\n",
      "epoch: 8 step: 970, loss is 0.0008045150898396969\n",
      "epoch: 8 step: 971, loss is 0.0027988862711936235\n",
      "epoch: 8 step: 972, loss is 0.0003471007803454995\n",
      "epoch: 8 step: 973, loss is 0.0006231118459254503\n",
      "epoch: 8 step: 974, loss is 0.011430558748543262\n",
      "epoch: 8 step: 975, loss is 4.6256744099082425e-05\n",
      "epoch: 8 step: 976, loss is 0.0015632176073268056\n",
      "epoch: 8 step: 977, loss is 0.006775607354938984\n",
      "epoch: 8 step: 978, loss is 0.0014805833343416452\n",
      "epoch: 8 step: 979, loss is 0.12413183599710464\n",
      "epoch: 8 step: 980, loss is 0.00037598947528749704\n",
      "epoch: 8 step: 981, loss is 0.007406638003885746\n",
      "epoch: 8 step: 982, loss is 0.0002856312203221023\n",
      "epoch: 8 step: 983, loss is 0.002822520909830928\n",
      "epoch: 8 step: 984, loss is 0.012738476507365704\n",
      "epoch: 8 step: 985, loss is 0.003934940788894892\n",
      "epoch: 8 step: 986, loss is 0.22049929201602936\n",
      "epoch: 8 step: 987, loss is 9.124614007305354e-05\n",
      "epoch: 8 step: 988, loss is 0.06369046866893768\n",
      "epoch: 8 step: 989, loss is 0.004771044943481684\n",
      "epoch: 8 step: 990, loss is 0.02087542973458767\n",
      "epoch: 8 step: 991, loss is 0.015424794517457485\n",
      "epoch: 8 step: 992, loss is 0.009514929726719856\n",
      "epoch: 8 step: 993, loss is 0.0004959130892530084\n",
      "epoch: 8 step: 994, loss is 0.005202957428991795\n",
      "epoch: 8 step: 995, loss is 0.03270294889807701\n",
      "epoch: 8 step: 996, loss is 4.616446312866174e-05\n",
      "epoch: 8 step: 997, loss is 0.008530755527317524\n",
      "epoch: 8 step: 998, loss is 0.005614318419247866\n",
      "epoch: 8 step: 999, loss is 0.0010244916193187237\n",
      "epoch: 8 step: 1000, loss is 0.0013161315582692623\n",
      "epoch: 8 step: 1001, loss is 0.0007094574975781143\n",
      "epoch: 8 step: 1002, loss is 0.030616719275712967\n",
      "epoch: 8 step: 1003, loss is 0.0016965055838227272\n",
      "epoch: 8 step: 1004, loss is 8.562633593101054e-05\n",
      "epoch: 8 step: 1005, loss is 0.002143807942047715\n",
      "epoch: 8 step: 1006, loss is 9.725943527882919e-06\n",
      "epoch: 8 step: 1007, loss is 0.00019966014951933175\n",
      "epoch: 8 step: 1008, loss is 0.0032563975546509027\n",
      "epoch: 8 step: 1009, loss is 0.0018077288987115026\n",
      "epoch: 8 step: 1010, loss is 0.0007777636055834591\n",
      "epoch: 8 step: 1011, loss is 0.001757038407959044\n",
      "epoch: 8 step: 1012, loss is 0.024440398439764977\n",
      "epoch: 8 step: 1013, loss is 0.003453509882092476\n",
      "epoch: 8 step: 1014, loss is 0.015731047838926315\n",
      "epoch: 8 step: 1015, loss is 0.0012550980318337679\n",
      "epoch: 8 step: 1016, loss is 6.599783955607563e-05\n",
      "epoch: 8 step: 1017, loss is 0.0003601777134463191\n",
      "epoch: 8 step: 1018, loss is 0.0078112161718308926\n",
      "epoch: 8 step: 1019, loss is 0.03481731563806534\n",
      "epoch: 8 step: 1020, loss is 0.0009426952456124127\n",
      "epoch: 8 step: 1021, loss is 0.0006813463987782598\n",
      "epoch: 8 step: 1022, loss is 0.0032322362530976534\n",
      "epoch: 8 step: 1023, loss is 0.00010881639900617301\n",
      "epoch: 8 step: 1024, loss is 0.0005611451342701912\n",
      "epoch: 8 step: 1025, loss is 0.00216477713547647\n",
      "epoch: 8 step: 1026, loss is 0.0006027448107488453\n",
      "epoch: 8 step: 1027, loss is 0.00019423713092692196\n",
      "epoch: 8 step: 1028, loss is 0.0004486187535803765\n",
      "epoch: 8 step: 1029, loss is 0.0005173477111384273\n",
      "epoch: 8 step: 1030, loss is 0.00011714328138623387\n",
      "epoch: 8 step: 1031, loss is 4.6760007535340264e-05\n",
      "epoch: 8 step: 1032, loss is 0.003925031516700983\n",
      "epoch: 8 step: 1033, loss is 3.395601743250154e-05\n",
      "epoch: 8 step: 1034, loss is 5.085109478386585e-06\n",
      "epoch: 8 step: 1035, loss is 0.00529124541208148\n",
      "epoch: 8 step: 1036, loss is 0.001023986842483282\n",
      "epoch: 8 step: 1037, loss is 0.01589851640164852\n",
      "epoch: 8 step: 1038, loss is 7.813890988472849e-05\n",
      "epoch: 8 step: 1039, loss is 0.0010515580652281642\n",
      "epoch: 8 step: 1040, loss is 0.0037359853740781546\n",
      "epoch: 8 step: 1041, loss is 0.0006457139970734715\n",
      "epoch: 8 step: 1042, loss is 0.0012595937587320805\n",
      "epoch: 8 step: 1043, loss is 0.008414899930357933\n",
      "epoch: 8 step: 1044, loss is 0.00012177055759821087\n",
      "epoch: 8 step: 1045, loss is 0.019983284175395966\n",
      "epoch: 8 step: 1046, loss is 0.023722577840089798\n",
      "epoch: 8 step: 1047, loss is 0.002118433592841029\n",
      "epoch: 8 step: 1048, loss is 0.000919348793104291\n",
      "epoch: 8 step: 1049, loss is 0.011211841367185116\n",
      "epoch: 8 step: 1050, loss is 0.0008970504277385771\n",
      "epoch: 8 step: 1051, loss is 0.0030931150540709496\n",
      "epoch: 8 step: 1052, loss is 0.03838739171624184\n",
      "epoch: 8 step: 1053, loss is 0.0005463112029246986\n",
      "epoch: 8 step: 1054, loss is 0.001154705765657127\n",
      "epoch: 8 step: 1055, loss is 0.009384154342114925\n",
      "epoch: 8 step: 1056, loss is 0.004112167749553919\n",
      "epoch: 8 step: 1057, loss is 0.030955811962485313\n",
      "epoch: 8 step: 1058, loss is 0.017266683280467987\n",
      "epoch: 8 step: 1059, loss is 0.00042879796819761395\n",
      "epoch: 8 step: 1060, loss is 0.00013384791964199394\n",
      "epoch: 8 step: 1061, loss is 0.0003125739749521017\n",
      "epoch: 8 step: 1062, loss is 0.004878271371126175\n",
      "epoch: 8 step: 1063, loss is 0.024589067324995995\n",
      "epoch: 8 step: 1064, loss is 0.000197651403141208\n",
      "epoch: 8 step: 1065, loss is 0.00019501481438055634\n",
      "epoch: 8 step: 1066, loss is 0.0114324651658535\n",
      "epoch: 8 step: 1067, loss is 0.1711599975824356\n",
      "epoch: 8 step: 1068, loss is 0.006773499306291342\n",
      "epoch: 8 step: 1069, loss is 0.0008087484748102725\n",
      "epoch: 8 step: 1070, loss is 0.18947438895702362\n",
      "epoch: 8 step: 1071, loss is 0.0009455481776967645\n",
      "epoch: 8 step: 1072, loss is 0.002570721786469221\n",
      "epoch: 8 step: 1073, loss is 0.00013440329348668456\n",
      "epoch: 8 step: 1074, loss is 4.6127424866426736e-05\n",
      "epoch: 8 step: 1075, loss is 0.1691238433122635\n",
      "epoch: 8 step: 1076, loss is 3.099553578067571e-05\n",
      "epoch: 8 step: 1077, loss is 0.020876890048384666\n",
      "epoch: 8 step: 1078, loss is 0.000902585859876126\n",
      "epoch: 8 step: 1079, loss is 0.0003439627471379936\n",
      "epoch: 8 step: 1080, loss is 0.0005228125955909491\n",
      "epoch: 8 step: 1081, loss is 0.012277613393962383\n",
      "epoch: 8 step: 1082, loss is 0.027215231209993362\n",
      "epoch: 8 step: 1083, loss is 0.00041338245500810444\n",
      "epoch: 8 step: 1084, loss is 0.06852678209543228\n",
      "epoch: 8 step: 1085, loss is 0.012356595136225224\n",
      "epoch: 8 step: 1086, loss is 0.00013358578144107014\n",
      "epoch: 8 step: 1087, loss is 2.201873212470673e-05\n",
      "epoch: 8 step: 1088, loss is 0.0015194413717836142\n",
      "epoch: 8 step: 1089, loss is 0.1862698197364807\n",
      "epoch: 8 step: 1090, loss is 0.007243522442877293\n",
      "epoch: 8 step: 1091, loss is 0.008646066300570965\n",
      "epoch: 8 step: 1092, loss is 0.008385339751839638\n",
      "epoch: 8 step: 1093, loss is 0.0056757256388664246\n",
      "epoch: 8 step: 1094, loss is 0.013470606878399849\n",
      "epoch: 8 step: 1095, loss is 0.00660522747784853\n",
      "epoch: 8 step: 1096, loss is 0.00013462825154419988\n",
      "epoch: 8 step: 1097, loss is 0.005056572146713734\n",
      "epoch: 8 step: 1098, loss is 0.12136397510766983\n",
      "epoch: 8 step: 1099, loss is 0.001193162752315402\n",
      "epoch: 8 step: 1100, loss is 0.0027154937852174044\n",
      "epoch: 8 step: 1101, loss is 0.0003765950968954712\n",
      "epoch: 8 step: 1102, loss is 0.03247857466340065\n",
      "epoch: 8 step: 1103, loss is 0.03767763823270798\n",
      "epoch: 8 step: 1104, loss is 0.0030013590585440397\n",
      "epoch: 8 step: 1105, loss is 0.019352653995156288\n",
      "epoch: 8 step: 1106, loss is 0.2282763421535492\n",
      "epoch: 8 step: 1107, loss is 0.03817230090498924\n",
      "epoch: 8 step: 1108, loss is 0.0212537981569767\n",
      "epoch: 8 step: 1109, loss is 0.019576024264097214\n",
      "epoch: 8 step: 1110, loss is 0.0025973713491111994\n",
      "epoch: 8 step: 1111, loss is 0.004671584349125624\n",
      "epoch: 8 step: 1112, loss is 0.08093023300170898\n",
      "epoch: 8 step: 1113, loss is 0.005511761177331209\n",
      "epoch: 8 step: 1114, loss is 0.05341451242566109\n",
      "epoch: 8 step: 1115, loss is 0.10858288407325745\n",
      "epoch: 8 step: 1116, loss is 0.0005987731856293976\n",
      "epoch: 8 step: 1117, loss is 0.000539639440830797\n",
      "epoch: 8 step: 1118, loss is 5.009421511203982e-05\n",
      "epoch: 8 step: 1119, loss is 3.9520731661468744e-05\n",
      "epoch: 8 step: 1120, loss is 0.00028677345835603774\n",
      "epoch: 8 step: 1121, loss is 0.11049554497003555\n",
      "epoch: 8 step: 1122, loss is 0.024716779589653015\n",
      "epoch: 8 step: 1123, loss is 0.042495809495449066\n",
      "epoch: 8 step: 1124, loss is 0.018662545830011368\n",
      "epoch: 8 step: 1125, loss is 0.13405290246009827\n",
      "epoch: 8 step: 1126, loss is 0.003406553529202938\n",
      "epoch: 8 step: 1127, loss is 0.07486071437597275\n",
      "epoch: 8 step: 1128, loss is 0.0757233202457428\n",
      "epoch: 8 step: 1129, loss is 0.00021921267034485936\n",
      "epoch: 8 step: 1130, loss is 0.010264337994158268\n",
      "epoch: 8 step: 1131, loss is 0.0007281741709448397\n",
      "epoch: 8 step: 1132, loss is 0.10153091698884964\n",
      "epoch: 8 step: 1133, loss is 0.00014661882596556097\n",
      "epoch: 8 step: 1134, loss is 0.007027014624327421\n",
      "epoch: 8 step: 1135, loss is 0.013314384035766125\n",
      "epoch: 8 step: 1136, loss is 0.0010193086927756667\n",
      "epoch: 8 step: 1137, loss is 0.000899365812074393\n",
      "epoch: 8 step: 1138, loss is 0.003157947678118944\n",
      "epoch: 8 step: 1139, loss is 0.0004674269584938884\n",
      "epoch: 8 step: 1140, loss is 7.870194531278685e-05\n",
      "epoch: 8 step: 1141, loss is 0.00028952787397429347\n",
      "epoch: 8 step: 1142, loss is 0.00012433457595761865\n",
      "epoch: 8 step: 1143, loss is 0.0433114655315876\n",
      "epoch: 8 step: 1144, loss is 0.0012527374783530831\n",
      "epoch: 8 step: 1145, loss is 0.12771646678447723\n",
      "epoch: 8 step: 1146, loss is 0.00012706410780083388\n",
      "epoch: 8 step: 1147, loss is 0.10533255338668823\n",
      "epoch: 8 step: 1148, loss is 0.011236942373216152\n",
      "epoch: 8 step: 1149, loss is 0.0008353268494829535\n",
      "epoch: 8 step: 1150, loss is 0.03685373812913895\n",
      "epoch: 8 step: 1151, loss is 0.08989265561103821\n",
      "epoch: 8 step: 1152, loss is 0.00307956850156188\n",
      "epoch: 8 step: 1153, loss is 0.0016401550965383649\n",
      "epoch: 8 step: 1154, loss is 0.00046285928692668676\n",
      "epoch: 8 step: 1155, loss is 0.0055622197687625885\n",
      "epoch: 8 step: 1156, loss is 0.0023242791648954153\n",
      "epoch: 8 step: 1157, loss is 0.0002644057385623455\n",
      "epoch: 8 step: 1158, loss is 0.009890635497868061\n",
      "epoch: 8 step: 1159, loss is 0.009023793041706085\n",
      "epoch: 8 step: 1160, loss is 0.002622149884700775\n",
      "epoch: 8 step: 1161, loss is 0.005524315871298313\n",
      "epoch: 8 step: 1162, loss is 0.0010877406457439065\n",
      "epoch: 8 step: 1163, loss is 0.00014565935998689383\n",
      "epoch: 8 step: 1164, loss is 0.0018797263037413359\n",
      "epoch: 8 step: 1165, loss is 0.028332898393273354\n",
      "epoch: 8 step: 1166, loss is 0.0791933611035347\n",
      "epoch: 8 step: 1167, loss is 0.010509892366826534\n",
      "epoch: 8 step: 1168, loss is 0.00014368881238624454\n",
      "epoch: 8 step: 1169, loss is 0.0013262490974739194\n",
      "epoch: 8 step: 1170, loss is 0.020671607926487923\n",
      "epoch: 8 step: 1171, loss is 0.0073991259559988976\n",
      "epoch: 8 step: 1172, loss is 2.72216166194994e-05\n",
      "epoch: 8 step: 1173, loss is 0.0004020853666588664\n",
      "epoch: 8 step: 1174, loss is 0.0014091180637478828\n",
      "epoch: 8 step: 1175, loss is 0.009266162291169167\n",
      "epoch: 8 step: 1176, loss is 0.05792295187711716\n",
      "epoch: 8 step: 1177, loss is 0.006851131562143564\n",
      "epoch: 8 step: 1178, loss is 0.0009509107330814004\n",
      "epoch: 8 step: 1179, loss is 0.0004514324537012726\n",
      "epoch: 8 step: 1180, loss is 0.0012405632296577096\n",
      "epoch: 8 step: 1181, loss is 0.00859283097088337\n",
      "epoch: 8 step: 1182, loss is 0.05305803194642067\n",
      "epoch: 8 step: 1183, loss is 0.002673398470506072\n",
      "epoch: 8 step: 1184, loss is 0.0011452162871137261\n",
      "epoch: 8 step: 1185, loss is 0.012859142385423183\n",
      "epoch: 8 step: 1186, loss is 0.021462690085172653\n",
      "epoch: 8 step: 1187, loss is 5.2309216698631644e-05\n",
      "epoch: 8 step: 1188, loss is 0.03510573133826256\n",
      "epoch: 8 step: 1189, loss is 0.00010444517101859674\n",
      "epoch: 8 step: 1190, loss is 0.0005884058773517609\n",
      "epoch: 8 step: 1191, loss is 7.198599632829428e-05\n",
      "epoch: 8 step: 1192, loss is 0.0001754861732479185\n",
      "epoch: 8 step: 1193, loss is 0.006845507305115461\n",
      "epoch: 8 step: 1194, loss is 0.001151468139141798\n",
      "epoch: 8 step: 1195, loss is 0.00047220231499522924\n",
      "epoch: 8 step: 1196, loss is 0.005448685027658939\n",
      "epoch: 8 step: 1197, loss is 0.0010392324766144156\n",
      "epoch: 8 step: 1198, loss is 0.0004016007878817618\n",
      "epoch: 8 step: 1199, loss is 0.006868844851851463\n",
      "epoch: 8 step: 1200, loss is 0.30317893624305725\n",
      "epoch: 8 step: 1201, loss is 0.009088096208870411\n",
      "epoch: 8 step: 1202, loss is 0.0010368406074121594\n",
      "epoch: 8 step: 1203, loss is 0.0006693285540677607\n",
      "epoch: 8 step: 1204, loss is 0.0005363153759390116\n",
      "epoch: 8 step: 1205, loss is 0.03147589787840843\n",
      "epoch: 8 step: 1206, loss is 0.009044049307703972\n",
      "epoch: 8 step: 1207, loss is 0.0003482622269075364\n",
      "epoch: 8 step: 1208, loss is 0.03506307676434517\n",
      "epoch: 8 step: 1209, loss is 0.009617031551897526\n",
      "epoch: 8 step: 1210, loss is 0.0002891972253564745\n",
      "epoch: 8 step: 1211, loss is 0.07154332101345062\n",
      "epoch: 8 step: 1212, loss is 0.00028147472767159343\n",
      "epoch: 8 step: 1213, loss is 0.00028586978442035615\n",
      "epoch: 8 step: 1214, loss is 0.008360207080841064\n",
      "epoch: 8 step: 1215, loss is 0.00015912827802821994\n",
      "epoch: 8 step: 1216, loss is 0.010470458306372166\n",
      "epoch: 8 step: 1217, loss is 0.0014735549921169877\n",
      "epoch: 8 step: 1218, loss is 0.00033268023980781436\n",
      "epoch: 8 step: 1219, loss is 0.017678139731287956\n",
      "epoch: 8 step: 1220, loss is 0.00031842125463299453\n",
      "epoch: 8 step: 1221, loss is 0.0028961095958948135\n",
      "epoch: 8 step: 1222, loss is 0.00015199414337985218\n",
      "epoch: 8 step: 1223, loss is 0.02031235583126545\n",
      "epoch: 8 step: 1224, loss is 0.00019479947513900697\n",
      "epoch: 8 step: 1225, loss is 2.6430840080138296e-05\n",
      "epoch: 8 step: 1226, loss is 0.005118862725794315\n",
      "epoch: 8 step: 1227, loss is 0.00014252659457270056\n",
      "epoch: 8 step: 1228, loss is 0.08625970780849457\n",
      "epoch: 8 step: 1229, loss is 0.0067753177136182785\n",
      "epoch: 8 step: 1230, loss is 0.002398426178842783\n",
      "epoch: 8 step: 1231, loss is 0.005983689799904823\n",
      "epoch: 8 step: 1232, loss is 0.00112633325625211\n",
      "epoch: 8 step: 1233, loss is 0.0017818071646615863\n",
      "epoch: 8 step: 1234, loss is 0.022240063175559044\n",
      "epoch: 8 step: 1235, loss is 0.0006188463303260505\n",
      "epoch: 8 step: 1236, loss is 0.0075713275000452995\n",
      "epoch: 8 step: 1237, loss is 0.0003166448441334069\n",
      "epoch: 8 step: 1238, loss is 0.04014662280678749\n",
      "epoch: 8 step: 1239, loss is 8.968666952569038e-05\n",
      "epoch: 8 step: 1240, loss is 0.005758516024798155\n",
      "epoch: 8 step: 1241, loss is 0.006076705642044544\n",
      "epoch: 8 step: 1242, loss is 8.080913539743051e-05\n",
      "epoch: 8 step: 1243, loss is 0.0024922979064285755\n",
      "epoch: 8 step: 1244, loss is 0.0007248056936077774\n",
      "epoch: 8 step: 1245, loss is 0.009337055496871471\n",
      "epoch: 8 step: 1246, loss is 8.424752741120756e-05\n",
      "epoch: 8 step: 1247, loss is 0.0026377979665994644\n",
      "epoch: 8 step: 1248, loss is 0.2306540459394455\n",
      "epoch: 8 step: 1249, loss is 0.0032446272671222687\n",
      "epoch: 8 step: 1250, loss is 0.007717624772340059\n",
      "epoch: 8 step: 1251, loss is 0.0748932808637619\n",
      "epoch: 8 step: 1252, loss is 0.000841342203784734\n",
      "epoch: 8 step: 1253, loss is 0.3845239281654358\n",
      "epoch: 8 step: 1254, loss is 0.0027423505671322346\n",
      "epoch: 8 step: 1255, loss is 0.00038368115201592445\n",
      "epoch: 8 step: 1256, loss is 0.0009011623333208263\n",
      "epoch: 8 step: 1257, loss is 0.007739205379039049\n",
      "epoch: 8 step: 1258, loss is 0.21118387579917908\n",
      "epoch: 8 step: 1259, loss is 0.002057816367596388\n",
      "epoch: 8 step: 1260, loss is 0.0002139197604265064\n",
      "epoch: 8 step: 1261, loss is 0.04422193765640259\n",
      "epoch: 8 step: 1262, loss is 0.05299166589975357\n",
      "epoch: 8 step: 1263, loss is 0.002656591823324561\n",
      "epoch: 8 step: 1264, loss is 0.06688334792852402\n",
      "epoch: 8 step: 1265, loss is 0.0062880911864340305\n",
      "epoch: 8 step: 1266, loss is 0.02368762157857418\n",
      "epoch: 8 step: 1267, loss is 0.04783455282449722\n",
      "epoch: 8 step: 1268, loss is 0.0009919964941218495\n",
      "epoch: 8 step: 1269, loss is 0.018332086503505707\n",
      "epoch: 8 step: 1270, loss is 0.0029245689511299133\n",
      "epoch: 8 step: 1271, loss is 0.00022404588526114821\n",
      "epoch: 8 step: 1272, loss is 0.0031590110156685114\n",
      "epoch: 8 step: 1273, loss is 0.005292453803122044\n",
      "epoch: 8 step: 1274, loss is 0.00037255845381878316\n",
      "epoch: 8 step: 1275, loss is 0.005017852410674095\n",
      "epoch: 8 step: 1276, loss is 0.0011921963887289166\n",
      "epoch: 8 step: 1277, loss is 0.43305185437202454\n",
      "epoch: 8 step: 1278, loss is 0.015083427540957928\n",
      "epoch: 8 step: 1279, loss is 0.004335789009928703\n",
      "epoch: 8 step: 1280, loss is 0.0011622097808867693\n",
      "epoch: 8 step: 1281, loss is 0.025731507688760757\n",
      "epoch: 8 step: 1282, loss is 0.03803639113903046\n",
      "epoch: 8 step: 1283, loss is 0.009943862445652485\n",
      "epoch: 8 step: 1284, loss is 9.93338180705905e-05\n",
      "epoch: 8 step: 1285, loss is 0.0027482989244163036\n",
      "epoch: 8 step: 1286, loss is 0.00029493807232938707\n",
      "epoch: 8 step: 1287, loss is 0.0011974441586062312\n",
      "epoch: 8 step: 1288, loss is 0.00036299676867201924\n",
      "epoch: 8 step: 1289, loss is 0.0021325494162738323\n",
      "epoch: 8 step: 1290, loss is 0.006681527942419052\n",
      "epoch: 8 step: 1291, loss is 0.00042038611718453467\n",
      "epoch: 8 step: 1292, loss is 0.0002465384313836694\n",
      "epoch: 8 step: 1293, loss is 0.01835143193602562\n",
      "epoch: 8 step: 1294, loss is 0.08185281604528427\n",
      "epoch: 8 step: 1295, loss is 0.00020170045900158584\n",
      "epoch: 8 step: 1296, loss is 0.0002836262574419379\n",
      "epoch: 8 step: 1297, loss is 0.005199753679335117\n",
      "epoch: 8 step: 1298, loss is 0.02814381755888462\n",
      "epoch: 8 step: 1299, loss is 0.08340224623680115\n",
      "epoch: 8 step: 1300, loss is 0.008570615202188492\n",
      "epoch: 8 step: 1301, loss is 0.00858130119740963\n",
      "epoch: 8 step: 1302, loss is 0.08711624145507812\n",
      "epoch: 8 step: 1303, loss is 9.154826693702489e-05\n",
      "epoch: 8 step: 1304, loss is 0.044321879744529724\n",
      "epoch: 8 step: 1305, loss is 0.0019312120275571942\n",
      "epoch: 8 step: 1306, loss is 0.0006786022568121552\n",
      "epoch: 8 step: 1307, loss is 0.11747629940509796\n",
      "epoch: 8 step: 1308, loss is 0.07439185678958893\n",
      "epoch: 8 step: 1309, loss is 0.00037327728932723403\n",
      "epoch: 8 step: 1310, loss is 0.006625867448747158\n",
      "epoch: 8 step: 1311, loss is 0.0023929537273943424\n",
      "epoch: 8 step: 1312, loss is 6.491311069112271e-05\n",
      "epoch: 8 step: 1313, loss is 0.0015958357835188508\n",
      "epoch: 8 step: 1314, loss is 0.00020583654986694455\n",
      "epoch: 8 step: 1315, loss is 0.0002724933729041368\n",
      "epoch: 8 step: 1316, loss is 0.023975446820259094\n",
      "epoch: 8 step: 1317, loss is 0.0001887977123260498\n",
      "epoch: 8 step: 1318, loss is 0.005103227216750383\n",
      "epoch: 8 step: 1319, loss is 0.009011008776724339\n",
      "epoch: 8 step: 1320, loss is 0.00264944089576602\n",
      "epoch: 8 step: 1321, loss is 0.0017168407794088125\n",
      "epoch: 8 step: 1322, loss is 0.005237183533608913\n",
      "epoch: 8 step: 1323, loss is 0.005526226479560137\n",
      "epoch: 8 step: 1324, loss is 0.054395146667957306\n",
      "epoch: 8 step: 1325, loss is 0.03659847378730774\n",
      "epoch: 8 step: 1326, loss is 0.0002946355380117893\n",
      "epoch: 8 step: 1327, loss is 0.004029329866170883\n",
      "epoch: 8 step: 1328, loss is 0.0009466034825891256\n",
      "epoch: 8 step: 1329, loss is 0.000384931277949363\n",
      "epoch: 8 step: 1330, loss is 0.008297872729599476\n",
      "epoch: 8 step: 1331, loss is 0.01050009485334158\n",
      "epoch: 8 step: 1332, loss is 0.24586555361747742\n",
      "epoch: 8 step: 1333, loss is 0.00024226264213211834\n",
      "epoch: 8 step: 1334, loss is 0.04435154050588608\n",
      "epoch: 8 step: 1335, loss is 0.0006144865183159709\n",
      "epoch: 8 step: 1336, loss is 0.0003219484933651984\n",
      "epoch: 8 step: 1337, loss is 0.013926279731094837\n",
      "epoch: 8 step: 1338, loss is 0.008190693333745003\n",
      "epoch: 8 step: 1339, loss is 0.0010121343657374382\n",
      "epoch: 8 step: 1340, loss is 0.0009371345513500273\n",
      "epoch: 8 step: 1341, loss is 0.01572691649198532\n",
      "epoch: 8 step: 1342, loss is 0.04768936336040497\n",
      "epoch: 8 step: 1343, loss is 0.0032846059184521437\n",
      "epoch: 8 step: 1344, loss is 0.026892490684986115\n",
      "epoch: 8 step: 1345, loss is 0.001570601831190288\n",
      "epoch: 8 step: 1346, loss is 0.00011286760127404705\n",
      "epoch: 8 step: 1347, loss is 0.03188544884324074\n",
      "epoch: 8 step: 1348, loss is 0.05431969091296196\n",
      "epoch: 8 step: 1349, loss is 0.0015126412035897374\n",
      "epoch: 8 step: 1350, loss is 0.006405273452401161\n",
      "epoch: 8 step: 1351, loss is 0.0001415199221810326\n",
      "epoch: 8 step: 1352, loss is 0.000396689516492188\n",
      "epoch: 8 step: 1353, loss is 0.0009051571250893176\n",
      "epoch: 8 step: 1354, loss is 0.007663608528673649\n",
      "epoch: 8 step: 1355, loss is 3.288070729468018e-05\n",
      "epoch: 8 step: 1356, loss is 0.0203861016780138\n",
      "epoch: 8 step: 1357, loss is 0.0020418050698935986\n",
      "epoch: 8 step: 1358, loss is 0.00041836872696876526\n",
      "epoch: 8 step: 1359, loss is 0.013147960416972637\n",
      "epoch: 8 step: 1360, loss is 0.0009381922427564859\n",
      "epoch: 8 step: 1361, loss is 0.07747397571802139\n",
      "epoch: 8 step: 1362, loss is 0.006657920777797699\n",
      "epoch: 8 step: 1363, loss is 0.255156934261322\n",
      "epoch: 8 step: 1364, loss is 0.009322305209934711\n",
      "epoch: 8 step: 1365, loss is 0.0003766028385143727\n",
      "epoch: 8 step: 1366, loss is 0.04608074203133583\n",
      "epoch: 8 step: 1367, loss is 8.721643825992942e-05\n",
      "epoch: 8 step: 1368, loss is 0.002462207805365324\n",
      "epoch: 8 step: 1369, loss is 0.005605627316981554\n",
      "epoch: 8 step: 1370, loss is 0.005156461149454117\n",
      "epoch: 8 step: 1371, loss is 0.07195174694061279\n",
      "epoch: 8 step: 1372, loss is 0.003060409100726247\n",
      "epoch: 8 step: 1373, loss is 0.006159725598990917\n",
      "epoch: 8 step: 1374, loss is 0.0034499967005103827\n",
      "epoch: 8 step: 1375, loss is 0.0012314956402406096\n",
      "epoch: 8 step: 1376, loss is 0.2029033899307251\n",
      "epoch: 8 step: 1377, loss is 0.0043684327974915504\n",
      "epoch: 8 step: 1378, loss is 0.0020404376555234194\n",
      "epoch: 8 step: 1379, loss is 0.008036644198000431\n",
      "epoch: 8 step: 1380, loss is 0.05588757246732712\n",
      "epoch: 8 step: 1381, loss is 0.0014475862262770534\n",
      "epoch: 8 step: 1382, loss is 0.005443795118480921\n",
      "epoch: 8 step: 1383, loss is 0.0006419935962185264\n",
      "epoch: 8 step: 1384, loss is 0.006179674994200468\n",
      "epoch: 8 step: 1385, loss is 0.00029536127112805843\n",
      "epoch: 8 step: 1386, loss is 9.719334775581956e-05\n",
      "epoch: 8 step: 1387, loss is 0.0001657106331549585\n",
      "epoch: 8 step: 1388, loss is 0.023752890527248383\n",
      "epoch: 8 step: 1389, loss is 0.0007055648602545261\n",
      "epoch: 8 step: 1390, loss is 0.015801599249243736\n",
      "epoch: 8 step: 1391, loss is 0.040634769946336746\n",
      "epoch: 8 step: 1392, loss is 0.0010083448141813278\n",
      "epoch: 8 step: 1393, loss is 0.0002826921991072595\n",
      "epoch: 8 step: 1394, loss is 0.0029849077109247446\n",
      "epoch: 8 step: 1395, loss is 0.07878997921943665\n",
      "epoch: 8 step: 1396, loss is 0.0006225595134310424\n",
      "epoch: 8 step: 1397, loss is 0.18635255098342896\n",
      "epoch: 8 step: 1398, loss is 0.00012409407645463943\n",
      "epoch: 8 step: 1399, loss is 0.00024695537285879254\n",
      "epoch: 8 step: 1400, loss is 0.010053851641714573\n",
      "epoch: 8 step: 1401, loss is 0.0004773408581968397\n",
      "epoch: 8 step: 1402, loss is 0.008809424005448818\n",
      "epoch: 8 step: 1403, loss is 0.0002276196755701676\n",
      "epoch: 8 step: 1404, loss is 0.00036712182918563485\n",
      "epoch: 8 step: 1405, loss is 0.007950159721076488\n",
      "epoch: 8 step: 1406, loss is 0.0013979870127514005\n",
      "epoch: 8 step: 1407, loss is 6.562197813764215e-05\n",
      "epoch: 8 step: 1408, loss is 0.003406357252970338\n",
      "epoch: 8 step: 1409, loss is 0.004555843770503998\n",
      "epoch: 8 step: 1410, loss is 0.0023501920513808727\n",
      "epoch: 8 step: 1411, loss is 9.736140782479197e-05\n",
      "epoch: 8 step: 1412, loss is 0.01522041019052267\n",
      "epoch: 8 step: 1413, loss is 0.02914559282362461\n",
      "epoch: 8 step: 1414, loss is 0.000559380860067904\n",
      "epoch: 8 step: 1415, loss is 0.17378124594688416\n",
      "epoch: 8 step: 1416, loss is 0.005127766169607639\n",
      "epoch: 8 step: 1417, loss is 0.005223787855356932\n",
      "epoch: 8 step: 1418, loss is 0.0014440914383158088\n",
      "epoch: 8 step: 1419, loss is 0.014796719886362553\n",
      "epoch: 8 step: 1420, loss is 0.00021067331545054913\n",
      "epoch: 8 step: 1421, loss is 0.008692217990756035\n",
      "epoch: 8 step: 1422, loss is 0.049491576850414276\n",
      "epoch: 8 step: 1423, loss is 0.008246887475252151\n",
      "epoch: 8 step: 1424, loss is 0.028993334621191025\n",
      "epoch: 8 step: 1425, loss is 0.016338875517249107\n",
      "epoch: 8 step: 1426, loss is 0.03146378695964813\n",
      "epoch: 8 step: 1427, loss is 0.010678783990442753\n",
      "epoch: 8 step: 1428, loss is 0.002956344513222575\n",
      "epoch: 8 step: 1429, loss is 0.04668799787759781\n",
      "epoch: 8 step: 1430, loss is 0.0003067617944907397\n",
      "epoch: 8 step: 1431, loss is 0.0008075764635577798\n",
      "epoch: 8 step: 1432, loss is 0.0010386776411905885\n",
      "epoch: 8 step: 1433, loss is 0.007435619831085205\n",
      "epoch: 8 step: 1434, loss is 0.0005265699001029134\n",
      "epoch: 8 step: 1435, loss is 0.0024535423144698143\n",
      "epoch: 8 step: 1436, loss is 0.0049729859456419945\n",
      "epoch: 8 step: 1437, loss is 0.007126814220100641\n",
      "epoch: 8 step: 1438, loss is 0.18579775094985962\n",
      "epoch: 8 step: 1439, loss is 0.0013425068464130163\n",
      "epoch: 8 step: 1440, loss is 0.011269739829003811\n",
      "epoch: 8 step: 1441, loss is 6.785470759496093e-05\n",
      "epoch: 8 step: 1442, loss is 0.12988772988319397\n",
      "epoch: 8 step: 1443, loss is 0.0033335702028125525\n",
      "epoch: 8 step: 1444, loss is 0.11152085661888123\n",
      "epoch: 8 step: 1445, loss is 9.96391027001664e-05\n",
      "epoch: 8 step: 1446, loss is 0.0022514783777296543\n",
      "epoch: 8 step: 1447, loss is 0.00020994889200665057\n",
      "epoch: 8 step: 1448, loss is 5.15604324391461e-06\n",
      "epoch: 8 step: 1449, loss is 0.005641700699925423\n",
      "epoch: 8 step: 1450, loss is 0.0012362009147182107\n",
      "epoch: 8 step: 1451, loss is 2.0708372176159173e-05\n",
      "epoch: 8 step: 1452, loss is 6.731382018188015e-05\n",
      "epoch: 8 step: 1453, loss is 0.05346834659576416\n",
      "epoch: 8 step: 1454, loss is 0.00017069053137674928\n",
      "epoch: 8 step: 1455, loss is 2.942136416095309e-05\n",
      "epoch: 8 step: 1456, loss is 0.010065154172480106\n",
      "epoch: 8 step: 1457, loss is 0.0658116266131401\n",
      "epoch: 8 step: 1458, loss is 9.933845285559073e-05\n",
      "epoch: 8 step: 1459, loss is 0.014845099300146103\n",
      "epoch: 8 step: 1460, loss is 0.030133143067359924\n",
      "epoch: 8 step: 1461, loss is 0.001623283140361309\n",
      "epoch: 8 step: 1462, loss is 0.003773588454350829\n",
      "epoch: 8 step: 1463, loss is 0.0020411210134625435\n",
      "epoch: 8 step: 1464, loss is 0.017973504960536957\n",
      "epoch: 8 step: 1465, loss is 0.029867807403206825\n",
      "epoch: 8 step: 1466, loss is 0.0005191845120862126\n",
      "epoch: 8 step: 1467, loss is 6.26702603767626e-05\n",
      "epoch: 8 step: 1468, loss is 0.018890567123889923\n",
      "epoch: 8 step: 1469, loss is 0.027566678822040558\n",
      "epoch: 8 step: 1470, loss is 2.0852630768786184e-05\n",
      "epoch: 8 step: 1471, loss is 0.00040573894511908293\n",
      "epoch: 8 step: 1472, loss is 0.06473803520202637\n",
      "epoch: 8 step: 1473, loss is 0.00016177129873540252\n",
      "epoch: 8 step: 1474, loss is 0.013729317113757133\n",
      "epoch: 8 step: 1475, loss is 9.943035365722608e-06\n",
      "epoch: 8 step: 1476, loss is 0.00028934195870533586\n",
      "epoch: 8 step: 1477, loss is 0.00904997531324625\n",
      "epoch: 8 step: 1478, loss is 0.08591502159833908\n",
      "epoch: 8 step: 1479, loss is 0.006998029071837664\n",
      "epoch: 8 step: 1480, loss is 0.05988237261772156\n",
      "epoch: 8 step: 1481, loss is 0.00890156626701355\n",
      "epoch: 8 step: 1482, loss is 0.0007464963709935546\n",
      "epoch: 8 step: 1483, loss is 0.0013949512504041195\n",
      "epoch: 8 step: 1484, loss is 0.00022673283820040524\n",
      "epoch: 8 step: 1485, loss is 0.0025390316732227802\n",
      "epoch: 8 step: 1486, loss is 0.0009174797451123595\n",
      "epoch: 8 step: 1487, loss is 0.0004014261066913605\n",
      "epoch: 8 step: 1488, loss is 0.0017320216866210103\n",
      "epoch: 8 step: 1489, loss is 0.01467827521264553\n",
      "epoch: 8 step: 1490, loss is 0.02503795735538006\n",
      "epoch: 8 step: 1491, loss is 0.0008031580946408212\n",
      "epoch: 8 step: 1492, loss is 0.022240616381168365\n",
      "epoch: 8 step: 1493, loss is 0.007342848461121321\n",
      "epoch: 8 step: 1494, loss is 6.119472527643666e-05\n",
      "epoch: 8 step: 1495, loss is 0.0021415050141513348\n",
      "epoch: 8 step: 1496, loss is 0.04708990454673767\n",
      "epoch: 8 step: 1497, loss is 0.005088068079203367\n",
      "epoch: 8 step: 1498, loss is 0.009814095683395863\n",
      "epoch: 8 step: 1499, loss is 0.004215001128613949\n",
      "epoch: 8 step: 1500, loss is 0.0038504016119986773\n",
      "epoch: 8 step: 1501, loss is 0.00010395013669040054\n",
      "epoch: 8 step: 1502, loss is 0.006500908173620701\n",
      "epoch: 8 step: 1503, loss is 8.370602881768718e-05\n",
      "epoch: 8 step: 1504, loss is 0.00040805572643876076\n",
      "epoch: 8 step: 1505, loss is 0.00046815603855066\n",
      "epoch: 8 step: 1506, loss is 0.015212112106382847\n",
      "epoch: 8 step: 1507, loss is 0.005505896173417568\n",
      "epoch: 8 step: 1508, loss is 0.0031456593424081802\n",
      "epoch: 8 step: 1509, loss is 0.013448823243379593\n",
      "epoch: 8 step: 1510, loss is 0.001384414965286851\n",
      "epoch: 8 step: 1511, loss is 0.0030891159549355507\n",
      "epoch: 8 step: 1512, loss is 0.042903780937194824\n",
      "epoch: 8 step: 1513, loss is 0.00016065908130258322\n",
      "epoch: 8 step: 1514, loss is 0.0554099977016449\n",
      "epoch: 8 step: 1515, loss is 0.00015822748537175357\n",
      "epoch: 8 step: 1516, loss is 0.000840389693621546\n",
      "epoch: 8 step: 1517, loss is 0.07862409949302673\n",
      "epoch: 8 step: 1518, loss is 3.059579830733128e-05\n",
      "epoch: 8 step: 1519, loss is 9.02968313312158e-05\n",
      "epoch: 8 step: 1520, loss is 2.5130395442829467e-05\n",
      "epoch: 8 step: 1521, loss is 0.003566337749361992\n",
      "epoch: 8 step: 1522, loss is 0.0025110687129199505\n",
      "epoch: 8 step: 1523, loss is 4.108102439204231e-05\n",
      "epoch: 8 step: 1524, loss is 0.0023205792531371117\n",
      "epoch: 8 step: 1525, loss is 0.004131586290895939\n",
      "epoch: 8 step: 1526, loss is 0.012456271797418594\n",
      "epoch: 8 step: 1527, loss is 6.629898416576907e-05\n",
      "epoch: 8 step: 1528, loss is 0.00039193901466205716\n",
      "epoch: 8 step: 1529, loss is 0.015496993437409401\n",
      "epoch: 8 step: 1530, loss is 0.001287999446503818\n",
      "epoch: 8 step: 1531, loss is 0.013067295774817467\n",
      "epoch: 8 step: 1532, loss is 0.00014765591186005622\n",
      "epoch: 8 step: 1533, loss is 0.0019943201914429665\n",
      "epoch: 8 step: 1534, loss is 0.0007066612015478313\n",
      "epoch: 8 step: 1535, loss is 0.0028902380727231503\n",
      "epoch: 8 step: 1536, loss is 0.004374338313937187\n",
      "epoch: 8 step: 1537, loss is 0.005649952217936516\n",
      "epoch: 8 step: 1538, loss is 0.0029845188837498426\n",
      "epoch: 8 step: 1539, loss is 0.0008572145598009229\n",
      "epoch: 8 step: 1540, loss is 0.00723604578524828\n",
      "epoch: 8 step: 1541, loss is 0.0002021874679485336\n",
      "epoch: 8 step: 1542, loss is 0.007220386061817408\n",
      "epoch: 8 step: 1543, loss is 5.997795960865915e-05\n",
      "epoch: 8 step: 1544, loss is 0.0001021104326355271\n",
      "epoch: 8 step: 1545, loss is 0.00011617920245043933\n",
      "epoch: 8 step: 1546, loss is 0.004059807397425175\n",
      "epoch: 8 step: 1547, loss is 0.00015206883836071938\n",
      "epoch: 8 step: 1548, loss is 0.00021748260769527406\n",
      "epoch: 8 step: 1549, loss is 0.0001932410814333707\n",
      "epoch: 8 step: 1550, loss is 0.0011254488490521908\n",
      "epoch: 8 step: 1551, loss is 0.0026351180858910084\n",
      "epoch: 8 step: 1552, loss is 0.00036026223096996546\n",
      "epoch: 8 step: 1553, loss is 1.920412432809826e-05\n",
      "epoch: 8 step: 1554, loss is 0.078460194170475\n",
      "epoch: 8 step: 1555, loss is 0.0003974880964960903\n",
      "epoch: 8 step: 1556, loss is 0.010519670322537422\n",
      "epoch: 8 step: 1557, loss is 0.002500134287402034\n",
      "epoch: 8 step: 1558, loss is 0.035835158079862595\n",
      "epoch: 8 step: 1559, loss is 0.004523305222392082\n",
      "epoch: 8 step: 1560, loss is 0.004063046537339687\n",
      "epoch: 8 step: 1561, loss is 0.007655266672372818\n",
      "epoch: 8 step: 1562, loss is 0.000112159650598187\n",
      "epoch: 8 step: 1563, loss is 0.16571545600891113\n",
      "epoch: 8 step: 1564, loss is 0.0003039190487470478\n",
      "epoch: 8 step: 1565, loss is 0.00023706354841124266\n",
      "epoch: 8 step: 1566, loss is 0.0012054353719577193\n",
      "epoch: 8 step: 1567, loss is 0.02203025110065937\n",
      "epoch: 8 step: 1568, loss is 0.0001757703721523285\n",
      "epoch: 8 step: 1569, loss is 0.00010992663010256365\n",
      "epoch: 8 step: 1570, loss is 6.221319927135482e-05\n",
      "epoch: 8 step: 1571, loss is 0.006828781217336655\n",
      "epoch: 8 step: 1572, loss is 0.17633233964443207\n",
      "epoch: 8 step: 1573, loss is 0.10776084661483765\n",
      "epoch: 8 step: 1574, loss is 0.0001931028818944469\n",
      "epoch: 8 step: 1575, loss is 0.0014588412595912814\n",
      "epoch: 8 step: 1576, loss is 0.00011897767399204895\n",
      "epoch: 8 step: 1577, loss is 0.0009629240375943482\n",
      "epoch: 8 step: 1578, loss is 0.010224908590316772\n",
      "epoch: 8 step: 1579, loss is 0.051859840750694275\n",
      "epoch: 8 step: 1580, loss is 0.14957530796527863\n",
      "epoch: 8 step: 1581, loss is 0.00023961043916642666\n",
      "epoch: 8 step: 1582, loss is 0.0005072189960628748\n",
      "epoch: 8 step: 1583, loss is 0.00022241791884880513\n",
      "epoch: 8 step: 1584, loss is 0.00035835476592183113\n",
      "epoch: 8 step: 1585, loss is 0.0007655919180251658\n",
      "epoch: 8 step: 1586, loss is 0.05442367121577263\n",
      "epoch: 8 step: 1587, loss is 0.001623641001060605\n",
      "epoch: 8 step: 1588, loss is 7.927726983325556e-06\n",
      "epoch: 8 step: 1589, loss is 0.03721452131867409\n",
      "epoch: 8 step: 1590, loss is 0.012461128644645214\n",
      "epoch: 8 step: 1591, loss is 0.015531511977314949\n",
      "epoch: 8 step: 1592, loss is 0.00817379355430603\n",
      "epoch: 8 step: 1593, loss is 0.041401538997888565\n",
      "epoch: 8 step: 1594, loss is 0.0006155420560389757\n",
      "epoch: 8 step: 1595, loss is 0.11573657393455505\n",
      "epoch: 8 step: 1596, loss is 0.00525743467733264\n",
      "epoch: 8 step: 1597, loss is 0.021127302199602127\n",
      "epoch: 8 step: 1598, loss is 0.0001816525764297694\n",
      "epoch: 8 step: 1599, loss is 0.006803292315453291\n",
      "epoch: 8 step: 1600, loss is 0.13965940475463867\n",
      "epoch: 8 step: 1601, loss is 0.0017827062401920557\n",
      "epoch: 8 step: 1602, loss is 2.2642925614491105e-05\n",
      "epoch: 8 step: 1603, loss is 0.04083002731204033\n",
      "epoch: 8 step: 1604, loss is 0.045087575912475586\n",
      "epoch: 8 step: 1605, loss is 0.045459847897291183\n",
      "epoch: 8 step: 1606, loss is 0.006475092843174934\n",
      "epoch: 8 step: 1607, loss is 0.0011069440515711904\n",
      "epoch: 8 step: 1608, loss is 0.002253083512187004\n",
      "epoch: 8 step: 1609, loss is 0.013644675724208355\n",
      "epoch: 8 step: 1610, loss is 0.06258918344974518\n",
      "epoch: 8 step: 1611, loss is 0.00011241570609854534\n",
      "epoch: 8 step: 1612, loss is 0.1580357849597931\n",
      "epoch: 8 step: 1613, loss is 0.008689220994710922\n",
      "epoch: 8 step: 1614, loss is 0.001981605077162385\n",
      "epoch: 8 step: 1615, loss is 0.0003612720174714923\n",
      "epoch: 8 step: 1616, loss is 0.0012898842105641961\n",
      "epoch: 8 step: 1617, loss is 0.015365872532129288\n",
      "epoch: 8 step: 1618, loss is 0.007339948322623968\n",
      "epoch: 8 step: 1619, loss is 0.00027181411860510707\n",
      "epoch: 8 step: 1620, loss is 0.0030225252266973257\n",
      "epoch: 8 step: 1621, loss is 0.0062601505778729916\n",
      "epoch: 8 step: 1622, loss is 0.10175870358943939\n",
      "epoch: 8 step: 1623, loss is 0.0004412055423017591\n",
      "epoch: 8 step: 1624, loss is 6.81463789078407e-05\n",
      "epoch: 8 step: 1625, loss is 0.0018037429545074701\n",
      "epoch: 8 step: 1626, loss is 0.031821347773075104\n",
      "epoch: 8 step: 1627, loss is 0.10903934389352798\n",
      "epoch: 8 step: 1628, loss is 0.04806564375758171\n",
      "epoch: 8 step: 1629, loss is 0.00010806629143189639\n",
      "epoch: 8 step: 1630, loss is 0.0007178641390055418\n",
      "epoch: 8 step: 1631, loss is 0.0035441850777715445\n",
      "epoch: 8 step: 1632, loss is 0.12032655626535416\n",
      "epoch: 8 step: 1633, loss is 0.045434001833200455\n",
      "epoch: 8 step: 1634, loss is 0.001032948843203485\n",
      "epoch: 8 step: 1635, loss is 0.025213537737727165\n",
      "epoch: 8 step: 1636, loss is 0.03504238650202751\n",
      "epoch: 8 step: 1637, loss is 1.7590502466191538e-05\n",
      "epoch: 8 step: 1638, loss is 0.0004110169247724116\n",
      "epoch: 8 step: 1639, loss is 0.004701671656221151\n",
      "epoch: 8 step: 1640, loss is 0.013319715857505798\n",
      "epoch: 8 step: 1641, loss is 0.0004246519529260695\n",
      "epoch: 8 step: 1642, loss is 0.004736968781799078\n",
      "epoch: 8 step: 1643, loss is 0.020070020109415054\n",
      "epoch: 8 step: 1644, loss is 0.010240868665277958\n",
      "epoch: 8 step: 1645, loss is 0.00028245802968740463\n",
      "epoch: 8 step: 1646, loss is 0.2190660983324051\n",
      "epoch: 8 step: 1647, loss is 0.07525522261857986\n",
      "epoch: 8 step: 1648, loss is 0.016299430280923843\n",
      "epoch: 8 step: 1649, loss is 0.005462447181344032\n",
      "epoch: 8 step: 1650, loss is 0.14630179107189178\n",
      "epoch: 8 step: 1651, loss is 0.0250882375985384\n",
      "epoch: 8 step: 1652, loss is 0.0014068646123632789\n",
      "epoch: 8 step: 1653, loss is 0.00016761240840423852\n",
      "epoch: 8 step: 1654, loss is 0.002415413735434413\n",
      "epoch: 8 step: 1655, loss is 0.0007031665300019085\n",
      "epoch: 8 step: 1656, loss is 0.027100903913378716\n",
      "epoch: 8 step: 1657, loss is 0.00016790759400464594\n",
      "epoch: 8 step: 1658, loss is 0.0015389012405648828\n",
      "epoch: 8 step: 1659, loss is 0.000947064720094204\n",
      "epoch: 8 step: 1660, loss is 0.0006320149404928088\n",
      "epoch: 8 step: 1661, loss is 0.007780811283737421\n",
      "epoch: 8 step: 1662, loss is 0.0005612912937067449\n",
      "epoch: 8 step: 1663, loss is 3.055531487916596e-05\n",
      "epoch: 8 step: 1664, loss is 0.0028455432038754225\n",
      "epoch: 8 step: 1665, loss is 0.02247314155101776\n",
      "epoch: 8 step: 1666, loss is 0.00018175653531216085\n",
      "epoch: 8 step: 1667, loss is 0.029379671439528465\n",
      "epoch: 8 step: 1668, loss is 0.014345964416861534\n",
      "epoch: 8 step: 1669, loss is 0.0006579926703125238\n",
      "epoch: 8 step: 1670, loss is 0.016825785860419273\n",
      "epoch: 8 step: 1671, loss is 0.03305875137448311\n",
      "epoch: 8 step: 1672, loss is 0.0016853971173986793\n",
      "epoch: 8 step: 1673, loss is 0.0005805733380839229\n",
      "epoch: 8 step: 1674, loss is 0.0004223619180265814\n",
      "epoch: 8 step: 1675, loss is 0.0009359234245494008\n",
      "epoch: 8 step: 1676, loss is 0.00033004660508595407\n",
      "epoch: 8 step: 1677, loss is 0.0007720927824266255\n",
      "epoch: 8 step: 1678, loss is 0.004499782808125019\n",
      "epoch: 8 step: 1679, loss is 0.0031201669480651617\n",
      "epoch: 8 step: 1680, loss is 0.00037224299740046263\n",
      "epoch: 8 step: 1681, loss is 0.1007203757762909\n",
      "epoch: 8 step: 1682, loss is 0.00012121713371016085\n",
      "epoch: 8 step: 1683, loss is 0.004473966080695391\n",
      "epoch: 8 step: 1684, loss is 0.0006476432899944484\n",
      "epoch: 8 step: 1685, loss is 0.022297868505120277\n",
      "epoch: 8 step: 1686, loss is 0.0010829465463757515\n",
      "epoch: 8 step: 1687, loss is 0.00030826713191345334\n",
      "epoch: 8 step: 1688, loss is 0.011956721544265747\n",
      "epoch: 8 step: 1689, loss is 0.0010966500267386436\n",
      "epoch: 8 step: 1690, loss is 6.87620704411529e-05\n",
      "epoch: 8 step: 1691, loss is 0.00031406464404426515\n",
      "epoch: 8 step: 1692, loss is 4.628960232366808e-05\n",
      "epoch: 8 step: 1693, loss is 0.0008075450314208865\n",
      "epoch: 8 step: 1694, loss is 0.010134397074580193\n",
      "epoch: 8 step: 1695, loss is 0.00013723813754040748\n",
      "epoch: 8 step: 1696, loss is 0.07038252055644989\n",
      "epoch: 8 step: 1697, loss is 2.3024284018902108e-05\n",
      "epoch: 8 step: 1698, loss is 0.06323160976171494\n",
      "epoch: 8 step: 1699, loss is 0.002921550301834941\n",
      "epoch: 8 step: 1700, loss is 0.0019134476315230131\n",
      "epoch: 8 step: 1701, loss is 0.00013980481890030205\n",
      "epoch: 8 step: 1702, loss is 0.00922941043972969\n",
      "epoch: 8 step: 1703, loss is 0.004610050935298204\n",
      "epoch: 8 step: 1704, loss is 0.00045698281610384583\n",
      "epoch: 8 step: 1705, loss is 0.014931811019778252\n",
      "epoch: 8 step: 1706, loss is 0.0003730747848749161\n",
      "epoch: 8 step: 1707, loss is 0.0495419055223465\n",
      "epoch: 8 step: 1708, loss is 7.7554774179589e-05\n",
      "epoch: 8 step: 1709, loss is 0.0020424090325832367\n",
      "epoch: 8 step: 1710, loss is 0.005432572215795517\n",
      "epoch: 8 step: 1711, loss is 0.028637124225497246\n",
      "epoch: 8 step: 1712, loss is 0.002524999435991049\n",
      "epoch: 8 step: 1713, loss is 0.0001602147676749155\n",
      "epoch: 8 step: 1714, loss is 0.08539178967475891\n",
      "epoch: 8 step: 1715, loss is 3.1285224395105615e-05\n",
      "epoch: 8 step: 1716, loss is 0.0010117016499862075\n",
      "epoch: 8 step: 1717, loss is 0.03640326112508774\n",
      "epoch: 8 step: 1718, loss is 0.00029782994533888996\n",
      "epoch: 8 step: 1719, loss is 0.11052250117063522\n",
      "epoch: 8 step: 1720, loss is 0.0003532058035489172\n",
      "epoch: 8 step: 1721, loss is 0.10618650168180466\n",
      "epoch: 8 step: 1722, loss is 0.0001356977882096544\n",
      "epoch: 8 step: 1723, loss is 0.0008687471854500473\n",
      "epoch: 8 step: 1724, loss is 0.009215861558914185\n",
      "epoch: 8 step: 1725, loss is 0.0019813431426882744\n",
      "epoch: 8 step: 1726, loss is 0.0008630739757791162\n",
      "epoch: 8 step: 1727, loss is 0.012150564230978489\n",
      "epoch: 8 step: 1728, loss is 0.0040926504880189896\n",
      "epoch: 8 step: 1729, loss is 0.0007132923929020762\n",
      "epoch: 8 step: 1730, loss is 0.05444546788930893\n",
      "epoch: 8 step: 1731, loss is 0.006969355046749115\n",
      "epoch: 8 step: 1732, loss is 0.0300417710095644\n",
      "epoch: 8 step: 1733, loss is 0.00419932184740901\n",
      "epoch: 8 step: 1734, loss is 0.004613989964127541\n",
      "epoch: 8 step: 1735, loss is 0.017954465001821518\n",
      "epoch: 8 step: 1736, loss is 0.010734859853982925\n",
      "epoch: 8 step: 1737, loss is 0.056947674602270126\n",
      "epoch: 8 step: 1738, loss is 0.021044153720140457\n",
      "epoch: 8 step: 1739, loss is 0.006800140254199505\n",
      "epoch: 8 step: 1740, loss is 0.021695248782634735\n",
      "epoch: 8 step: 1741, loss is 9.713718463899568e-05\n",
      "epoch: 8 step: 1742, loss is 0.3248177766799927\n",
      "epoch: 8 step: 1743, loss is 0.0031283798161894083\n",
      "epoch: 8 step: 1744, loss is 0.0008289385586977005\n",
      "epoch: 8 step: 1745, loss is 0.002304916037246585\n",
      "epoch: 8 step: 1746, loss is 0.0009511485695838928\n",
      "epoch: 8 step: 1747, loss is 0.0004763978940900415\n",
      "epoch: 8 step: 1748, loss is 0.034221503883600235\n",
      "epoch: 8 step: 1749, loss is 0.005061146803200245\n",
      "epoch: 8 step: 1750, loss is 0.003555142553523183\n",
      "epoch: 8 step: 1751, loss is 0.0026750131510198116\n",
      "epoch: 8 step: 1752, loss is 0.0003514316922519356\n",
      "epoch: 8 step: 1753, loss is 0.030836554244160652\n",
      "epoch: 8 step: 1754, loss is 0.0025862802285701036\n",
      "epoch: 8 step: 1755, loss is 9.421008144272491e-05\n",
      "epoch: 8 step: 1756, loss is 0.0013233307981863618\n",
      "epoch: 8 step: 1757, loss is 0.000295839156024158\n",
      "epoch: 8 step: 1758, loss is 0.0017123358557000756\n",
      "epoch: 8 step: 1759, loss is 0.09671897441148758\n",
      "epoch: 8 step: 1760, loss is 0.00061296351486817\n",
      "epoch: 8 step: 1761, loss is 0.0003455768746789545\n",
      "epoch: 8 step: 1762, loss is 0.05285295099020004\n",
      "epoch: 8 step: 1763, loss is 0.10401734709739685\n",
      "epoch: 8 step: 1764, loss is 0.0001652274077059701\n",
      "epoch: 8 step: 1765, loss is 0.0022904968354851007\n",
      "epoch: 8 step: 1766, loss is 0.021147264167666435\n",
      "epoch: 8 step: 1767, loss is 0.21117089688777924\n",
      "epoch: 8 step: 1768, loss is 0.0015035487012937665\n",
      "epoch: 8 step: 1769, loss is 0.005011928267776966\n",
      "epoch: 8 step: 1770, loss is 0.1818486750125885\n",
      "epoch: 8 step: 1771, loss is 0.005189917050302029\n",
      "epoch: 8 step: 1772, loss is 0.0005404844414442778\n",
      "epoch: 8 step: 1773, loss is 0.0008017828222364187\n",
      "epoch: 8 step: 1774, loss is 0.0005297361640259624\n",
      "epoch: 8 step: 1775, loss is 0.00013204329297877848\n",
      "epoch: 8 step: 1776, loss is 0.0005796659388579428\n",
      "epoch: 8 step: 1777, loss is 0.0213761068880558\n",
      "epoch: 8 step: 1778, loss is 0.0136178620159626\n",
      "epoch: 8 step: 1779, loss is 0.024053551256656647\n",
      "epoch: 8 step: 1780, loss is 0.0002387489512329921\n",
      "epoch: 8 step: 1781, loss is 0.0008953978540375829\n",
      "epoch: 8 step: 1782, loss is 0.0018381972331553698\n",
      "epoch: 8 step: 1783, loss is 0.04900981858372688\n",
      "epoch: 8 step: 1784, loss is 0.0009631324792280793\n",
      "epoch: 8 step: 1785, loss is 0.03210534155368805\n",
      "epoch: 8 step: 1786, loss is 0.0897270068526268\n",
      "epoch: 8 step: 1787, loss is 0.007902977056801319\n",
      "epoch: 8 step: 1788, loss is 0.024569563567638397\n",
      "epoch: 8 step: 1789, loss is 0.00011224898480577394\n",
      "epoch: 8 step: 1790, loss is 0.0010950156720355153\n",
      "epoch: 8 step: 1791, loss is 0.0011332371504977345\n",
      "epoch: 8 step: 1792, loss is 0.0001565909042255953\n",
      "epoch: 8 step: 1793, loss is 0.0038407896645367146\n",
      "epoch: 8 step: 1794, loss is 0.000214238534681499\n",
      "epoch: 8 step: 1795, loss is 0.004580950364470482\n",
      "epoch: 8 step: 1796, loss is 0.00014460802776739\n",
      "epoch: 8 step: 1797, loss is 0.008147463202476501\n",
      "epoch: 8 step: 1798, loss is 7.417819142574444e-05\n",
      "epoch: 8 step: 1799, loss is 0.0015735117485746741\n",
      "epoch: 8 step: 1800, loss is 0.03442661464214325\n",
      "epoch: 8 step: 1801, loss is 0.000799169356469065\n",
      "epoch: 8 step: 1802, loss is 0.00304733170196414\n",
      "epoch: 8 step: 1803, loss is 0.00014120589185040444\n",
      "epoch: 8 step: 1804, loss is 0.04009460285305977\n",
      "epoch: 8 step: 1805, loss is 0.11073119938373566\n",
      "epoch: 8 step: 1806, loss is 0.04853048920631409\n",
      "epoch: 8 step: 1807, loss is 0.00014395167818292975\n",
      "epoch: 8 step: 1808, loss is 0.00044798257295042276\n",
      "epoch: 8 step: 1809, loss is 5.277295349515043e-05\n",
      "epoch: 8 step: 1810, loss is 0.008603768423199654\n",
      "epoch: 8 step: 1811, loss is 0.00018201902275905013\n",
      "epoch: 8 step: 1812, loss is 0.01241249218583107\n",
      "epoch: 8 step: 1813, loss is 0.028530946001410484\n",
      "epoch: 8 step: 1814, loss is 0.0011856979690492153\n",
      "epoch: 8 step: 1815, loss is 0.016077600419521332\n",
      "epoch: 8 step: 1816, loss is 0.00021371465118136257\n",
      "epoch: 8 step: 1817, loss is 0.01793927326798439\n",
      "epoch: 8 step: 1818, loss is 0.00011856685887323692\n",
      "epoch: 8 step: 1819, loss is 0.0010321818990632892\n",
      "epoch: 8 step: 1820, loss is 0.00014747036038897932\n",
      "epoch: 8 step: 1821, loss is 0.0002928089816123247\n",
      "epoch: 8 step: 1822, loss is 0.0003550187684595585\n",
      "epoch: 8 step: 1823, loss is 0.02060883305966854\n",
      "epoch: 8 step: 1824, loss is 0.004936397075653076\n",
      "epoch: 8 step: 1825, loss is 0.0010025078663602471\n",
      "epoch: 8 step: 1826, loss is 0.10435754060745239\n",
      "epoch: 8 step: 1827, loss is 0.04905609041452408\n",
      "epoch: 8 step: 1828, loss is 0.004214567597955465\n",
      "epoch: 8 step: 1829, loss is 0.0299240555614233\n",
      "epoch: 8 step: 1830, loss is 0.00010664494038792327\n",
      "epoch: 8 step: 1831, loss is 0.0005123373703099787\n",
      "epoch: 8 step: 1832, loss is 0.003916514106094837\n",
      "epoch: 8 step: 1833, loss is 0.06476067751646042\n",
      "epoch: 8 step: 1834, loss is 0.10094195604324341\n",
      "epoch: 8 step: 1835, loss is 0.0007130402373149991\n",
      "epoch: 8 step: 1836, loss is 0.014064788818359375\n",
      "epoch: 8 step: 1837, loss is 0.22026865184307098\n",
      "epoch: 8 step: 1838, loss is 0.00019286698079667985\n",
      "epoch: 8 step: 1839, loss is 0.0005494278739206493\n",
      "epoch: 8 step: 1840, loss is 0.00037507808883674443\n",
      "epoch: 8 step: 1841, loss is 0.0006261716480366886\n",
      "epoch: 8 step: 1842, loss is 0.03426024317741394\n",
      "epoch: 8 step: 1843, loss is 0.0005580598954111338\n",
      "epoch: 8 step: 1844, loss is 0.00495939701795578\n",
      "epoch: 8 step: 1845, loss is 0.017781900241971016\n",
      "epoch: 8 step: 1846, loss is 0.014917419292032719\n",
      "epoch: 8 step: 1847, loss is 0.0005324643570929766\n",
      "epoch: 8 step: 1848, loss is 0.06134844198822975\n",
      "epoch: 8 step: 1849, loss is 0.015514155849814415\n",
      "epoch: 8 step: 1850, loss is 0.0005354353343136609\n",
      "epoch: 8 step: 1851, loss is 0.0979631096124649\n",
      "epoch: 8 step: 1852, loss is 0.00019561307271942496\n",
      "epoch: 8 step: 1853, loss is 0.047143060714006424\n",
      "epoch: 8 step: 1854, loss is 0.007133196573704481\n",
      "epoch: 8 step: 1855, loss is 0.003484465414658189\n",
      "epoch: 8 step: 1856, loss is 0.009305919520556927\n",
      "epoch: 8 step: 1857, loss is 0.0009615104645490646\n",
      "epoch: 8 step: 1858, loss is 0.0006667683483101428\n",
      "epoch: 8 step: 1859, loss is 0.0042753745801746845\n",
      "epoch: 8 step: 1860, loss is 0.004203969147056341\n",
      "epoch: 8 step: 1861, loss is 0.05268492549657822\n",
      "epoch: 8 step: 1862, loss is 0.11268608272075653\n",
      "epoch: 8 step: 1863, loss is 0.027281401678919792\n",
      "epoch: 8 step: 1864, loss is 0.40643084049224854\n",
      "epoch: 8 step: 1865, loss is 0.015061648562550545\n",
      "epoch: 8 step: 1866, loss is 0.002148239640519023\n",
      "epoch: 8 step: 1867, loss is 0.0010959835490211844\n",
      "epoch: 8 step: 1868, loss is 0.0011583729647099972\n",
      "epoch: 8 step: 1869, loss is 0.0015541911125183105\n",
      "epoch: 8 step: 1870, loss is 0.025297440588474274\n",
      "epoch: 8 step: 1871, loss is 0.032295845448970795\n",
      "epoch: 8 step: 1872, loss is 0.04339710623025894\n",
      "epoch: 8 step: 1873, loss is 0.017128178849816322\n",
      "epoch: 8 step: 1874, loss is 0.00433054706081748\n",
      "epoch: 8 step: 1875, loss is 0.011211331933736801\n",
      "Train epoch time: 16206.648 ms, per step time: 8.644 ms\n",
      "epoch: 9 step: 1, loss is 0.00021740811644122005\n",
      "epoch: 9 step: 2, loss is 0.003156850812956691\n",
      "epoch: 9 step: 3, loss is 0.018343014642596245\n",
      "epoch: 9 step: 4, loss is 0.008949932642281055\n",
      "epoch: 9 step: 5, loss is 0.006651283707469702\n",
      "epoch: 9 step: 6, loss is 0.03274453058838844\n",
      "epoch: 9 step: 7, loss is 0.003609277540817857\n",
      "epoch: 9 step: 8, loss is 0.007713961880654097\n",
      "epoch: 9 step: 9, loss is 0.043135885149240494\n",
      "epoch: 9 step: 10, loss is 0.017810584977269173\n",
      "epoch: 9 step: 11, loss is 0.007438682019710541\n",
      "epoch: 9 step: 12, loss is 0.002049167873337865\n",
      "epoch: 9 step: 13, loss is 0.020263731479644775\n",
      "epoch: 9 step: 14, loss is 0.0026371467392891645\n",
      "epoch: 9 step: 15, loss is 0.01668507233262062\n",
      "epoch: 9 step: 16, loss is 0.0007911981665529311\n",
      "epoch: 9 step: 17, loss is 0.00014297636516857892\n",
      "epoch: 9 step: 18, loss is 0.003369393525645137\n",
      "epoch: 9 step: 19, loss is 0.01973281055688858\n",
      "epoch: 9 step: 20, loss is 0.0003006474580615759\n",
      "epoch: 9 step: 21, loss is 0.00024833239149302244\n",
      "epoch: 9 step: 22, loss is 0.008723980747163296\n",
      "epoch: 9 step: 23, loss is 0.0001354281121166423\n",
      "epoch: 9 step: 24, loss is 0.006306174676865339\n",
      "epoch: 9 step: 25, loss is 0.004957378376275301\n",
      "epoch: 9 step: 26, loss is 0.009933684952557087\n",
      "epoch: 9 step: 27, loss is 0.017508404329419136\n",
      "epoch: 9 step: 28, loss is 0.004783733282238245\n",
      "epoch: 9 step: 29, loss is 0.024228869006037712\n",
      "epoch: 9 step: 30, loss is 0.0006753327907063067\n",
      "epoch: 9 step: 31, loss is 0.008807164616882801\n",
      "epoch: 9 step: 32, loss is 0.0009397593094035983\n",
      "epoch: 9 step: 33, loss is 2.14779738598736e-05\n",
      "epoch: 9 step: 34, loss is 0.00012427727051544935\n",
      "epoch: 9 step: 35, loss is 0.000312274438329041\n",
      "epoch: 9 step: 36, loss is 0.004216317553073168\n",
      "epoch: 9 step: 37, loss is 0.015223008580505848\n",
      "epoch: 9 step: 38, loss is 0.00963965617120266\n",
      "epoch: 9 step: 39, loss is 0.0023013639729470015\n",
      "epoch: 9 step: 40, loss is 0.03612470254302025\n",
      "epoch: 9 step: 41, loss is 0.0004254808882251382\n",
      "epoch: 9 step: 42, loss is 0.0071526626124978065\n",
      "epoch: 9 step: 43, loss is 0.0017469412414357066\n",
      "epoch: 9 step: 44, loss is 0.0036516250111162663\n",
      "epoch: 9 step: 45, loss is 0.00047283846652135253\n",
      "epoch: 9 step: 46, loss is 0.005564847029745579\n",
      "epoch: 9 step: 47, loss is 0.0047094617038965225\n",
      "epoch: 9 step: 48, loss is 0.0002275255974382162\n",
      "epoch: 9 step: 49, loss is 0.0009534372948110104\n",
      "epoch: 9 step: 50, loss is 3.395305247977376e-05\n",
      "epoch: 9 step: 51, loss is 0.00117662048432976\n",
      "epoch: 9 step: 52, loss is 6.222401862032712e-05\n",
      "epoch: 9 step: 53, loss is 0.005122683942317963\n",
      "epoch: 9 step: 54, loss is 9.021314326673746e-05\n",
      "epoch: 9 step: 55, loss is 0.0005070668994449079\n",
      "epoch: 9 step: 56, loss is 3.639468559413217e-05\n",
      "epoch: 9 step: 57, loss is 0.0021553300321102142\n",
      "epoch: 9 step: 58, loss is 0.0017644783947616816\n",
      "epoch: 9 step: 59, loss is 0.00010046779061667621\n",
      "epoch: 9 step: 60, loss is 0.0011267352383583784\n",
      "epoch: 9 step: 61, loss is 0.0002501601120457053\n",
      "epoch: 9 step: 62, loss is 0.000300808169413358\n",
      "epoch: 9 step: 63, loss is 0.00034282883279956877\n",
      "epoch: 9 step: 64, loss is 3.860854121739976e-05\n",
      "epoch: 9 step: 65, loss is 0.002616647630929947\n",
      "epoch: 9 step: 66, loss is 0.00010944056703010574\n",
      "epoch: 9 step: 67, loss is 0.0003631002618931234\n",
      "epoch: 9 step: 68, loss is 0.00019840325694531202\n",
      "epoch: 9 step: 69, loss is 0.0003889720537699759\n",
      "epoch: 9 step: 70, loss is 0.0009350718464702368\n",
      "epoch: 9 step: 71, loss is 0.008554785512387753\n",
      "epoch: 9 step: 72, loss is 0.00041478630737401545\n",
      "epoch: 9 step: 73, loss is 0.0012653392041102052\n",
      "epoch: 9 step: 74, loss is 9.124039934249595e-05\n",
      "epoch: 9 step: 75, loss is 0.008550774306058884\n",
      "epoch: 9 step: 76, loss is 0.0023175880778580904\n",
      "epoch: 9 step: 77, loss is 0.00037259040982462466\n",
      "epoch: 9 step: 78, loss is 0.0006180060445331037\n",
      "epoch: 9 step: 79, loss is 0.00058783870190382\n",
      "epoch: 9 step: 80, loss is 1.7943842976819724e-05\n",
      "epoch: 9 step: 81, loss is 0.0001256474934052676\n",
      "epoch: 9 step: 82, loss is 0.026342332363128662\n",
      "epoch: 9 step: 83, loss is 0.00618706364184618\n",
      "epoch: 9 step: 84, loss is 0.008381431922316551\n",
      "epoch: 9 step: 85, loss is 0.0019030687399208546\n",
      "epoch: 9 step: 86, loss is 0.0014510162873193622\n",
      "epoch: 9 step: 87, loss is 0.00876355729997158\n",
      "epoch: 9 step: 88, loss is 0.0002101848367601633\n",
      "epoch: 9 step: 89, loss is 7.256211392814294e-05\n",
      "epoch: 9 step: 90, loss is 0.0002592688542790711\n",
      "epoch: 9 step: 91, loss is 0.042836207896471024\n",
      "epoch: 9 step: 92, loss is 0.0006196856265887618\n",
      "epoch: 9 step: 93, loss is 0.00932554341852665\n",
      "epoch: 9 step: 94, loss is 0.0009533780394122005\n",
      "epoch: 9 step: 95, loss is 0.021759092807769775\n",
      "epoch: 9 step: 96, loss is 0.04777075722813606\n",
      "epoch: 9 step: 97, loss is 0.028949182480573654\n",
      "epoch: 9 step: 98, loss is 0.00027624244103208184\n",
      "epoch: 9 step: 99, loss is 0.015178578905761242\n",
      "epoch: 9 step: 100, loss is 3.188381379004568e-05\n",
      "epoch: 9 step: 101, loss is 0.00014334634761326015\n",
      "epoch: 9 step: 102, loss is 0.000814007013104856\n",
      "epoch: 9 step: 103, loss is 2.7136684366269037e-05\n",
      "epoch: 9 step: 104, loss is 0.0037568581756204367\n",
      "epoch: 9 step: 105, loss is 0.0038070527371019125\n",
      "epoch: 9 step: 106, loss is 0.0012682558735832572\n",
      "epoch: 9 step: 107, loss is 0.0004882871289737523\n",
      "epoch: 9 step: 108, loss is 0.0003092822153121233\n",
      "epoch: 9 step: 109, loss is 0.0042871637269854546\n",
      "epoch: 9 step: 110, loss is 2.6770419935928658e-05\n",
      "epoch: 9 step: 111, loss is 0.015341026708483696\n",
      "epoch: 9 step: 112, loss is 2.9760605684714392e-05\n",
      "epoch: 9 step: 113, loss is 0.007168455049395561\n",
      "epoch: 9 step: 114, loss is 0.0026000880170613527\n",
      "epoch: 9 step: 115, loss is 0.00035442080115899444\n",
      "epoch: 9 step: 116, loss is 0.09454319626092911\n",
      "epoch: 9 step: 117, loss is 8.555958629585803e-05\n",
      "epoch: 9 step: 118, loss is 4.735977927339263e-05\n",
      "epoch: 9 step: 119, loss is 7.254823867697269e-05\n",
      "epoch: 9 step: 120, loss is 0.00574661698192358\n",
      "epoch: 9 step: 121, loss is 0.0022834292612969875\n",
      "epoch: 9 step: 122, loss is 0.04530298337340355\n",
      "epoch: 9 step: 123, loss is 1.1198808351764455e-05\n",
      "epoch: 9 step: 124, loss is 0.0177262332290411\n",
      "epoch: 9 step: 125, loss is 0.027736902236938477\n",
      "epoch: 9 step: 126, loss is 2.8936899980180897e-05\n",
      "epoch: 9 step: 127, loss is 0.0004188237071502954\n",
      "epoch: 9 step: 128, loss is 0.0010973417665809393\n",
      "epoch: 9 step: 129, loss is 0.0008760862401686609\n",
      "epoch: 9 step: 130, loss is 0.0011519180843606591\n",
      "epoch: 9 step: 131, loss is 0.0004948984133079648\n",
      "epoch: 9 step: 132, loss is 0.00012914722901768982\n",
      "epoch: 9 step: 133, loss is 0.0007192385965026915\n",
      "epoch: 9 step: 134, loss is 0.0004586431314237416\n",
      "epoch: 9 step: 135, loss is 8.874935156200081e-05\n",
      "epoch: 9 step: 136, loss is 0.057234637439250946\n",
      "epoch: 9 step: 137, loss is 0.00011294244177406654\n",
      "epoch: 9 step: 138, loss is 0.00011812002776423469\n",
      "epoch: 9 step: 139, loss is 0.0029271638486534357\n",
      "epoch: 9 step: 140, loss is 5.242324550636113e-05\n",
      "epoch: 9 step: 141, loss is 0.0018210930284112692\n",
      "epoch: 9 step: 142, loss is 0.0049027977511286736\n",
      "epoch: 9 step: 143, loss is 0.13598449528217316\n",
      "epoch: 9 step: 144, loss is 0.07381293922662735\n",
      "epoch: 9 step: 145, loss is 0.0013038377510383725\n",
      "epoch: 9 step: 146, loss is 0.00013492339348886162\n",
      "epoch: 9 step: 147, loss is 0.0002698369789868593\n",
      "epoch: 9 step: 148, loss is 0.00011591147631406784\n",
      "epoch: 9 step: 149, loss is 0.034356970340013504\n",
      "epoch: 9 step: 150, loss is 0.0002496484958101064\n",
      "epoch: 9 step: 151, loss is 0.00040943684871308506\n",
      "epoch: 9 step: 152, loss is 0.0005279284087009728\n",
      "epoch: 9 step: 153, loss is 0.013342808000743389\n",
      "epoch: 9 step: 154, loss is 0.037545040249824524\n",
      "epoch: 9 step: 155, loss is 0.01613878645002842\n",
      "epoch: 9 step: 156, loss is 2.4682127332198434e-05\n",
      "epoch: 9 step: 157, loss is 8.66284899530001e-05\n",
      "epoch: 9 step: 158, loss is 0.00030890709604136646\n",
      "epoch: 9 step: 159, loss is 0.007376768160611391\n",
      "epoch: 9 step: 160, loss is 0.06802063435316086\n",
      "epoch: 9 step: 161, loss is 0.0008286289521493018\n",
      "epoch: 9 step: 162, loss is 0.01963934861123562\n",
      "epoch: 9 step: 163, loss is 0.0011796439066529274\n",
      "epoch: 9 step: 164, loss is 8.902051922632381e-05\n",
      "epoch: 9 step: 165, loss is 0.003444971516728401\n",
      "epoch: 9 step: 166, loss is 0.0002009428571909666\n",
      "epoch: 9 step: 167, loss is 0.00043545369408093393\n",
      "epoch: 9 step: 168, loss is 0.0003771378833334893\n",
      "epoch: 9 step: 169, loss is 9.586353553459048e-05\n",
      "epoch: 9 step: 170, loss is 0.004004701040685177\n",
      "epoch: 9 step: 171, loss is 0.00014697281585540622\n",
      "epoch: 9 step: 172, loss is 0.05884462594985962\n",
      "epoch: 9 step: 173, loss is 0.005914803594350815\n",
      "epoch: 9 step: 174, loss is 0.0007362395408563316\n",
      "epoch: 9 step: 175, loss is 0.001702053239569068\n",
      "epoch: 9 step: 176, loss is 0.00039889069739729166\n",
      "epoch: 9 step: 177, loss is 0.0006097484147176147\n",
      "epoch: 9 step: 178, loss is 0.0007630528998561203\n",
      "epoch: 9 step: 179, loss is 0.0020359004847705364\n",
      "epoch: 9 step: 180, loss is 0.0031116849277168512\n",
      "epoch: 9 step: 181, loss is 0.031657375395298004\n",
      "epoch: 9 step: 182, loss is 0.029291445389389992\n",
      "epoch: 9 step: 183, loss is 0.06342541426420212\n",
      "epoch: 9 step: 184, loss is 0.0015928633511066437\n",
      "epoch: 9 step: 185, loss is 0.005385729484260082\n",
      "epoch: 9 step: 186, loss is 0.00011290657857898623\n",
      "epoch: 9 step: 187, loss is 0.0671963170170784\n",
      "epoch: 9 step: 188, loss is 0.00783437117934227\n",
      "epoch: 9 step: 189, loss is 0.0028715424705296755\n",
      "epoch: 9 step: 190, loss is 0.015005452558398247\n",
      "epoch: 9 step: 191, loss is 6.054613186279312e-05\n",
      "epoch: 9 step: 192, loss is 0.05324515700340271\n",
      "epoch: 9 step: 193, loss is 0.006982369348406792\n",
      "epoch: 9 step: 194, loss is 0.0009581036283634603\n",
      "epoch: 9 step: 195, loss is 0.0001246560423169285\n",
      "epoch: 9 step: 196, loss is 0.0008431959431618452\n",
      "epoch: 9 step: 197, loss is 0.00011391995212761685\n",
      "epoch: 9 step: 198, loss is 0.0033528227359056473\n",
      "epoch: 9 step: 199, loss is 0.018184645101428032\n",
      "epoch: 9 step: 200, loss is 0.0024480284191668034\n",
      "epoch: 9 step: 201, loss is 0.0029648742638528347\n",
      "epoch: 9 step: 202, loss is 0.060671497136354446\n",
      "epoch: 9 step: 203, loss is 0.013162107206881046\n",
      "epoch: 9 step: 204, loss is 0.06121981143951416\n",
      "epoch: 9 step: 205, loss is 3.44719119311776e-05\n",
      "epoch: 9 step: 206, loss is 0.09598896652460098\n",
      "epoch: 9 step: 207, loss is 0.00284564308822155\n",
      "epoch: 9 step: 208, loss is 0.00590047100558877\n",
      "epoch: 9 step: 209, loss is 0.023860255256295204\n",
      "epoch: 9 step: 210, loss is 2.5965766781155253e-06\n",
      "epoch: 9 step: 211, loss is 5.8861947763944045e-05\n",
      "epoch: 9 step: 212, loss is 0.0005416712374426425\n",
      "epoch: 9 step: 213, loss is 0.0002815569459926337\n",
      "epoch: 9 step: 214, loss is 0.0020457105711102486\n",
      "epoch: 9 step: 215, loss is 0.00018795720825437456\n",
      "epoch: 9 step: 216, loss is 0.00010857634333660826\n",
      "epoch: 9 step: 217, loss is 9.987787780119106e-05\n",
      "epoch: 9 step: 218, loss is 0.00018412593635730445\n",
      "epoch: 9 step: 219, loss is 0.000263778812950477\n",
      "epoch: 9 step: 220, loss is 0.004845978692173958\n",
      "epoch: 9 step: 221, loss is 0.00016178459918592125\n",
      "epoch: 9 step: 222, loss is 0.0001414010621374473\n",
      "epoch: 9 step: 223, loss is 0.0034693533089011908\n",
      "epoch: 9 step: 224, loss is 0.03897620737552643\n",
      "epoch: 9 step: 225, loss is 0.004116262309253216\n",
      "epoch: 9 step: 226, loss is 0.004560795612633228\n",
      "epoch: 9 step: 227, loss is 0.0022373520769178867\n",
      "epoch: 9 step: 228, loss is 0.01135776937007904\n",
      "epoch: 9 step: 229, loss is 0.00275320652872324\n",
      "epoch: 9 step: 230, loss is 0.02807464264333248\n",
      "epoch: 9 step: 231, loss is 5.550875357585028e-05\n",
      "epoch: 9 step: 232, loss is 0.039458949118852615\n",
      "epoch: 9 step: 233, loss is 0.017108866944909096\n",
      "epoch: 9 step: 234, loss is 0.0018432056531310081\n",
      "epoch: 9 step: 235, loss is 0.03692386671900749\n",
      "epoch: 9 step: 236, loss is 0.005972377955913544\n",
      "epoch: 9 step: 237, loss is 0.08280842006206512\n",
      "epoch: 9 step: 238, loss is 0.019940104335546494\n",
      "epoch: 9 step: 239, loss is 1.1194995749974623e-05\n",
      "epoch: 9 step: 240, loss is 0.0026493428740650415\n",
      "epoch: 9 step: 241, loss is 0.0014593882951885462\n",
      "epoch: 9 step: 242, loss is 6.400578422471881e-05\n",
      "epoch: 9 step: 243, loss is 0.0002484187134541571\n",
      "epoch: 9 step: 244, loss is 0.005750300828367472\n",
      "epoch: 9 step: 245, loss is 4.704141247202642e-05\n",
      "epoch: 9 step: 246, loss is 0.026325825601816177\n",
      "epoch: 9 step: 247, loss is 0.020520444959402084\n",
      "epoch: 9 step: 248, loss is 0.0012384448200464249\n",
      "epoch: 9 step: 249, loss is 0.04389379173517227\n",
      "epoch: 9 step: 250, loss is 0.00023002158559393138\n",
      "epoch: 9 step: 251, loss is 0.0432635173201561\n",
      "epoch: 9 step: 252, loss is 0.0005253629060462117\n",
      "epoch: 9 step: 253, loss is 3.809457120951265e-05\n",
      "epoch: 9 step: 254, loss is 0.00277595198713243\n",
      "epoch: 9 step: 255, loss is 0.0010615584906190634\n",
      "epoch: 9 step: 256, loss is 0.0008933763019740582\n",
      "epoch: 9 step: 257, loss is 0.012864907272160053\n",
      "epoch: 9 step: 258, loss is 0.0023834819439798594\n",
      "epoch: 9 step: 259, loss is 0.01305627916008234\n",
      "epoch: 9 step: 260, loss is 0.00015888958296272904\n",
      "epoch: 9 step: 261, loss is 0.0011772160651162267\n",
      "epoch: 9 step: 262, loss is 0.00014469833695329726\n",
      "epoch: 9 step: 263, loss is 0.0014480712125077844\n",
      "epoch: 9 step: 264, loss is 1.2938612599100452e-05\n",
      "epoch: 9 step: 265, loss is 0.00015985601930879056\n",
      "epoch: 9 step: 266, loss is 0.05473117530345917\n",
      "epoch: 9 step: 267, loss is 0.0003620453644543886\n",
      "epoch: 9 step: 268, loss is 0.014703990891575813\n",
      "epoch: 9 step: 269, loss is 0.0009335383074358106\n",
      "epoch: 9 step: 270, loss is 0.009136265143752098\n",
      "epoch: 9 step: 271, loss is 1.4389913303602953e-05\n",
      "epoch: 9 step: 272, loss is 0.02566361613571644\n",
      "epoch: 9 step: 273, loss is 0.00011590659414650872\n",
      "epoch: 9 step: 274, loss is 0.02723764441907406\n",
      "epoch: 9 step: 275, loss is 0.0004808409430552274\n",
      "epoch: 9 step: 276, loss is 0.00022863573394715786\n",
      "epoch: 9 step: 277, loss is 6.672452855127631e-06\n",
      "epoch: 9 step: 278, loss is 0.00024467622279189527\n",
      "epoch: 9 step: 279, loss is 0.007164778187870979\n",
      "epoch: 9 step: 280, loss is 9.79745527729392e-05\n",
      "epoch: 9 step: 281, loss is 0.002712986897677183\n",
      "epoch: 9 step: 282, loss is 0.0007651959895156324\n",
      "epoch: 9 step: 283, loss is 0.12946465611457825\n",
      "epoch: 9 step: 284, loss is 0.02393464930355549\n",
      "epoch: 9 step: 285, loss is 0.006825373042374849\n",
      "epoch: 9 step: 286, loss is 0.00010156405187444761\n",
      "epoch: 9 step: 287, loss is 0.08008349686861038\n",
      "epoch: 9 step: 288, loss is 6.528701487695798e-05\n",
      "epoch: 9 step: 289, loss is 2.656481410667766e-05\n",
      "epoch: 9 step: 290, loss is 0.021251631900668144\n",
      "epoch: 9 step: 291, loss is 0.02320018596947193\n",
      "epoch: 9 step: 292, loss is 0.0030134685803204775\n",
      "epoch: 9 step: 293, loss is 0.0002648413646966219\n",
      "epoch: 9 step: 294, loss is 0.000990366912446916\n",
      "epoch: 9 step: 295, loss is 0.009512473829090595\n",
      "epoch: 9 step: 296, loss is 0.05877215042710304\n",
      "epoch: 9 step: 297, loss is 0.0005380975781008601\n",
      "epoch: 9 step: 298, loss is 0.010840866714715958\n",
      "epoch: 9 step: 299, loss is 1.7675858543952927e-05\n",
      "epoch: 9 step: 300, loss is 3.8309870433295146e-05\n",
      "epoch: 9 step: 301, loss is 0.013714040629565716\n",
      "epoch: 9 step: 302, loss is 0.0009537350852042437\n",
      "epoch: 9 step: 303, loss is 0.013239337131381035\n",
      "epoch: 9 step: 304, loss is 0.00021773706248495728\n",
      "epoch: 9 step: 305, loss is 0.00904116127640009\n",
      "epoch: 9 step: 306, loss is 0.0077040670439600945\n",
      "epoch: 9 step: 307, loss is 0.004580401815474033\n",
      "epoch: 9 step: 308, loss is 0.16581080853939056\n",
      "epoch: 9 step: 309, loss is 0.06954946368932724\n",
      "epoch: 9 step: 310, loss is 0.0038386061787605286\n",
      "epoch: 9 step: 311, loss is 0.006055014207959175\n",
      "epoch: 9 step: 312, loss is 0.0014499268727377057\n",
      "epoch: 9 step: 313, loss is 9.24839114304632e-05\n",
      "epoch: 9 step: 314, loss is 0.00791816134005785\n",
      "epoch: 9 step: 315, loss is 0.0005947200697846711\n",
      "epoch: 9 step: 316, loss is 6.232798477867618e-05\n",
      "epoch: 9 step: 317, loss is 0.0006737987860105932\n",
      "epoch: 9 step: 318, loss is 0.00040650489972904325\n",
      "epoch: 9 step: 319, loss is 3.327936792629771e-05\n",
      "epoch: 9 step: 320, loss is 0.0010264398297294974\n",
      "epoch: 9 step: 321, loss is 0.0001938582572620362\n",
      "epoch: 9 step: 322, loss is 0.024635253474116325\n",
      "epoch: 9 step: 323, loss is 0.0005424944101832807\n",
      "epoch: 9 step: 324, loss is 0.05519692599773407\n",
      "epoch: 9 step: 325, loss is 0.00034179206704720855\n",
      "epoch: 9 step: 326, loss is 0.022578103467822075\n",
      "epoch: 9 step: 327, loss is 0.0013975726906210184\n",
      "epoch: 9 step: 328, loss is 0.0035737156867980957\n",
      "epoch: 9 step: 329, loss is 0.001588535262271762\n",
      "epoch: 9 step: 330, loss is 0.0001430465345038101\n",
      "epoch: 9 step: 331, loss is 0.01192974392324686\n",
      "epoch: 9 step: 332, loss is 0.02787020616233349\n",
      "epoch: 9 step: 333, loss is 0.010234111919999123\n",
      "epoch: 9 step: 334, loss is 0.055659838020801544\n",
      "epoch: 9 step: 335, loss is 0.002771789440885186\n",
      "epoch: 9 step: 336, loss is 0.007043527904897928\n",
      "epoch: 9 step: 337, loss is 3.4211592719657347e-05\n",
      "epoch: 9 step: 338, loss is 5.615216650767252e-05\n",
      "epoch: 9 step: 339, loss is 0.00013724237214773893\n",
      "epoch: 9 step: 340, loss is 0.10549356788396835\n",
      "epoch: 9 step: 341, loss is 1.760873965395149e-05\n",
      "epoch: 9 step: 342, loss is 0.0012465298641473055\n",
      "epoch: 9 step: 343, loss is 0.013804680667817593\n",
      "epoch: 9 step: 344, loss is 0.00023602684086654335\n",
      "epoch: 9 step: 345, loss is 9.626186511013657e-05\n",
      "epoch: 9 step: 346, loss is 0.009041589684784412\n",
      "epoch: 9 step: 347, loss is 0.003047666745260358\n",
      "epoch: 9 step: 348, loss is 7.684790762141347e-05\n",
      "epoch: 9 step: 349, loss is 0.0014756785240024328\n",
      "epoch: 9 step: 350, loss is 0.0020990660414099693\n",
      "epoch: 9 step: 351, loss is 0.010908633470535278\n",
      "epoch: 9 step: 352, loss is 0.011819267645478249\n",
      "epoch: 9 step: 353, loss is 0.00034958735341206193\n",
      "epoch: 9 step: 354, loss is 0.13681122660636902\n",
      "epoch: 9 step: 355, loss is 0.0026045036502182484\n",
      "epoch: 9 step: 356, loss is 0.11307597160339355\n",
      "epoch: 9 step: 357, loss is 0.0011463729897513986\n",
      "epoch: 9 step: 358, loss is 0.013668369501829147\n",
      "epoch: 9 step: 359, loss is 0.003129296936094761\n",
      "epoch: 9 step: 360, loss is 0.0037153910379856825\n",
      "epoch: 9 step: 361, loss is 0.03853290155529976\n",
      "epoch: 9 step: 362, loss is 0.009058878757059574\n",
      "epoch: 9 step: 363, loss is 0.00563652440905571\n",
      "epoch: 9 step: 364, loss is 0.0011546446476131678\n",
      "epoch: 9 step: 365, loss is 0.175776407122612\n",
      "epoch: 9 step: 366, loss is 0.016359075903892517\n",
      "epoch: 9 step: 367, loss is 0.008870257996022701\n",
      "epoch: 9 step: 368, loss is 0.017624013125896454\n",
      "epoch: 9 step: 369, loss is 0.01573099195957184\n",
      "epoch: 9 step: 370, loss is 0.0029139085672795773\n",
      "epoch: 9 step: 371, loss is 0.03778050094842911\n",
      "epoch: 9 step: 372, loss is 0.0016426828224211931\n",
      "epoch: 9 step: 373, loss is 0.005296971183270216\n",
      "epoch: 9 step: 374, loss is 0.00015132089902181178\n",
      "epoch: 9 step: 375, loss is 3.0446259188465774e-05\n",
      "epoch: 9 step: 376, loss is 0.04165089875459671\n",
      "epoch: 9 step: 377, loss is 0.00010112609015777707\n",
      "epoch: 9 step: 378, loss is 0.056301891803741455\n",
      "epoch: 9 step: 379, loss is 0.00029618764529004693\n",
      "epoch: 9 step: 380, loss is 0.00580991106107831\n",
      "epoch: 9 step: 381, loss is 0.007574382703751326\n",
      "epoch: 9 step: 382, loss is 0.00039894040673971176\n",
      "epoch: 9 step: 383, loss is 1.024149969452992e-05\n",
      "epoch: 9 step: 384, loss is 0.002581512089818716\n",
      "epoch: 9 step: 385, loss is 0.003755392273887992\n",
      "epoch: 9 step: 386, loss is 0.003388080047443509\n",
      "epoch: 9 step: 387, loss is 0.008369749411940575\n",
      "epoch: 9 step: 388, loss is 0.00011866765271406621\n",
      "epoch: 9 step: 389, loss is 0.00021523878967855126\n",
      "epoch: 9 step: 390, loss is 0.007233411073684692\n",
      "epoch: 9 step: 391, loss is 1.6009513274184428e-05\n",
      "epoch: 9 step: 392, loss is 0.10068614035844803\n",
      "epoch: 9 step: 393, loss is 6.696987111354247e-05\n",
      "epoch: 9 step: 394, loss is 0.028582662343978882\n",
      "epoch: 9 step: 395, loss is 3.125313378404826e-05\n",
      "epoch: 9 step: 396, loss is 0.0004897406906820834\n",
      "epoch: 9 step: 397, loss is 0.01504470594227314\n",
      "epoch: 9 step: 398, loss is 0.027195528149604797\n",
      "epoch: 9 step: 399, loss is 0.2000252604484558\n",
      "epoch: 9 step: 400, loss is 0.008829501457512379\n",
      "epoch: 9 step: 401, loss is 0.0018490772927179933\n",
      "epoch: 9 step: 402, loss is 0.007498921360820532\n",
      "epoch: 9 step: 403, loss is 0.00035298988223075867\n",
      "epoch: 9 step: 404, loss is 0.004244051408022642\n",
      "epoch: 9 step: 405, loss is 4.32440428994596e-05\n",
      "epoch: 9 step: 406, loss is 0.005869481712579727\n",
      "epoch: 9 step: 407, loss is 0.0026725479401648045\n",
      "epoch: 9 step: 408, loss is 0.023811714723706245\n",
      "epoch: 9 step: 409, loss is 0.002437393181025982\n",
      "epoch: 9 step: 410, loss is 5.301534110913053e-05\n",
      "epoch: 9 step: 411, loss is 0.003295103320851922\n",
      "epoch: 9 step: 412, loss is 0.0004001242923550308\n",
      "epoch: 9 step: 413, loss is 0.00026662935852073133\n",
      "epoch: 9 step: 414, loss is 0.0002992829540744424\n",
      "epoch: 9 step: 415, loss is 0.0034430203959345818\n",
      "epoch: 9 step: 416, loss is 0.00022171466844156384\n",
      "epoch: 9 step: 417, loss is 0.0010662247659638524\n",
      "epoch: 9 step: 418, loss is 0.000201123533770442\n",
      "epoch: 9 step: 419, loss is 0.0006314024212770164\n",
      "epoch: 9 step: 420, loss is 0.0009541336912661791\n",
      "epoch: 9 step: 421, loss is 0.0001988187723327428\n",
      "epoch: 9 step: 422, loss is 0.0005282905185595155\n",
      "epoch: 9 step: 423, loss is 0.00039452454075217247\n",
      "epoch: 9 step: 424, loss is 0.03880102559924126\n",
      "epoch: 9 step: 425, loss is 0.0027885958552360535\n",
      "epoch: 9 step: 426, loss is 0.005893123336136341\n",
      "epoch: 9 step: 427, loss is 0.005065762437880039\n",
      "epoch: 9 step: 428, loss is 0.012606631964445114\n",
      "epoch: 9 step: 429, loss is 0.19280117750167847\n",
      "epoch: 9 step: 430, loss is 0.0008310143020935357\n",
      "epoch: 9 step: 431, loss is 0.002002614550292492\n",
      "epoch: 9 step: 432, loss is 0.025764351710677147\n",
      "epoch: 9 step: 433, loss is 0.0002630187082104385\n",
      "epoch: 9 step: 434, loss is 0.003572709858417511\n",
      "epoch: 9 step: 435, loss is 0.00013059985940344632\n",
      "epoch: 9 step: 436, loss is 0.0006637435872107744\n",
      "epoch: 9 step: 437, loss is 0.0006505115306936204\n",
      "epoch: 9 step: 438, loss is 0.00020100883557461202\n",
      "epoch: 9 step: 439, loss is 0.021708592772483826\n",
      "epoch: 9 step: 440, loss is 0.024039264768362045\n",
      "epoch: 9 step: 441, loss is 0.00109946820884943\n",
      "epoch: 9 step: 442, loss is 6.5011961851269e-05\n",
      "epoch: 9 step: 443, loss is 0.0189551692456007\n",
      "epoch: 9 step: 444, loss is 0.00040091402479447424\n",
      "epoch: 9 step: 445, loss is 0.0004186813021078706\n",
      "epoch: 9 step: 446, loss is 7.096635090420023e-05\n",
      "epoch: 9 step: 447, loss is 0.00012994247663300484\n",
      "epoch: 9 step: 448, loss is 0.014883177354931831\n",
      "epoch: 9 step: 449, loss is 0.19043561816215515\n",
      "epoch: 9 step: 450, loss is 0.00038446878897957504\n",
      "epoch: 9 step: 451, loss is 0.1353491246700287\n",
      "epoch: 9 step: 452, loss is 0.0023227278143167496\n",
      "epoch: 9 step: 453, loss is 0.004279605112969875\n",
      "epoch: 9 step: 454, loss is 0.026733528822660446\n",
      "epoch: 9 step: 455, loss is 0.019955381751060486\n",
      "epoch: 9 step: 456, loss is 0.002023258712142706\n",
      "epoch: 9 step: 457, loss is 0.00582562480121851\n",
      "epoch: 9 step: 458, loss is 0.047567687928676605\n",
      "epoch: 9 step: 459, loss is 0.0018304955447092652\n",
      "epoch: 9 step: 460, loss is 0.0006356705562211573\n",
      "epoch: 9 step: 461, loss is 0.0005119926063343883\n",
      "epoch: 9 step: 462, loss is 0.0005281126941554248\n",
      "epoch: 9 step: 463, loss is 0.0004282720619812608\n",
      "epoch: 9 step: 464, loss is 0.0015234265010803938\n",
      "epoch: 9 step: 465, loss is 6.594676233362406e-05\n",
      "epoch: 9 step: 466, loss is 0.018293842673301697\n",
      "epoch: 9 step: 467, loss is 0.004032195545732975\n",
      "epoch: 9 step: 468, loss is 0.000901352206710726\n",
      "epoch: 9 step: 469, loss is 0.008285951800644398\n",
      "epoch: 9 step: 470, loss is 0.0001455428428016603\n",
      "epoch: 9 step: 471, loss is 0.001427811337634921\n",
      "epoch: 9 step: 472, loss is 0.007049140520393848\n",
      "epoch: 9 step: 473, loss is 0.0006884108879603446\n",
      "epoch: 9 step: 474, loss is 0.005056596826761961\n",
      "epoch: 9 step: 475, loss is 0.013941842131316662\n",
      "epoch: 9 step: 476, loss is 0.014508605934679508\n",
      "epoch: 9 step: 477, loss is 0.0020884124096482992\n",
      "epoch: 9 step: 478, loss is 0.0032782256603240967\n",
      "epoch: 9 step: 479, loss is 0.00011193020327482373\n",
      "epoch: 9 step: 480, loss is 7.429498509736732e-05\n",
      "epoch: 9 step: 481, loss is 0.00022074973094277084\n",
      "epoch: 9 step: 482, loss is 0.00016871877596713603\n",
      "epoch: 9 step: 483, loss is 0.06688164919614792\n",
      "epoch: 9 step: 484, loss is 0.00045967669575475156\n",
      "epoch: 9 step: 485, loss is 0.0016546393744647503\n",
      "epoch: 9 step: 486, loss is 4.2659754399210215e-05\n",
      "epoch: 9 step: 487, loss is 0.0043824827298521996\n",
      "epoch: 9 step: 488, loss is 0.0013729125494137406\n",
      "epoch: 9 step: 489, loss is 4.900567000731826e-05\n",
      "epoch: 9 step: 490, loss is 0.01113663986325264\n",
      "epoch: 9 step: 491, loss is 2.68624207819812e-05\n",
      "epoch: 9 step: 492, loss is 0.0029098093509674072\n",
      "epoch: 9 step: 493, loss is 0.0009289499139413238\n",
      "epoch: 9 step: 494, loss is 0.0017453344771638513\n",
      "epoch: 9 step: 495, loss is 0.0003668938297778368\n",
      "epoch: 9 step: 496, loss is 0.04849136248230934\n",
      "epoch: 9 step: 497, loss is 6.166128878248855e-05\n",
      "epoch: 9 step: 498, loss is 0.02197256311774254\n",
      "epoch: 9 step: 499, loss is 0.001061771297827363\n",
      "epoch: 9 step: 500, loss is 0.0006108043598942459\n",
      "epoch: 9 step: 501, loss is 0.0001806390646379441\n",
      "epoch: 9 step: 502, loss is 0.001123699708841741\n",
      "epoch: 9 step: 503, loss is 9.687770943855867e-05\n",
      "epoch: 9 step: 504, loss is 0.0007827140507288277\n",
      "epoch: 9 step: 505, loss is 0.006239569745957851\n",
      "epoch: 9 step: 506, loss is 0.006883340422064066\n",
      "epoch: 9 step: 507, loss is 0.00011216594430152327\n",
      "epoch: 9 step: 508, loss is 0.086807020008564\n",
      "epoch: 9 step: 509, loss is 0.019734513014554977\n",
      "epoch: 9 step: 510, loss is 0.009646335616707802\n",
      "epoch: 9 step: 511, loss is 0.00014954665675759315\n",
      "epoch: 9 step: 512, loss is 0.001668784418143332\n",
      "epoch: 9 step: 513, loss is 0.00287299114279449\n",
      "epoch: 9 step: 514, loss is 0.0074285948649048805\n",
      "epoch: 9 step: 515, loss is 0.001517787459306419\n",
      "epoch: 9 step: 516, loss is 5.211093230172992e-05\n",
      "epoch: 9 step: 517, loss is 0.00011541238200152293\n",
      "epoch: 9 step: 518, loss is 0.0008027644944377244\n",
      "epoch: 9 step: 519, loss is 1.100498775485903e-05\n",
      "epoch: 9 step: 520, loss is 0.0011794870952144265\n",
      "epoch: 9 step: 521, loss is 0.23846082389354706\n",
      "epoch: 9 step: 522, loss is 0.08534660190343857\n",
      "epoch: 9 step: 523, loss is 4.198874012217857e-05\n",
      "epoch: 9 step: 524, loss is 0.013932354748249054\n",
      "epoch: 9 step: 525, loss is 0.0011185709154233336\n",
      "epoch: 9 step: 526, loss is 0.00535314716398716\n",
      "epoch: 9 step: 527, loss is 0.0012862293515354395\n",
      "epoch: 9 step: 528, loss is 0.009280546568334103\n",
      "epoch: 9 step: 529, loss is 0.012395890429615974\n",
      "epoch: 9 step: 530, loss is 0.0013064246159046888\n",
      "epoch: 9 step: 531, loss is 0.011733360588550568\n",
      "epoch: 9 step: 532, loss is 0.0026982855051755905\n",
      "epoch: 9 step: 533, loss is 0.000818922184407711\n",
      "epoch: 9 step: 534, loss is 0.002517490880563855\n",
      "epoch: 9 step: 535, loss is 0.00019943011284340173\n",
      "epoch: 9 step: 536, loss is 8.004341361811385e-05\n",
      "epoch: 9 step: 537, loss is 2.0384146409924142e-05\n",
      "epoch: 9 step: 538, loss is 0.0006687996792607009\n",
      "epoch: 9 step: 539, loss is 0.003029086859896779\n",
      "epoch: 9 step: 540, loss is 0.0170754324644804\n",
      "epoch: 9 step: 541, loss is 0.00046509699313901365\n",
      "epoch: 9 step: 542, loss is 0.05496976152062416\n",
      "epoch: 9 step: 543, loss is 0.09030906856060028\n",
      "epoch: 9 step: 544, loss is 0.0004258016706444323\n",
      "epoch: 9 step: 545, loss is 0.01713467575609684\n",
      "epoch: 9 step: 546, loss is 0.024624884128570557\n",
      "epoch: 9 step: 547, loss is 0.005936437286436558\n",
      "epoch: 9 step: 548, loss is 0.0016302306903526187\n",
      "epoch: 9 step: 549, loss is 0.00554676353931427\n",
      "epoch: 9 step: 550, loss is 0.00014969070616643876\n",
      "epoch: 9 step: 551, loss is 2.6723404516815208e-05\n",
      "epoch: 9 step: 552, loss is 5.701775444322266e-05\n",
      "epoch: 9 step: 553, loss is 0.0023049539886415005\n",
      "epoch: 9 step: 554, loss is 0.0003476215060800314\n",
      "epoch: 9 step: 555, loss is 0.00017021983512677252\n",
      "epoch: 9 step: 556, loss is 0.0003834862436633557\n",
      "epoch: 9 step: 557, loss is 0.04032962769269943\n",
      "epoch: 9 step: 558, loss is 0.03632338345050812\n",
      "epoch: 9 step: 559, loss is 9.85361693892628e-05\n",
      "epoch: 9 step: 560, loss is 0.16460666060447693\n",
      "epoch: 9 step: 561, loss is 0.0587783120572567\n",
      "epoch: 9 step: 562, loss is 0.0004874062433373183\n",
      "epoch: 9 step: 563, loss is 0.19586366415023804\n",
      "epoch: 9 step: 564, loss is 0.007398160174489021\n",
      "epoch: 9 step: 565, loss is 0.0001233616640092805\n",
      "epoch: 9 step: 566, loss is 0.027057990431785583\n",
      "epoch: 9 step: 567, loss is 0.0009485129849053919\n",
      "epoch: 9 step: 568, loss is 0.00248288013972342\n",
      "epoch: 9 step: 569, loss is 7.701723370701075e-05\n",
      "epoch: 9 step: 570, loss is 0.0002556054387241602\n",
      "epoch: 9 step: 571, loss is 0.001055276021361351\n",
      "epoch: 9 step: 572, loss is 0.0030353355687111616\n",
      "epoch: 9 step: 573, loss is 0.005128239281475544\n",
      "epoch: 9 step: 574, loss is 0.010369186289608479\n",
      "epoch: 9 step: 575, loss is 0.0452602282166481\n",
      "epoch: 9 step: 576, loss is 0.0007524435059167445\n",
      "epoch: 9 step: 577, loss is 0.0005815635086037219\n",
      "epoch: 9 step: 578, loss is 0.04113747924566269\n",
      "epoch: 9 step: 579, loss is 0.01515056099742651\n",
      "epoch: 9 step: 580, loss is 0.003253467846661806\n",
      "epoch: 9 step: 581, loss is 0.0011856288183480501\n",
      "epoch: 9 step: 582, loss is 0.001938631641678512\n",
      "epoch: 9 step: 583, loss is 0.1689443588256836\n",
      "epoch: 9 step: 584, loss is 0.004793127998709679\n",
      "epoch: 9 step: 585, loss is 0.0015407344326376915\n",
      "epoch: 9 step: 586, loss is 0.006853771395981312\n",
      "epoch: 9 step: 587, loss is 0.006451704539358616\n",
      "epoch: 9 step: 588, loss is 0.04920221120119095\n",
      "epoch: 9 step: 589, loss is 0.00015312882896978408\n",
      "epoch: 9 step: 590, loss is 0.0009279684163630009\n",
      "epoch: 9 step: 591, loss is 0.004006149247288704\n",
      "epoch: 9 step: 592, loss is 0.013171551749110222\n",
      "epoch: 9 step: 593, loss is 0.02809639647603035\n",
      "epoch: 9 step: 594, loss is 0.0027864347212016582\n",
      "epoch: 9 step: 595, loss is 0.0019767095800489187\n",
      "epoch: 9 step: 596, loss is 0.00026375657762400806\n",
      "epoch: 9 step: 597, loss is 0.039608657360076904\n",
      "epoch: 9 step: 598, loss is 0.00012384800356812775\n",
      "epoch: 9 step: 599, loss is 0.00306503614410758\n",
      "epoch: 9 step: 600, loss is 0.0001946388219948858\n",
      "epoch: 9 step: 601, loss is 0.006841909606009722\n",
      "epoch: 9 step: 602, loss is 0.029089009389281273\n",
      "epoch: 9 step: 603, loss is 0.0014771101996302605\n",
      "epoch: 9 step: 604, loss is 0.024254070594906807\n",
      "epoch: 9 step: 605, loss is 0.024461546912789345\n",
      "epoch: 9 step: 606, loss is 0.00032029973226599395\n",
      "epoch: 9 step: 607, loss is 0.0011588888010010123\n",
      "epoch: 9 step: 608, loss is 5.2937604777980596e-05\n",
      "epoch: 9 step: 609, loss is 0.0029479486402124166\n",
      "epoch: 9 step: 610, loss is 0.005758795887231827\n",
      "epoch: 9 step: 611, loss is 0.0006139419274404645\n",
      "epoch: 9 step: 612, loss is 0.004041404463350773\n",
      "epoch: 9 step: 613, loss is 0.022709544748067856\n",
      "epoch: 9 step: 614, loss is 0.002522626891732216\n",
      "epoch: 9 step: 615, loss is 0.0006508042570203543\n",
      "epoch: 9 step: 616, loss is 0.00080257368972525\n",
      "epoch: 9 step: 617, loss is 3.228440255043097e-05\n",
      "epoch: 9 step: 618, loss is 0.003300267271697521\n",
      "epoch: 9 step: 619, loss is 0.0018657646141946316\n",
      "epoch: 9 step: 620, loss is 0.12591470777988434\n",
      "epoch: 9 step: 621, loss is 0.05003897845745087\n",
      "epoch: 9 step: 622, loss is 0.0022539158817380667\n",
      "epoch: 9 step: 623, loss is 1.8619686670717783e-05\n",
      "epoch: 9 step: 624, loss is 0.0001243740989593789\n",
      "epoch: 9 step: 625, loss is 0.002317382488399744\n",
      "epoch: 9 step: 626, loss is 0.0027217294555157423\n",
      "epoch: 9 step: 627, loss is 3.5787605156656355e-05\n",
      "epoch: 9 step: 628, loss is 7.341049058595672e-05\n",
      "epoch: 9 step: 629, loss is 0.0004052685690112412\n",
      "epoch: 9 step: 630, loss is 0.003000410972163081\n",
      "epoch: 9 step: 631, loss is 0.0007125550182536244\n",
      "epoch: 9 step: 632, loss is 0.0011308357352390885\n",
      "epoch: 9 step: 633, loss is 2.4844863219186664e-05\n",
      "epoch: 9 step: 634, loss is 3.3613439882174134e-05\n",
      "epoch: 9 step: 635, loss is 0.001052526873536408\n",
      "epoch: 9 step: 636, loss is 0.07231433689594269\n",
      "epoch: 9 step: 637, loss is 0.000361245300155133\n",
      "epoch: 9 step: 638, loss is 0.0016151448944583535\n",
      "epoch: 9 step: 639, loss is 0.008987215347588062\n",
      "epoch: 9 step: 640, loss is 0.13760694861412048\n",
      "epoch: 9 step: 641, loss is 0.06600576639175415\n",
      "epoch: 9 step: 642, loss is 0.00021366807050071657\n",
      "epoch: 9 step: 643, loss is 0.00329651590436697\n",
      "epoch: 9 step: 644, loss is 0.01662927120923996\n",
      "epoch: 9 step: 645, loss is 0.0028311836067587137\n",
      "epoch: 9 step: 646, loss is 0.00027062345179729164\n",
      "epoch: 9 step: 647, loss is 0.00044705442269332707\n",
      "epoch: 9 step: 648, loss is 0.0006495325942523777\n",
      "epoch: 9 step: 649, loss is 0.029674150049686432\n",
      "epoch: 9 step: 650, loss is 0.0014919290551915765\n",
      "epoch: 9 step: 651, loss is 0.0025928758550435305\n",
      "epoch: 9 step: 652, loss is 0.002153936307877302\n",
      "epoch: 9 step: 653, loss is 0.004010356497019529\n",
      "epoch: 9 step: 654, loss is 0.00022845751664135605\n",
      "epoch: 9 step: 655, loss is 0.0014432374155148864\n",
      "epoch: 9 step: 656, loss is 0.00885961763560772\n",
      "epoch: 9 step: 657, loss is 0.004135987255722284\n",
      "epoch: 9 step: 658, loss is 0.00013847509399056435\n",
      "epoch: 9 step: 659, loss is 0.00034313712967559695\n",
      "epoch: 9 step: 660, loss is 0.0033417523372918367\n",
      "epoch: 9 step: 661, loss is 9.403703734278679e-05\n",
      "epoch: 9 step: 662, loss is 7.73381907492876e-05\n",
      "epoch: 9 step: 663, loss is 0.0003730638709384948\n",
      "epoch: 9 step: 664, loss is 0.005526718683540821\n",
      "epoch: 9 step: 665, loss is 0.010377761907875538\n",
      "epoch: 9 step: 666, loss is 0.010237086564302444\n",
      "epoch: 9 step: 667, loss is 0.0025481265038251877\n",
      "epoch: 9 step: 668, loss is 0.0013296457473188639\n",
      "epoch: 9 step: 669, loss is 0.024952780455350876\n",
      "epoch: 9 step: 670, loss is 0.011431926861405373\n",
      "epoch: 9 step: 671, loss is 0.000147415412357077\n",
      "epoch: 9 step: 672, loss is 0.00263007078319788\n",
      "epoch: 9 step: 673, loss is 0.00021598288731183857\n",
      "epoch: 9 step: 674, loss is 0.0016131451120600104\n",
      "epoch: 9 step: 675, loss is 9.131042133958545e-06\n",
      "epoch: 9 step: 676, loss is 0.00010581556853139773\n",
      "epoch: 9 step: 677, loss is 0.00013172188482712954\n",
      "epoch: 9 step: 678, loss is 0.0016744798049330711\n",
      "epoch: 9 step: 679, loss is 8.386015906580724e-06\n",
      "epoch: 9 step: 680, loss is 0.0005714505678042769\n",
      "epoch: 9 step: 681, loss is 0.013699864968657494\n",
      "epoch: 9 step: 682, loss is 0.0019263217691332102\n",
      "epoch: 9 step: 683, loss is 8.597515989094973e-05\n",
      "epoch: 9 step: 684, loss is 5.722781497752294e-05\n",
      "epoch: 9 step: 685, loss is 4.3461091991048306e-05\n",
      "epoch: 9 step: 686, loss is 0.038118068128824234\n",
      "epoch: 9 step: 687, loss is 0.09864140301942825\n",
      "epoch: 9 step: 688, loss is 0.00034860585583373904\n",
      "epoch: 9 step: 689, loss is 0.001373056205920875\n",
      "epoch: 9 step: 690, loss is 0.0029851305298507214\n",
      "epoch: 9 step: 691, loss is 0.0002518806140869856\n",
      "epoch: 9 step: 692, loss is 0.0010388285154476762\n",
      "epoch: 9 step: 693, loss is 0.006757980212569237\n",
      "epoch: 9 step: 694, loss is 0.0006992148119024932\n",
      "epoch: 9 step: 695, loss is 0.0001137234503403306\n",
      "epoch: 9 step: 696, loss is 0.0005075830849818885\n",
      "epoch: 9 step: 697, loss is 0.0024624487850815058\n",
      "epoch: 9 step: 698, loss is 0.006593382451683283\n",
      "epoch: 9 step: 699, loss is 4.1563165723346174e-05\n",
      "epoch: 9 step: 700, loss is 0.01865271106362343\n",
      "epoch: 9 step: 701, loss is 0.006318246014416218\n",
      "epoch: 9 step: 702, loss is 0.005812499672174454\n",
      "epoch: 9 step: 703, loss is 0.02486080676317215\n",
      "epoch: 9 step: 704, loss is 0.00044357814476825297\n",
      "epoch: 9 step: 705, loss is 0.00010411519178887829\n",
      "epoch: 9 step: 706, loss is 0.009000301361083984\n",
      "epoch: 9 step: 707, loss is 0.0003692680038511753\n",
      "epoch: 9 step: 708, loss is 0.0010364045156165957\n",
      "epoch: 9 step: 709, loss is 0.0009319261298514903\n",
      "epoch: 9 step: 710, loss is 0.045606162399053574\n",
      "epoch: 9 step: 711, loss is 6.938391015864909e-05\n",
      "epoch: 9 step: 712, loss is 0.021659722551703453\n",
      "epoch: 9 step: 713, loss is 0.0007759344298392534\n",
      "epoch: 9 step: 714, loss is 0.001968035940080881\n",
      "epoch: 9 step: 715, loss is 0.00027298019267618656\n",
      "epoch: 9 step: 716, loss is 0.07191137969493866\n",
      "epoch: 9 step: 717, loss is 0.0015243630623444915\n",
      "epoch: 9 step: 718, loss is 0.000123838588478975\n",
      "epoch: 9 step: 719, loss is 9.971888357540593e-05\n",
      "epoch: 9 step: 720, loss is 0.08469720929861069\n",
      "epoch: 9 step: 721, loss is 0.012140905484557152\n",
      "epoch: 9 step: 722, loss is 0.005659077782183886\n",
      "epoch: 9 step: 723, loss is 0.18958552181720734\n",
      "epoch: 9 step: 724, loss is 0.00022992395679466426\n",
      "epoch: 9 step: 725, loss is 0.0022652638144791126\n",
      "epoch: 9 step: 726, loss is 5.7299184845760465e-05\n",
      "epoch: 9 step: 727, loss is 0.0001754026161506772\n",
      "epoch: 9 step: 728, loss is 0.0018208042019978166\n",
      "epoch: 9 step: 729, loss is 0.0013592417817562819\n",
      "epoch: 9 step: 730, loss is 0.2283870279788971\n",
      "epoch: 9 step: 731, loss is 0.0004148971929680556\n",
      "epoch: 9 step: 732, loss is 0.11935205012559891\n",
      "epoch: 9 step: 733, loss is 0.021799808368086815\n",
      "epoch: 9 step: 734, loss is 4.189587343716994e-05\n",
      "epoch: 9 step: 735, loss is 0.02651713415980339\n",
      "epoch: 9 step: 736, loss is 8.89735238160938e-05\n",
      "epoch: 9 step: 737, loss is 0.0006427847547456622\n",
      "epoch: 9 step: 738, loss is 0.0009927528444677591\n",
      "epoch: 9 step: 739, loss is 0.0038491422310471535\n",
      "epoch: 9 step: 740, loss is 3.761316474992782e-05\n",
      "epoch: 9 step: 741, loss is 0.0006565921939909458\n",
      "epoch: 9 step: 742, loss is 0.007877837866544724\n",
      "epoch: 9 step: 743, loss is 0.006726551800966263\n",
      "epoch: 9 step: 744, loss is 0.004038941115140915\n",
      "epoch: 9 step: 745, loss is 0.00045163364848122\n",
      "epoch: 9 step: 746, loss is 0.00021250572171993554\n",
      "epoch: 9 step: 747, loss is 5.542533835978247e-05\n",
      "epoch: 9 step: 748, loss is 0.008422628045082092\n",
      "epoch: 9 step: 749, loss is 9.66461084317416e-05\n",
      "epoch: 9 step: 750, loss is 0.010434351861476898\n",
      "epoch: 9 step: 751, loss is 0.002103429287672043\n",
      "epoch: 9 step: 752, loss is 0.0032136554364115\n",
      "epoch: 9 step: 753, loss is 0.0023064110428094864\n",
      "epoch: 9 step: 754, loss is 0.00016986954142339528\n",
      "epoch: 9 step: 755, loss is 0.026940826326608658\n",
      "epoch: 9 step: 756, loss is 0.0017139872070401907\n",
      "epoch: 9 step: 757, loss is 0.000158629787620157\n",
      "epoch: 9 step: 758, loss is 0.0006767318118363619\n",
      "epoch: 9 step: 759, loss is 0.0020021244417876005\n",
      "epoch: 9 step: 760, loss is 0.06140582635998726\n",
      "epoch: 9 step: 761, loss is 0.00290084071457386\n",
      "epoch: 9 step: 762, loss is 0.0006214334280230105\n",
      "epoch: 9 step: 763, loss is 0.00015807876479811966\n",
      "epoch: 9 step: 764, loss is 0.00015109135711099952\n",
      "epoch: 9 step: 765, loss is 7.084045500960201e-05\n",
      "epoch: 9 step: 766, loss is 0.005734709091484547\n",
      "epoch: 9 step: 767, loss is 0.003738763742148876\n",
      "epoch: 9 step: 768, loss is 0.014700138010084629\n",
      "epoch: 9 step: 769, loss is 0.0005111386999487877\n",
      "epoch: 9 step: 770, loss is 0.002971051726490259\n",
      "epoch: 9 step: 771, loss is 0.0398559644818306\n",
      "epoch: 9 step: 772, loss is 0.00025866739451885223\n",
      "epoch: 9 step: 773, loss is 0.005503574851900339\n",
      "epoch: 9 step: 774, loss is 0.03413193300366402\n",
      "epoch: 9 step: 775, loss is 0.0790044292807579\n",
      "epoch: 9 step: 776, loss is 0.0056818644516170025\n",
      "epoch: 9 step: 777, loss is 0.0010497943731024861\n",
      "epoch: 9 step: 778, loss is 0.00012580105976667255\n",
      "epoch: 9 step: 779, loss is 0.00032200993155129254\n",
      "epoch: 9 step: 780, loss is 4.9951031542150304e-05\n",
      "epoch: 9 step: 781, loss is 0.00044510935549624264\n",
      "epoch: 9 step: 782, loss is 0.001576523296535015\n",
      "epoch: 9 step: 783, loss is 0.0059810057282447815\n",
      "epoch: 9 step: 784, loss is 0.0030594351701438427\n",
      "epoch: 9 step: 785, loss is 0.14215177297592163\n",
      "epoch: 9 step: 786, loss is 0.0005296987947076559\n",
      "epoch: 9 step: 787, loss is 0.04012421891093254\n",
      "epoch: 9 step: 788, loss is 0.002938601654022932\n",
      "epoch: 9 step: 789, loss is 0.0014649968361482024\n",
      "epoch: 9 step: 790, loss is 0.0014216406270861626\n",
      "epoch: 9 step: 791, loss is 0.005301883444190025\n",
      "epoch: 9 step: 792, loss is 6.661712541244924e-05\n",
      "epoch: 9 step: 793, loss is 0.004441095516085625\n",
      "epoch: 9 step: 794, loss is 0.005443528760224581\n",
      "epoch: 9 step: 795, loss is 4.110332520212978e-05\n",
      "epoch: 9 step: 796, loss is 0.0014893534826114774\n",
      "epoch: 9 step: 797, loss is 0.0008197433780878782\n",
      "epoch: 9 step: 798, loss is 0.00036568945506587625\n",
      "epoch: 9 step: 799, loss is 0.0003914876142516732\n",
      "epoch: 9 step: 800, loss is 0.019212346524000168\n",
      "epoch: 9 step: 801, loss is 0.00744085106998682\n",
      "epoch: 9 step: 802, loss is 0.0005583350430242717\n",
      "epoch: 9 step: 803, loss is 5.223611879046075e-05\n",
      "epoch: 9 step: 804, loss is 0.0001480394566897303\n",
      "epoch: 9 step: 805, loss is 3.743769048014656e-05\n",
      "epoch: 9 step: 806, loss is 0.00042362179374322295\n",
      "epoch: 9 step: 807, loss is 0.00420558825135231\n",
      "epoch: 9 step: 808, loss is 0.07671304047107697\n",
      "epoch: 9 step: 809, loss is 0.00014761427883058786\n",
      "epoch: 9 step: 810, loss is 5.4416344937635586e-05\n",
      "epoch: 9 step: 811, loss is 0.0018830513581633568\n",
      "epoch: 9 step: 812, loss is 1.8622489733388647e-05\n",
      "epoch: 9 step: 813, loss is 4.519380672718398e-05\n",
      "epoch: 9 step: 814, loss is 7.302415906451643e-05\n",
      "epoch: 9 step: 815, loss is 0.007270581088960171\n",
      "epoch: 9 step: 816, loss is 0.0005040828255005181\n",
      "epoch: 9 step: 817, loss is 0.0006607442046515644\n",
      "epoch: 9 step: 818, loss is 0.00636981101706624\n",
      "epoch: 9 step: 819, loss is 0.11589755117893219\n",
      "epoch: 9 step: 820, loss is 0.0017206603661179543\n",
      "epoch: 9 step: 821, loss is 0.003609228180721402\n",
      "epoch: 9 step: 822, loss is 0.003537580603733659\n",
      "epoch: 9 step: 823, loss is 0.04546025022864342\n",
      "epoch: 9 step: 824, loss is 0.0037624298129230738\n",
      "epoch: 9 step: 825, loss is 3.665157419163734e-05\n",
      "epoch: 9 step: 826, loss is 7.75945300119929e-05\n",
      "epoch: 9 step: 827, loss is 0.0003494336560834199\n",
      "epoch: 9 step: 828, loss is 0.001787047367542982\n",
      "epoch: 9 step: 829, loss is 0.007225422188639641\n",
      "epoch: 9 step: 830, loss is 0.0018607262754812837\n",
      "epoch: 9 step: 831, loss is 0.0001941140362760052\n",
      "epoch: 9 step: 832, loss is 0.0011717104353010654\n",
      "epoch: 9 step: 833, loss is 0.018294135108590126\n",
      "epoch: 9 step: 834, loss is 9.657179907662794e-05\n",
      "epoch: 9 step: 835, loss is 0.00047484482638537884\n",
      "epoch: 9 step: 836, loss is 5.0597038352862e-05\n",
      "epoch: 9 step: 837, loss is 0.00013327316264621913\n",
      "epoch: 9 step: 838, loss is 0.10963522642850876\n",
      "epoch: 9 step: 839, loss is 0.0009702169918455184\n",
      "epoch: 9 step: 840, loss is 0.00025308559997938573\n",
      "epoch: 9 step: 841, loss is 0.0007856568554416299\n",
      "epoch: 9 step: 842, loss is 1.789744055713527e-05\n",
      "epoch: 9 step: 843, loss is 3.01626914733788e-05\n",
      "epoch: 9 step: 844, loss is 0.03688333183526993\n",
      "epoch: 9 step: 845, loss is 0.0037076910957694054\n",
      "epoch: 9 step: 846, loss is 0.002937637735158205\n",
      "epoch: 9 step: 847, loss is 1.3637823940371163e-05\n",
      "epoch: 9 step: 848, loss is 0.0019323569722473621\n",
      "epoch: 9 step: 849, loss is 0.013978444039821625\n",
      "epoch: 9 step: 850, loss is 0.0003271221648901701\n",
      "epoch: 9 step: 851, loss is 0.0007558962679468095\n",
      "epoch: 9 step: 852, loss is 0.0010776525596156716\n",
      "epoch: 9 step: 853, loss is 0.0037433081306517124\n",
      "epoch: 9 step: 854, loss is 0.0014111448545008898\n",
      "epoch: 9 step: 855, loss is 0.05305973440408707\n",
      "epoch: 9 step: 856, loss is 0.0011499749962240458\n",
      "epoch: 9 step: 857, loss is 1.3430391845759004e-05\n",
      "epoch: 9 step: 858, loss is 0.002151236869394779\n",
      "epoch: 9 step: 859, loss is 0.013842032290995121\n",
      "epoch: 9 step: 860, loss is 0.008881449699401855\n",
      "epoch: 9 step: 861, loss is 0.018394209444522858\n",
      "epoch: 9 step: 862, loss is 0.0018595800502225757\n",
      "epoch: 9 step: 863, loss is 0.056258685886859894\n",
      "epoch: 9 step: 864, loss is 0.000828439777251333\n",
      "epoch: 9 step: 865, loss is 0.002919590100646019\n",
      "epoch: 9 step: 866, loss is 0.09276329725980759\n",
      "epoch: 9 step: 867, loss is 0.0019155738409608603\n",
      "epoch: 9 step: 868, loss is 0.06557625532150269\n",
      "epoch: 9 step: 869, loss is 0.0003914954431820661\n",
      "epoch: 9 step: 870, loss is 0.0001506694679846987\n",
      "epoch: 9 step: 871, loss is 0.0005987089243717492\n",
      "epoch: 9 step: 872, loss is 0.0023106029257178307\n",
      "epoch: 9 step: 873, loss is 0.002537719439715147\n",
      "epoch: 9 step: 874, loss is 0.005635081324726343\n",
      "epoch: 9 step: 875, loss is 0.023101231083273888\n",
      "epoch: 9 step: 876, loss is 0.07585255056619644\n",
      "epoch: 9 step: 877, loss is 0.14074382185935974\n",
      "epoch: 9 step: 878, loss is 0.004241585731506348\n",
      "epoch: 9 step: 879, loss is 0.005339241586625576\n",
      "epoch: 9 step: 880, loss is 0.009169047698378563\n",
      "epoch: 9 step: 881, loss is 0.025480283424258232\n",
      "epoch: 9 step: 882, loss is 0.0016284775920212269\n",
      "epoch: 9 step: 883, loss is 0.017770549282431602\n",
      "epoch: 9 step: 884, loss is 0.0041951644234359264\n",
      "epoch: 9 step: 885, loss is 0.021138165146112442\n",
      "epoch: 9 step: 886, loss is 0.00784104224294424\n",
      "epoch: 9 step: 887, loss is 0.008138698525726795\n",
      "epoch: 9 step: 888, loss is 0.0009750968893058598\n",
      "epoch: 9 step: 889, loss is 0.130351722240448\n",
      "epoch: 9 step: 890, loss is 0.015663137659430504\n",
      "epoch: 9 step: 891, loss is 0.09269876033067703\n",
      "epoch: 9 step: 892, loss is 0.0002881423570215702\n",
      "epoch: 9 step: 893, loss is 0.011099808849394321\n",
      "epoch: 9 step: 894, loss is 0.00016178861551452428\n",
      "epoch: 9 step: 895, loss is 0.08993157744407654\n",
      "epoch: 9 step: 896, loss is 0.15767265856266022\n",
      "epoch: 9 step: 897, loss is 0.11161891371011734\n",
      "epoch: 9 step: 898, loss is 0.008359183557331562\n",
      "epoch: 9 step: 899, loss is 2.9987460948177613e-05\n",
      "epoch: 9 step: 900, loss is 0.0006459162686951458\n",
      "epoch: 9 step: 901, loss is 0.011509834788739681\n",
      "epoch: 9 step: 902, loss is 0.020149486139416695\n",
      "epoch: 9 step: 903, loss is 0.002096355427056551\n",
      "epoch: 9 step: 904, loss is 0.00018319624359719455\n",
      "epoch: 9 step: 905, loss is 0.002432752400636673\n",
      "epoch: 9 step: 906, loss is 0.02971537783741951\n",
      "epoch: 9 step: 907, loss is 0.06892669200897217\n",
      "epoch: 9 step: 908, loss is 0.0006195413297973573\n",
      "epoch: 9 step: 909, loss is 0.0006433887756429613\n",
      "epoch: 9 step: 910, loss is 0.049713678658008575\n",
      "epoch: 9 step: 911, loss is 0.001124159898608923\n",
      "epoch: 9 step: 912, loss is 0.17931291460990906\n",
      "epoch: 9 step: 913, loss is 0.0005815089680254459\n",
      "epoch: 9 step: 914, loss is 0.0006477348506450653\n",
      "epoch: 9 step: 915, loss is 0.003764670342206955\n",
      "epoch: 9 step: 916, loss is 0.00012014420644845814\n",
      "epoch: 9 step: 917, loss is 0.007649501319974661\n",
      "epoch: 9 step: 918, loss is 0.0013030655682086945\n",
      "epoch: 9 step: 919, loss is 0.04591633379459381\n",
      "epoch: 9 step: 920, loss is 0.0016558354254812002\n",
      "epoch: 9 step: 921, loss is 0.0007384218624792993\n",
      "epoch: 9 step: 922, loss is 0.017279991880059242\n",
      "epoch: 9 step: 923, loss is 0.00012530587264336646\n",
      "epoch: 9 step: 924, loss is 0.00012803447316400707\n",
      "epoch: 9 step: 925, loss is 0.05103380233049393\n",
      "epoch: 9 step: 926, loss is 0.0006192627479322255\n",
      "epoch: 9 step: 927, loss is 0.0005853634793311357\n",
      "epoch: 9 step: 928, loss is 0.00011457990331109613\n",
      "epoch: 9 step: 929, loss is 0.0021045643370598555\n",
      "epoch: 9 step: 930, loss is 0.02745286002755165\n",
      "epoch: 9 step: 931, loss is 0.04571367800235748\n",
      "epoch: 9 step: 932, loss is 0.0004289419448468834\n",
      "epoch: 9 step: 933, loss is 0.021645208820700645\n",
      "epoch: 9 step: 934, loss is 0.030787797644734383\n",
      "epoch: 9 step: 935, loss is 0.0002549607597757131\n",
      "epoch: 9 step: 936, loss is 0.00024204641522374004\n",
      "epoch: 9 step: 937, loss is 8.653999429952819e-06\n",
      "epoch: 9 step: 938, loss is 0.0013961083022877574\n",
      "epoch: 9 step: 939, loss is 0.0006735255010426044\n",
      "epoch: 9 step: 940, loss is 0.001668761484324932\n",
      "epoch: 9 step: 941, loss is 0.00033011488267220557\n",
      "epoch: 9 step: 942, loss is 0.0035963733680546284\n",
      "epoch: 9 step: 943, loss is 8.783031807979569e-05\n",
      "epoch: 9 step: 944, loss is 0.005751306191086769\n",
      "epoch: 9 step: 945, loss is 0.0023055681958794594\n",
      "epoch: 9 step: 946, loss is 0.00013412951375357807\n",
      "epoch: 9 step: 947, loss is 0.0003440022992435843\n",
      "epoch: 9 step: 948, loss is 0.040953829884529114\n",
      "epoch: 9 step: 949, loss is 0.0037831070367246866\n",
      "epoch: 9 step: 950, loss is 0.004682208877056837\n",
      "epoch: 9 step: 951, loss is 0.038274556398391724\n",
      "epoch: 9 step: 952, loss is 4.6887700591469184e-05\n",
      "epoch: 9 step: 953, loss is 0.00017729285173118114\n",
      "epoch: 9 step: 954, loss is 0.0008344576926901937\n",
      "epoch: 9 step: 955, loss is 0.0012887390330433846\n",
      "epoch: 9 step: 956, loss is 0.04021402820944786\n",
      "epoch: 9 step: 957, loss is 0.0016782014863565564\n",
      "epoch: 9 step: 958, loss is 0.0016477744793519378\n",
      "epoch: 9 step: 959, loss is 0.001088628196157515\n",
      "epoch: 9 step: 960, loss is 0.001523012644611299\n",
      "epoch: 9 step: 961, loss is 0.00013818978914059699\n",
      "epoch: 9 step: 962, loss is 0.2728295922279358\n",
      "epoch: 9 step: 963, loss is 0.0014377788174897432\n",
      "epoch: 9 step: 964, loss is 0.0014093943173065782\n",
      "epoch: 9 step: 965, loss is 0.004435550421476364\n",
      "epoch: 9 step: 966, loss is 0.0001671259815338999\n",
      "epoch: 9 step: 967, loss is 0.000993649591691792\n",
      "epoch: 9 step: 968, loss is 0.0031598322093486786\n",
      "epoch: 9 step: 969, loss is 0.0005141947767697275\n",
      "epoch: 9 step: 970, loss is 0.0006965111242607236\n",
      "epoch: 9 step: 971, loss is 0.0004154295602347702\n",
      "epoch: 9 step: 972, loss is 0.0021663769148290157\n",
      "epoch: 9 step: 973, loss is 0.0037547042593359947\n",
      "epoch: 9 step: 974, loss is 0.0010469387052580714\n",
      "epoch: 9 step: 975, loss is 0.09785068035125732\n",
      "epoch: 9 step: 976, loss is 0.0029995145741850138\n",
      "epoch: 9 step: 977, loss is 0.00021372619085013866\n",
      "epoch: 9 step: 978, loss is 0.0004666844615712762\n",
      "epoch: 9 step: 979, loss is 0.0532577708363533\n",
      "epoch: 9 step: 980, loss is 0.0004332807438913733\n",
      "epoch: 9 step: 981, loss is 0.0005905607249587774\n",
      "epoch: 9 step: 982, loss is 0.00010188929445575923\n",
      "epoch: 9 step: 983, loss is 0.010887833312153816\n",
      "epoch: 9 step: 984, loss is 0.03406602889299393\n",
      "epoch: 9 step: 985, loss is 0.00012335527571849525\n",
      "epoch: 9 step: 986, loss is 0.004682352300733328\n",
      "epoch: 9 step: 987, loss is 0.0019072453724220395\n",
      "epoch: 9 step: 988, loss is 0.009887183085083961\n",
      "epoch: 9 step: 989, loss is 0.00025188972358591855\n",
      "epoch: 9 step: 990, loss is 0.00632115313783288\n",
      "epoch: 9 step: 991, loss is 0.0270626749843359\n",
      "epoch: 9 step: 992, loss is 0.00011636087583610788\n",
      "epoch: 9 step: 993, loss is 0.1074698269367218\n",
      "epoch: 9 step: 994, loss is 0.011146433651447296\n",
      "epoch: 9 step: 995, loss is 0.0008431171299889684\n",
      "epoch: 9 step: 996, loss is 0.01357974298298359\n",
      "epoch: 9 step: 997, loss is 0.00044325264752842486\n",
      "epoch: 9 step: 998, loss is 0.001366041018627584\n",
      "epoch: 9 step: 999, loss is 0.02561938390135765\n",
      "epoch: 9 step: 1000, loss is 0.14303478598594666\n",
      "epoch: 9 step: 1001, loss is 0.010222267359495163\n",
      "epoch: 9 step: 1002, loss is 5.879084346815944e-05\n",
      "epoch: 9 step: 1003, loss is 0.0004182713746558875\n",
      "epoch: 9 step: 1004, loss is 0.0042353179305791855\n",
      "epoch: 9 step: 1005, loss is 0.012735240161418915\n",
      "epoch: 9 step: 1006, loss is 0.002727791201323271\n",
      "epoch: 9 step: 1007, loss is 2.8011891117785126e-05\n",
      "epoch: 9 step: 1008, loss is 0.09938619285821915\n",
      "epoch: 9 step: 1009, loss is 0.00014490741887129843\n",
      "epoch: 9 step: 1010, loss is 0.005518453195691109\n",
      "epoch: 9 step: 1011, loss is 0.004116438329219818\n",
      "epoch: 9 step: 1012, loss is 0.0013041243655607104\n",
      "epoch: 9 step: 1013, loss is 0.0019374571274966002\n",
      "epoch: 9 step: 1014, loss is 2.3396636606776156e-05\n",
      "epoch: 9 step: 1015, loss is 0.0009084644261747599\n",
      "epoch: 9 step: 1016, loss is 0.00025509920669719577\n",
      "epoch: 9 step: 1017, loss is 0.020097048953175545\n",
      "epoch: 9 step: 1018, loss is 0.004109831992536783\n",
      "epoch: 9 step: 1019, loss is 0.0007324342732317746\n",
      "epoch: 9 step: 1020, loss is 0.00035268388455733657\n",
      "epoch: 9 step: 1021, loss is 0.0025460142642259598\n",
      "epoch: 9 step: 1022, loss is 0.0005630237283185124\n",
      "epoch: 9 step: 1023, loss is 0.0027958485297858715\n",
      "epoch: 9 step: 1024, loss is 0.0001843931822804734\n",
      "epoch: 9 step: 1025, loss is 0.0005468809395097196\n",
      "epoch: 9 step: 1026, loss is 0.12991304695606232\n",
      "epoch: 9 step: 1027, loss is 0.0049610924907028675\n",
      "epoch: 9 step: 1028, loss is 0.012215103954076767\n",
      "epoch: 9 step: 1029, loss is 0.0004681844438891858\n",
      "epoch: 9 step: 1030, loss is 0.00027092505479231477\n",
      "epoch: 9 step: 1031, loss is 0.0013770383084192872\n",
      "epoch: 9 step: 1032, loss is 0.0008656737045384943\n",
      "epoch: 9 step: 1033, loss is 0.005121783819049597\n",
      "epoch: 9 step: 1034, loss is 0.026827372610569\n",
      "epoch: 9 step: 1035, loss is 0.09902989864349365\n",
      "epoch: 9 step: 1036, loss is 0.00035080991801805794\n",
      "epoch: 9 step: 1037, loss is 0.0023154846858233213\n",
      "epoch: 9 step: 1038, loss is 0.00575434323400259\n",
      "epoch: 9 step: 1039, loss is 1.6702633729437366e-05\n",
      "epoch: 9 step: 1040, loss is 0.01608632504940033\n",
      "epoch: 9 step: 1041, loss is 0.016689039766788483\n",
      "epoch: 9 step: 1042, loss is 0.00911491084843874\n",
      "epoch: 9 step: 1043, loss is 0.0016606170684099197\n",
      "epoch: 9 step: 1044, loss is 0.02960430644452572\n",
      "epoch: 9 step: 1045, loss is 0.00010624509741319343\n",
      "epoch: 9 step: 1046, loss is 0.002408662112429738\n",
      "epoch: 9 step: 1047, loss is 7.76733213569969e-05\n",
      "epoch: 9 step: 1048, loss is 0.0038873457815498114\n",
      "epoch: 9 step: 1049, loss is 0.0005754026351496577\n",
      "epoch: 9 step: 1050, loss is 0.00034091866109520197\n",
      "epoch: 9 step: 1051, loss is 0.0014937128871679306\n",
      "epoch: 9 step: 1052, loss is 0.001630759914405644\n",
      "epoch: 9 step: 1053, loss is 0.0033603052143007517\n",
      "epoch: 9 step: 1054, loss is 0.31312745809555054\n",
      "epoch: 9 step: 1055, loss is 5.026979124522768e-05\n",
      "epoch: 9 step: 1056, loss is 2.6375348625151673e-06\n",
      "epoch: 9 step: 1057, loss is 0.006196700967848301\n",
      "epoch: 9 step: 1058, loss is 0.022639818489551544\n",
      "epoch: 9 step: 1059, loss is 0.04604180529713631\n",
      "epoch: 9 step: 1060, loss is 0.00016212702030315995\n",
      "epoch: 9 step: 1061, loss is 0.000568042101804167\n",
      "epoch: 9 step: 1062, loss is 0.0015661153011023998\n",
      "epoch: 9 step: 1063, loss is 0.10479091107845306\n",
      "epoch: 9 step: 1064, loss is 0.00012825621524825692\n",
      "epoch: 9 step: 1065, loss is 0.040727317333221436\n",
      "epoch: 9 step: 1066, loss is 0.0010241474956274033\n",
      "epoch: 9 step: 1067, loss is 0.09343641251325607\n",
      "epoch: 9 step: 1068, loss is 0.0035350085236132145\n",
      "epoch: 9 step: 1069, loss is 0.0005016911891289055\n",
      "epoch: 9 step: 1070, loss is 0.03547283262014389\n",
      "epoch: 9 step: 1071, loss is 0.0010861867340281606\n",
      "epoch: 9 step: 1072, loss is 0.001802451559342444\n",
      "epoch: 9 step: 1073, loss is 0.004383698105812073\n",
      "epoch: 9 step: 1074, loss is 0.00046446992200799286\n",
      "epoch: 9 step: 1075, loss is 9.83875579549931e-05\n",
      "epoch: 9 step: 1076, loss is 0.0014254035195335746\n",
      "epoch: 9 step: 1077, loss is 0.0023372163996100426\n",
      "epoch: 9 step: 1078, loss is 0.0007477952167391777\n",
      "epoch: 9 step: 1079, loss is 0.0007008170941844583\n",
      "epoch: 9 step: 1080, loss is 0.000648976129014045\n",
      "epoch: 9 step: 1081, loss is 0.035160865634679794\n",
      "epoch: 9 step: 1082, loss is 0.002153122564777732\n",
      "epoch: 9 step: 1083, loss is 0.0035924306139349937\n",
      "epoch: 9 step: 1084, loss is 2.870044409064576e-05\n",
      "epoch: 9 step: 1085, loss is 0.0003966976946685463\n",
      "epoch: 9 step: 1086, loss is 0.0006205213139764965\n",
      "epoch: 9 step: 1087, loss is 0.0007754628895781934\n",
      "epoch: 9 step: 1088, loss is 0.001231069560162723\n",
      "epoch: 9 step: 1089, loss is 0.0015449654310941696\n",
      "epoch: 9 step: 1090, loss is 0.00047525332774966955\n",
      "epoch: 9 step: 1091, loss is 0.0010578695219010115\n",
      "epoch: 9 step: 1092, loss is 0.0003137167659588158\n",
      "epoch: 9 step: 1093, loss is 0.08211594074964523\n",
      "epoch: 9 step: 1094, loss is 0.001446383772417903\n",
      "epoch: 9 step: 1095, loss is 0.004889973904937506\n",
      "epoch: 9 step: 1096, loss is 0.0004737376293633133\n",
      "epoch: 9 step: 1097, loss is 0.041366808116436005\n",
      "epoch: 9 step: 1098, loss is 0.00038331878022290766\n",
      "epoch: 9 step: 1099, loss is 0.0028961217030882835\n",
      "epoch: 9 step: 1100, loss is 0.0018060768488794565\n",
      "epoch: 9 step: 1101, loss is 0.0026897508651018143\n",
      "epoch: 9 step: 1102, loss is 0.0017413385212421417\n",
      "epoch: 9 step: 1103, loss is 0.04999355599284172\n",
      "epoch: 9 step: 1104, loss is 0.002160055562853813\n",
      "epoch: 9 step: 1105, loss is 0.00017304407083429396\n",
      "epoch: 9 step: 1106, loss is 0.00014889519661664963\n",
      "epoch: 9 step: 1107, loss is 0.008918742649257183\n",
      "epoch: 9 step: 1108, loss is 0.0001092056481866166\n",
      "epoch: 9 step: 1109, loss is 0.0005690993857569993\n",
      "epoch: 9 step: 1110, loss is 0.00011951974011026323\n",
      "epoch: 9 step: 1111, loss is 0.014275741763412952\n",
      "epoch: 9 step: 1112, loss is 0.037061456590890884\n",
      "epoch: 9 step: 1113, loss is 0.018090447410941124\n",
      "epoch: 9 step: 1114, loss is 0.009782237000763416\n",
      "epoch: 9 step: 1115, loss is 0.012693717144429684\n",
      "epoch: 9 step: 1116, loss is 0.0004147611325606704\n",
      "epoch: 9 step: 1117, loss is 0.019280333071947098\n",
      "epoch: 9 step: 1118, loss is 0.0011254142737016082\n",
      "epoch: 9 step: 1119, loss is 0.0049621653743088245\n",
      "epoch: 9 step: 1120, loss is 0.02836735174059868\n",
      "epoch: 9 step: 1121, loss is 0.000968043867032975\n",
      "epoch: 9 step: 1122, loss is 6.42461673123762e-05\n",
      "epoch: 9 step: 1123, loss is 0.14878547191619873\n",
      "epoch: 9 step: 1124, loss is 9.01760213309899e-05\n",
      "epoch: 9 step: 1125, loss is 0.010358466766774654\n",
      "epoch: 9 step: 1126, loss is 0.0060311309061944485\n",
      "epoch: 9 step: 1127, loss is 0.03332799673080444\n",
      "epoch: 9 step: 1128, loss is 0.001272471621632576\n",
      "epoch: 9 step: 1129, loss is 0.0001526320120319724\n",
      "epoch: 9 step: 1130, loss is 5.3419127652887255e-05\n",
      "epoch: 9 step: 1131, loss is 6.349619070533663e-05\n",
      "epoch: 9 step: 1132, loss is 0.009478690102696419\n",
      "epoch: 9 step: 1133, loss is 0.00011065539729315788\n",
      "epoch: 9 step: 1134, loss is 0.0014391412260010839\n",
      "epoch: 9 step: 1135, loss is 0.032084088772535324\n",
      "epoch: 9 step: 1136, loss is 0.05069170892238617\n",
      "epoch: 9 step: 1137, loss is 0.00019925518427044153\n",
      "epoch: 9 step: 1138, loss is 0.013458079658448696\n",
      "epoch: 9 step: 1139, loss is 0.001457473379559815\n",
      "epoch: 9 step: 1140, loss is 0.2016458958387375\n",
      "epoch: 9 step: 1141, loss is 0.0006841901922598481\n",
      "epoch: 9 step: 1142, loss is 2.4270617359434254e-05\n",
      "epoch: 9 step: 1143, loss is 0.0005184552865102887\n",
      "epoch: 9 step: 1144, loss is 0.00010478968033567071\n",
      "epoch: 9 step: 1145, loss is 0.03967480733990669\n",
      "epoch: 9 step: 1146, loss is 0.00024082782329060137\n",
      "epoch: 9 step: 1147, loss is 0.0007652502972632647\n",
      "epoch: 9 step: 1148, loss is 0.005729570519179106\n",
      "epoch: 9 step: 1149, loss is 0.010952519252896309\n",
      "epoch: 9 step: 1150, loss is 0.00016034375585149974\n",
      "epoch: 9 step: 1151, loss is 0.001236132113263011\n",
      "epoch: 9 step: 1152, loss is 0.00017146178288385272\n",
      "epoch: 9 step: 1153, loss is 0.005028638523072004\n",
      "epoch: 9 step: 1154, loss is 0.00027435406809672713\n",
      "epoch: 9 step: 1155, loss is 0.0428619422018528\n",
      "epoch: 9 step: 1156, loss is 7.911912689451128e-05\n",
      "epoch: 9 step: 1157, loss is 5.811574283143273e-06\n",
      "epoch: 9 step: 1158, loss is 0.0021907282061874866\n",
      "epoch: 9 step: 1159, loss is 0.001218411372974515\n",
      "epoch: 9 step: 1160, loss is 0.0011940521653741598\n",
      "epoch: 9 step: 1161, loss is 4.667453686124645e-05\n",
      "epoch: 9 step: 1162, loss is 0.00043258187361061573\n",
      "epoch: 9 step: 1163, loss is 0.00016356795094907284\n",
      "epoch: 9 step: 1164, loss is 0.0006918630679138005\n",
      "epoch: 9 step: 1165, loss is 0.014061100780963898\n",
      "epoch: 9 step: 1166, loss is 0.005006172228604555\n",
      "epoch: 9 step: 1167, loss is 0.0019134677713736892\n",
      "epoch: 9 step: 1168, loss is 0.09527787566184998\n",
      "epoch: 9 step: 1169, loss is 0.0032610278576612473\n",
      "epoch: 9 step: 1170, loss is 0.003666030941531062\n",
      "epoch: 9 step: 1171, loss is 0.00361172785051167\n",
      "epoch: 9 step: 1172, loss is 0.000271494296612218\n",
      "epoch: 9 step: 1173, loss is 0.00011944295692956075\n",
      "epoch: 9 step: 1174, loss is 0.006582664791494608\n",
      "epoch: 9 step: 1175, loss is 0.007951606065034866\n",
      "epoch: 9 step: 1176, loss is 0.08122551441192627\n",
      "epoch: 9 step: 1177, loss is 0.08690588176250458\n",
      "epoch: 9 step: 1178, loss is 6.414345261873677e-05\n",
      "epoch: 9 step: 1179, loss is 0.004121353849768639\n",
      "epoch: 9 step: 1180, loss is 0.0001546834537293762\n",
      "epoch: 9 step: 1181, loss is 0.0024789508897811174\n",
      "epoch: 9 step: 1182, loss is 0.00017361922073177993\n",
      "epoch: 9 step: 1183, loss is 0.008827698417007923\n",
      "epoch: 9 step: 1184, loss is 0.02870943397283554\n",
      "epoch: 9 step: 1185, loss is 0.0049360222183167934\n",
      "epoch: 9 step: 1186, loss is 0.0008209104416891932\n",
      "epoch: 9 step: 1187, loss is 0.011608243919909\n",
      "epoch: 9 step: 1188, loss is 0.0002703897771425545\n",
      "epoch: 9 step: 1189, loss is 0.00013361596211325377\n",
      "epoch: 9 step: 1190, loss is 0.00013661487901117653\n",
      "epoch: 9 step: 1191, loss is 0.2752656638622284\n",
      "epoch: 9 step: 1192, loss is 0.0108485147356987\n",
      "epoch: 9 step: 1193, loss is 0.0821419507265091\n",
      "epoch: 9 step: 1194, loss is 0.0008107801550067961\n",
      "epoch: 9 step: 1195, loss is 0.00029900099616497755\n",
      "epoch: 9 step: 1196, loss is 0.007172047160565853\n",
      "epoch: 9 step: 1197, loss is 0.01597513258457184\n",
      "epoch: 9 step: 1198, loss is 0.1654175966978073\n",
      "epoch: 9 step: 1199, loss is 0.0001813883864087984\n",
      "epoch: 9 step: 1200, loss is 1.7898461010190658e-05\n",
      "epoch: 9 step: 1201, loss is 0.0025595701299607754\n",
      "epoch: 9 step: 1202, loss is 0.00018420498236082494\n",
      "epoch: 9 step: 1203, loss is 0.0009683337993919849\n",
      "epoch: 9 step: 1204, loss is 5.129849159857258e-05\n",
      "epoch: 9 step: 1205, loss is 0.00017431291053071618\n",
      "epoch: 9 step: 1206, loss is 0.04113715514540672\n",
      "epoch: 9 step: 1207, loss is 0.0014257939765229821\n",
      "epoch: 9 step: 1208, loss is 0.03731561452150345\n",
      "epoch: 9 step: 1209, loss is 4.81692086395924e-06\n",
      "epoch: 9 step: 1210, loss is 0.0006946015637367964\n",
      "epoch: 9 step: 1211, loss is 0.0001647060562390834\n",
      "epoch: 9 step: 1212, loss is 0.00014539124094881117\n",
      "epoch: 9 step: 1213, loss is 7.585048479086254e-06\n",
      "epoch: 9 step: 1214, loss is 0.0008059748215600848\n",
      "epoch: 9 step: 1215, loss is 0.33096131682395935\n",
      "epoch: 9 step: 1216, loss is 0.03844310715794563\n",
      "epoch: 9 step: 1217, loss is 6.01282408752013e-05\n",
      "epoch: 9 step: 1218, loss is 0.0002833368780557066\n",
      "epoch: 9 step: 1219, loss is 0.0024684101808816195\n",
      "epoch: 9 step: 1220, loss is 0.00598856620490551\n",
      "epoch: 9 step: 1221, loss is 5.877863077330403e-05\n",
      "epoch: 9 step: 1222, loss is 0.0004850590485148132\n",
      "epoch: 9 step: 1223, loss is 0.002671529771760106\n",
      "epoch: 9 step: 1224, loss is 0.0018585686339065433\n",
      "epoch: 9 step: 1225, loss is 0.0014656750718131661\n",
      "epoch: 9 step: 1226, loss is 0.008013532496988773\n",
      "epoch: 9 step: 1227, loss is 0.0031907232478260994\n",
      "epoch: 9 step: 1228, loss is 0.16851651668548584\n",
      "epoch: 9 step: 1229, loss is 8.744111255509779e-05\n",
      "epoch: 9 step: 1230, loss is 0.0070934295654296875\n",
      "epoch: 9 step: 1231, loss is 0.18730293214321136\n",
      "epoch: 9 step: 1232, loss is 2.88021910819225e-05\n",
      "epoch: 9 step: 1233, loss is 0.017883425578475\n",
      "epoch: 9 step: 1234, loss is 0.00010479120828676969\n",
      "epoch: 9 step: 1235, loss is 0.0003045028424821794\n",
      "epoch: 9 step: 1236, loss is 4.3400650611147285e-06\n",
      "epoch: 9 step: 1237, loss is 0.00117549579590559\n",
      "epoch: 9 step: 1238, loss is 7.554362673545256e-05\n",
      "epoch: 9 step: 1239, loss is 0.022108815610408783\n",
      "epoch: 9 step: 1240, loss is 0.07296717911958694\n",
      "epoch: 9 step: 1241, loss is 0.028302986174821854\n",
      "epoch: 9 step: 1242, loss is 0.0012525932397693396\n",
      "epoch: 9 step: 1243, loss is 0.00010635535727487877\n",
      "epoch: 9 step: 1244, loss is 0.00023208846687339246\n",
      "epoch: 9 step: 1245, loss is 0.00026634937967173755\n",
      "epoch: 9 step: 1246, loss is 8.649407391203567e-05\n",
      "epoch: 9 step: 1247, loss is 0.00010403084888821468\n",
      "epoch: 9 step: 1248, loss is 4.018093750346452e-05\n",
      "epoch: 9 step: 1249, loss is 0.06955806165933609\n",
      "epoch: 9 step: 1250, loss is 0.00027715059695765376\n",
      "epoch: 9 step: 1251, loss is 0.00019105899264104664\n",
      "epoch: 9 step: 1252, loss is 0.0029369841795414686\n",
      "epoch: 9 step: 1253, loss is 0.13485009968280792\n",
      "epoch: 9 step: 1254, loss is 6.952323019504547e-05\n",
      "epoch: 9 step: 1255, loss is 0.09936445206403732\n",
      "epoch: 9 step: 1256, loss is 0.031001685187220573\n",
      "epoch: 9 step: 1257, loss is 0.00038856867467984557\n",
      "epoch: 9 step: 1258, loss is 0.09749183803796768\n",
      "epoch: 9 step: 1259, loss is 2.2663018171442673e-05\n",
      "epoch: 9 step: 1260, loss is 0.1411607414484024\n",
      "epoch: 9 step: 1261, loss is 0.19802425801753998\n",
      "epoch: 9 step: 1262, loss is 0.0013009782414883375\n",
      "epoch: 9 step: 1263, loss is 0.00764025654643774\n",
      "epoch: 9 step: 1264, loss is 0.0011020605452358723\n",
      "epoch: 9 step: 1265, loss is 0.047713425010442734\n",
      "epoch: 9 step: 1266, loss is 0.0006961125764064491\n",
      "epoch: 9 step: 1267, loss is 0.0007728208438493311\n",
      "epoch: 9 step: 1268, loss is 0.0050395457074046135\n",
      "epoch: 9 step: 1269, loss is 3.1136816687649116e-05\n",
      "epoch: 9 step: 1270, loss is 0.05890964716672897\n",
      "epoch: 9 step: 1271, loss is 0.02520059607923031\n",
      "epoch: 9 step: 1272, loss is 0.004994054790586233\n",
      "epoch: 9 step: 1273, loss is 0.0006161323399282992\n",
      "epoch: 9 step: 1274, loss is 0.00630510039627552\n",
      "epoch: 9 step: 1275, loss is 0.2286219447851181\n",
      "epoch: 9 step: 1276, loss is 0.27845728397369385\n",
      "epoch: 9 step: 1277, loss is 0.003486711997538805\n",
      "epoch: 9 step: 1278, loss is 0.00832353625446558\n",
      "epoch: 9 step: 1279, loss is 0.0035386814270168543\n",
      "epoch: 9 step: 1280, loss is 0.14024482667446136\n",
      "epoch: 9 step: 1281, loss is 0.0026963751297444105\n",
      "epoch: 9 step: 1282, loss is 0.042958248406648636\n",
      "epoch: 9 step: 1283, loss is 0.07172107696533203\n",
      "epoch: 9 step: 1284, loss is 0.00014560302952304482\n",
      "epoch: 9 step: 1285, loss is 0.002155779395252466\n",
      "epoch: 9 step: 1286, loss is 0.007987208664417267\n",
      "epoch: 9 step: 1287, loss is 0.0012535101268440485\n",
      "epoch: 9 step: 1288, loss is 0.003483240259811282\n",
      "epoch: 9 step: 1289, loss is 0.0009393739746883512\n",
      "epoch: 9 step: 1290, loss is 0.029388323426246643\n",
      "epoch: 9 step: 1291, loss is 0.0031631572637706995\n",
      "epoch: 9 step: 1292, loss is 0.004466696176677942\n",
      "epoch: 9 step: 1293, loss is 0.007699767593294382\n",
      "epoch: 9 step: 1294, loss is 0.04180128127336502\n",
      "epoch: 9 step: 1295, loss is 0.003891006112098694\n",
      "epoch: 9 step: 1296, loss is 0.010974438861012459\n",
      "epoch: 9 step: 1297, loss is 0.004439541604369879\n",
      "epoch: 9 step: 1298, loss is 0.00366738042794168\n",
      "epoch: 9 step: 1299, loss is 0.0002497273962944746\n",
      "epoch: 9 step: 1300, loss is 0.005217208527028561\n",
      "epoch: 9 step: 1301, loss is 0.0025572096928954124\n",
      "epoch: 9 step: 1302, loss is 0.004254575353115797\n",
      "epoch: 9 step: 1303, loss is 0.0014899037778377533\n",
      "epoch: 9 step: 1304, loss is 0.07929059118032455\n",
      "epoch: 9 step: 1305, loss is 0.004652914125472307\n",
      "epoch: 9 step: 1306, loss is 0.001370617770589888\n",
      "epoch: 9 step: 1307, loss is 0.0028326832689344883\n",
      "epoch: 9 step: 1308, loss is 9.255606710212305e-05\n",
      "epoch: 9 step: 1309, loss is 0.0001681833091424778\n",
      "epoch: 9 step: 1310, loss is 0.014089432545006275\n",
      "epoch: 9 step: 1311, loss is 0.0060053616762161255\n",
      "epoch: 9 step: 1312, loss is 0.0018214961746707559\n",
      "epoch: 9 step: 1313, loss is 0.006798631511628628\n",
      "epoch: 9 step: 1314, loss is 0.006617241073399782\n",
      "epoch: 9 step: 1315, loss is 0.00010213076166110113\n",
      "epoch: 9 step: 1316, loss is 0.0001090480072889477\n",
      "epoch: 9 step: 1317, loss is 0.0016836397117003798\n",
      "epoch: 9 step: 1318, loss is 0.0031226270366460085\n",
      "epoch: 9 step: 1319, loss is 0.00033758272184059024\n",
      "epoch: 9 step: 1320, loss is 0.10805243253707886\n",
      "epoch: 9 step: 1321, loss is 0.0005046366131864488\n",
      "epoch: 9 step: 1322, loss is 0.0005639052833430469\n",
      "epoch: 9 step: 1323, loss is 0.0010772644309327006\n",
      "epoch: 9 step: 1324, loss is 0.0007753378013148904\n",
      "epoch: 9 step: 1325, loss is 0.0002414066984783858\n",
      "epoch: 9 step: 1326, loss is 0.19049610197544098\n",
      "epoch: 9 step: 1327, loss is 0.0006415226380340755\n",
      "epoch: 9 step: 1328, loss is 0.0007764502661302686\n",
      "epoch: 9 step: 1329, loss is 0.0005501894047483802\n",
      "epoch: 9 step: 1330, loss is 0.0003132844576612115\n",
      "epoch: 9 step: 1331, loss is 0.0064113144762814045\n",
      "epoch: 9 step: 1332, loss is 0.0691460520029068\n",
      "epoch: 9 step: 1333, loss is 0.002412127796560526\n",
      "epoch: 9 step: 1334, loss is 0.0023892889730632305\n",
      "epoch: 9 step: 1335, loss is 0.00617611687630415\n",
      "epoch: 9 step: 1336, loss is 0.00022745164460502565\n",
      "epoch: 9 step: 1337, loss is 7.087786798365414e-05\n",
      "epoch: 9 step: 1338, loss is 0.0005050257313996553\n",
      "epoch: 9 step: 1339, loss is 0.0010810606181621552\n",
      "epoch: 9 step: 1340, loss is 0.01960468292236328\n",
      "epoch: 9 step: 1341, loss is 0.04974128678441048\n",
      "epoch: 9 step: 1342, loss is 0.14167331159114838\n",
      "epoch: 9 step: 1343, loss is 0.00024359513190574944\n",
      "epoch: 9 step: 1344, loss is 0.0001445167581550777\n",
      "epoch: 9 step: 1345, loss is 0.002804076299071312\n",
      "epoch: 9 step: 1346, loss is 0.0006738351657986641\n",
      "epoch: 9 step: 1347, loss is 0.040812186896800995\n",
      "epoch: 9 step: 1348, loss is 9.971434337785468e-05\n",
      "epoch: 9 step: 1349, loss is 0.00525687076151371\n",
      "epoch: 9 step: 1350, loss is 0.0675828829407692\n",
      "epoch: 9 step: 1351, loss is 0.00040607620030641556\n",
      "epoch: 9 step: 1352, loss is 0.0008954837103374302\n",
      "epoch: 9 step: 1353, loss is 0.020215928554534912\n",
      "epoch: 9 step: 1354, loss is 0.00047397849266417325\n",
      "epoch: 9 step: 1355, loss is 0.00030392405460588634\n",
      "epoch: 9 step: 1356, loss is 0.024186931550502777\n",
      "epoch: 9 step: 1357, loss is 0.005643362645059824\n",
      "epoch: 9 step: 1358, loss is 0.005510101094841957\n",
      "epoch: 9 step: 1359, loss is 0.011065722443163395\n",
      "epoch: 9 step: 1360, loss is 0.031378667801618576\n",
      "epoch: 9 step: 1361, loss is 0.0012632206780835986\n",
      "epoch: 9 step: 1362, loss is 0.00010133656178368255\n",
      "epoch: 9 step: 1363, loss is 0.0006646576803177595\n",
      "epoch: 9 step: 1364, loss is 0.016819201409816742\n",
      "epoch: 9 step: 1365, loss is 0.0003518116136547178\n",
      "epoch: 9 step: 1366, loss is 0.00040823977906256914\n",
      "epoch: 9 step: 1367, loss is 0.0013856322038918734\n",
      "epoch: 9 step: 1368, loss is 0.001269388711079955\n",
      "epoch: 9 step: 1369, loss is 0.08336438983678818\n",
      "epoch: 9 step: 1370, loss is 0.0020666129421442747\n",
      "epoch: 9 step: 1371, loss is 0.039676472544670105\n",
      "epoch: 9 step: 1372, loss is 0.0024570515379309654\n",
      "epoch: 9 step: 1373, loss is 5.897267328691669e-06\n",
      "epoch: 9 step: 1374, loss is 0.0007042639190331101\n",
      "epoch: 9 step: 1375, loss is 0.1709577590227127\n",
      "epoch: 9 step: 1376, loss is 0.0015444268938153982\n",
      "epoch: 9 step: 1377, loss is 0.00391276553273201\n",
      "epoch: 9 step: 1378, loss is 0.00020060250244569033\n",
      "epoch: 9 step: 1379, loss is 0.00549626350402832\n",
      "epoch: 9 step: 1380, loss is 0.004244024399667978\n",
      "epoch: 9 step: 1381, loss is 0.0007019020849838853\n",
      "epoch: 9 step: 1382, loss is 0.013765940442681313\n",
      "epoch: 9 step: 1383, loss is 0.024849360808730125\n",
      "epoch: 9 step: 1384, loss is 0.0015417084796354175\n",
      "epoch: 9 step: 1385, loss is 0.09841188788414001\n",
      "epoch: 9 step: 1386, loss is 0.011047246865928173\n",
      "epoch: 9 step: 1387, loss is 0.04640946909785271\n",
      "epoch: 9 step: 1388, loss is 0.034134089946746826\n",
      "epoch: 9 step: 1389, loss is 0.001196199795231223\n",
      "epoch: 9 step: 1390, loss is 0.019635707139968872\n",
      "epoch: 9 step: 1391, loss is 0.04999597370624542\n",
      "epoch: 9 step: 1392, loss is 6.315013888524845e-05\n",
      "epoch: 9 step: 1393, loss is 0.00031219839002005756\n",
      "epoch: 9 step: 1394, loss is 0.0011384731624275446\n",
      "epoch: 9 step: 1395, loss is 0.0007491002324968576\n",
      "epoch: 9 step: 1396, loss is 0.002158727962523699\n",
      "epoch: 9 step: 1397, loss is 0.006157154217362404\n",
      "epoch: 9 step: 1398, loss is 0.05596695467829704\n",
      "epoch: 9 step: 1399, loss is 0.000465249118860811\n",
      "epoch: 9 step: 1400, loss is 0.00015722874377388507\n",
      "epoch: 9 step: 1401, loss is 0.003965986426919699\n",
      "epoch: 9 step: 1402, loss is 8.393969619646668e-05\n",
      "epoch: 9 step: 1403, loss is 0.006497979164123535\n",
      "epoch: 9 step: 1404, loss is 0.024891814216971397\n",
      "epoch: 9 step: 1405, loss is 0.002804874675348401\n",
      "epoch: 9 step: 1406, loss is 0.00013369700172916055\n",
      "epoch: 9 step: 1407, loss is 2.9568091122200713e-05\n",
      "epoch: 9 step: 1408, loss is 0.0016720269341021776\n",
      "epoch: 9 step: 1409, loss is 0.0006257639033719897\n",
      "epoch: 9 step: 1410, loss is 0.0002713118738029152\n",
      "epoch: 9 step: 1411, loss is 0.010540086776018143\n",
      "epoch: 9 step: 1412, loss is 0.00242538470774889\n",
      "epoch: 9 step: 1413, loss is 0.002525425748899579\n",
      "epoch: 9 step: 1414, loss is 0.0011652831453830004\n",
      "epoch: 9 step: 1415, loss is 0.00017950359324458987\n",
      "epoch: 9 step: 1416, loss is 0.0017687249928712845\n",
      "epoch: 9 step: 1417, loss is 0.01118198037147522\n",
      "epoch: 9 step: 1418, loss is 0.0014242054894566536\n",
      "epoch: 9 step: 1419, loss is 0.031036503612995148\n",
      "epoch: 9 step: 1420, loss is 0.0021374246571213007\n",
      "epoch: 9 step: 1421, loss is 0.0015302919782698154\n",
      "epoch: 9 step: 1422, loss is 0.0035169124603271484\n",
      "epoch: 9 step: 1423, loss is 0.002234220039099455\n",
      "epoch: 9 step: 1424, loss is 0.001267522107809782\n",
      "epoch: 9 step: 1425, loss is 0.00036085801548324525\n",
      "epoch: 9 step: 1426, loss is 0.0024698455817997456\n",
      "epoch: 9 step: 1427, loss is 0.0003575062146410346\n",
      "epoch: 9 step: 1428, loss is 6.578037573490292e-05\n",
      "epoch: 9 step: 1429, loss is 0.03834281489253044\n",
      "epoch: 9 step: 1430, loss is 0.06914005428552628\n",
      "epoch: 9 step: 1431, loss is 0.0024853518698364496\n",
      "epoch: 9 step: 1432, loss is 0.006705353036522865\n",
      "epoch: 9 step: 1433, loss is 0.0009522911859676242\n",
      "epoch: 9 step: 1434, loss is 0.006564398296177387\n",
      "epoch: 9 step: 1435, loss is 0.02261263318359852\n",
      "epoch: 9 step: 1436, loss is 0.0022898800671100616\n",
      "epoch: 9 step: 1437, loss is 0.012057003565132618\n",
      "epoch: 9 step: 1438, loss is 0.0047600287944078445\n",
      "epoch: 9 step: 1439, loss is 0.005452190060168505\n",
      "epoch: 9 step: 1440, loss is 0.00014151405775919557\n",
      "epoch: 9 step: 1441, loss is 0.00010581700189504772\n",
      "epoch: 9 step: 1442, loss is 0.0016484708758071065\n",
      "epoch: 9 step: 1443, loss is 0.010338549502193928\n",
      "epoch: 9 step: 1444, loss is 0.005670968908816576\n",
      "epoch: 9 step: 1445, loss is 0.0036503630690276623\n",
      "epoch: 9 step: 1446, loss is 2.9502363759092987e-05\n",
      "epoch: 9 step: 1447, loss is 0.011481840163469315\n",
      "epoch: 9 step: 1448, loss is 0.0015576307196170092\n",
      "epoch: 9 step: 1449, loss is 0.0016334321117028594\n",
      "epoch: 9 step: 1450, loss is 0.0013501120265573263\n",
      "epoch: 9 step: 1451, loss is 0.00014987334725447\n",
      "epoch: 9 step: 1452, loss is 0.001432893448509276\n",
      "epoch: 9 step: 1453, loss is 0.02269875630736351\n",
      "epoch: 9 step: 1454, loss is 0.00018026705947704613\n",
      "epoch: 9 step: 1455, loss is 0.0034651823807507753\n",
      "epoch: 9 step: 1456, loss is 0.0026140117552131414\n",
      "epoch: 9 step: 1457, loss is 0.000492551364004612\n",
      "epoch: 9 step: 1458, loss is 0.0021494256798177958\n",
      "epoch: 9 step: 1459, loss is 0.00048036593943834305\n",
      "epoch: 9 step: 1460, loss is 0.013476643711328506\n",
      "epoch: 9 step: 1461, loss is 0.03205655515193939\n",
      "epoch: 9 step: 1462, loss is 0.000447550235548988\n",
      "epoch: 9 step: 1463, loss is 0.002529192017391324\n",
      "epoch: 9 step: 1464, loss is 0.00016331099322997034\n",
      "epoch: 9 step: 1465, loss is 0.0034645949490368366\n",
      "epoch: 9 step: 1466, loss is 0.03242690488696098\n",
      "epoch: 9 step: 1467, loss is 0.0001940515503520146\n",
      "epoch: 9 step: 1468, loss is 0.00031851447420194745\n",
      "epoch: 9 step: 1469, loss is 0.00013523439702112228\n",
      "epoch: 9 step: 1470, loss is 0.0013769951183348894\n",
      "epoch: 9 step: 1471, loss is 0.0006969767273403704\n",
      "epoch: 9 step: 1472, loss is 0.0038755980785936117\n",
      "epoch: 9 step: 1473, loss is 0.000317642668960616\n",
      "epoch: 9 step: 1474, loss is 0.02280181460082531\n",
      "epoch: 9 step: 1475, loss is 0.014013690873980522\n",
      "epoch: 9 step: 1476, loss is 1.8257043848279864e-05\n",
      "epoch: 9 step: 1477, loss is 0.09216523170471191\n",
      "epoch: 9 step: 1478, loss is 0.0018866565078496933\n",
      "epoch: 9 step: 1479, loss is 0.004141745623201132\n",
      "epoch: 9 step: 1480, loss is 0.012575670145452023\n",
      "epoch: 9 step: 1481, loss is 0.0003816162352450192\n",
      "epoch: 9 step: 1482, loss is 0.004224089905619621\n",
      "epoch: 9 step: 1483, loss is 0.000405746279284358\n",
      "epoch: 9 step: 1484, loss is 0.0010261426214128733\n",
      "epoch: 9 step: 1485, loss is 0.0005163238383829594\n",
      "epoch: 9 step: 1486, loss is 0.015967199578881264\n",
      "epoch: 9 step: 1487, loss is 0.05288216099143028\n",
      "epoch: 9 step: 1488, loss is 0.0003174985176883638\n",
      "epoch: 9 step: 1489, loss is 0.00819027703255415\n",
      "epoch: 9 step: 1490, loss is 0.0001079028588719666\n",
      "epoch: 9 step: 1491, loss is 8.400680962949991e-05\n",
      "epoch: 9 step: 1492, loss is 0.04279226437211037\n",
      "epoch: 9 step: 1493, loss is 0.0006840656860731542\n",
      "epoch: 9 step: 1494, loss is 6.606853276025504e-05\n",
      "epoch: 9 step: 1495, loss is 0.0005007334402762353\n",
      "epoch: 9 step: 1496, loss is 0.0001234370720339939\n",
      "epoch: 9 step: 1497, loss is 0.04598446562886238\n",
      "epoch: 9 step: 1498, loss is 0.0015103581827133894\n",
      "epoch: 9 step: 1499, loss is 0.002879504347220063\n",
      "epoch: 9 step: 1500, loss is 0.00011766164971049875\n",
      "epoch: 9 step: 1501, loss is 0.0001822375343181193\n",
      "epoch: 9 step: 1502, loss is 0.014488897286355495\n",
      "epoch: 9 step: 1503, loss is 0.0008318431209772825\n",
      "epoch: 9 step: 1504, loss is 0.013177303597331047\n",
      "epoch: 9 step: 1505, loss is 0.00014681174070574343\n",
      "epoch: 9 step: 1506, loss is 0.00475770328193903\n",
      "epoch: 9 step: 1507, loss is 0.03460418060421944\n",
      "epoch: 9 step: 1508, loss is 0.0031484151259064674\n",
      "epoch: 9 step: 1509, loss is 0.00012256983609404415\n",
      "epoch: 9 step: 1510, loss is 0.20204998552799225\n",
      "epoch: 9 step: 1511, loss is 0.0880555808544159\n",
      "epoch: 9 step: 1512, loss is 0.0006650884170085192\n",
      "epoch: 9 step: 1513, loss is 0.015219043008983135\n",
      "epoch: 9 step: 1514, loss is 0.001517528435215354\n",
      "epoch: 9 step: 1515, loss is 0.013743064366281033\n",
      "epoch: 9 step: 1516, loss is 0.0012836814858019352\n",
      "epoch: 9 step: 1517, loss is 0.03315500542521477\n",
      "epoch: 9 step: 1518, loss is 0.0974854901432991\n",
      "epoch: 9 step: 1519, loss is 0.0016779426950961351\n",
      "epoch: 9 step: 1520, loss is 0.002759958617389202\n",
      "epoch: 9 step: 1521, loss is 0.0005005032289773226\n",
      "epoch: 9 step: 1522, loss is 5.086043893243186e-05\n",
      "epoch: 9 step: 1523, loss is 0.0543559268116951\n",
      "epoch: 9 step: 1524, loss is 0.0034345791209489107\n",
      "epoch: 9 step: 1525, loss is 0.0034569979179650545\n",
      "epoch: 9 step: 1526, loss is 0.0004527315904852003\n",
      "epoch: 9 step: 1527, loss is 0.0004971236921846867\n",
      "epoch: 9 step: 1528, loss is 7.802296750014648e-05\n",
      "epoch: 9 step: 1529, loss is 5.223888365435414e-05\n",
      "epoch: 9 step: 1530, loss is 0.0001668441400397569\n",
      "epoch: 9 step: 1531, loss is 0.007920232601463795\n",
      "epoch: 9 step: 1532, loss is 8.802470983937383e-05\n",
      "epoch: 9 step: 1533, loss is 0.026290711015462875\n",
      "epoch: 9 step: 1534, loss is 0.005848914850503206\n",
      "epoch: 9 step: 1535, loss is 0.008745582774281502\n",
      "epoch: 9 step: 1536, loss is 0.15096278488636017\n",
      "epoch: 9 step: 1537, loss is 0.005258845165371895\n",
      "epoch: 9 step: 1538, loss is 0.020322568714618683\n",
      "epoch: 9 step: 1539, loss is 0.019726475700736046\n",
      "epoch: 9 step: 1540, loss is 0.0003715856000781059\n",
      "epoch: 9 step: 1541, loss is 0.10651411861181259\n",
      "epoch: 9 step: 1542, loss is 0.12327560037374496\n",
      "epoch: 9 step: 1543, loss is 0.0018141167238354683\n",
      "epoch: 9 step: 1544, loss is 0.00028133735759183764\n",
      "epoch: 9 step: 1545, loss is 0.00012783517013303936\n",
      "epoch: 9 step: 1546, loss is 0.07930034399032593\n",
      "epoch: 9 step: 1547, loss is 0.00013801395834889263\n",
      "epoch: 9 step: 1548, loss is 0.0007397180888801813\n",
      "epoch: 9 step: 1549, loss is 0.0006864319439046085\n",
      "epoch: 9 step: 1550, loss is 0.03092357888817787\n",
      "epoch: 9 step: 1551, loss is 0.00019189968588761985\n",
      "epoch: 9 step: 1552, loss is 0.01601248048245907\n",
      "epoch: 9 step: 1553, loss is 0.003646173747256398\n",
      "epoch: 9 step: 1554, loss is 0.0003422006848268211\n",
      "epoch: 9 step: 1555, loss is 0.1499457061290741\n",
      "epoch: 9 step: 1556, loss is 0.0009574526338838041\n",
      "epoch: 9 step: 1557, loss is 0.000406174105592072\n",
      "epoch: 9 step: 1558, loss is 0.0010344010079279542\n",
      "epoch: 9 step: 1559, loss is 0.012486555613577366\n",
      "epoch: 9 step: 1560, loss is 0.08972857147455215\n",
      "epoch: 9 step: 1561, loss is 0.02386915311217308\n",
      "epoch: 9 step: 1562, loss is 0.00011514429206727073\n",
      "epoch: 9 step: 1563, loss is 0.014680720865726471\n",
      "epoch: 9 step: 1564, loss is 8.371782314497977e-05\n",
      "epoch: 9 step: 1565, loss is 0.0011240433668717742\n",
      "epoch: 9 step: 1566, loss is 0.011609790846705437\n",
      "epoch: 9 step: 1567, loss is 0.08760310709476471\n",
      "epoch: 9 step: 1568, loss is 0.0019255358492955565\n",
      "epoch: 9 step: 1569, loss is 0.0037364645395427942\n",
      "epoch: 9 step: 1570, loss is 0.0063745188526809216\n",
      "epoch: 9 step: 1571, loss is 0.00011377500050002709\n",
      "epoch: 9 step: 1572, loss is 0.00022709608310833573\n",
      "epoch: 9 step: 1573, loss is 0.0037070622202008963\n",
      "epoch: 9 step: 1574, loss is 0.010939241386950016\n",
      "epoch: 9 step: 1575, loss is 0.0033592507243156433\n",
      "epoch: 9 step: 1576, loss is 0.005861247889697552\n",
      "epoch: 9 step: 1577, loss is 0.0004566167772281915\n",
      "epoch: 9 step: 1578, loss is 0.031061828136444092\n",
      "epoch: 9 step: 1579, loss is 0.0008865657728165388\n",
      "epoch: 9 step: 1580, loss is 0.00048534185043536127\n",
      "epoch: 9 step: 1581, loss is 0.0011451555183157325\n",
      "epoch: 9 step: 1582, loss is 0.0006162385689094663\n",
      "epoch: 9 step: 1583, loss is 0.1477360725402832\n",
      "epoch: 9 step: 1584, loss is 0.04524698108434677\n",
      "epoch: 9 step: 1585, loss is 0.00878944993019104\n",
      "epoch: 9 step: 1586, loss is 0.06108909845352173\n",
      "epoch: 9 step: 1587, loss is 0.0007971837185323238\n",
      "epoch: 9 step: 1588, loss is 0.0038006531540304422\n",
      "epoch: 9 step: 1589, loss is 0.00017716908769216388\n",
      "epoch: 9 step: 1590, loss is 0.0007556483033113182\n",
      "epoch: 9 step: 1591, loss is 0.0002755809109658003\n",
      "epoch: 9 step: 1592, loss is 0.09766718745231628\n",
      "epoch: 9 step: 1593, loss is 0.00037113812868483365\n",
      "epoch: 9 step: 1594, loss is 0.0005436049541458488\n",
      "epoch: 9 step: 1595, loss is 0.001042794669046998\n",
      "epoch: 9 step: 1596, loss is 0.0004745973274111748\n",
      "epoch: 9 step: 1597, loss is 0.0001553949696244672\n",
      "epoch: 9 step: 1598, loss is 0.0014680124586448073\n",
      "epoch: 9 step: 1599, loss is 2.95628015010152e-05\n",
      "epoch: 9 step: 1600, loss is 0.006690918002277613\n",
      "epoch: 9 step: 1601, loss is 0.045709144324064255\n",
      "epoch: 9 step: 1602, loss is 0.006719197146594524\n",
      "epoch: 9 step: 1603, loss is 0.00014822008961346\n",
      "epoch: 9 step: 1604, loss is 0.00015268441347870976\n",
      "epoch: 9 step: 1605, loss is 0.001611239043995738\n",
      "epoch: 9 step: 1606, loss is 3.3068306947825477e-05\n",
      "epoch: 9 step: 1607, loss is 0.0030976145062595606\n",
      "epoch: 9 step: 1608, loss is 0.00010601758549455553\n",
      "epoch: 9 step: 1609, loss is 0.04337882250547409\n",
      "epoch: 9 step: 1610, loss is 0.16000808775424957\n",
      "epoch: 9 step: 1611, loss is 0.002717596013098955\n",
      "epoch: 9 step: 1612, loss is 0.12685959041118622\n",
      "epoch: 9 step: 1613, loss is 0.0020217937417328358\n",
      "epoch: 9 step: 1614, loss is 0.029473286122083664\n",
      "epoch: 9 step: 1615, loss is 0.007940125651657581\n",
      "epoch: 9 step: 1616, loss is 0.002565177157521248\n",
      "epoch: 9 step: 1617, loss is 0.0159943588078022\n",
      "epoch: 9 step: 1618, loss is 0.00013623639824800193\n",
      "epoch: 9 step: 1619, loss is 0.001809427049010992\n",
      "epoch: 9 step: 1620, loss is 0.09958594292402267\n",
      "epoch: 9 step: 1621, loss is 0.010275878012180328\n",
      "epoch: 9 step: 1622, loss is 0.00028179350192658603\n",
      "epoch: 9 step: 1623, loss is 0.01343594677746296\n",
      "epoch: 9 step: 1624, loss is 9.949799277819693e-05\n",
      "epoch: 9 step: 1625, loss is 0.0006267364369705319\n",
      "epoch: 9 step: 1626, loss is 0.00028931733686476946\n",
      "epoch: 9 step: 1627, loss is 0.007724772673100233\n",
      "epoch: 9 step: 1628, loss is 0.011932694353163242\n",
      "epoch: 9 step: 1629, loss is 0.000288370531052351\n",
      "epoch: 9 step: 1630, loss is 0.020121563225984573\n",
      "epoch: 9 step: 1631, loss is 0.0006189715350046754\n",
      "epoch: 9 step: 1632, loss is 0.000876596721354872\n",
      "epoch: 9 step: 1633, loss is 8.602590241935104e-05\n",
      "epoch: 9 step: 1634, loss is 0.0006732927286066115\n",
      "epoch: 9 step: 1635, loss is 0.04702134057879448\n",
      "epoch: 9 step: 1636, loss is 0.02072971872985363\n",
      "epoch: 9 step: 1637, loss is 6.749824387952685e-05\n",
      "epoch: 9 step: 1638, loss is 0.0003042694297619164\n",
      "epoch: 9 step: 1639, loss is 0.00029947166331112385\n",
      "epoch: 9 step: 1640, loss is 0.0008590316865593195\n",
      "epoch: 9 step: 1641, loss is 0.0003701703390106559\n",
      "epoch: 9 step: 1642, loss is 0.00040285405702888966\n",
      "epoch: 9 step: 1643, loss is 0.03673039749264717\n",
      "epoch: 9 step: 1644, loss is 0.0007888462860137224\n",
      "epoch: 9 step: 1645, loss is 0.0073929536156356335\n",
      "epoch: 9 step: 1646, loss is 0.02197008579969406\n",
      "epoch: 9 step: 1647, loss is 3.6560435546562076e-05\n",
      "epoch: 9 step: 1648, loss is 0.0003593996516428888\n",
      "epoch: 9 step: 1649, loss is 0.05004102736711502\n",
      "epoch: 9 step: 1650, loss is 0.001839986420236528\n",
      "epoch: 9 step: 1651, loss is 0.006979594472795725\n",
      "epoch: 9 step: 1652, loss is 0.0009474874241277575\n",
      "epoch: 9 step: 1653, loss is 0.00046330521581694484\n",
      "epoch: 9 step: 1654, loss is 0.00032009996357373893\n",
      "epoch: 9 step: 1655, loss is 0.00034775419044308364\n",
      "epoch: 9 step: 1656, loss is 0.0009717870852909982\n",
      "epoch: 9 step: 1657, loss is 0.02354598604142666\n",
      "epoch: 9 step: 1658, loss is 0.012888486497104168\n",
      "epoch: 9 step: 1659, loss is 0.0011530359042808414\n",
      "epoch: 9 step: 1660, loss is 0.0005382419913075864\n",
      "epoch: 9 step: 1661, loss is 4.457057002582587e-05\n",
      "epoch: 9 step: 1662, loss is 0.00048773057642392814\n",
      "epoch: 9 step: 1663, loss is 0.003572672139853239\n",
      "epoch: 9 step: 1664, loss is 0.00010332954116165638\n",
      "epoch: 9 step: 1665, loss is 0.00010553220636211336\n",
      "epoch: 9 step: 1666, loss is 0.00223642960190773\n",
      "epoch: 9 step: 1667, loss is 0.002087437082082033\n",
      "epoch: 9 step: 1668, loss is 0.0010419207392260432\n",
      "epoch: 9 step: 1669, loss is 0.011677118949592113\n",
      "epoch: 9 step: 1670, loss is 0.00014363699301611632\n",
      "epoch: 9 step: 1671, loss is 0.003134853672236204\n",
      "epoch: 9 step: 1672, loss is 0.0013123019598424435\n",
      "epoch: 9 step: 1673, loss is 0.0020066811703145504\n",
      "epoch: 9 step: 1674, loss is 0.002722987672314048\n",
      "epoch: 9 step: 1675, loss is 0.001142535824328661\n",
      "epoch: 9 step: 1676, loss is 0.0009872731752693653\n",
      "epoch: 9 step: 1677, loss is 0.04868015646934509\n",
      "epoch: 9 step: 1678, loss is 7.645487494301051e-05\n",
      "epoch: 9 step: 1679, loss is 0.031465042382478714\n",
      "epoch: 9 step: 1680, loss is 0.0031722085550427437\n",
      "epoch: 9 step: 1681, loss is 0.022153330966830254\n",
      "epoch: 9 step: 1682, loss is 0.004033808130770922\n",
      "epoch: 9 step: 1683, loss is 0.0035585786681622267\n",
      "epoch: 9 step: 1684, loss is 1.9503708244883455e-05\n",
      "epoch: 9 step: 1685, loss is 0.009434307925403118\n",
      "epoch: 9 step: 1686, loss is 0.06497462838888168\n",
      "epoch: 9 step: 1687, loss is 0.00041510959272272885\n",
      "epoch: 9 step: 1688, loss is 0.01868719980120659\n",
      "epoch: 9 step: 1689, loss is 2.6934430934488773e-06\n",
      "epoch: 9 step: 1690, loss is 0.0007680202252231538\n",
      "epoch: 9 step: 1691, loss is 0.00255591026507318\n",
      "epoch: 9 step: 1692, loss is 0.0018477519042789936\n",
      "epoch: 9 step: 1693, loss is 0.05720032751560211\n",
      "epoch: 9 step: 1694, loss is 0.0004305634938646108\n",
      "epoch: 9 step: 1695, loss is 0.00030002245330251753\n",
      "epoch: 9 step: 1696, loss is 0.00011942720448132604\n",
      "epoch: 9 step: 1697, loss is 0.044663116335868835\n",
      "epoch: 9 step: 1698, loss is 0.00014507905871141702\n",
      "epoch: 9 step: 1699, loss is 0.005475845653563738\n",
      "epoch: 9 step: 1700, loss is 0.29676133394241333\n",
      "epoch: 9 step: 1701, loss is 0.014069902710616589\n",
      "epoch: 9 step: 1702, loss is 0.00014277955051511526\n",
      "epoch: 9 step: 1703, loss is 0.016387181356549263\n",
      "epoch: 9 step: 1704, loss is 0.004213385749608278\n",
      "epoch: 9 step: 1705, loss is 0.019603591412305832\n",
      "epoch: 9 step: 1706, loss is 0.0026792099233716726\n",
      "epoch: 9 step: 1707, loss is 0.038165923207998276\n",
      "epoch: 9 step: 1708, loss is 6.183583900565282e-05\n",
      "epoch: 9 step: 1709, loss is 0.00011442908726166934\n",
      "epoch: 9 step: 1710, loss is 5.915509245824069e-05\n",
      "epoch: 9 step: 1711, loss is 0.00016979544307105243\n",
      "epoch: 9 step: 1712, loss is 0.0005767683614976704\n",
      "epoch: 9 step: 1713, loss is 0.0028841225430369377\n",
      "epoch: 9 step: 1714, loss is 0.0026009909342974424\n",
      "epoch: 9 step: 1715, loss is 0.000333098549162969\n",
      "epoch: 9 step: 1716, loss is 0.0005205535562708974\n",
      "epoch: 9 step: 1717, loss is 0.017644641920924187\n",
      "epoch: 9 step: 1718, loss is 5.204795888857916e-05\n",
      "epoch: 9 step: 1719, loss is 4.551879828795791e-05\n",
      "epoch: 9 step: 1720, loss is 0.008035927079617977\n",
      "epoch: 9 step: 1721, loss is 0.000673753151204437\n",
      "epoch: 9 step: 1722, loss is 0.013793999329209328\n",
      "epoch: 9 step: 1723, loss is 0.05875437334179878\n",
      "epoch: 9 step: 1724, loss is 0.014190619811415672\n",
      "epoch: 9 step: 1725, loss is 0.018899288028478622\n",
      "epoch: 9 step: 1726, loss is 0.0004954725154675543\n",
      "epoch: 9 step: 1727, loss is 0.00271482951939106\n",
      "epoch: 9 step: 1728, loss is 0.0014383314410224557\n",
      "epoch: 9 step: 1729, loss is 0.12522615492343903\n",
      "epoch: 9 step: 1730, loss is 0.016806451603770256\n",
      "epoch: 9 step: 1731, loss is 0.0008197976858355105\n",
      "epoch: 9 step: 1732, loss is 0.01264191698282957\n",
      "epoch: 9 step: 1733, loss is 0.010966092348098755\n",
      "epoch: 9 step: 1734, loss is 0.015479651279747486\n",
      "epoch: 9 step: 1735, loss is 0.0002478114911355078\n",
      "epoch: 9 step: 1736, loss is 0.004635617136955261\n",
      "epoch: 9 step: 1737, loss is 0.0005626563797704875\n",
      "epoch: 9 step: 1738, loss is 0.0004304059548303485\n",
      "epoch: 9 step: 1739, loss is 0.0045588961802423\n",
      "epoch: 9 step: 1740, loss is 7.244715379783884e-05\n",
      "epoch: 9 step: 1741, loss is 0.006201423704624176\n",
      "epoch: 9 step: 1742, loss is 0.011969130486249924\n",
      "epoch: 9 step: 1743, loss is 8.293739665532485e-05\n",
      "epoch: 9 step: 1744, loss is 0.03579474613070488\n",
      "epoch: 9 step: 1745, loss is 0.0011811804724857211\n",
      "epoch: 9 step: 1746, loss is 0.001224273582920432\n",
      "epoch: 9 step: 1747, loss is 0.0005983954761177301\n",
      "epoch: 9 step: 1748, loss is 0.0007175856735557318\n",
      "epoch: 9 step: 1749, loss is 0.0002721430500969291\n",
      "epoch: 9 step: 1750, loss is 0.00024401702103205025\n",
      "epoch: 9 step: 1751, loss is 0.00035701997694559395\n",
      "epoch: 9 step: 1752, loss is 0.0006630210555158556\n",
      "epoch: 9 step: 1753, loss is 0.00010802211909322068\n",
      "epoch: 9 step: 1754, loss is 0.010097779333591461\n",
      "epoch: 9 step: 1755, loss is 0.0006558914901688695\n",
      "epoch: 9 step: 1756, loss is 0.003983067814260721\n",
      "epoch: 9 step: 1757, loss is 0.000709163025021553\n",
      "epoch: 9 step: 1758, loss is 0.0007679913542233407\n",
      "epoch: 9 step: 1759, loss is 0.024396125227212906\n",
      "epoch: 9 step: 1760, loss is 0.05875641480088234\n",
      "epoch: 9 step: 1761, loss is 0.03523745387792587\n",
      "epoch: 9 step: 1762, loss is 0.0010827616788446903\n",
      "epoch: 9 step: 1763, loss is 0.019444283097982407\n",
      "epoch: 9 step: 1764, loss is 0.00040602628723718226\n",
      "epoch: 9 step: 1765, loss is 0.15476740896701813\n",
      "epoch: 9 step: 1766, loss is 0.08004980534315109\n",
      "epoch: 9 step: 1767, loss is 0.0011133106891065836\n",
      "epoch: 9 step: 1768, loss is 0.0002288119721924886\n",
      "epoch: 9 step: 1769, loss is 0.00032809548429213464\n",
      "epoch: 9 step: 1770, loss is 0.06278983503580093\n",
      "epoch: 9 step: 1771, loss is 0.002493335632607341\n",
      "epoch: 9 step: 1772, loss is 0.035752855241298676\n",
      "epoch: 9 step: 1773, loss is 0.00045584107283502817\n",
      "epoch: 9 step: 1774, loss is 0.0012723628897219896\n",
      "epoch: 9 step: 1775, loss is 0.08302945643663406\n",
      "epoch: 9 step: 1776, loss is 0.10009336471557617\n",
      "epoch: 9 step: 1777, loss is 0.0038080033846199512\n",
      "epoch: 9 step: 1778, loss is 0.0018961247988045216\n",
      "epoch: 9 step: 1779, loss is 0.019146809354424477\n",
      "epoch: 9 step: 1780, loss is 0.0001440025371266529\n",
      "epoch: 9 step: 1781, loss is 0.0014495145296677947\n",
      "epoch: 9 step: 1782, loss is 0.022202160209417343\n",
      "epoch: 9 step: 1783, loss is 0.003285996848717332\n",
      "epoch: 9 step: 1784, loss is 0.025604503229260445\n",
      "epoch: 9 step: 1785, loss is 7.200077379820868e-05\n",
      "epoch: 9 step: 1786, loss is 0.01110704056918621\n",
      "epoch: 9 step: 1787, loss is 0.00022566350526176393\n",
      "epoch: 9 step: 1788, loss is 0.032040901482105255\n",
      "epoch: 9 step: 1789, loss is 0.046244267374277115\n",
      "epoch: 9 step: 1790, loss is 0.0015195717569440603\n",
      "epoch: 9 step: 1791, loss is 0.0004437885363586247\n",
      "epoch: 9 step: 1792, loss is 0.0015757568180561066\n",
      "epoch: 9 step: 1793, loss is 0.0721416100859642\n",
      "epoch: 9 step: 1794, loss is 0.000685368082486093\n",
      "epoch: 9 step: 1795, loss is 7.768311479594558e-05\n",
      "epoch: 9 step: 1796, loss is 0.0065641701221466064\n",
      "epoch: 9 step: 1797, loss is 0.0001346906356047839\n",
      "epoch: 9 step: 1798, loss is 0.012020878493785858\n",
      "epoch: 9 step: 1799, loss is 0.0004646188172046095\n",
      "epoch: 9 step: 1800, loss is 0.0005168645875528455\n",
      "epoch: 9 step: 1801, loss is 0.01788991689682007\n",
      "epoch: 9 step: 1802, loss is 0.00037294765934348106\n",
      "epoch: 9 step: 1803, loss is 1.0367887625761796e-05\n",
      "epoch: 9 step: 1804, loss is 0.0007147475844249129\n",
      "epoch: 9 step: 1805, loss is 0.004529922269284725\n",
      "epoch: 9 step: 1806, loss is 0.2630040645599365\n",
      "epoch: 9 step: 1807, loss is 0.028569485992193222\n",
      "epoch: 9 step: 1808, loss is 0.008484949357807636\n",
      "epoch: 9 step: 1809, loss is 0.0002626956265885383\n",
      "epoch: 9 step: 1810, loss is 0.0001330922095803544\n",
      "epoch: 9 step: 1811, loss is 0.002329297363758087\n",
      "epoch: 9 step: 1812, loss is 5.1314149459358305e-05\n",
      "epoch: 9 step: 1813, loss is 0.0009916217532008886\n",
      "epoch: 9 step: 1814, loss is 0.0004894249141216278\n",
      "epoch: 9 step: 1815, loss is 0.0004191415209788829\n",
      "epoch: 9 step: 1816, loss is 0.0037183291278779507\n",
      "epoch: 9 step: 1817, loss is 0.006416629534214735\n",
      "epoch: 9 step: 1818, loss is 0.0020431801676750183\n",
      "epoch: 9 step: 1819, loss is 0.009302341379225254\n",
      "epoch: 9 step: 1820, loss is 0.0015116846188902855\n",
      "epoch: 9 step: 1821, loss is 0.012965730391442776\n",
      "epoch: 9 step: 1822, loss is 0.00046473165275529027\n",
      "epoch: 9 step: 1823, loss is 0.0018828017637133598\n",
      "epoch: 9 step: 1824, loss is 0.0036099296994507313\n",
      "epoch: 9 step: 1825, loss is 0.010818151757121086\n",
      "epoch: 9 step: 1826, loss is 0.013286208733916283\n",
      "epoch: 9 step: 1827, loss is 0.00015359236567746848\n",
      "epoch: 9 step: 1828, loss is 0.000184859280125238\n",
      "epoch: 9 step: 1829, loss is 0.0018964952323585749\n",
      "epoch: 9 step: 1830, loss is 0.00011828879360109568\n",
      "epoch: 9 step: 1831, loss is 9.396208770340309e-05\n",
      "epoch: 9 step: 1832, loss is 0.004103119019418955\n",
      "epoch: 9 step: 1833, loss is 0.07221995294094086\n",
      "epoch: 9 step: 1834, loss is 0.32368725538253784\n",
      "epoch: 9 step: 1835, loss is 0.0980725884437561\n",
      "epoch: 9 step: 1836, loss is 0.00716578820720315\n",
      "epoch: 9 step: 1837, loss is 0.004095243290066719\n",
      "epoch: 9 step: 1838, loss is 0.09589102119207382\n",
      "epoch: 9 step: 1839, loss is 0.004927448928356171\n",
      "epoch: 9 step: 1840, loss is 0.0010319497669115663\n",
      "epoch: 9 step: 1841, loss is 0.10485020279884338\n",
      "epoch: 9 step: 1842, loss is 0.00036643739440478384\n",
      "epoch: 9 step: 1843, loss is 0.0634816363453865\n",
      "epoch: 9 step: 1844, loss is 0.00048130040522664785\n",
      "epoch: 9 step: 1845, loss is 0.27839377522468567\n",
      "epoch: 9 step: 1846, loss is 0.012620042078197002\n",
      "epoch: 9 step: 1847, loss is 0.019375210627913475\n",
      "epoch: 9 step: 1848, loss is 0.00014329751138575375\n",
      "epoch: 9 step: 1849, loss is 0.05564727634191513\n",
      "epoch: 9 step: 1850, loss is 0.0013720126589760184\n",
      "epoch: 9 step: 1851, loss is 0.0010018524480983615\n",
      "epoch: 9 step: 1852, loss is 0.0004633699427358806\n",
      "epoch: 9 step: 1853, loss is 0.007064307574182749\n",
      "epoch: 9 step: 1854, loss is 0.04270373657345772\n",
      "epoch: 9 step: 1855, loss is 0.09936998039484024\n",
      "epoch: 9 step: 1856, loss is 0.007180462591350079\n",
      "epoch: 9 step: 1857, loss is 0.021660011261701584\n",
      "epoch: 9 step: 1858, loss is 0.0017676242860034108\n",
      "epoch: 9 step: 1859, loss is 0.07872392237186432\n",
      "epoch: 9 step: 1860, loss is 0.0009719402296468616\n",
      "epoch: 9 step: 1861, loss is 0.005739906802773476\n",
      "epoch: 9 step: 1862, loss is 0.019408633932471275\n",
      "epoch: 9 step: 1863, loss is 0.0231012012809515\n",
      "epoch: 9 step: 1864, loss is 0.0011367001570761204\n",
      "epoch: 9 step: 1865, loss is 0.22388312220573425\n",
      "epoch: 9 step: 1866, loss is 0.0035346297081559896\n",
      "epoch: 9 step: 1867, loss is 0.00022070336854085326\n",
      "epoch: 9 step: 1868, loss is 0.0064943451434373856\n",
      "epoch: 9 step: 1869, loss is 0.001091250218451023\n",
      "epoch: 9 step: 1870, loss is 0.03141268342733383\n",
      "epoch: 9 step: 1871, loss is 0.0003933425759896636\n",
      "epoch: 9 step: 1872, loss is 0.0008064003195613623\n",
      "epoch: 9 step: 1873, loss is 0.00017258936713915318\n",
      "epoch: 9 step: 1874, loss is 0.021994449198246002\n",
      "epoch: 9 step: 1875, loss is 0.06650150567293167\n",
      "Train epoch time: 17119.319 ms, per step time: 9.130 ms\n",
      "epoch: 10 step: 1, loss is 0.0019965532701462507\n",
      "epoch: 10 step: 2, loss is 0.00022674945648759604\n",
      "epoch: 10 step: 3, loss is 0.0006234995671547949\n",
      "epoch: 10 step: 4, loss is 0.10302990674972534\n",
      "epoch: 10 step: 5, loss is 0.0006467184284701943\n",
      "epoch: 10 step: 6, loss is 0.02620571292936802\n",
      "epoch: 10 step: 7, loss is 0.0006231917650438845\n",
      "epoch: 10 step: 8, loss is 0.011097715236246586\n",
      "epoch: 10 step: 9, loss is 0.0025107748806476593\n",
      "epoch: 10 step: 10, loss is 0.023160764947533607\n",
      "epoch: 10 step: 11, loss is 0.001460178173147142\n",
      "epoch: 10 step: 12, loss is 0.013447633013129234\n",
      "epoch: 10 step: 13, loss is 0.005867501255124807\n",
      "epoch: 10 step: 14, loss is 0.0003203192027285695\n",
      "epoch: 10 step: 15, loss is 0.07746771723031998\n",
      "epoch: 10 step: 16, loss is 0.0005095390370115638\n",
      "epoch: 10 step: 17, loss is 0.0013536644401028752\n",
      "epoch: 10 step: 18, loss is 0.0019051635172218084\n",
      "epoch: 10 step: 19, loss is 0.0022786192130297422\n",
      "epoch: 10 step: 20, loss is 0.004287820775061846\n",
      "epoch: 10 step: 21, loss is 0.0021529491059482098\n",
      "epoch: 10 step: 22, loss is 0.08865615725517273\n",
      "epoch: 10 step: 23, loss is 0.009845010004937649\n",
      "epoch: 10 step: 24, loss is 0.0009773994097486138\n",
      "epoch: 10 step: 25, loss is 0.0017656300915405154\n",
      "epoch: 10 step: 26, loss is 0.0020815932657569647\n",
      "epoch: 10 step: 27, loss is 0.01693756878376007\n",
      "epoch: 10 step: 28, loss is 0.008127085864543915\n",
      "epoch: 10 step: 29, loss is 0.00015262037049978971\n",
      "epoch: 10 step: 30, loss is 0.0012833476066589355\n",
      "epoch: 10 step: 31, loss is 0.002047116169705987\n",
      "epoch: 10 step: 32, loss is 0.0015660783974453807\n",
      "epoch: 10 step: 33, loss is 0.013038775883615017\n",
      "epoch: 10 step: 34, loss is 0.011730826459825039\n",
      "epoch: 10 step: 35, loss is 0.00020662578754127026\n",
      "epoch: 10 step: 36, loss is 0.017908748239278793\n",
      "epoch: 10 step: 37, loss is 0.00019864588102791458\n",
      "epoch: 10 step: 38, loss is 0.0019110535504296422\n",
      "epoch: 10 step: 39, loss is 0.0002460636314935982\n",
      "epoch: 10 step: 40, loss is 0.0405300036072731\n",
      "epoch: 10 step: 41, loss is 0.0006477864226326346\n",
      "epoch: 10 step: 42, loss is 0.015459811314940453\n",
      "epoch: 10 step: 43, loss is 0.0030004926957190037\n",
      "epoch: 10 step: 44, loss is 0.004012611694633961\n",
      "epoch: 10 step: 45, loss is 0.01998460479080677\n",
      "epoch: 10 step: 46, loss is 0.0007595012430101633\n",
      "epoch: 10 step: 47, loss is 0.008824247866868973\n",
      "epoch: 10 step: 48, loss is 0.027179693803191185\n",
      "epoch: 10 step: 49, loss is 0.11463861912488937\n",
      "epoch: 10 step: 50, loss is 0.002206447534263134\n",
      "epoch: 10 step: 51, loss is 0.043123770505189896\n",
      "epoch: 10 step: 52, loss is 0.0003879666037391871\n",
      "epoch: 10 step: 53, loss is 0.00010552014282438904\n",
      "epoch: 10 step: 54, loss is 0.003460216335952282\n",
      "epoch: 10 step: 55, loss is 0.0011927613522857428\n",
      "epoch: 10 step: 56, loss is 0.0017686793580651283\n",
      "epoch: 10 step: 57, loss is 0.04006336256861687\n",
      "epoch: 10 step: 58, loss is 0.01538805291056633\n",
      "epoch: 10 step: 59, loss is 0.00032206071773543954\n",
      "epoch: 10 step: 60, loss is 0.009984881617128849\n",
      "epoch: 10 step: 61, loss is 0.0018636559834703803\n",
      "epoch: 10 step: 62, loss is 0.0007006463129073381\n",
      "epoch: 10 step: 63, loss is 0.019696637988090515\n",
      "epoch: 10 step: 64, loss is 8.046135189943016e-05\n",
      "epoch: 10 step: 65, loss is 0.0060811396688222885\n",
      "epoch: 10 step: 66, loss is 0.07535324990749359\n",
      "epoch: 10 step: 67, loss is 0.0011353344889357686\n",
      "epoch: 10 step: 68, loss is 0.0009147301316261292\n",
      "epoch: 10 step: 69, loss is 0.0004887271206825972\n",
      "epoch: 10 step: 70, loss is 0.06486011296510696\n",
      "epoch: 10 step: 71, loss is 0.004121532663702965\n",
      "epoch: 10 step: 72, loss is 0.007808869704604149\n",
      "epoch: 10 step: 73, loss is 0.00043657547212205827\n",
      "epoch: 10 step: 74, loss is 1.488977432018146e-05\n",
      "epoch: 10 step: 75, loss is 0.03412053734064102\n",
      "epoch: 10 step: 76, loss is 0.00046308146556839347\n",
      "epoch: 10 step: 77, loss is 0.054258789867162704\n",
      "epoch: 10 step: 78, loss is 0.00491568585857749\n",
      "epoch: 10 step: 79, loss is 0.0003144667425658554\n",
      "epoch: 10 step: 80, loss is 0.0003140354820061475\n",
      "epoch: 10 step: 81, loss is 0.024643277749419212\n",
      "epoch: 10 step: 82, loss is 0.0011164285242557526\n",
      "epoch: 10 step: 83, loss is 0.0006455189432017505\n",
      "epoch: 10 step: 84, loss is 0.007287611719220877\n",
      "epoch: 10 step: 85, loss is 0.0026370228733867407\n",
      "epoch: 10 step: 86, loss is 0.001236518262885511\n",
      "epoch: 10 step: 87, loss is 0.0024681512732058764\n",
      "epoch: 10 step: 88, loss is 0.003165489761158824\n",
      "epoch: 10 step: 89, loss is 0.00032438692869618535\n",
      "epoch: 10 step: 90, loss is 0.00024321020464412868\n",
      "epoch: 10 step: 91, loss is 1.6576859707129188e-05\n",
      "epoch: 10 step: 92, loss is 0.00026531642652116716\n",
      "epoch: 10 step: 93, loss is 0.0009790798649191856\n",
      "epoch: 10 step: 94, loss is 0.0009659317438490689\n",
      "epoch: 10 step: 95, loss is 0.016383910551667213\n",
      "epoch: 10 step: 96, loss is 0.0015163368079811335\n",
      "epoch: 10 step: 97, loss is 0.24721302092075348\n",
      "epoch: 10 step: 98, loss is 0.0008241536561399698\n",
      "epoch: 10 step: 99, loss is 0.0012615120504051447\n",
      "epoch: 10 step: 100, loss is 0.021020134910941124\n",
      "epoch: 10 step: 101, loss is 0.0012037137057632208\n",
      "epoch: 10 step: 102, loss is 0.00010891902638832107\n",
      "epoch: 10 step: 103, loss is 0.0006434756214730442\n",
      "epoch: 10 step: 104, loss is 0.0005245872307568789\n",
      "epoch: 10 step: 105, loss is 0.005852864123880863\n",
      "epoch: 10 step: 106, loss is 0.008408691734075546\n",
      "epoch: 10 step: 107, loss is 8.25915631139651e-05\n",
      "epoch: 10 step: 108, loss is 7.813925185473636e-05\n",
      "epoch: 10 step: 109, loss is 0.0016919664340093732\n",
      "epoch: 10 step: 110, loss is 0.024544833227992058\n",
      "epoch: 10 step: 111, loss is 0.005216989666223526\n",
      "epoch: 10 step: 112, loss is 0.2242727130651474\n",
      "epoch: 10 step: 113, loss is 0.12159454077482224\n",
      "epoch: 10 step: 114, loss is 0.000930845330003649\n",
      "epoch: 10 step: 115, loss is 0.0027892165817320347\n",
      "epoch: 10 step: 116, loss is 0.0071440632455050945\n",
      "epoch: 10 step: 117, loss is 0.047379665076732635\n",
      "epoch: 10 step: 118, loss is 0.0137327890843153\n",
      "epoch: 10 step: 119, loss is 0.014882316812872887\n",
      "epoch: 10 step: 120, loss is 0.016577772796154022\n",
      "epoch: 10 step: 121, loss is 0.013931252993643284\n",
      "epoch: 10 step: 122, loss is 0.0010367650538682938\n",
      "epoch: 10 step: 123, loss is 0.0003619522030930966\n",
      "epoch: 10 step: 124, loss is 0.007394613232463598\n",
      "epoch: 10 step: 125, loss is 8.488311141263694e-05\n",
      "epoch: 10 step: 126, loss is 0.0026327045634388924\n",
      "epoch: 10 step: 127, loss is 0.005736074410378933\n",
      "epoch: 10 step: 128, loss is 0.0006731526809744537\n",
      "epoch: 10 step: 129, loss is 0.00021391655900515616\n",
      "epoch: 10 step: 130, loss is 0.0006984766805544496\n",
      "epoch: 10 step: 131, loss is 0.001477996353060007\n",
      "epoch: 10 step: 132, loss is 0.0015240939101204276\n",
      "epoch: 10 step: 133, loss is 0.0001433077413821593\n",
      "epoch: 10 step: 134, loss is 0.013716434128582478\n",
      "epoch: 10 step: 135, loss is 0.0681505873799324\n",
      "epoch: 10 step: 136, loss is 0.0004311520606279373\n",
      "epoch: 10 step: 137, loss is 0.00018752794130705297\n",
      "epoch: 10 step: 138, loss is 0.00022629517479799688\n",
      "epoch: 10 step: 139, loss is 0.07075487822294235\n",
      "epoch: 10 step: 140, loss is 0.001751852803863585\n",
      "epoch: 10 step: 141, loss is 0.001011540531180799\n",
      "epoch: 10 step: 142, loss is 0.015181196853518486\n",
      "epoch: 10 step: 143, loss is 0.00028536471654661\n",
      "epoch: 10 step: 144, loss is 0.006268358323723078\n",
      "epoch: 10 step: 145, loss is 0.01619068533182144\n",
      "epoch: 10 step: 146, loss is 0.00013402619515545666\n",
      "epoch: 10 step: 147, loss is 0.0051270402036607265\n",
      "epoch: 10 step: 148, loss is 0.0008846805430948734\n",
      "epoch: 10 step: 149, loss is 0.00026213275850750506\n",
      "epoch: 10 step: 150, loss is 0.00015751436876598746\n",
      "epoch: 10 step: 151, loss is 0.03994520381093025\n",
      "epoch: 10 step: 152, loss is 0.03807276859879494\n",
      "epoch: 10 step: 153, loss is 4.191669722786173e-05\n",
      "epoch: 10 step: 154, loss is 0.008520776405930519\n",
      "epoch: 10 step: 155, loss is 0.0018600227776914835\n",
      "epoch: 10 step: 156, loss is 0.0007273680530488491\n",
      "epoch: 10 step: 157, loss is 0.00035753456177189946\n",
      "epoch: 10 step: 158, loss is 0.007958020083606243\n",
      "epoch: 10 step: 159, loss is 0.00042826851131394506\n",
      "epoch: 10 step: 160, loss is 0.1206296980381012\n",
      "epoch: 10 step: 161, loss is 9.22276740311645e-05\n",
      "epoch: 10 step: 162, loss is 0.0006054038531146944\n",
      "epoch: 10 step: 163, loss is 0.036574460566043854\n",
      "epoch: 10 step: 164, loss is 0.0019147956045344472\n",
      "epoch: 10 step: 165, loss is 0.006424179300665855\n",
      "epoch: 10 step: 166, loss is 0.0004108747816644609\n",
      "epoch: 10 step: 167, loss is 0.0002581562730483711\n",
      "epoch: 10 step: 168, loss is 0.002211980754509568\n",
      "epoch: 10 step: 169, loss is 0.006333081517368555\n",
      "epoch: 10 step: 170, loss is 0.0017238163854926825\n",
      "epoch: 10 step: 171, loss is 4.973605246050283e-05\n",
      "epoch: 10 step: 172, loss is 0.010474767535924911\n",
      "epoch: 10 step: 173, loss is 0.0011107097379863262\n",
      "epoch: 10 step: 174, loss is 0.0025226245634257793\n",
      "epoch: 10 step: 175, loss is 0.0008650627569295466\n",
      "epoch: 10 step: 176, loss is 0.004028661176562309\n",
      "epoch: 10 step: 177, loss is 0.002564818598330021\n",
      "epoch: 10 step: 178, loss is 0.12383091449737549\n",
      "epoch: 10 step: 179, loss is 0.0003453231474850327\n",
      "epoch: 10 step: 180, loss is 0.09065661579370499\n",
      "epoch: 10 step: 181, loss is 0.0048773935995996\n",
      "epoch: 10 step: 182, loss is 0.05081648379564285\n",
      "epoch: 10 step: 183, loss is 0.007195201236754656\n",
      "epoch: 10 step: 184, loss is 0.0013545588590204716\n",
      "epoch: 10 step: 185, loss is 0.0003509325033519417\n",
      "epoch: 10 step: 186, loss is 0.00047862136852927506\n",
      "epoch: 10 step: 187, loss is 0.007327404338866472\n",
      "epoch: 10 step: 188, loss is 0.00024298463540617377\n",
      "epoch: 10 step: 189, loss is 0.06738600134849548\n",
      "epoch: 10 step: 190, loss is 0.003069238970056176\n",
      "epoch: 10 step: 191, loss is 0.0035581854172050953\n",
      "epoch: 10 step: 192, loss is 0.019054928794503212\n",
      "epoch: 10 step: 193, loss is 0.005373907275497913\n",
      "epoch: 10 step: 194, loss is 0.006011192686855793\n",
      "epoch: 10 step: 195, loss is 0.0009707962744869292\n",
      "epoch: 10 step: 196, loss is 0.0007477534818463027\n",
      "epoch: 10 step: 197, loss is 0.0029988205060362816\n",
      "epoch: 10 step: 198, loss is 0.0022115593310445547\n",
      "epoch: 10 step: 199, loss is 0.008260468021035194\n",
      "epoch: 10 step: 200, loss is 2.7252906875219196e-05\n",
      "epoch: 10 step: 201, loss is 0.0010847622761502862\n",
      "epoch: 10 step: 202, loss is 0.004030819516628981\n",
      "epoch: 10 step: 203, loss is 0.00018117937725037336\n",
      "epoch: 10 step: 204, loss is 0.00011473277845652774\n",
      "epoch: 10 step: 205, loss is 0.0006095874705351889\n",
      "epoch: 10 step: 206, loss is 9.19289595913142e-05\n",
      "epoch: 10 step: 207, loss is 0.0001391648402204737\n",
      "epoch: 10 step: 208, loss is 0.0002300072810612619\n",
      "epoch: 10 step: 209, loss is 0.0001359807065455243\n",
      "epoch: 10 step: 210, loss is 0.00010567878052825108\n",
      "epoch: 10 step: 211, loss is 0.0017627874622121453\n",
      "epoch: 10 step: 212, loss is 0.0009445443865843117\n",
      "epoch: 10 step: 213, loss is 0.008404398337006569\n",
      "epoch: 10 step: 214, loss is 0.001819590455852449\n",
      "epoch: 10 step: 215, loss is 0.003743060864508152\n",
      "epoch: 10 step: 216, loss is 0.012185036204755306\n",
      "epoch: 10 step: 217, loss is 0.04172617197036743\n",
      "epoch: 10 step: 218, loss is 0.0005511672934517264\n",
      "epoch: 10 step: 219, loss is 0.00861012376844883\n",
      "epoch: 10 step: 220, loss is 0.0018264371901750565\n",
      "epoch: 10 step: 221, loss is 0.05655256286263466\n",
      "epoch: 10 step: 222, loss is 0.00010125445987796411\n",
      "epoch: 10 step: 223, loss is 0.0008484836434945464\n",
      "epoch: 10 step: 224, loss is 1.9363518731552176e-05\n",
      "epoch: 10 step: 225, loss is 0.0015009825583547354\n",
      "epoch: 10 step: 226, loss is 0.0006399137200787663\n",
      "epoch: 10 step: 227, loss is 0.00042208057129755616\n",
      "epoch: 10 step: 228, loss is 0.0022816264536231756\n",
      "epoch: 10 step: 229, loss is 0.00019320726278237998\n",
      "epoch: 10 step: 230, loss is 0.015637367963790894\n",
      "epoch: 10 step: 231, loss is 0.020579170435667038\n",
      "epoch: 10 step: 232, loss is 0.0005331477732397616\n",
      "epoch: 10 step: 233, loss is 0.0005437688087113202\n",
      "epoch: 10 step: 234, loss is 0.0004379212623462081\n",
      "epoch: 10 step: 235, loss is 0.00017529413162264973\n",
      "epoch: 10 step: 236, loss is 8.34793463582173e-05\n",
      "epoch: 10 step: 237, loss is 0.1790546029806137\n",
      "epoch: 10 step: 238, loss is 5.147382034920156e-05\n",
      "epoch: 10 step: 239, loss is 0.03681425750255585\n",
      "epoch: 10 step: 240, loss is 0.0018643455114215612\n",
      "epoch: 10 step: 241, loss is 0.003691906575113535\n",
      "epoch: 10 step: 242, loss is 0.0006231358274817467\n",
      "epoch: 10 step: 243, loss is 0.00016142714594025165\n",
      "epoch: 10 step: 244, loss is 0.0007655225927010179\n",
      "epoch: 10 step: 245, loss is 0.0033678810577839613\n",
      "epoch: 10 step: 246, loss is 0.0014345560921356082\n",
      "epoch: 10 step: 247, loss is 0.0012201335048303008\n",
      "epoch: 10 step: 248, loss is 0.004368102177977562\n",
      "epoch: 10 step: 249, loss is 0.0018571598920971155\n",
      "epoch: 10 step: 250, loss is 0.00430144602432847\n",
      "epoch: 10 step: 251, loss is 0.0033327387645840645\n",
      "epoch: 10 step: 252, loss is 8.976709068519995e-05\n",
      "epoch: 10 step: 253, loss is 0.0001273827365366742\n",
      "epoch: 10 step: 254, loss is 0.0031554936431348324\n",
      "epoch: 10 step: 255, loss is 0.006529500242322683\n",
      "epoch: 10 step: 256, loss is 0.005683128722012043\n",
      "epoch: 10 step: 257, loss is 0.00021951382223051041\n",
      "epoch: 10 step: 258, loss is 0.15960919857025146\n",
      "epoch: 10 step: 259, loss is 0.016312522813677788\n",
      "epoch: 10 step: 260, loss is 0.01849881187081337\n",
      "epoch: 10 step: 261, loss is 0.0011285499203950167\n",
      "epoch: 10 step: 262, loss is 0.001420497545041144\n",
      "epoch: 10 step: 263, loss is 0.05014229193329811\n",
      "epoch: 10 step: 264, loss is 0.004069150425493717\n",
      "epoch: 10 step: 265, loss is 0.0002901436237152666\n",
      "epoch: 10 step: 266, loss is 0.0008134086383506656\n",
      "epoch: 10 step: 267, loss is 1.2896010048280004e-05\n",
      "epoch: 10 step: 268, loss is 0.017550718039274216\n",
      "epoch: 10 step: 269, loss is 0.0005547275650314987\n",
      "epoch: 10 step: 270, loss is 2.2773341697757132e-05\n",
      "epoch: 10 step: 271, loss is 0.00040276796789839864\n",
      "epoch: 10 step: 272, loss is 0.002223985968157649\n",
      "epoch: 10 step: 273, loss is 0.011677670292556286\n",
      "epoch: 10 step: 274, loss is 0.004678213503211737\n",
      "epoch: 10 step: 275, loss is 5.368225629354129e-06\n",
      "epoch: 10 step: 276, loss is 0.00045307696564123034\n",
      "epoch: 10 step: 277, loss is 0.0019954689778387547\n",
      "epoch: 10 step: 278, loss is 0.0003818288678303361\n",
      "epoch: 10 step: 279, loss is 0.0017814693273976445\n",
      "epoch: 10 step: 280, loss is 6.598322943318635e-05\n",
      "epoch: 10 step: 281, loss is 0.0001502327504567802\n",
      "epoch: 10 step: 282, loss is 0.0005134193343110383\n",
      "epoch: 10 step: 283, loss is 0.16153660416603088\n",
      "epoch: 10 step: 284, loss is 0.0006445351173169911\n",
      "epoch: 10 step: 285, loss is 9.797621896723285e-05\n",
      "epoch: 10 step: 286, loss is 0.0036145190242677927\n",
      "epoch: 10 step: 287, loss is 0.0021776726935058832\n",
      "epoch: 10 step: 288, loss is 2.5108565750997514e-05\n",
      "epoch: 10 step: 289, loss is 0.021783974021673203\n",
      "epoch: 10 step: 290, loss is 0.0073594762943685055\n",
      "epoch: 10 step: 291, loss is 0.03140513226389885\n",
      "epoch: 10 step: 292, loss is 4.975490446668118e-05\n",
      "epoch: 10 step: 293, loss is 0.010073771700263023\n",
      "epoch: 10 step: 294, loss is 0.026043621823191643\n",
      "epoch: 10 step: 295, loss is 0.00021244736853986979\n",
      "epoch: 10 step: 296, loss is 0.11842411011457443\n",
      "epoch: 10 step: 297, loss is 5.9900459746131673e-05\n",
      "epoch: 10 step: 298, loss is 0.0029948765877634287\n",
      "epoch: 10 step: 299, loss is 4.440287739271298e-05\n",
      "epoch: 10 step: 300, loss is 4.3006315536331385e-05\n",
      "epoch: 10 step: 301, loss is 0.00038210468483157456\n",
      "epoch: 10 step: 302, loss is 0.00023082297411747277\n",
      "epoch: 10 step: 303, loss is 9.904538455884904e-05\n",
      "epoch: 10 step: 304, loss is 0.0002514429797884077\n",
      "epoch: 10 step: 305, loss is 4.3328760511940345e-05\n",
      "epoch: 10 step: 306, loss is 0.009072109125554562\n",
      "epoch: 10 step: 307, loss is 0.005862793885171413\n",
      "epoch: 10 step: 308, loss is 0.000492377148475498\n",
      "epoch: 10 step: 309, loss is 0.011605406180024147\n",
      "epoch: 10 step: 310, loss is 0.009449578821659088\n",
      "epoch: 10 step: 311, loss is 0.036107007414102554\n",
      "epoch: 10 step: 312, loss is 7.449784607160836e-05\n",
      "epoch: 10 step: 313, loss is 0.008243798278272152\n",
      "epoch: 10 step: 314, loss is 0.0011425220873206854\n",
      "epoch: 10 step: 315, loss is 0.00033192936098203063\n",
      "epoch: 10 step: 316, loss is 0.003898318624123931\n",
      "epoch: 10 step: 317, loss is 3.28253663610667e-05\n",
      "epoch: 10 step: 318, loss is 0.0005286165978759527\n",
      "epoch: 10 step: 319, loss is 0.00016507843974977732\n",
      "epoch: 10 step: 320, loss is 0.006634152960032225\n",
      "epoch: 10 step: 321, loss is 0.0003980413603130728\n",
      "epoch: 10 step: 322, loss is 0.002918030833825469\n",
      "epoch: 10 step: 323, loss is 0.0006345949368551373\n",
      "epoch: 10 step: 324, loss is 4.6594650484621525e-05\n",
      "epoch: 10 step: 325, loss is 0.004639945924282074\n",
      "epoch: 10 step: 326, loss is 0.09345145523548126\n",
      "epoch: 10 step: 327, loss is 0.005984238814562559\n",
      "epoch: 10 step: 328, loss is 0.004891969729214907\n",
      "epoch: 10 step: 329, loss is 0.00032320694299414754\n",
      "epoch: 10 step: 330, loss is 0.023175301030278206\n",
      "epoch: 10 step: 331, loss is 0.00033307133708149195\n",
      "epoch: 10 step: 332, loss is 0.00023751295520924032\n",
      "epoch: 10 step: 333, loss is 0.00022883967903908342\n",
      "epoch: 10 step: 334, loss is 0.028010010719299316\n",
      "epoch: 10 step: 335, loss is 0.009881436824798584\n",
      "epoch: 10 step: 336, loss is 0.0020245595369488\n",
      "epoch: 10 step: 337, loss is 0.0008478547097183764\n",
      "epoch: 10 step: 338, loss is 0.000316537800244987\n",
      "epoch: 10 step: 339, loss is 5.555475581786595e-05\n",
      "epoch: 10 step: 340, loss is 0.00023291278921533376\n",
      "epoch: 10 step: 341, loss is 2.083918298012577e-05\n",
      "epoch: 10 step: 342, loss is 9.774330101208761e-05\n",
      "epoch: 10 step: 343, loss is 0.00046777352690696716\n",
      "epoch: 10 step: 344, loss is 2.9387480026343837e-05\n",
      "epoch: 10 step: 345, loss is 0.005077858921140432\n",
      "epoch: 10 step: 346, loss is 0.00011138589616166428\n",
      "epoch: 10 step: 347, loss is 0.029589416459202766\n",
      "epoch: 10 step: 348, loss is 0.005058603826910257\n",
      "epoch: 10 step: 349, loss is 0.005404920782893896\n",
      "epoch: 10 step: 350, loss is 0.0037674810737371445\n",
      "epoch: 10 step: 351, loss is 2.953878720290959e-05\n",
      "epoch: 10 step: 352, loss is 4.999958764528856e-05\n",
      "epoch: 10 step: 353, loss is 0.00023209332721307874\n",
      "epoch: 10 step: 354, loss is 0.0001359744928777218\n",
      "epoch: 10 step: 355, loss is 0.017943060025572777\n",
      "epoch: 10 step: 356, loss is 0.002014600671827793\n",
      "epoch: 10 step: 357, loss is 0.027509815990924835\n",
      "epoch: 10 step: 358, loss is 0.016041269525885582\n",
      "epoch: 10 step: 359, loss is 0.00043005411862395704\n",
      "epoch: 10 step: 360, loss is 0.0033667017705738544\n",
      "epoch: 10 step: 361, loss is 0.016456671059131622\n",
      "epoch: 10 step: 362, loss is 0.00031359639251604676\n",
      "epoch: 10 step: 363, loss is 0.0005957564571872354\n",
      "epoch: 10 step: 364, loss is 0.0013212183257564902\n",
      "epoch: 10 step: 365, loss is 0.048978447914123535\n",
      "epoch: 10 step: 366, loss is 0.010196329094469547\n",
      "epoch: 10 step: 367, loss is 0.00642031105235219\n",
      "epoch: 10 step: 368, loss is 0.0035662981681525707\n",
      "epoch: 10 step: 369, loss is 0.0005685142823494971\n",
      "epoch: 10 step: 370, loss is 0.0009116786532104015\n",
      "epoch: 10 step: 371, loss is 0.0004045365785714239\n",
      "epoch: 10 step: 372, loss is 0.026538638398051262\n",
      "epoch: 10 step: 373, loss is 0.0034078112803399563\n",
      "epoch: 10 step: 374, loss is 0.0021623477805405855\n",
      "epoch: 10 step: 375, loss is 0.3192189335823059\n",
      "epoch: 10 step: 376, loss is 0.010451670736074448\n",
      "epoch: 10 step: 377, loss is 0.0003549352695699781\n",
      "epoch: 10 step: 378, loss is 0.017444606870412827\n",
      "epoch: 10 step: 379, loss is 0.0003424727183301002\n",
      "epoch: 10 step: 380, loss is 0.0021718237549066544\n",
      "epoch: 10 step: 381, loss is 0.00026393201551400125\n",
      "epoch: 10 step: 382, loss is 0.0006280516972765326\n",
      "epoch: 10 step: 383, loss is 0.0003791959024965763\n",
      "epoch: 10 step: 384, loss is 0.07414434105157852\n",
      "epoch: 10 step: 385, loss is 0.05942903831601143\n",
      "epoch: 10 step: 386, loss is 0.0048792981542646885\n",
      "epoch: 10 step: 387, loss is 0.0011290714610368013\n",
      "epoch: 10 step: 388, loss is 0.061359893530607224\n",
      "epoch: 10 step: 389, loss is 0.0029354405123740435\n",
      "epoch: 10 step: 390, loss is 0.001099425950087607\n",
      "epoch: 10 step: 391, loss is 8.98703292477876e-05\n",
      "epoch: 10 step: 392, loss is 0.0032917645294219255\n",
      "epoch: 10 step: 393, loss is 6.492566899396479e-05\n",
      "epoch: 10 step: 394, loss is 0.019537221640348434\n",
      "epoch: 10 step: 395, loss is 0.00012675458856392652\n",
      "epoch: 10 step: 396, loss is 0.010959755629301071\n",
      "epoch: 10 step: 397, loss is 0.000214345651329495\n",
      "epoch: 10 step: 398, loss is 0.0030595490243285894\n",
      "epoch: 10 step: 399, loss is 0.0016969215357676148\n",
      "epoch: 10 step: 400, loss is 0.003935130778700113\n",
      "epoch: 10 step: 401, loss is 0.00039546022890135646\n",
      "epoch: 10 step: 402, loss is 0.0018754614284262061\n",
      "epoch: 10 step: 403, loss is 0.12123899906873703\n",
      "epoch: 10 step: 404, loss is 0.0011283762287348509\n",
      "epoch: 10 step: 405, loss is 0.005020026583224535\n",
      "epoch: 10 step: 406, loss is 0.00043265128624625504\n",
      "epoch: 10 step: 407, loss is 0.00022667631856165826\n",
      "epoch: 10 step: 408, loss is 0.0016582449898123741\n",
      "epoch: 10 step: 409, loss is 6.520026363432407e-05\n",
      "epoch: 10 step: 410, loss is 8.120999700622633e-05\n",
      "epoch: 10 step: 411, loss is 5.319712363416329e-05\n",
      "epoch: 10 step: 412, loss is 0.0023607842158526182\n",
      "epoch: 10 step: 413, loss is 0.004043884575366974\n",
      "epoch: 10 step: 414, loss is 0.008743785321712494\n",
      "epoch: 10 step: 415, loss is 0.01426767185330391\n",
      "epoch: 10 step: 416, loss is 0.009847396053373814\n",
      "epoch: 10 step: 417, loss is 0.0387306772172451\n",
      "epoch: 10 step: 418, loss is 0.00018679008644539863\n",
      "epoch: 10 step: 419, loss is 7.31981490389444e-05\n",
      "epoch: 10 step: 420, loss is 0.01527342852205038\n",
      "epoch: 10 step: 421, loss is 6.242247764021158e-05\n",
      "epoch: 10 step: 422, loss is 0.0006874914397485554\n",
      "epoch: 10 step: 423, loss is 1.4833043678663671e-05\n",
      "epoch: 10 step: 424, loss is 0.0008112348150461912\n",
      "epoch: 10 step: 425, loss is 3.8986119761830196e-05\n",
      "epoch: 10 step: 426, loss is 0.001341989729553461\n",
      "epoch: 10 step: 427, loss is 0.017059139907360077\n",
      "epoch: 10 step: 428, loss is 3.299910531495698e-05\n",
      "epoch: 10 step: 429, loss is 0.00012588832760229707\n",
      "epoch: 10 step: 430, loss is 0.0017638220451772213\n",
      "epoch: 10 step: 431, loss is 0.0001277202827623114\n",
      "epoch: 10 step: 432, loss is 7.869322143960744e-05\n",
      "epoch: 10 step: 433, loss is 0.001031034393236041\n",
      "epoch: 10 step: 434, loss is 0.009748837910592556\n",
      "epoch: 10 step: 435, loss is 0.0025505085941404104\n",
      "epoch: 10 step: 436, loss is 0.0590270534157753\n",
      "epoch: 10 step: 437, loss is 6.138258322607726e-05\n",
      "epoch: 10 step: 438, loss is 5.977004548185505e-05\n",
      "epoch: 10 step: 439, loss is 0.006437909789383411\n",
      "epoch: 10 step: 440, loss is 0.009985879063606262\n",
      "epoch: 10 step: 441, loss is 0.0031216409988701344\n",
      "epoch: 10 step: 442, loss is 0.0011152873048558831\n",
      "epoch: 10 step: 443, loss is 0.00215180148370564\n",
      "epoch: 10 step: 444, loss is 0.00047411082778126\n",
      "epoch: 10 step: 445, loss is 0.00032288514194078743\n",
      "epoch: 10 step: 446, loss is 0.002019337145611644\n",
      "epoch: 10 step: 447, loss is 0.001354958862066269\n",
      "epoch: 10 step: 448, loss is 0.00016679092368576676\n",
      "epoch: 10 step: 449, loss is 0.0007206241134554148\n",
      "epoch: 10 step: 450, loss is 0.0072194733656942844\n",
      "epoch: 10 step: 451, loss is 0.0016698359977453947\n",
      "epoch: 10 step: 452, loss is 0.001042882795445621\n",
      "epoch: 10 step: 453, loss is 0.00012258256901986897\n",
      "epoch: 10 step: 454, loss is 5.615377449430525e-05\n",
      "epoch: 10 step: 455, loss is 0.04489005729556084\n",
      "epoch: 10 step: 456, loss is 0.002279243664816022\n",
      "epoch: 10 step: 457, loss is 0.0004351839015726\n",
      "epoch: 10 step: 458, loss is 7.3300311669299845e-06\n",
      "epoch: 10 step: 459, loss is 6.237378693185747e-05\n",
      "epoch: 10 step: 460, loss is 0.00080212636385113\n",
      "epoch: 10 step: 461, loss is 0.004424221348017454\n",
      "epoch: 10 step: 462, loss is 0.015056484378874302\n",
      "epoch: 10 step: 463, loss is 0.006148732267320156\n",
      "epoch: 10 step: 464, loss is 0.0504947192966938\n",
      "epoch: 10 step: 465, loss is 0.010340787470340729\n",
      "epoch: 10 step: 466, loss is 0.005578947253525257\n",
      "epoch: 10 step: 467, loss is 0.0002703582285903394\n",
      "epoch: 10 step: 468, loss is 0.009591639041900635\n",
      "epoch: 10 step: 469, loss is 0.11047252267599106\n",
      "epoch: 10 step: 470, loss is 0.00021350440511014313\n",
      "epoch: 10 step: 471, loss is 0.00036156392889097333\n",
      "epoch: 10 step: 472, loss is 0.0008073135395534337\n",
      "epoch: 10 step: 473, loss is 0.022388147190213203\n",
      "epoch: 10 step: 474, loss is 0.05421151593327522\n",
      "epoch: 10 step: 475, loss is 0.0006848559714853764\n",
      "epoch: 10 step: 476, loss is 0.0010178002994507551\n",
      "epoch: 10 step: 477, loss is 3.691854089993285e-06\n",
      "epoch: 10 step: 478, loss is 0.0003168561670463532\n",
      "epoch: 10 step: 479, loss is 8.394420001422986e-05\n",
      "epoch: 10 step: 480, loss is 0.000650951114948839\n",
      "epoch: 10 step: 481, loss is 0.0007275889511220157\n",
      "epoch: 10 step: 482, loss is 0.0018508764915168285\n",
      "epoch: 10 step: 483, loss is 0.0016254965448752046\n",
      "epoch: 10 step: 484, loss is 0.00037489779060706496\n",
      "epoch: 10 step: 485, loss is 0.02151280641555786\n",
      "epoch: 10 step: 486, loss is 8.373436867259443e-05\n",
      "epoch: 10 step: 487, loss is 0.001037096488289535\n",
      "epoch: 10 step: 488, loss is 3.843441299977712e-05\n",
      "epoch: 10 step: 489, loss is 0.0024321689270436764\n",
      "epoch: 10 step: 490, loss is 6.334600038826466e-05\n",
      "epoch: 10 step: 491, loss is 0.009949210099875927\n",
      "epoch: 10 step: 492, loss is 0.0004395778232719749\n",
      "epoch: 10 step: 493, loss is 0.00249648024328053\n",
      "epoch: 10 step: 494, loss is 0.03781760111451149\n",
      "epoch: 10 step: 495, loss is 0.00042299050255678594\n",
      "epoch: 10 step: 496, loss is 0.0013590850867331028\n",
      "epoch: 10 step: 497, loss is 0.0009568110690452158\n",
      "epoch: 10 step: 498, loss is 0.0017378156771883368\n",
      "epoch: 10 step: 499, loss is 0.00016129855066537857\n",
      "epoch: 10 step: 500, loss is 0.0034152932930737734\n",
      "epoch: 10 step: 501, loss is 0.02817169949412346\n",
      "epoch: 10 step: 502, loss is 0.0010713592637330294\n",
      "epoch: 10 step: 503, loss is 0.00020006098202429712\n",
      "epoch: 10 step: 504, loss is 0.0021070409566164017\n",
      "epoch: 10 step: 505, loss is 0.012567821890115738\n",
      "epoch: 10 step: 506, loss is 0.16490377485752106\n",
      "epoch: 10 step: 507, loss is 0.1720212996006012\n",
      "epoch: 10 step: 508, loss is 0.01789690926671028\n",
      "epoch: 10 step: 509, loss is 0.00252476055175066\n",
      "epoch: 10 step: 510, loss is 0.0024230079725384712\n",
      "epoch: 10 step: 511, loss is 0.0011996530229225755\n",
      "epoch: 10 step: 512, loss is 0.00919911079108715\n",
      "epoch: 10 step: 513, loss is 0.0004238613764755428\n",
      "epoch: 10 step: 514, loss is 0.000829444732517004\n",
      "epoch: 10 step: 515, loss is 0.0003483230830170214\n",
      "epoch: 10 step: 516, loss is 0.001969334203749895\n",
      "epoch: 10 step: 517, loss is 0.0006700571393594146\n",
      "epoch: 10 step: 518, loss is 0.0002746031677816063\n",
      "epoch: 10 step: 519, loss is 0.1343938261270523\n",
      "epoch: 10 step: 520, loss is 0.00022624789562541991\n",
      "epoch: 10 step: 521, loss is 0.00642727455124259\n",
      "epoch: 10 step: 522, loss is 0.02814171463251114\n",
      "epoch: 10 step: 523, loss is 0.0037878556177020073\n",
      "epoch: 10 step: 524, loss is 0.00034270985634066164\n",
      "epoch: 10 step: 525, loss is 0.0017312976997345686\n",
      "epoch: 10 step: 526, loss is 0.0014258570736274123\n",
      "epoch: 10 step: 527, loss is 0.014056028798222542\n",
      "epoch: 10 step: 528, loss is 2.9313096092664637e-05\n",
      "epoch: 10 step: 529, loss is 0.003330023493617773\n",
      "epoch: 10 step: 530, loss is 0.020477039739489555\n",
      "epoch: 10 step: 531, loss is 0.0028567833360284567\n",
      "epoch: 10 step: 532, loss is 0.009539380669593811\n",
      "epoch: 10 step: 533, loss is 0.0052459510043263435\n",
      "epoch: 10 step: 534, loss is 0.0002455279463902116\n",
      "epoch: 10 step: 535, loss is 0.0003631572180893272\n",
      "epoch: 10 step: 536, loss is 0.024065716192126274\n",
      "epoch: 10 step: 537, loss is 0.006121801678091288\n",
      "epoch: 10 step: 538, loss is 0.00033326196717098355\n",
      "epoch: 10 step: 539, loss is 0.020613690838217735\n",
      "epoch: 10 step: 540, loss is 0.004127714782953262\n",
      "epoch: 10 step: 541, loss is 0.001660438603721559\n",
      "epoch: 10 step: 542, loss is 0.0002554463571868837\n",
      "epoch: 10 step: 543, loss is 0.0013830673415213823\n",
      "epoch: 10 step: 544, loss is 0.021084191277623177\n",
      "epoch: 10 step: 545, loss is 0.0216048713773489\n",
      "epoch: 10 step: 546, loss is 0.0012333645718172193\n",
      "epoch: 10 step: 547, loss is 0.0030220546759665012\n",
      "epoch: 10 step: 548, loss is 0.00036297389306128025\n",
      "epoch: 10 step: 549, loss is 8.383598469663411e-05\n",
      "epoch: 10 step: 550, loss is 0.0005049094907008111\n",
      "epoch: 10 step: 551, loss is 0.0014017048524692655\n",
      "epoch: 10 step: 552, loss is 0.030250556766986847\n",
      "epoch: 10 step: 553, loss is 0.004609546158462763\n",
      "epoch: 10 step: 554, loss is 0.0011287026572972536\n",
      "epoch: 10 step: 555, loss is 0.00030833849450573325\n",
      "epoch: 10 step: 556, loss is 0.000521755893714726\n",
      "epoch: 10 step: 557, loss is 7.428663957398385e-05\n",
      "epoch: 10 step: 558, loss is 0.00030232706922106445\n",
      "epoch: 10 step: 559, loss is 0.003675269428640604\n",
      "epoch: 10 step: 560, loss is 0.004524186719208956\n",
      "epoch: 10 step: 561, loss is 7.88882898632437e-05\n",
      "epoch: 10 step: 562, loss is 0.000821382156573236\n",
      "epoch: 10 step: 563, loss is 0.07213423401117325\n",
      "epoch: 10 step: 564, loss is 0.0004539615474641323\n",
      "epoch: 10 step: 565, loss is 4.9626083637122065e-05\n",
      "epoch: 10 step: 566, loss is 0.0005193784600123763\n",
      "epoch: 10 step: 567, loss is 0.0003202143998350948\n",
      "epoch: 10 step: 568, loss is 2.8594899049494416e-05\n",
      "epoch: 10 step: 569, loss is 0.00021203054348006845\n",
      "epoch: 10 step: 570, loss is 0.0014266500947996974\n",
      "epoch: 10 step: 571, loss is 0.0015054993564262986\n",
      "epoch: 10 step: 572, loss is 0.0007031293353065848\n",
      "epoch: 10 step: 573, loss is 0.021078480407595634\n",
      "epoch: 10 step: 574, loss is 0.04802147299051285\n",
      "epoch: 10 step: 575, loss is 0.0001425419468432665\n",
      "epoch: 10 step: 576, loss is 0.027653981000185013\n",
      "epoch: 10 step: 577, loss is 5.63242138014175e-05\n",
      "epoch: 10 step: 578, loss is 0.03691304847598076\n",
      "epoch: 10 step: 579, loss is 0.003709167707711458\n",
      "epoch: 10 step: 580, loss is 0.0011705175274983048\n",
      "epoch: 10 step: 581, loss is 0.00038782067713327706\n",
      "epoch: 10 step: 582, loss is 0.004354765173047781\n",
      "epoch: 10 step: 583, loss is 0.00041110109305009246\n",
      "epoch: 10 step: 584, loss is 1.4497852134809364e-05\n",
      "epoch: 10 step: 585, loss is 0.0008113557123579085\n",
      "epoch: 10 step: 586, loss is 0.00043544056825339794\n",
      "epoch: 10 step: 587, loss is 0.0015649222768843174\n",
      "epoch: 10 step: 588, loss is 0.0001245014718733728\n",
      "epoch: 10 step: 589, loss is 0.07771646976470947\n",
      "epoch: 10 step: 590, loss is 0.00010704659507609904\n",
      "epoch: 10 step: 591, loss is 0.008705055341124535\n",
      "epoch: 10 step: 592, loss is 0.0005003750557079911\n",
      "epoch: 10 step: 593, loss is 0.0002819022920448333\n",
      "epoch: 10 step: 594, loss is 0.008377937600016594\n",
      "epoch: 10 step: 595, loss is 0.00035088846925646067\n",
      "epoch: 10 step: 596, loss is 0.005020471289753914\n",
      "epoch: 10 step: 597, loss is 0.002516969107091427\n",
      "epoch: 10 step: 598, loss is 0.0011013088515028358\n",
      "epoch: 10 step: 599, loss is 0.0015577010344713926\n",
      "epoch: 10 step: 600, loss is 0.0003041016752831638\n",
      "epoch: 10 step: 601, loss is 0.00014427516725845635\n",
      "epoch: 10 step: 602, loss is 0.010686689056456089\n",
      "epoch: 10 step: 603, loss is 0.00014727760571986437\n",
      "epoch: 10 step: 604, loss is 0.0006663117674179375\n",
      "epoch: 10 step: 605, loss is 0.0022929704282432795\n",
      "epoch: 10 step: 606, loss is 0.001896299421787262\n",
      "epoch: 10 step: 607, loss is 0.0004990143934264779\n",
      "epoch: 10 step: 608, loss is 0.0022881089244037867\n",
      "epoch: 10 step: 609, loss is 0.004395324736833572\n",
      "epoch: 10 step: 610, loss is 0.025302009657025337\n",
      "epoch: 10 step: 611, loss is 0.0007097303168848157\n",
      "epoch: 10 step: 612, loss is 0.004037827253341675\n",
      "epoch: 10 step: 613, loss is 0.009641604498028755\n",
      "epoch: 10 step: 614, loss is 0.009534747339785099\n",
      "epoch: 10 step: 615, loss is 0.006171614862978458\n",
      "epoch: 10 step: 616, loss is 0.0027503608725965023\n",
      "epoch: 10 step: 617, loss is 0.0018779321108013391\n",
      "epoch: 10 step: 618, loss is 0.008736644871532917\n",
      "epoch: 10 step: 619, loss is 0.00026686376077122986\n",
      "epoch: 10 step: 620, loss is 6.632857548538595e-05\n",
      "epoch: 10 step: 621, loss is 0.0004294767277315259\n",
      "epoch: 10 step: 622, loss is 0.007166794966906309\n",
      "epoch: 10 step: 623, loss is 0.00017707407823763788\n",
      "epoch: 10 step: 624, loss is 0.0001394055288983509\n",
      "epoch: 10 step: 625, loss is 0.008317562751471996\n",
      "epoch: 10 step: 626, loss is 0.0010064609814435244\n",
      "epoch: 10 step: 627, loss is 0.06738346070051193\n",
      "epoch: 10 step: 628, loss is 0.0004961570375598967\n",
      "epoch: 10 step: 629, loss is 0.0007905091042630374\n",
      "epoch: 10 step: 630, loss is 0.006363199558109045\n",
      "epoch: 10 step: 631, loss is 0.0070818765088915825\n",
      "epoch: 10 step: 632, loss is 0.0002969694905914366\n",
      "epoch: 10 step: 633, loss is 0.00031479899189434946\n",
      "epoch: 10 step: 634, loss is 0.001864476129412651\n",
      "epoch: 10 step: 635, loss is 0.0011989984195679426\n",
      "epoch: 10 step: 636, loss is 0.03153982385993004\n",
      "epoch: 10 step: 637, loss is 0.0008890751632861793\n",
      "epoch: 10 step: 638, loss is 0.001674925209954381\n",
      "epoch: 10 step: 639, loss is 0.00045683298958465457\n",
      "epoch: 10 step: 640, loss is 0.0006472672102972865\n",
      "epoch: 10 step: 641, loss is 0.0017497906228527427\n",
      "epoch: 10 step: 642, loss is 0.006708098575472832\n",
      "epoch: 10 step: 643, loss is 0.09660240262746811\n",
      "epoch: 10 step: 644, loss is 0.04390714690089226\n",
      "epoch: 10 step: 645, loss is 5.7512363127898425e-05\n",
      "epoch: 10 step: 646, loss is 9.246147237718105e-05\n",
      "epoch: 10 step: 647, loss is 0.0034744907170534134\n",
      "epoch: 10 step: 648, loss is 0.0025188683066517115\n",
      "epoch: 10 step: 649, loss is 0.00213548238389194\n",
      "epoch: 10 step: 650, loss is 0.0003768033639062196\n",
      "epoch: 10 step: 651, loss is 4.064886161359027e-05\n",
      "epoch: 10 step: 652, loss is 0.001269911415874958\n",
      "epoch: 10 step: 653, loss is 9.45914289331995e-05\n",
      "epoch: 10 step: 654, loss is 0.005327879451215267\n",
      "epoch: 10 step: 655, loss is 4.205574805382639e-05\n",
      "epoch: 10 step: 656, loss is 0.01367997843772173\n",
      "epoch: 10 step: 657, loss is 0.00022343199816532433\n",
      "epoch: 10 step: 658, loss is 0.006235878448933363\n",
      "epoch: 10 step: 659, loss is 0.0018992302939295769\n",
      "epoch: 10 step: 660, loss is 0.0007855081930756569\n",
      "epoch: 10 step: 661, loss is 0.00018548777734395117\n",
      "epoch: 10 step: 662, loss is 5.3071791626280174e-05\n",
      "epoch: 10 step: 663, loss is 0.00018976775754708797\n",
      "epoch: 10 step: 664, loss is 0.07445283979177475\n",
      "epoch: 10 step: 665, loss is 0.1957920342683792\n",
      "epoch: 10 step: 666, loss is 0.00011461269605206326\n",
      "epoch: 10 step: 667, loss is 0.000418408919358626\n",
      "epoch: 10 step: 668, loss is 0.01931474730372429\n",
      "epoch: 10 step: 669, loss is 2.5523077056277543e-05\n",
      "epoch: 10 step: 670, loss is 0.0034263429697602987\n",
      "epoch: 10 step: 671, loss is 0.04497992619872093\n",
      "epoch: 10 step: 672, loss is 0.00039790544542483985\n",
      "epoch: 10 step: 673, loss is 0.0005151156219653785\n",
      "epoch: 10 step: 674, loss is 0.00619767839089036\n",
      "epoch: 10 step: 675, loss is 0.000409754371503368\n",
      "epoch: 10 step: 676, loss is 0.00016422187036368996\n",
      "epoch: 10 step: 677, loss is 0.00013746162585448474\n",
      "epoch: 10 step: 678, loss is 0.0011492534540593624\n",
      "epoch: 10 step: 679, loss is 6.93698093527928e-05\n",
      "epoch: 10 step: 680, loss is 0.0001094322360586375\n",
      "epoch: 10 step: 681, loss is 0.0032345119398087263\n",
      "epoch: 10 step: 682, loss is 0.0001864032819867134\n",
      "epoch: 10 step: 683, loss is 0.0003165452799294144\n",
      "epoch: 10 step: 684, loss is 0.002507084747776389\n",
      "epoch: 10 step: 685, loss is 1.0621188266668469e-05\n",
      "epoch: 10 step: 686, loss is 0.002086712047457695\n",
      "epoch: 10 step: 687, loss is 9.722655522637069e-05\n",
      "epoch: 10 step: 688, loss is 4.666927270591259e-05\n",
      "epoch: 10 step: 689, loss is 0.00015314649499487132\n",
      "epoch: 10 step: 690, loss is 0.010560191236436367\n",
      "epoch: 10 step: 691, loss is 0.00018677179468795657\n",
      "epoch: 10 step: 692, loss is 9.016461990540847e-05\n",
      "epoch: 10 step: 693, loss is 0.027673425152897835\n",
      "epoch: 10 step: 694, loss is 0.014425652101635933\n",
      "epoch: 10 step: 695, loss is 3.673578248708509e-05\n",
      "epoch: 10 step: 696, loss is 5.567159678321332e-05\n",
      "epoch: 10 step: 697, loss is 2.126732397300657e-05\n",
      "epoch: 10 step: 698, loss is 0.014309559017419815\n",
      "epoch: 10 step: 699, loss is 0.001096797059290111\n",
      "epoch: 10 step: 700, loss is 0.024219496175646782\n",
      "epoch: 10 step: 701, loss is 0.0010380259482190013\n",
      "epoch: 10 step: 702, loss is 0.01760057359933853\n",
      "epoch: 10 step: 703, loss is 0.0008339978521689773\n",
      "epoch: 10 step: 704, loss is 0.016786955296993256\n",
      "epoch: 10 step: 705, loss is 0.0003131783741991967\n",
      "epoch: 10 step: 706, loss is 0.00010008587560150772\n",
      "epoch: 10 step: 707, loss is 0.00907665304839611\n",
      "epoch: 10 step: 708, loss is 4.363512925920077e-05\n",
      "epoch: 10 step: 709, loss is 0.0006773482309654355\n",
      "epoch: 10 step: 710, loss is 7.231843483168632e-05\n",
      "epoch: 10 step: 711, loss is 0.13162191212177277\n",
      "epoch: 10 step: 712, loss is 0.00021083388128317893\n",
      "epoch: 10 step: 713, loss is 0.013994131237268448\n",
      "epoch: 10 step: 714, loss is 0.00042448166641406715\n",
      "epoch: 10 step: 715, loss is 0.0009629628621041775\n",
      "epoch: 10 step: 716, loss is 2.7735815820051357e-05\n",
      "epoch: 10 step: 717, loss is 0.011048968881368637\n",
      "epoch: 10 step: 718, loss is 0.0003558747412171215\n",
      "epoch: 10 step: 719, loss is 0.002949582180008292\n",
      "epoch: 10 step: 720, loss is 0.00018590841500554234\n",
      "epoch: 10 step: 721, loss is 0.0004836842999793589\n",
      "epoch: 10 step: 722, loss is 0.054670631885528564\n",
      "epoch: 10 step: 723, loss is 9.195166057907045e-05\n",
      "epoch: 10 step: 724, loss is 0.03355832025408745\n",
      "epoch: 10 step: 725, loss is 0.0002858604129869491\n",
      "epoch: 10 step: 726, loss is 0.006566304713487625\n",
      "epoch: 10 step: 727, loss is 0.001656258711591363\n",
      "epoch: 10 step: 728, loss is 0.00048493847134523094\n",
      "epoch: 10 step: 729, loss is 0.0007610765169374645\n",
      "epoch: 10 step: 730, loss is 0.00029339565662667155\n",
      "epoch: 10 step: 731, loss is 0.00026017564232461154\n",
      "epoch: 10 step: 732, loss is 0.015830665826797485\n",
      "epoch: 10 step: 733, loss is 0.0021300273947417736\n",
      "epoch: 10 step: 734, loss is 0.00020359066547825933\n",
      "epoch: 10 step: 735, loss is 0.0002958696859423071\n",
      "epoch: 10 step: 736, loss is 5.947699537500739e-05\n",
      "epoch: 10 step: 737, loss is 0.00019595999037846923\n",
      "epoch: 10 step: 738, loss is 0.010887923650443554\n",
      "epoch: 10 step: 739, loss is 0.00011738665489247069\n",
      "epoch: 10 step: 740, loss is 0.008429266512393951\n",
      "epoch: 10 step: 741, loss is 2.335001772735268e-05\n",
      "epoch: 10 step: 742, loss is 0.040253523737192154\n",
      "epoch: 10 step: 743, loss is 0.00018563735648058355\n",
      "epoch: 10 step: 744, loss is 0.0003904169425368309\n",
      "epoch: 10 step: 745, loss is 0.00017108366591855884\n",
      "epoch: 10 step: 746, loss is 0.040972575545310974\n",
      "epoch: 10 step: 747, loss is 6.523194770124974e-06\n",
      "epoch: 10 step: 748, loss is 0.00022019693278707564\n",
      "epoch: 10 step: 749, loss is 0.0006358562386594713\n",
      "epoch: 10 step: 750, loss is 0.0005204131011851132\n",
      "epoch: 10 step: 751, loss is 0.00024054202367551625\n",
      "epoch: 10 step: 752, loss is 0.0003780646075028926\n",
      "epoch: 10 step: 753, loss is 0.0017388970591127872\n",
      "epoch: 10 step: 754, loss is 0.00015766186697874218\n",
      "epoch: 10 step: 755, loss is 0.0005623240722343326\n",
      "epoch: 10 step: 756, loss is 0.00934575218707323\n",
      "epoch: 10 step: 757, loss is 1.2512354260252323e-05\n",
      "epoch: 10 step: 758, loss is 0.013826507143676281\n",
      "epoch: 10 step: 759, loss is 0.030072107911109924\n",
      "epoch: 10 step: 760, loss is 0.00027231397689320147\n",
      "epoch: 10 step: 761, loss is 4.832531703868881e-05\n",
      "epoch: 10 step: 762, loss is 0.0012814184883609414\n",
      "epoch: 10 step: 763, loss is 0.2269211858510971\n",
      "epoch: 10 step: 764, loss is 0.08751451969146729\n",
      "epoch: 10 step: 765, loss is 3.419968925300054e-05\n",
      "epoch: 10 step: 766, loss is 0.00026956695364788175\n",
      "epoch: 10 step: 767, loss is 0.0017441378440707922\n",
      "epoch: 10 step: 768, loss is 0.00038014212623238564\n",
      "epoch: 10 step: 769, loss is 0.0003294752386864275\n",
      "epoch: 10 step: 770, loss is 0.0013741010334342718\n",
      "epoch: 10 step: 771, loss is 0.0009801051346585155\n",
      "epoch: 10 step: 772, loss is 0.000327370798913762\n",
      "epoch: 10 step: 773, loss is 0.00014833040768280625\n",
      "epoch: 10 step: 774, loss is 0.0006068177754059434\n",
      "epoch: 10 step: 775, loss is 0.001205233740620315\n",
      "epoch: 10 step: 776, loss is 0.00010699646372813731\n",
      "epoch: 10 step: 777, loss is 7.365215424215421e-06\n",
      "epoch: 10 step: 778, loss is 0.0003248572174925357\n",
      "epoch: 10 step: 779, loss is 2.043651693384163e-05\n",
      "epoch: 10 step: 780, loss is 0.0022029858082532883\n",
      "epoch: 10 step: 781, loss is 0.002612672047689557\n",
      "epoch: 10 step: 782, loss is 0.016607381403446198\n",
      "epoch: 10 step: 783, loss is 0.14620758593082428\n",
      "epoch: 10 step: 784, loss is 0.0023640647996217012\n",
      "epoch: 10 step: 785, loss is 1.8982462279382162e-05\n",
      "epoch: 10 step: 786, loss is 0.0014413795433938503\n",
      "epoch: 10 step: 787, loss is 0.0004369296657387167\n",
      "epoch: 10 step: 788, loss is 0.0008759969496168196\n",
      "epoch: 10 step: 789, loss is 4.190357503830455e-05\n",
      "epoch: 10 step: 790, loss is 8.712710405234247e-05\n",
      "epoch: 10 step: 791, loss is 4.379881647764705e-05\n",
      "epoch: 10 step: 792, loss is 0.0003130054974462837\n",
      "epoch: 10 step: 793, loss is 7.641691627213731e-05\n",
      "epoch: 10 step: 794, loss is 0.0003448736388236284\n",
      "epoch: 10 step: 795, loss is 8.821883966447785e-06\n",
      "epoch: 10 step: 796, loss is 0.07021760195493698\n",
      "epoch: 10 step: 797, loss is 0.001439381972886622\n",
      "epoch: 10 step: 798, loss is 5.9130303270649165e-05\n",
      "epoch: 10 step: 799, loss is 0.00022383594478014857\n",
      "epoch: 10 step: 800, loss is 0.00033869186881929636\n",
      "epoch: 10 step: 801, loss is 0.00044757171417586505\n",
      "epoch: 10 step: 802, loss is 0.00023359475017059594\n",
      "epoch: 10 step: 803, loss is 0.00042429816676303744\n",
      "epoch: 10 step: 804, loss is 0.002546473406255245\n",
      "epoch: 10 step: 805, loss is 1.6204767234739847e-05\n",
      "epoch: 10 step: 806, loss is 0.0023707826621830463\n",
      "epoch: 10 step: 807, loss is 0.013503494672477245\n",
      "epoch: 10 step: 808, loss is 0.09590224176645279\n",
      "epoch: 10 step: 809, loss is 0.00045761349610984325\n",
      "epoch: 10 step: 810, loss is 4.563602033158531e-06\n",
      "epoch: 10 step: 811, loss is 0.0002228817029390484\n",
      "epoch: 10 step: 812, loss is 1.7984944861382246e-05\n",
      "epoch: 10 step: 813, loss is 0.00015672104200348258\n",
      "epoch: 10 step: 814, loss is 0.0012747709406539798\n",
      "epoch: 10 step: 815, loss is 0.008475946262478828\n",
      "epoch: 10 step: 816, loss is 0.2758277952671051\n",
      "epoch: 10 step: 817, loss is 0.0025060533080250025\n",
      "epoch: 10 step: 818, loss is 1.5179719412117265e-05\n",
      "epoch: 10 step: 819, loss is 0.00011394226021366194\n",
      "epoch: 10 step: 820, loss is 0.01819879561662674\n",
      "epoch: 10 step: 821, loss is 0.00023222887830343097\n",
      "epoch: 10 step: 822, loss is 0.00019639961828943342\n",
      "epoch: 10 step: 823, loss is 0.0036094586830586195\n",
      "epoch: 10 step: 824, loss is 0.00039052971987985075\n",
      "epoch: 10 step: 825, loss is 0.010838842019438744\n",
      "epoch: 10 step: 826, loss is 0.014319639652967453\n",
      "epoch: 10 step: 827, loss is 0.050414543598890305\n",
      "epoch: 10 step: 828, loss is 0.0003789344918914139\n",
      "epoch: 10 step: 829, loss is 0.0016470995033159852\n",
      "epoch: 10 step: 830, loss is 0.0004795531858690083\n",
      "epoch: 10 step: 831, loss is 0.0007059400668367743\n",
      "epoch: 10 step: 832, loss is 0.00018332706531509757\n",
      "epoch: 10 step: 833, loss is 0.049907948821783066\n",
      "epoch: 10 step: 834, loss is 0.0014358081389218569\n",
      "epoch: 10 step: 835, loss is 0.0013363766483962536\n",
      "epoch: 10 step: 836, loss is 0.001705150119960308\n",
      "epoch: 10 step: 837, loss is 0.0015959149459376931\n",
      "epoch: 10 step: 838, loss is 0.004120940342545509\n",
      "epoch: 10 step: 839, loss is 0.01969587244093418\n",
      "epoch: 10 step: 840, loss is 0.00010569735604804009\n",
      "epoch: 10 step: 841, loss is 0.11507271975278854\n",
      "epoch: 10 step: 842, loss is 0.001215704483911395\n",
      "epoch: 10 step: 843, loss is 0.00766904978081584\n",
      "epoch: 10 step: 844, loss is 0.00022529535635840148\n",
      "epoch: 10 step: 845, loss is 6.491637032013386e-05\n",
      "epoch: 10 step: 846, loss is 3.766578447539359e-05\n",
      "epoch: 10 step: 847, loss is 0.00013872100680600852\n",
      "epoch: 10 step: 848, loss is 0.00023864841205067933\n",
      "epoch: 10 step: 849, loss is 0.00017832950106821954\n",
      "epoch: 10 step: 850, loss is 0.0063094510696828365\n",
      "epoch: 10 step: 851, loss is 6.072310043236939e-06\n",
      "epoch: 10 step: 852, loss is 0.004559336695820093\n",
      "epoch: 10 step: 853, loss is 0.018147006630897522\n",
      "epoch: 10 step: 854, loss is 9.719064109958708e-05\n",
      "epoch: 10 step: 855, loss is 8.289514516945928e-05\n",
      "epoch: 10 step: 856, loss is 0.0014959550462663174\n",
      "epoch: 10 step: 857, loss is 0.0003480213927105069\n",
      "epoch: 10 step: 858, loss is 0.0017434979090467095\n",
      "epoch: 10 step: 859, loss is 7.353528053499758e-05\n",
      "epoch: 10 step: 860, loss is 0.004685183055698872\n",
      "epoch: 10 step: 861, loss is 0.0005317253526300192\n",
      "epoch: 10 step: 862, loss is 0.030698951333761215\n",
      "epoch: 10 step: 863, loss is 0.0004574825579766184\n",
      "epoch: 10 step: 864, loss is 0.00015501714369747788\n",
      "epoch: 10 step: 865, loss is 0.0009456769330427051\n",
      "epoch: 10 step: 866, loss is 4.934696335112676e-05\n",
      "epoch: 10 step: 867, loss is 0.00020391149155329913\n",
      "epoch: 10 step: 868, loss is 0.0002477609377820045\n",
      "epoch: 10 step: 869, loss is 0.003825575578957796\n",
      "epoch: 10 step: 870, loss is 0.0012344904243946075\n",
      "epoch: 10 step: 871, loss is 0.0014742257772013545\n",
      "epoch: 10 step: 872, loss is 0.0012309723533689976\n",
      "epoch: 10 step: 873, loss is 0.011138933710753918\n",
      "epoch: 10 step: 874, loss is 0.015540855005383492\n",
      "epoch: 10 step: 875, loss is 0.000876246253028512\n",
      "epoch: 10 step: 876, loss is 0.0035501408856362104\n",
      "epoch: 10 step: 877, loss is 0.0018554094713181257\n",
      "epoch: 10 step: 878, loss is 0.001221253303810954\n",
      "epoch: 10 step: 879, loss is 0.0004451982968021184\n",
      "epoch: 10 step: 880, loss is 9.497276187175885e-05\n",
      "epoch: 10 step: 881, loss is 0.0006773890345357358\n",
      "epoch: 10 step: 882, loss is 0.0013289658818393946\n",
      "epoch: 10 step: 883, loss is 0.0941639393568039\n",
      "epoch: 10 step: 884, loss is 0.00011722138879122213\n",
      "epoch: 10 step: 885, loss is 0.0004915817989967763\n",
      "epoch: 10 step: 886, loss is 0.00046475842827931046\n",
      "epoch: 10 step: 887, loss is 0.06864306330680847\n",
      "epoch: 10 step: 888, loss is 0.0013339754659682512\n",
      "epoch: 10 step: 889, loss is 0.0007429407560266554\n",
      "epoch: 10 step: 890, loss is 5.383135430747643e-06\n",
      "epoch: 10 step: 891, loss is 0.00011779625492636114\n",
      "epoch: 10 step: 892, loss is 0.0025530592538416386\n",
      "epoch: 10 step: 893, loss is 0.00048394690384157\n",
      "epoch: 10 step: 894, loss is 0.0007089625578373671\n",
      "epoch: 10 step: 895, loss is 0.002672129077836871\n",
      "epoch: 10 step: 896, loss is 0.00046408706111833453\n",
      "epoch: 10 step: 897, loss is 0.006646524649113417\n",
      "epoch: 10 step: 898, loss is 0.04552799090743065\n",
      "epoch: 10 step: 899, loss is 0.003031347878277302\n",
      "epoch: 10 step: 900, loss is 1.2212973160785623e-05\n",
      "epoch: 10 step: 901, loss is 5.336695539881475e-05\n",
      "epoch: 10 step: 902, loss is 0.005840091034770012\n",
      "epoch: 10 step: 903, loss is 0.014956338331103325\n",
      "epoch: 10 step: 904, loss is 0.0004970117588527501\n",
      "epoch: 10 step: 905, loss is 0.0002815738844219595\n",
      "epoch: 10 step: 906, loss is 0.0003930944367311895\n",
      "epoch: 10 step: 907, loss is 0.00012529759260360152\n",
      "epoch: 10 step: 908, loss is 3.624190139817074e-05\n",
      "epoch: 10 step: 909, loss is 0.00031596634653396904\n",
      "epoch: 10 step: 910, loss is 2.45539649768034e-05\n",
      "epoch: 10 step: 911, loss is 0.0002411135210422799\n",
      "epoch: 10 step: 912, loss is 0.00025107726105488837\n",
      "epoch: 10 step: 913, loss is 0.00044493074528872967\n",
      "epoch: 10 step: 914, loss is 0.0319560244679451\n",
      "epoch: 10 step: 915, loss is 3.354188083903864e-05\n",
      "epoch: 10 step: 916, loss is 2.0852214220212772e-05\n",
      "epoch: 10 step: 917, loss is 0.005720627028495073\n",
      "epoch: 10 step: 918, loss is 0.00011983954027527943\n",
      "epoch: 10 step: 919, loss is 2.0168077753623948e-05\n",
      "epoch: 10 step: 920, loss is 0.00017746741650626063\n",
      "epoch: 10 step: 921, loss is 0.01998688280582428\n",
      "epoch: 10 step: 922, loss is 0.0018763589905574918\n",
      "epoch: 10 step: 923, loss is 0.003056782064959407\n",
      "epoch: 10 step: 924, loss is 0.00020671615493483841\n",
      "epoch: 10 step: 925, loss is 6.203862722031772e-05\n",
      "epoch: 10 step: 926, loss is 0.00014640188601333648\n",
      "epoch: 10 step: 927, loss is 0.0009505759226158261\n",
      "epoch: 10 step: 928, loss is 0.039827633649110794\n",
      "epoch: 10 step: 929, loss is 0.0030844463035464287\n",
      "epoch: 10 step: 930, loss is 3.2708339858800173e-06\n",
      "epoch: 10 step: 931, loss is 0.05156872794032097\n",
      "epoch: 10 step: 932, loss is 7.647981692571193e-05\n",
      "epoch: 10 step: 933, loss is 0.0006680525839328766\n",
      "epoch: 10 step: 934, loss is 0.21193943917751312\n",
      "epoch: 10 step: 935, loss is 0.13694801926612854\n",
      "epoch: 10 step: 936, loss is 0.008381024934351444\n",
      "epoch: 10 step: 937, loss is 0.004119286313652992\n",
      "epoch: 10 step: 938, loss is 4.799250382347964e-05\n",
      "epoch: 10 step: 939, loss is 4.164349229540676e-05\n",
      "epoch: 10 step: 940, loss is 0.0009388800244778395\n",
      "epoch: 10 step: 941, loss is 0.0011465717107057571\n",
      "epoch: 10 step: 942, loss is 0.039549581706523895\n",
      "epoch: 10 step: 943, loss is 0.0006526481010951102\n",
      "epoch: 10 step: 944, loss is 0.029098238795995712\n",
      "epoch: 10 step: 945, loss is 0.0006842776783742011\n",
      "epoch: 10 step: 946, loss is 0.0010073226876556873\n",
      "epoch: 10 step: 947, loss is 0.00531348679214716\n",
      "epoch: 10 step: 948, loss is 0.03332072123885155\n",
      "epoch: 10 step: 949, loss is 0.00012623524526134133\n",
      "epoch: 10 step: 950, loss is 0.001590552506968379\n",
      "epoch: 10 step: 951, loss is 0.022109923884272575\n",
      "epoch: 10 step: 952, loss is 8.475979848299176e-05\n",
      "epoch: 10 step: 953, loss is 0.05229475721716881\n",
      "epoch: 10 step: 954, loss is 0.006174514535814524\n",
      "epoch: 10 step: 955, loss is 5.199828956392594e-05\n",
      "epoch: 10 step: 956, loss is 0.0016285794554278255\n",
      "epoch: 10 step: 957, loss is 0.00011144136806251481\n",
      "epoch: 10 step: 958, loss is 0.0007678844849579036\n",
      "epoch: 10 step: 959, loss is 0.00020172189397271723\n",
      "epoch: 10 step: 960, loss is 0.020461371168494225\n",
      "epoch: 10 step: 961, loss is 0.000979169155471027\n",
      "epoch: 10 step: 962, loss is 0.03520387411117554\n",
      "epoch: 10 step: 963, loss is 0.0006415621610358357\n",
      "epoch: 10 step: 964, loss is 0.0020461285021156073\n",
      "epoch: 10 step: 965, loss is 0.0006867108750157058\n",
      "epoch: 10 step: 966, loss is 0.0013428071979433298\n",
      "epoch: 10 step: 967, loss is 0.006219508592039347\n",
      "epoch: 10 step: 968, loss is 0.0003646187833510339\n",
      "epoch: 10 step: 969, loss is 0.00025217115762643516\n",
      "epoch: 10 step: 970, loss is 0.04331483319401741\n",
      "epoch: 10 step: 971, loss is 0.008973819203674793\n",
      "epoch: 10 step: 972, loss is 0.00022290615015663207\n",
      "epoch: 10 step: 973, loss is 8.231064566643909e-05\n",
      "epoch: 10 step: 974, loss is 0.01759832538664341\n",
      "epoch: 10 step: 975, loss is 0.002403275342658162\n",
      "epoch: 10 step: 976, loss is 3.273032052675262e-05\n",
      "epoch: 10 step: 977, loss is 0.0005936672096140683\n",
      "epoch: 10 step: 978, loss is 0.00010245537851005793\n",
      "epoch: 10 step: 979, loss is 5.545685417018831e-05\n",
      "epoch: 10 step: 980, loss is 5.1670872380782384e-06\n",
      "epoch: 10 step: 981, loss is 0.0001379854656988755\n",
      "epoch: 10 step: 982, loss is 1.0878375178435817e-05\n",
      "epoch: 10 step: 983, loss is 9.618065814720467e-05\n",
      "epoch: 10 step: 984, loss is 0.00017159579147119075\n",
      "epoch: 10 step: 985, loss is 0.0002152132074115798\n",
      "epoch: 10 step: 986, loss is 1.8967881260323338e-05\n",
      "epoch: 10 step: 987, loss is 0.03133247047662735\n",
      "epoch: 10 step: 988, loss is 2.2634512788499705e-05\n",
      "epoch: 10 step: 989, loss is 2.354405751248123e-06\n",
      "epoch: 10 step: 990, loss is 0.00025296935928054154\n",
      "epoch: 10 step: 991, loss is 0.00014776384341530502\n",
      "epoch: 10 step: 992, loss is 3.6186080251354724e-05\n",
      "epoch: 10 step: 993, loss is 0.0004476719768717885\n",
      "epoch: 10 step: 994, loss is 0.190015509724617\n",
      "epoch: 10 step: 995, loss is 0.004444067366421223\n",
      "epoch: 10 step: 996, loss is 4.179178722552024e-05\n",
      "epoch: 10 step: 997, loss is 0.000639286357909441\n",
      "epoch: 10 step: 998, loss is 0.005974557716399431\n",
      "epoch: 10 step: 999, loss is 0.0006401996361091733\n",
      "epoch: 10 step: 1000, loss is 4.8881272959988564e-05\n",
      "epoch: 10 step: 1001, loss is 0.0027111663948744535\n",
      "epoch: 10 step: 1002, loss is 0.00039394735358655453\n",
      "epoch: 10 step: 1003, loss is 0.0008721297490410507\n",
      "epoch: 10 step: 1004, loss is 0.00014247035142034292\n",
      "epoch: 10 step: 1005, loss is 0.0004584958660416305\n",
      "epoch: 10 step: 1006, loss is 0.005305361934006214\n",
      "epoch: 10 step: 1007, loss is 5.845757550559938e-05\n",
      "epoch: 10 step: 1008, loss is 0.0011979422997683287\n",
      "epoch: 10 step: 1009, loss is 0.00012143411004217342\n",
      "epoch: 10 step: 1010, loss is 0.02547641471028328\n",
      "epoch: 10 step: 1011, loss is 0.00014271665713749826\n",
      "epoch: 10 step: 1012, loss is 1.2655653335968964e-05\n",
      "epoch: 10 step: 1013, loss is 1.746487214404624e-05\n",
      "epoch: 10 step: 1014, loss is 0.0009105810895562172\n",
      "epoch: 10 step: 1015, loss is 0.008639524690806866\n",
      "epoch: 10 step: 1016, loss is 0.0016035514418035746\n",
      "epoch: 10 step: 1017, loss is 0.0012406037421897054\n",
      "epoch: 10 step: 1018, loss is 0.004392913077026606\n",
      "epoch: 10 step: 1019, loss is 0.010335988365113735\n",
      "epoch: 10 step: 1020, loss is 0.0042753382585942745\n",
      "epoch: 10 step: 1021, loss is 0.012432890012860298\n",
      "epoch: 10 step: 1022, loss is 0.0004097356286365539\n",
      "epoch: 10 step: 1023, loss is 0.0008783694356679916\n",
      "epoch: 10 step: 1024, loss is 0.0005166828050278127\n",
      "epoch: 10 step: 1025, loss is 0.00018146906222682446\n",
      "epoch: 10 step: 1026, loss is 0.0009374963701702654\n",
      "epoch: 10 step: 1027, loss is 7.629387255292386e-05\n",
      "epoch: 10 step: 1028, loss is 0.06800519675016403\n",
      "epoch: 10 step: 1029, loss is 0.00031937973108142614\n",
      "epoch: 10 step: 1030, loss is 0.003067352343350649\n",
      "epoch: 10 step: 1031, loss is 0.004755973815917969\n",
      "epoch: 10 step: 1032, loss is 6.193077570060268e-05\n",
      "epoch: 10 step: 1033, loss is 0.0024344013072550297\n",
      "epoch: 10 step: 1034, loss is 2.1137018848094158e-05\n",
      "epoch: 10 step: 1035, loss is 4.705365063273348e-05\n",
      "epoch: 10 step: 1036, loss is 0.000128691375721246\n",
      "epoch: 10 step: 1037, loss is 0.0033614058047533035\n",
      "epoch: 10 step: 1038, loss is 0.0007860743789933622\n",
      "epoch: 10 step: 1039, loss is 0.00043541548075154424\n",
      "epoch: 10 step: 1040, loss is 0.00016710979980416596\n",
      "epoch: 10 step: 1041, loss is 8.024658018257469e-05\n",
      "epoch: 10 step: 1042, loss is 7.732214726274833e-05\n",
      "epoch: 10 step: 1043, loss is 0.00023899285588413477\n",
      "epoch: 10 step: 1044, loss is 7.281482976395637e-05\n",
      "epoch: 10 step: 1045, loss is 7.119383371900767e-05\n",
      "epoch: 10 step: 1046, loss is 5.0057718908647075e-05\n",
      "epoch: 10 step: 1047, loss is 0.0028912832494825125\n",
      "epoch: 10 step: 1048, loss is 0.09430807083845139\n",
      "epoch: 10 step: 1049, loss is 0.005154846236109734\n",
      "epoch: 10 step: 1050, loss is 0.00024089486396405846\n",
      "epoch: 10 step: 1051, loss is 0.0014856430934742093\n",
      "epoch: 10 step: 1052, loss is 1.8771783288684674e-05\n",
      "epoch: 10 step: 1053, loss is 0.005335555877536535\n",
      "epoch: 10 step: 1054, loss is 7.982837269082665e-05\n",
      "epoch: 10 step: 1055, loss is 0.0002957661054097116\n",
      "epoch: 10 step: 1056, loss is 0.12325780093669891\n",
      "epoch: 10 step: 1057, loss is 0.00019563779642339796\n",
      "epoch: 10 step: 1058, loss is 0.001364980824291706\n",
      "epoch: 10 step: 1059, loss is 0.00046462684986181557\n",
      "epoch: 10 step: 1060, loss is 0.0003005103790201247\n",
      "epoch: 10 step: 1061, loss is 0.008942710235714912\n",
      "epoch: 10 step: 1062, loss is 0.00022902847558725625\n",
      "epoch: 10 step: 1063, loss is 0.011861042119562626\n",
      "epoch: 10 step: 1064, loss is 0.06920906901359558\n",
      "epoch: 10 step: 1065, loss is 0.021428605541586876\n",
      "epoch: 10 step: 1066, loss is 0.018459204584360123\n",
      "epoch: 10 step: 1067, loss is 1.1327927495585755e-05\n",
      "epoch: 10 step: 1068, loss is 3.0696686735609546e-05\n",
      "epoch: 10 step: 1069, loss is 9.632905857870355e-05\n",
      "epoch: 10 step: 1070, loss is 1.4776023817830719e-05\n",
      "epoch: 10 step: 1071, loss is 0.0031151375733315945\n",
      "epoch: 10 step: 1072, loss is 0.00038112117908895016\n",
      "epoch: 10 step: 1073, loss is 0.033299967646598816\n",
      "epoch: 10 step: 1074, loss is 2.983878403028939e-05\n",
      "epoch: 10 step: 1075, loss is 0.0003531380498316139\n",
      "epoch: 10 step: 1076, loss is 3.992466372437775e-05\n",
      "epoch: 10 step: 1077, loss is 0.015100279822945595\n",
      "epoch: 10 step: 1078, loss is 0.02066841907799244\n",
      "epoch: 10 step: 1079, loss is 0.00026659309514798224\n",
      "epoch: 10 step: 1080, loss is 0.004836345091462135\n",
      "epoch: 10 step: 1081, loss is 0.002396133029833436\n",
      "epoch: 10 step: 1082, loss is 1.1152458682772703e-05\n",
      "epoch: 10 step: 1083, loss is 0.009216357953846455\n",
      "epoch: 10 step: 1084, loss is 3.357077730470337e-05\n",
      "epoch: 10 step: 1085, loss is 7.21933101885952e-05\n",
      "epoch: 10 step: 1086, loss is 0.06421811133623123\n",
      "epoch: 10 step: 1087, loss is 0.0002225376374553889\n",
      "epoch: 10 step: 1088, loss is 0.0004461269127205014\n",
      "epoch: 10 step: 1089, loss is 0.0011615058174356818\n",
      "epoch: 10 step: 1090, loss is 8.89167349669151e-05\n",
      "epoch: 10 step: 1091, loss is 0.0004476898757275194\n",
      "epoch: 10 step: 1092, loss is 0.03646416962146759\n",
      "epoch: 10 step: 1093, loss is 0.0007918981136754155\n",
      "epoch: 10 step: 1094, loss is 0.0012467147316783667\n",
      "epoch: 10 step: 1095, loss is 0.0004730838700197637\n",
      "epoch: 10 step: 1096, loss is 0.01962035521864891\n",
      "epoch: 10 step: 1097, loss is 0.05980927124619484\n",
      "epoch: 10 step: 1098, loss is 0.00013074536400381476\n",
      "epoch: 10 step: 1099, loss is 0.004574027843773365\n",
      "epoch: 10 step: 1100, loss is 0.062144361436367035\n",
      "epoch: 10 step: 1101, loss is 0.0009871306829154491\n",
      "epoch: 10 step: 1102, loss is 0.04394323006272316\n",
      "epoch: 10 step: 1103, loss is 0.028368236497044563\n",
      "epoch: 10 step: 1104, loss is 0.003219547215849161\n",
      "epoch: 10 step: 1105, loss is 0.000948991219047457\n",
      "epoch: 10 step: 1106, loss is 0.00021799876412842423\n",
      "epoch: 10 step: 1107, loss is 3.1898780434858054e-05\n",
      "epoch: 10 step: 1108, loss is 0.041920702904462814\n",
      "epoch: 10 step: 1109, loss is 0.004115591756999493\n",
      "epoch: 10 step: 1110, loss is 0.0019090292043983936\n",
      "epoch: 10 step: 1111, loss is 0.010023090057075024\n",
      "epoch: 10 step: 1112, loss is 0.0017246664501726627\n",
      "epoch: 10 step: 1113, loss is 0.00954048428684473\n",
      "epoch: 10 step: 1114, loss is 0.00240019871853292\n",
      "epoch: 10 step: 1115, loss is 1.925304241012782e-05\n",
      "epoch: 10 step: 1116, loss is 6.277731154114008e-05\n",
      "epoch: 10 step: 1117, loss is 0.025510413572192192\n",
      "epoch: 10 step: 1118, loss is 0.00011433140025474131\n",
      "epoch: 10 step: 1119, loss is 0.003153708064928651\n",
      "epoch: 10 step: 1120, loss is 0.0008296017185784876\n",
      "epoch: 10 step: 1121, loss is 0.01018571387976408\n",
      "epoch: 10 step: 1122, loss is 4.7615747462259606e-05\n",
      "epoch: 10 step: 1123, loss is 9.125587530434132e-05\n",
      "epoch: 10 step: 1124, loss is 0.00027418401441536844\n",
      "epoch: 10 step: 1125, loss is 0.00038533363840542734\n",
      "epoch: 10 step: 1126, loss is 0.03984052687883377\n",
      "epoch: 10 step: 1127, loss is 0.0010357087012380362\n",
      "epoch: 10 step: 1128, loss is 0.008426324464380741\n",
      "epoch: 10 step: 1129, loss is 0.0011162664741277695\n",
      "epoch: 10 step: 1130, loss is 0.20595380663871765\n",
      "epoch: 10 step: 1131, loss is 0.00044485958642326295\n",
      "epoch: 10 step: 1132, loss is 0.00480811670422554\n",
      "epoch: 10 step: 1133, loss is 0.0011180348228663206\n",
      "epoch: 10 step: 1134, loss is 0.008868463337421417\n",
      "epoch: 10 step: 1135, loss is 0.00013991762534715235\n",
      "epoch: 10 step: 1136, loss is 2.4875027520465665e-05\n",
      "epoch: 10 step: 1137, loss is 0.029704660177230835\n",
      "epoch: 10 step: 1138, loss is 5.6153432524297386e-05\n",
      "epoch: 10 step: 1139, loss is 0.007402980700135231\n",
      "epoch: 10 step: 1140, loss is 0.0006868606433272362\n",
      "epoch: 10 step: 1141, loss is 0.0017257739091292024\n",
      "epoch: 10 step: 1142, loss is 0.0006325444555841386\n",
      "epoch: 10 step: 1143, loss is 0.0006100201280787587\n",
      "epoch: 10 step: 1144, loss is 7.870103581808507e-05\n",
      "epoch: 10 step: 1145, loss is 0.0001437159371562302\n",
      "epoch: 10 step: 1146, loss is 0.0011305413208901882\n",
      "epoch: 10 step: 1147, loss is 0.006391187664121389\n",
      "epoch: 10 step: 1148, loss is 0.011507384479045868\n",
      "epoch: 10 step: 1149, loss is 0.002604499226436019\n",
      "epoch: 10 step: 1150, loss is 0.003646041499450803\n",
      "epoch: 10 step: 1151, loss is 0.0026172308716923\n",
      "epoch: 10 step: 1152, loss is 0.014106201939284801\n",
      "epoch: 10 step: 1153, loss is 0.006589106284081936\n",
      "epoch: 10 step: 1154, loss is 0.020015722140669823\n",
      "epoch: 10 step: 1155, loss is 0.029978875070810318\n",
      "epoch: 10 step: 1156, loss is 3.5423930967226624e-05\n",
      "epoch: 10 step: 1157, loss is 0.01666567102074623\n",
      "epoch: 10 step: 1158, loss is 0.00045996971311978996\n",
      "epoch: 10 step: 1159, loss is 0.015709534287452698\n",
      "epoch: 10 step: 1160, loss is 0.022850120440125465\n",
      "epoch: 10 step: 1161, loss is 0.004446726758033037\n",
      "epoch: 10 step: 1162, loss is 7.958504284033552e-05\n",
      "epoch: 10 step: 1163, loss is 0.0014489053282886744\n",
      "epoch: 10 step: 1164, loss is 0.0004924595123156905\n",
      "epoch: 10 step: 1165, loss is 0.004329400602728128\n",
      "epoch: 10 step: 1166, loss is 0.0006557598826475441\n",
      "epoch: 10 step: 1167, loss is 0.14636999368667603\n",
      "epoch: 10 step: 1168, loss is 1.5069355868035927e-05\n",
      "epoch: 10 step: 1169, loss is 0.0002068972389679402\n",
      "epoch: 10 step: 1170, loss is 0.0005497225211001933\n",
      "epoch: 10 step: 1171, loss is 9.566779226588551e-06\n",
      "epoch: 10 step: 1172, loss is 0.00034557856270112097\n",
      "epoch: 10 step: 1173, loss is 0.047053344547748566\n",
      "epoch: 10 step: 1174, loss is 0.0007406866061501205\n",
      "epoch: 10 step: 1175, loss is 0.04112435132265091\n",
      "epoch: 10 step: 1176, loss is 0.000876342412084341\n",
      "epoch: 10 step: 1177, loss is 0.00021596103033516556\n",
      "epoch: 10 step: 1178, loss is 0.0003067806828767061\n",
      "epoch: 10 step: 1179, loss is 0.00692928209900856\n",
      "epoch: 10 step: 1180, loss is 0.008113961666822433\n",
      "epoch: 10 step: 1181, loss is 0.09008172899484634\n",
      "epoch: 10 step: 1182, loss is 0.0004660468839574605\n",
      "epoch: 10 step: 1183, loss is 0.002607126487419009\n",
      "epoch: 10 step: 1184, loss is 0.007371597923338413\n",
      "epoch: 10 step: 1185, loss is 0.010144049301743507\n",
      "epoch: 10 step: 1186, loss is 0.0036824571434408426\n",
      "epoch: 10 step: 1187, loss is 0.0006499638548120856\n",
      "epoch: 10 step: 1188, loss is 0.003952847793698311\n",
      "epoch: 10 step: 1189, loss is 0.0004980512312613428\n",
      "epoch: 10 step: 1190, loss is 0.00015827317838557065\n",
      "epoch: 10 step: 1191, loss is 0.00691137695685029\n",
      "epoch: 10 step: 1192, loss is 0.00015011597133707255\n",
      "epoch: 10 step: 1193, loss is 0.0010659247636795044\n",
      "epoch: 10 step: 1194, loss is 2.964279701700434e-05\n",
      "epoch: 10 step: 1195, loss is 0.001531337620690465\n",
      "epoch: 10 step: 1196, loss is 0.00029110111063346267\n",
      "epoch: 10 step: 1197, loss is 0.0002460078685544431\n",
      "epoch: 10 step: 1198, loss is 0.03184259682893753\n",
      "epoch: 10 step: 1199, loss is 0.00464306864887476\n",
      "epoch: 10 step: 1200, loss is 0.0006809959886595607\n",
      "epoch: 10 step: 1201, loss is 2.693412625376368e-06\n",
      "epoch: 10 step: 1202, loss is 0.0001130620003095828\n",
      "epoch: 10 step: 1203, loss is 0.018125567585229874\n",
      "epoch: 10 step: 1204, loss is 0.0011114416411146522\n",
      "epoch: 10 step: 1205, loss is 0.0001577019429532811\n",
      "epoch: 10 step: 1206, loss is 1.6502857761224732e-05\n",
      "epoch: 10 step: 1207, loss is 0.00013456938904710114\n",
      "epoch: 10 step: 1208, loss is 0.0005360849900171161\n",
      "epoch: 10 step: 1209, loss is 0.0004109699802938849\n",
      "epoch: 10 step: 1210, loss is 0.0022007478401064873\n",
      "epoch: 10 step: 1211, loss is 0.004733673296868801\n",
      "epoch: 10 step: 1212, loss is 0.02124638296663761\n",
      "epoch: 10 step: 1213, loss is 0.0003929997328668833\n",
      "epoch: 10 step: 1214, loss is 0.0007247136672958732\n",
      "epoch: 10 step: 1215, loss is 0.0001686387840891257\n",
      "epoch: 10 step: 1216, loss is 0.0004240629496052861\n",
      "epoch: 10 step: 1217, loss is 5.3149946324992925e-05\n",
      "epoch: 10 step: 1218, loss is 0.00012189551489427686\n",
      "epoch: 10 step: 1219, loss is 0.00011722143244696781\n",
      "epoch: 10 step: 1220, loss is 4.691262074629776e-05\n",
      "epoch: 10 step: 1221, loss is 2.9541795356635703e-06\n",
      "epoch: 10 step: 1222, loss is 7.655909030290786e-06\n",
      "epoch: 10 step: 1223, loss is 2.0968098397133872e-05\n",
      "epoch: 10 step: 1224, loss is 9.555974247632548e-05\n",
      "epoch: 10 step: 1225, loss is 5.5878324928926304e-05\n",
      "epoch: 10 step: 1226, loss is 1.2466524822229985e-05\n",
      "epoch: 10 step: 1227, loss is 0.010042090900242329\n",
      "epoch: 10 step: 1228, loss is 0.006467882543802261\n",
      "epoch: 10 step: 1229, loss is 0.06086253747344017\n",
      "epoch: 10 step: 1230, loss is 6.048663999536075e-05\n",
      "epoch: 10 step: 1231, loss is 0.05927233397960663\n",
      "epoch: 10 step: 1232, loss is 0.000756114546675235\n",
      "epoch: 10 step: 1233, loss is 0.0006470602820627391\n",
      "epoch: 10 step: 1234, loss is 7.73773263063049e-06\n",
      "epoch: 10 step: 1235, loss is 0.09058041125535965\n",
      "epoch: 10 step: 1236, loss is 0.002058163983747363\n",
      "epoch: 10 step: 1237, loss is 0.00010282665607519448\n",
      "epoch: 10 step: 1238, loss is 0.013770638033747673\n",
      "epoch: 10 step: 1239, loss is 0.10050266981124878\n",
      "epoch: 10 step: 1240, loss is 0.06796594709157944\n",
      "epoch: 10 step: 1241, loss is 0.011570807546377182\n",
      "epoch: 10 step: 1242, loss is 3.300717435195111e-05\n",
      "epoch: 10 step: 1243, loss is 7.573000038973987e-05\n",
      "epoch: 10 step: 1244, loss is 2.0408417185535654e-05\n",
      "epoch: 10 step: 1245, loss is 0.027889417484402657\n",
      "epoch: 10 step: 1246, loss is 0.00025046252994798124\n",
      "epoch: 10 step: 1247, loss is 0.0001234531810041517\n",
      "epoch: 10 step: 1248, loss is 0.0005778014310635626\n",
      "epoch: 10 step: 1249, loss is 0.004627964459359646\n",
      "epoch: 10 step: 1250, loss is 0.05531548336148262\n",
      "epoch: 10 step: 1251, loss is 0.00043594560702331364\n",
      "epoch: 10 step: 1252, loss is 0.001525871455669403\n",
      "epoch: 10 step: 1253, loss is 9.742634574649855e-05\n",
      "epoch: 10 step: 1254, loss is 0.00035171693889424205\n",
      "epoch: 10 step: 1255, loss is 5.56894410692621e-05\n",
      "epoch: 10 step: 1256, loss is 0.0018222698708996177\n",
      "epoch: 10 step: 1257, loss is 0.0030240132473409176\n",
      "epoch: 10 step: 1258, loss is 0.0023822165094316006\n",
      "epoch: 10 step: 1259, loss is 4.793817424797453e-05\n",
      "epoch: 10 step: 1260, loss is 0.0004398943274281919\n",
      "epoch: 10 step: 1261, loss is 0.0003601521602831781\n",
      "epoch: 10 step: 1262, loss is 0.00013842791668139398\n",
      "epoch: 10 step: 1263, loss is 0.06732476502656937\n",
      "epoch: 10 step: 1264, loss is 0.003737993771210313\n",
      "epoch: 10 step: 1265, loss is 0.0023089279420673847\n",
      "epoch: 10 step: 1266, loss is 0.0003160312189720571\n",
      "epoch: 10 step: 1267, loss is 0.0005805923719890416\n",
      "epoch: 10 step: 1268, loss is 0.0038306366186589003\n",
      "epoch: 10 step: 1269, loss is 0.004909970331937075\n",
      "epoch: 10 step: 1270, loss is 0.00471061747521162\n",
      "epoch: 10 step: 1271, loss is 0.020426135510206223\n",
      "epoch: 10 step: 1272, loss is 0.006268695928156376\n",
      "epoch: 10 step: 1273, loss is 0.0015676355687901378\n",
      "epoch: 10 step: 1274, loss is 0.0147488908842206\n",
      "epoch: 10 step: 1275, loss is 0.002675999654456973\n",
      "epoch: 10 step: 1276, loss is 0.0001894631568575278\n",
      "epoch: 10 step: 1277, loss is 0.0007934589521028101\n",
      "epoch: 10 step: 1278, loss is 0.00018551548419054598\n",
      "epoch: 10 step: 1279, loss is 0.030218595638871193\n",
      "epoch: 10 step: 1280, loss is 0.0002777366025839001\n",
      "epoch: 10 step: 1281, loss is 0.0007158400258049369\n",
      "epoch: 10 step: 1282, loss is 0.0002415239141555503\n",
      "epoch: 10 step: 1283, loss is 0.0033760133665055037\n",
      "epoch: 10 step: 1284, loss is 0.001833479618653655\n",
      "epoch: 10 step: 1285, loss is 0.0033875033259391785\n",
      "epoch: 10 step: 1286, loss is 0.01984591595828533\n",
      "epoch: 10 step: 1287, loss is 0.026324495673179626\n",
      "epoch: 10 step: 1288, loss is 0.03897758945822716\n",
      "epoch: 10 step: 1289, loss is 0.00038357623270712793\n",
      "epoch: 10 step: 1290, loss is 0.0007874738075770438\n",
      "epoch: 10 step: 1291, loss is 9.894535469356924e-05\n",
      "epoch: 10 step: 1292, loss is 0.0006002549198456109\n",
      "epoch: 10 step: 1293, loss is 0.0242779403924942\n",
      "epoch: 10 step: 1294, loss is 0.0006198956980369985\n",
      "epoch: 10 step: 1295, loss is 0.009483404457569122\n",
      "epoch: 10 step: 1296, loss is 0.04921633377671242\n",
      "epoch: 10 step: 1297, loss is 0.00019491376588121057\n",
      "epoch: 10 step: 1298, loss is 3.04155964840902e-05\n",
      "epoch: 10 step: 1299, loss is 0.0010222921846434474\n",
      "epoch: 10 step: 1300, loss is 0.0036681091878563166\n",
      "epoch: 10 step: 1301, loss is 0.00846734270453453\n",
      "epoch: 10 step: 1302, loss is 3.7008445360697806e-05\n",
      "epoch: 10 step: 1303, loss is 0.001407309086062014\n",
      "epoch: 10 step: 1304, loss is 9.686835983302444e-05\n",
      "epoch: 10 step: 1305, loss is 0.0003823743900284171\n",
      "epoch: 10 step: 1306, loss is 0.0020443254616111517\n",
      "epoch: 10 step: 1307, loss is 0.010752524249255657\n",
      "epoch: 10 step: 1308, loss is 0.08853883296251297\n",
      "epoch: 10 step: 1309, loss is 0.005289704073220491\n",
      "epoch: 10 step: 1310, loss is 0.06878981739282608\n",
      "epoch: 10 step: 1311, loss is 0.005701088346540928\n",
      "epoch: 10 step: 1312, loss is 0.1569676399230957\n",
      "epoch: 10 step: 1313, loss is 0.05700722336769104\n",
      "epoch: 10 step: 1314, loss is 9.922767640091479e-05\n",
      "epoch: 10 step: 1315, loss is 0.015011992305517197\n",
      "epoch: 10 step: 1316, loss is 0.00010848692909348756\n",
      "epoch: 10 step: 1317, loss is 0.003055923618376255\n",
      "epoch: 10 step: 1318, loss is 0.00010396348079666495\n",
      "epoch: 10 step: 1319, loss is 2.9923516194685362e-05\n",
      "epoch: 10 step: 1320, loss is 0.0009001132566481829\n",
      "epoch: 10 step: 1321, loss is 0.00365602970123291\n",
      "epoch: 10 step: 1322, loss is 4.275471656001173e-05\n",
      "epoch: 10 step: 1323, loss is 0.19473296403884888\n",
      "epoch: 10 step: 1324, loss is 0.0008356358157470822\n",
      "epoch: 10 step: 1325, loss is 0.0003085328498855233\n",
      "epoch: 10 step: 1326, loss is 0.0002861279062926769\n",
      "epoch: 10 step: 1327, loss is 0.34599989652633667\n",
      "epoch: 10 step: 1328, loss is 0.021181905642151833\n",
      "epoch: 10 step: 1329, loss is 0.00020056456560268998\n",
      "epoch: 10 step: 1330, loss is 0.011504226364195347\n",
      "epoch: 10 step: 1331, loss is 0.016819678246974945\n",
      "epoch: 10 step: 1332, loss is 0.00010986175766447559\n",
      "epoch: 10 step: 1333, loss is 1.7940288671525195e-05\n",
      "epoch: 10 step: 1334, loss is 0.0001593480701558292\n",
      "epoch: 10 step: 1335, loss is 0.00020967800810467452\n",
      "epoch: 10 step: 1336, loss is 0.00356990285217762\n",
      "epoch: 10 step: 1337, loss is 0.0021031287033110857\n",
      "epoch: 10 step: 1338, loss is 0.0003857694682665169\n",
      "epoch: 10 step: 1339, loss is 0.0035920878872275352\n",
      "epoch: 10 step: 1340, loss is 0.023665038868784904\n",
      "epoch: 10 step: 1341, loss is 0.0002636331191752106\n",
      "epoch: 10 step: 1342, loss is 0.03289187327027321\n",
      "epoch: 10 step: 1343, loss is 0.02682899497449398\n",
      "epoch: 10 step: 1344, loss is 0.02307702600955963\n",
      "epoch: 10 step: 1345, loss is 0.011261488310992718\n",
      "epoch: 10 step: 1346, loss is 0.003115523373708129\n",
      "epoch: 10 step: 1347, loss is 0.00913268607109785\n",
      "epoch: 10 step: 1348, loss is 0.03203481435775757\n",
      "epoch: 10 step: 1349, loss is 6.219270289875567e-05\n",
      "epoch: 10 step: 1350, loss is 0.000499350018799305\n",
      "epoch: 10 step: 1351, loss is 0.00012469853390939534\n",
      "epoch: 10 step: 1352, loss is 0.04569786414504051\n",
      "epoch: 10 step: 1353, loss is 0.005207438953220844\n",
      "epoch: 10 step: 1354, loss is 0.0012381058186292648\n",
      "epoch: 10 step: 1355, loss is 0.0003397416730877012\n",
      "epoch: 10 step: 1356, loss is 0.040766872465610504\n",
      "epoch: 10 step: 1357, loss is 0.0011764622759073973\n",
      "epoch: 10 step: 1358, loss is 0.20142528414726257\n",
      "epoch: 10 step: 1359, loss is 0.0022015655413269997\n",
      "epoch: 10 step: 1360, loss is 0.0007139135268516839\n",
      "epoch: 10 step: 1361, loss is 0.03215966746211052\n",
      "epoch: 10 step: 1362, loss is 0.010371212847530842\n",
      "epoch: 10 step: 1363, loss is 0.002901750849559903\n",
      "epoch: 10 step: 1364, loss is 0.14498093724250793\n",
      "epoch: 10 step: 1365, loss is 0.004904293920844793\n",
      "epoch: 10 step: 1366, loss is 0.1591602861881256\n",
      "epoch: 10 step: 1367, loss is 0.043607719242572784\n",
      "epoch: 10 step: 1368, loss is 0.004158664960414171\n",
      "epoch: 10 step: 1369, loss is 1.1999705748166889e-05\n",
      "epoch: 10 step: 1370, loss is 0.010499886237084866\n",
      "epoch: 10 step: 1371, loss is 0.03349892050027847\n",
      "epoch: 10 step: 1372, loss is 3.8916507037356496e-05\n",
      "epoch: 10 step: 1373, loss is 0.003600365249440074\n",
      "epoch: 10 step: 1374, loss is 0.00202439958229661\n",
      "epoch: 10 step: 1375, loss is 0.000604165019467473\n",
      "epoch: 10 step: 1376, loss is 0.00035477036726661026\n",
      "epoch: 10 step: 1377, loss is 9.046182094607502e-05\n",
      "epoch: 10 step: 1378, loss is 0.0008894104976207018\n",
      "epoch: 10 step: 1379, loss is 0.0011595969554036856\n",
      "epoch: 10 step: 1380, loss is 0.0029005249962210655\n",
      "epoch: 10 step: 1381, loss is 0.002577512990683317\n",
      "epoch: 10 step: 1382, loss is 0.002271998440846801\n",
      "epoch: 10 step: 1383, loss is 0.0001473682641517371\n",
      "epoch: 10 step: 1384, loss is 2.5569534045644104e-05\n",
      "epoch: 10 step: 1385, loss is 4.779661594511708e-06\n",
      "epoch: 10 step: 1386, loss is 0.0004054621676914394\n",
      "epoch: 10 step: 1387, loss is 0.022579487413167953\n",
      "epoch: 10 step: 1388, loss is 0.05039329081773758\n",
      "epoch: 10 step: 1389, loss is 0.003911687061190605\n",
      "epoch: 10 step: 1390, loss is 0.22351117432117462\n",
      "epoch: 10 step: 1391, loss is 0.11601126939058304\n",
      "epoch: 10 step: 1392, loss is 0.07585030049085617\n",
      "epoch: 10 step: 1393, loss is 0.00023133066133596003\n",
      "epoch: 10 step: 1394, loss is 0.007685789838433266\n",
      "epoch: 10 step: 1395, loss is 2.3936076104291715e-05\n",
      "epoch: 10 step: 1396, loss is 4.71736493636854e-05\n",
      "epoch: 10 step: 1397, loss is 0.0003244844847358763\n",
      "epoch: 10 step: 1398, loss is 4.8386453272541985e-05\n",
      "epoch: 10 step: 1399, loss is 0.035112421959638596\n",
      "epoch: 10 step: 1400, loss is 0.008318616077303886\n",
      "epoch: 10 step: 1401, loss is 0.09697719663381577\n",
      "epoch: 10 step: 1402, loss is 0.11912909150123596\n",
      "epoch: 10 step: 1403, loss is 0.00021462798758875579\n",
      "epoch: 10 step: 1404, loss is 4.547119533526711e-05\n",
      "epoch: 10 step: 1405, loss is 0.025247929617762566\n",
      "epoch: 10 step: 1406, loss is 3.8910278817638755e-05\n",
      "epoch: 10 step: 1407, loss is 0.0002277610037708655\n",
      "epoch: 10 step: 1408, loss is 2.9244953111629002e-05\n",
      "epoch: 10 step: 1409, loss is 0.024183951318264008\n",
      "epoch: 10 step: 1410, loss is 0.00021369410387706012\n",
      "epoch: 10 step: 1411, loss is 0.0003987760574091226\n",
      "epoch: 10 step: 1412, loss is 0.0019466356607154012\n",
      "epoch: 10 step: 1413, loss is 0.00019077843171544373\n",
      "epoch: 10 step: 1414, loss is 0.010427998378872871\n",
      "epoch: 10 step: 1415, loss is 0.03878628835082054\n",
      "epoch: 10 step: 1416, loss is 0.0006552172708325088\n",
      "epoch: 10 step: 1417, loss is 0.0007744979811832309\n",
      "epoch: 10 step: 1418, loss is 0.17071790993213654\n",
      "epoch: 10 step: 1419, loss is 0.06254804134368896\n",
      "epoch: 10 step: 1420, loss is 0.0038171433843672276\n",
      "epoch: 10 step: 1421, loss is 0.00012330213212408125\n",
      "epoch: 10 step: 1422, loss is 0.00022006886138115078\n",
      "epoch: 10 step: 1423, loss is 0.0008111437782645226\n",
      "epoch: 10 step: 1424, loss is 0.00017993788060266525\n",
      "epoch: 10 step: 1425, loss is 0.030507799237966537\n",
      "epoch: 10 step: 1426, loss is 5.893470370210707e-05\n",
      "epoch: 10 step: 1427, loss is 0.00021399272372946143\n",
      "epoch: 10 step: 1428, loss is 0.15720945596694946\n",
      "epoch: 10 step: 1429, loss is 0.032889265567064285\n",
      "epoch: 10 step: 1430, loss is 1.1699933565978426e-05\n",
      "epoch: 10 step: 1431, loss is 0.003956266213208437\n",
      "epoch: 10 step: 1432, loss is 0.019535589963197708\n",
      "epoch: 10 step: 1433, loss is 0.0019676696974784136\n",
      "epoch: 10 step: 1434, loss is 0.0003655562468338758\n",
      "epoch: 10 step: 1435, loss is 0.015037587843835354\n",
      "epoch: 10 step: 1436, loss is 6.856227264506742e-05\n",
      "epoch: 10 step: 1437, loss is 0.0011600740253925323\n",
      "epoch: 10 step: 1438, loss is 0.0030318410135805607\n",
      "epoch: 10 step: 1439, loss is 0.008960004895925522\n",
      "epoch: 10 step: 1440, loss is 0.0001804523926693946\n",
      "epoch: 10 step: 1441, loss is 0.0004795051063410938\n",
      "epoch: 10 step: 1442, loss is 0.00030356808565557003\n",
      "epoch: 10 step: 1443, loss is 0.019934244453907013\n",
      "epoch: 10 step: 1444, loss is 0.0003849979257211089\n",
      "epoch: 10 step: 1445, loss is 0.00920537393540144\n",
      "epoch: 10 step: 1446, loss is 0.03259747475385666\n",
      "epoch: 10 step: 1447, loss is 0.03550158813595772\n",
      "epoch: 10 step: 1448, loss is 0.0004537973436526954\n",
      "epoch: 10 step: 1449, loss is 0.00010680012201191857\n",
      "epoch: 10 step: 1450, loss is 0.0003001654986292124\n",
      "epoch: 10 step: 1451, loss is 0.00019770697690546513\n",
      "epoch: 10 step: 1452, loss is 0.00025103226653300226\n",
      "epoch: 10 step: 1453, loss is 1.6620273527223617e-05\n",
      "epoch: 10 step: 1454, loss is 0.0010281408904120326\n",
      "epoch: 10 step: 1455, loss is 6.0141130234114826e-05\n",
      "epoch: 10 step: 1456, loss is 0.0016368344658985734\n",
      "epoch: 10 step: 1457, loss is 0.0009485634509474039\n",
      "epoch: 10 step: 1458, loss is 0.03282070532441139\n",
      "epoch: 10 step: 1459, loss is 0.00028903555357828736\n",
      "epoch: 10 step: 1460, loss is 0.00022151916346047074\n",
      "epoch: 10 step: 1461, loss is 0.00040686698048375547\n",
      "epoch: 10 step: 1462, loss is 0.15874376893043518\n",
      "epoch: 10 step: 1463, loss is 0.007315529976040125\n",
      "epoch: 10 step: 1464, loss is 0.00012366291775833815\n",
      "epoch: 10 step: 1465, loss is 0.0130986999720335\n",
      "epoch: 10 step: 1466, loss is 0.03367265686392784\n",
      "epoch: 10 step: 1467, loss is 0.0033129698131233454\n",
      "epoch: 10 step: 1468, loss is 0.0012816451489925385\n",
      "epoch: 10 step: 1469, loss is 0.0025773574598133564\n",
      "epoch: 10 step: 1470, loss is 0.13203313946723938\n",
      "epoch: 10 step: 1471, loss is 0.0002481933042872697\n",
      "epoch: 10 step: 1472, loss is 0.0016200817190110683\n",
      "epoch: 10 step: 1473, loss is 0.00024204350484069437\n",
      "epoch: 10 step: 1474, loss is 0.0200142003595829\n",
      "epoch: 10 step: 1475, loss is 0.0008232231484726071\n",
      "epoch: 10 step: 1476, loss is 0.009199419058859348\n",
      "epoch: 10 step: 1477, loss is 0.00019507446268107742\n",
      "epoch: 10 step: 1478, loss is 0.0004743608587887138\n",
      "epoch: 10 step: 1479, loss is 0.010473481379449368\n",
      "epoch: 10 step: 1480, loss is 0.007093905936926603\n",
      "epoch: 10 step: 1481, loss is 0.16226275265216827\n",
      "epoch: 10 step: 1482, loss is 0.006890751421451569\n",
      "epoch: 10 step: 1483, loss is 0.00039998156717047095\n",
      "epoch: 10 step: 1484, loss is 0.025779355317354202\n",
      "epoch: 10 step: 1485, loss is 0.00020777281315531582\n",
      "epoch: 10 step: 1486, loss is 0.01567983627319336\n",
      "epoch: 10 step: 1487, loss is 0.0007148353615775704\n",
      "epoch: 10 step: 1488, loss is 0.005598711781203747\n",
      "epoch: 10 step: 1489, loss is 0.0011087242746725678\n",
      "epoch: 10 step: 1490, loss is 0.0018205024534836411\n",
      "epoch: 10 step: 1491, loss is 0.00027223594952374697\n",
      "epoch: 10 step: 1492, loss is 0.23322471976280212\n",
      "epoch: 10 step: 1493, loss is 0.00020453869365155697\n",
      "epoch: 10 step: 1494, loss is 0.00029414932942017913\n",
      "epoch: 10 step: 1495, loss is 0.00010444334475323558\n",
      "epoch: 10 step: 1496, loss is 0.0013631132896989584\n",
      "epoch: 10 step: 1497, loss is 0.00040753083885647357\n",
      "epoch: 10 step: 1498, loss is 0.0008031900506466627\n",
      "epoch: 10 step: 1499, loss is 0.0002052941854344681\n",
      "epoch: 10 step: 1500, loss is 0.0011747452663257718\n",
      "epoch: 10 step: 1501, loss is 0.0005098080728203058\n",
      "epoch: 10 step: 1502, loss is 0.0023865606635808945\n",
      "epoch: 10 step: 1503, loss is 0.06802456825971603\n",
      "epoch: 10 step: 1504, loss is 0.000558048312086612\n",
      "epoch: 10 step: 1505, loss is 0.00013780589506495744\n",
      "epoch: 10 step: 1506, loss is 0.0025806473568081856\n",
      "epoch: 10 step: 1507, loss is 0.01044441107660532\n",
      "epoch: 10 step: 1508, loss is 0.007244103588163853\n",
      "epoch: 10 step: 1509, loss is 0.00010874092549784109\n",
      "epoch: 10 step: 1510, loss is 0.028100494295358658\n",
      "epoch: 10 step: 1511, loss is 0.00342521327547729\n",
      "epoch: 10 step: 1512, loss is 0.012380572035908699\n",
      "epoch: 10 step: 1513, loss is 0.00046548680984415114\n",
      "epoch: 10 step: 1514, loss is 0.05209234729409218\n",
      "epoch: 10 step: 1515, loss is 0.00402978528290987\n",
      "epoch: 10 step: 1516, loss is 0.000969438289757818\n",
      "epoch: 10 step: 1517, loss is 0.0008349425042979419\n",
      "epoch: 10 step: 1518, loss is 0.020057979971170425\n",
      "epoch: 10 step: 1519, loss is 0.13686300814151764\n",
      "epoch: 10 step: 1520, loss is 0.0015525704948231578\n",
      "epoch: 10 step: 1521, loss is 0.0029976596124470234\n",
      "epoch: 10 step: 1522, loss is 0.004590439144521952\n",
      "epoch: 10 step: 1523, loss is 0.0001016245296341367\n",
      "epoch: 10 step: 1524, loss is 0.00034622495877556503\n",
      "epoch: 10 step: 1525, loss is 0.028683796525001526\n",
      "epoch: 10 step: 1526, loss is 0.0007461041095666587\n",
      "epoch: 10 step: 1527, loss is 0.00989499967545271\n",
      "epoch: 10 step: 1528, loss is 0.002057854551821947\n",
      "epoch: 10 step: 1529, loss is 0.0007157106301747262\n",
      "epoch: 10 step: 1530, loss is 0.0062788380309939384\n",
      "epoch: 10 step: 1531, loss is 0.04078312963247299\n",
      "epoch: 10 step: 1532, loss is 0.031939227133989334\n",
      "epoch: 10 step: 1533, loss is 0.0018094392726197839\n",
      "epoch: 10 step: 1534, loss is 0.0007943271193653345\n",
      "epoch: 10 step: 1535, loss is 0.0018774393247440457\n",
      "epoch: 10 step: 1536, loss is 5.1872477342840284e-05\n",
      "epoch: 10 step: 1537, loss is 0.00261822622269392\n",
      "epoch: 10 step: 1538, loss is 0.0035145035944879055\n",
      "epoch: 10 step: 1539, loss is 0.0010775236878544092\n",
      "epoch: 10 step: 1540, loss is 0.010468967258930206\n",
      "epoch: 10 step: 1541, loss is 3.986250885645859e-05\n",
      "epoch: 10 step: 1542, loss is 0.00026548781897872686\n",
      "epoch: 10 step: 1543, loss is 0.062211792916059494\n",
      "epoch: 10 step: 1544, loss is 0.001748988637700677\n",
      "epoch: 10 step: 1545, loss is 0.11947871744632721\n",
      "epoch: 10 step: 1546, loss is 0.0024899891577661037\n",
      "epoch: 10 step: 1547, loss is 0.0019541122019290924\n",
      "epoch: 10 step: 1548, loss is 0.003691489342600107\n",
      "epoch: 10 step: 1549, loss is 0.00508106779307127\n",
      "epoch: 10 step: 1550, loss is 0.01847684755921364\n",
      "epoch: 10 step: 1551, loss is 0.0003263310354668647\n",
      "epoch: 10 step: 1552, loss is 0.0015281903324648738\n",
      "epoch: 10 step: 1553, loss is 0.0015427032485604286\n",
      "epoch: 10 step: 1554, loss is 0.0005091452621854842\n",
      "epoch: 10 step: 1555, loss is 0.10359726846218109\n",
      "epoch: 10 step: 1556, loss is 0.005822991020977497\n",
      "epoch: 10 step: 1557, loss is 0.00026917108334600925\n",
      "epoch: 10 step: 1558, loss is 9.812518692342564e-05\n",
      "epoch: 10 step: 1559, loss is 0.00030697579495608807\n",
      "epoch: 10 step: 1560, loss is 0.008602079004049301\n",
      "epoch: 10 step: 1561, loss is 0.00031114014564082026\n",
      "epoch: 10 step: 1562, loss is 0.0021428142208606005\n",
      "epoch: 10 step: 1563, loss is 1.4756599739484955e-05\n",
      "epoch: 10 step: 1564, loss is 0.0009841275168582797\n",
      "epoch: 10 step: 1565, loss is 0.011169319972395897\n",
      "epoch: 10 step: 1566, loss is 0.007082831114530563\n",
      "epoch: 10 step: 1567, loss is 0.04235520586371422\n",
      "epoch: 10 step: 1568, loss is 0.0013145515695214272\n",
      "epoch: 10 step: 1569, loss is 0.0019333374220877886\n",
      "epoch: 10 step: 1570, loss is 0.0004132336762268096\n",
      "epoch: 10 step: 1571, loss is 0.00039259882760234177\n",
      "epoch: 10 step: 1572, loss is 0.015358326956629753\n",
      "epoch: 10 step: 1573, loss is 0.031599465757608414\n",
      "epoch: 10 step: 1574, loss is 0.038786880671978\n",
      "epoch: 10 step: 1575, loss is 0.14920085668563843\n",
      "epoch: 10 step: 1576, loss is 0.0048889657482504845\n",
      "epoch: 10 step: 1577, loss is 0.010408702306449413\n",
      "epoch: 10 step: 1578, loss is 0.0017347653629258275\n",
      "epoch: 10 step: 1579, loss is 0.14065375924110413\n",
      "epoch: 10 step: 1580, loss is 0.0002894372446462512\n",
      "epoch: 10 step: 1581, loss is 2.3895046979305334e-05\n",
      "epoch: 10 step: 1582, loss is 4.2076982936123386e-05\n",
      "epoch: 10 step: 1583, loss is 5.948456237092614e-05\n",
      "epoch: 10 step: 1584, loss is 0.0013626249274238944\n",
      "epoch: 10 step: 1585, loss is 0.004394290037453175\n",
      "epoch: 10 step: 1586, loss is 0.017318354919552803\n",
      "epoch: 10 step: 1587, loss is 0.00024105557531584054\n",
      "epoch: 10 step: 1588, loss is 0.11176282167434692\n",
      "epoch: 10 step: 1589, loss is 0.12570196390151978\n",
      "epoch: 10 step: 1590, loss is 0.06779880076646805\n",
      "epoch: 10 step: 1591, loss is 0.001188067253679037\n",
      "epoch: 10 step: 1592, loss is 0.0025315736420452595\n",
      "epoch: 10 step: 1593, loss is 0.0012781380210071802\n",
      "epoch: 10 step: 1594, loss is 0.005479409825056791\n",
      "epoch: 10 step: 1595, loss is 0.001090052304789424\n",
      "epoch: 10 step: 1596, loss is 0.0019548358395695686\n",
      "epoch: 10 step: 1597, loss is 0.00018622702918946743\n",
      "epoch: 10 step: 1598, loss is 0.020746709778904915\n",
      "epoch: 10 step: 1599, loss is 0.19285951554775238\n",
      "epoch: 10 step: 1600, loss is 0.005188795272260904\n",
      "epoch: 10 step: 1601, loss is 0.00012588498066179454\n",
      "epoch: 10 step: 1602, loss is 0.01013855915516615\n",
      "epoch: 10 step: 1603, loss is 0.007933351211249828\n",
      "epoch: 10 step: 1604, loss is 0.04219527170062065\n",
      "epoch: 10 step: 1605, loss is 0.003132258541882038\n",
      "epoch: 10 step: 1606, loss is 0.00022812238603364676\n",
      "epoch: 10 step: 1607, loss is 0.0012112848926335573\n",
      "epoch: 10 step: 1608, loss is 0.008363396860659122\n",
      "epoch: 10 step: 1609, loss is 0.060423340648412704\n",
      "epoch: 10 step: 1610, loss is 0.00031733830110169947\n",
      "epoch: 10 step: 1611, loss is 0.0009641327196732163\n",
      "epoch: 10 step: 1612, loss is 0.000269021256826818\n",
      "epoch: 10 step: 1613, loss is 0.005755376070737839\n",
      "epoch: 10 step: 1614, loss is 0.0002128325868397951\n",
      "epoch: 10 step: 1615, loss is 1.4935176295693964e-05\n",
      "epoch: 10 step: 1616, loss is 0.02054744027554989\n",
      "epoch: 10 step: 1617, loss is 0.0329258069396019\n",
      "epoch: 10 step: 1618, loss is 0.004475350491702557\n",
      "epoch: 10 step: 1619, loss is 6.024978938512504e-05\n",
      "epoch: 10 step: 1620, loss is 0.02111225575208664\n",
      "epoch: 10 step: 1621, loss is 0.00401848740875721\n",
      "epoch: 10 step: 1622, loss is 0.004671163856983185\n",
      "epoch: 10 step: 1623, loss is 0.06181935966014862\n",
      "epoch: 10 step: 1624, loss is 0.0017442313255742192\n",
      "epoch: 10 step: 1625, loss is 0.00041227080509997904\n",
      "epoch: 10 step: 1626, loss is 0.0002631766546983272\n",
      "epoch: 10 step: 1627, loss is 0.0023925823625177145\n",
      "epoch: 10 step: 1628, loss is 0.02005706913769245\n",
      "epoch: 10 step: 1629, loss is 0.0067176432348787785\n",
      "epoch: 10 step: 1630, loss is 0.019110357388854027\n",
      "epoch: 10 step: 1631, loss is 0.004284516908228397\n",
      "epoch: 10 step: 1632, loss is 0.01032106950879097\n",
      "epoch: 10 step: 1633, loss is 0.009958813898265362\n",
      "epoch: 10 step: 1634, loss is 0.0008188131032511592\n",
      "epoch: 10 step: 1635, loss is 0.005186094902455807\n",
      "epoch: 10 step: 1636, loss is 0.027913670986890793\n",
      "epoch: 10 step: 1637, loss is 0.00024524025502614677\n",
      "epoch: 10 step: 1638, loss is 0.004696141462773085\n",
      "epoch: 10 step: 1639, loss is 0.005149893928319216\n",
      "epoch: 10 step: 1640, loss is 0.00033999071456491947\n",
      "epoch: 10 step: 1641, loss is 0.0018093163380399346\n",
      "epoch: 10 step: 1642, loss is 0.0004310297954361886\n",
      "epoch: 10 step: 1643, loss is 0.0008148079505190253\n",
      "epoch: 10 step: 1644, loss is 0.08580198884010315\n",
      "epoch: 10 step: 1645, loss is 0.16446860134601593\n",
      "epoch: 10 step: 1646, loss is 0.09114917367696762\n",
      "epoch: 10 step: 1647, loss is 0.01223182026296854\n",
      "epoch: 10 step: 1648, loss is 0.007245647720992565\n",
      "epoch: 10 step: 1649, loss is 0.005400919355452061\n",
      "epoch: 10 step: 1650, loss is 1.1921457371499855e-05\n",
      "epoch: 10 step: 1651, loss is 0.0003989506221842021\n",
      "epoch: 10 step: 1652, loss is 0.01177828386425972\n",
      "epoch: 10 step: 1653, loss is 0.0003906699421349913\n",
      "epoch: 10 step: 1654, loss is 0.004468705505132675\n",
      "epoch: 10 step: 1655, loss is 0.00939677469432354\n",
      "epoch: 10 step: 1656, loss is 0.001078487024642527\n",
      "epoch: 10 step: 1657, loss is 0.0019652757328003645\n",
      "epoch: 10 step: 1658, loss is 0.006801908370107412\n",
      "epoch: 10 step: 1659, loss is 0.003004521131515503\n",
      "epoch: 10 step: 1660, loss is 0.0004626778536476195\n",
      "epoch: 10 step: 1661, loss is 0.00040233912295661867\n",
      "epoch: 10 step: 1662, loss is 0.00024381160619668663\n",
      "epoch: 10 step: 1663, loss is 0.00028317151009105146\n",
      "epoch: 10 step: 1664, loss is 9.450595098314807e-05\n",
      "epoch: 10 step: 1665, loss is 0.0005987182958051562\n",
      "epoch: 10 step: 1666, loss is 0.011085784062743187\n",
      "epoch: 10 step: 1667, loss is 4.2205439967801794e-05\n",
      "epoch: 10 step: 1668, loss is 0.00028783699963241816\n",
      "epoch: 10 step: 1669, loss is 0.00013143225805833936\n",
      "epoch: 10 step: 1670, loss is 2.8495849619503133e-05\n",
      "epoch: 10 step: 1671, loss is 0.0009436274413019419\n",
      "epoch: 10 step: 1672, loss is 0.03131292387843132\n",
      "epoch: 10 step: 1673, loss is 0.0003769340110011399\n",
      "epoch: 10 step: 1674, loss is 0.0006336903898045421\n",
      "epoch: 10 step: 1675, loss is 0.036075543612241745\n",
      "epoch: 10 step: 1676, loss is 0.0017751610139384866\n",
      "epoch: 10 step: 1677, loss is 0.00045255458098836243\n",
      "epoch: 10 step: 1678, loss is 0.0008031087345443666\n",
      "epoch: 10 step: 1679, loss is 0.016564346849918365\n",
      "epoch: 10 step: 1680, loss is 0.013527126051485538\n",
      "epoch: 10 step: 1681, loss is 0.001127177500165999\n",
      "epoch: 10 step: 1682, loss is 0.013142706826329231\n",
      "epoch: 10 step: 1683, loss is 0.00018034197273664176\n",
      "epoch: 10 step: 1684, loss is 0.0003532372647896409\n",
      "epoch: 10 step: 1685, loss is 0.00010918003681581467\n",
      "epoch: 10 step: 1686, loss is 0.0005766269168816507\n",
      "epoch: 10 step: 1687, loss is 0.007833776995539665\n",
      "epoch: 10 step: 1688, loss is 0.007914691232144833\n",
      "epoch: 10 step: 1689, loss is 0.0022643241100013256\n",
      "epoch: 10 step: 1690, loss is 0.00547660980373621\n",
      "epoch: 10 step: 1691, loss is 0.0009233251330442727\n",
      "epoch: 10 step: 1692, loss is 0.0001431987329851836\n",
      "epoch: 10 step: 1693, loss is 0.024146705865859985\n",
      "epoch: 10 step: 1694, loss is 0.0034345590975135565\n",
      "epoch: 10 step: 1695, loss is 1.9330349459778517e-05\n",
      "epoch: 10 step: 1696, loss is 0.00036379415541887283\n",
      "epoch: 10 step: 1697, loss is 0.04984138160943985\n",
      "epoch: 10 step: 1698, loss is 0.0010979876387864351\n",
      "epoch: 10 step: 1699, loss is 8.247953701356892e-06\n",
      "epoch: 10 step: 1700, loss is 0.0008151891524903476\n",
      "epoch: 10 step: 1701, loss is 0.00012974822311662138\n",
      "epoch: 10 step: 1702, loss is 0.00020730758842546493\n",
      "epoch: 10 step: 1703, loss is 0.017164772376418114\n",
      "epoch: 10 step: 1704, loss is 0.004784901160746813\n",
      "epoch: 10 step: 1705, loss is 2.621073872433044e-05\n",
      "epoch: 10 step: 1706, loss is 0.00041088651050813496\n",
      "epoch: 10 step: 1707, loss is 0.008079776540398598\n",
      "epoch: 10 step: 1708, loss is 0.00017655659758020192\n",
      "epoch: 10 step: 1709, loss is 1.7618829588172957e-05\n",
      "epoch: 10 step: 1710, loss is 0.0012767062289640307\n",
      "epoch: 10 step: 1711, loss is 0.008723546750843525\n",
      "epoch: 10 step: 1712, loss is 0.0005404663970693946\n",
      "epoch: 10 step: 1713, loss is 0.00019054661970585585\n",
      "epoch: 10 step: 1714, loss is 0.0039346711710095406\n",
      "epoch: 10 step: 1715, loss is 0.0011904871789738536\n",
      "epoch: 10 step: 1716, loss is 0.03461138904094696\n",
      "epoch: 10 step: 1717, loss is 0.0009122298215515912\n",
      "epoch: 10 step: 1718, loss is 0.0003554270078893751\n",
      "epoch: 10 step: 1719, loss is 0.00694636395201087\n",
      "epoch: 10 step: 1720, loss is 0.0001777889992808923\n",
      "epoch: 10 step: 1721, loss is 0.017353061586618423\n",
      "epoch: 10 step: 1722, loss is 3.2818530598888174e-05\n",
      "epoch: 10 step: 1723, loss is 0.014603340066969395\n",
      "epoch: 10 step: 1724, loss is 0.0002421333483653143\n",
      "epoch: 10 step: 1725, loss is 0.00016985388356260955\n",
      "epoch: 10 step: 1726, loss is 6.922260945430025e-05\n",
      "epoch: 10 step: 1727, loss is 0.009259856306016445\n",
      "epoch: 10 step: 1728, loss is 0.0017664562910795212\n",
      "epoch: 10 step: 1729, loss is 0.0022918975446373224\n",
      "epoch: 10 step: 1730, loss is 0.10534033179283142\n",
      "epoch: 10 step: 1731, loss is 0.010086428374052048\n",
      "epoch: 10 step: 1732, loss is 0.006116489414125681\n",
      "epoch: 10 step: 1733, loss is 0.000730057479813695\n",
      "epoch: 10 step: 1734, loss is 0.0001385633513564244\n",
      "epoch: 10 step: 1735, loss is 0.03171999007463455\n",
      "epoch: 10 step: 1736, loss is 0.006086999084800482\n",
      "epoch: 10 step: 1737, loss is 0.004057284444570541\n",
      "epoch: 10 step: 1738, loss is 4.517846537055448e-05\n",
      "epoch: 10 step: 1739, loss is 0.011163963004946709\n",
      "epoch: 10 step: 1740, loss is 0.0013918994227424264\n",
      "epoch: 10 step: 1741, loss is 0.0003357617824804038\n",
      "epoch: 10 step: 1742, loss is 0.0005041357944719493\n",
      "epoch: 10 step: 1743, loss is 0.0001396451989421621\n",
      "epoch: 10 step: 1744, loss is 0.0008460588287562132\n",
      "epoch: 10 step: 1745, loss is 0.0002323703229194507\n",
      "epoch: 10 step: 1746, loss is 0.0003683379909489304\n",
      "epoch: 10 step: 1747, loss is 0.0015464606694877148\n",
      "epoch: 10 step: 1748, loss is 0.0009225517860613763\n",
      "epoch: 10 step: 1749, loss is 9.509153460385278e-05\n",
      "epoch: 10 step: 1750, loss is 0.0005181105225346982\n",
      "epoch: 10 step: 1751, loss is 0.005840923171490431\n",
      "epoch: 10 step: 1752, loss is 0.0014841166557744145\n",
      "epoch: 10 step: 1753, loss is 0.0009944067569449544\n",
      "epoch: 10 step: 1754, loss is 0.0019648170564323664\n",
      "epoch: 10 step: 1755, loss is 0.0017601303989067674\n",
      "epoch: 10 step: 1756, loss is 0.0002695095899980515\n",
      "epoch: 10 step: 1757, loss is 0.0005806662375107408\n",
      "epoch: 10 step: 1758, loss is 0.00018301012460142374\n",
      "epoch: 10 step: 1759, loss is 0.00022535916650667787\n",
      "epoch: 10 step: 1760, loss is 0.00013400122406892478\n",
      "epoch: 10 step: 1761, loss is 8.311581768793985e-05\n",
      "epoch: 10 step: 1762, loss is 0.0073005808517336845\n",
      "epoch: 10 step: 1763, loss is 2.773478627204895e-05\n",
      "epoch: 10 step: 1764, loss is 0.0003932229592464864\n",
      "epoch: 10 step: 1765, loss is 0.0011627417989075184\n",
      "epoch: 10 step: 1766, loss is 0.00800697598606348\n",
      "epoch: 10 step: 1767, loss is 0.0011646809289231896\n",
      "epoch: 10 step: 1768, loss is 0.00028173968894407153\n",
      "epoch: 10 step: 1769, loss is 0.0033117958810180426\n",
      "epoch: 10 step: 1770, loss is 0.0010615114588290453\n",
      "epoch: 10 step: 1771, loss is 0.006435703951865435\n",
      "epoch: 10 step: 1772, loss is 0.00012989294191356748\n",
      "epoch: 10 step: 1773, loss is 3.1264211429515854e-05\n",
      "epoch: 10 step: 1774, loss is 0.02752857282757759\n",
      "epoch: 10 step: 1775, loss is 0.0003281654207967222\n",
      "epoch: 10 step: 1776, loss is 0.0007078091730363667\n",
      "epoch: 10 step: 1777, loss is 0.0007127778953872621\n",
      "epoch: 10 step: 1778, loss is 1.7299906176049262e-05\n",
      "epoch: 10 step: 1779, loss is 0.00814834050834179\n",
      "epoch: 10 step: 1780, loss is 4.218776666675694e-05\n",
      "epoch: 10 step: 1781, loss is 0.00028605558327399194\n",
      "epoch: 10 step: 1782, loss is 0.0001528880384285003\n",
      "epoch: 10 step: 1783, loss is 0.0045966519974172115\n",
      "epoch: 10 step: 1784, loss is 0.013615083880722523\n",
      "epoch: 10 step: 1785, loss is 0.00013367587234824896\n",
      "epoch: 10 step: 1786, loss is 0.002834103535860777\n",
      "epoch: 10 step: 1787, loss is 0.005768848583102226\n",
      "epoch: 10 step: 1788, loss is 0.07839358597993851\n",
      "epoch: 10 step: 1789, loss is 0.000715758535079658\n",
      "epoch: 10 step: 1790, loss is 0.0001916598412208259\n",
      "epoch: 10 step: 1791, loss is 0.0005992753431200981\n",
      "epoch: 10 step: 1792, loss is 7.00804012012668e-05\n",
      "epoch: 10 step: 1793, loss is 1.8575226931716315e-05\n",
      "epoch: 10 step: 1794, loss is 0.0013740722788497806\n",
      "epoch: 10 step: 1795, loss is 0.0008150548674166203\n",
      "epoch: 10 step: 1796, loss is 4.920017454423942e-05\n",
      "epoch: 10 step: 1797, loss is 0.00587669899687171\n",
      "epoch: 10 step: 1798, loss is 0.0028874720446765423\n",
      "epoch: 10 step: 1799, loss is 0.00975304190069437\n",
      "epoch: 10 step: 1800, loss is 2.7341500754118897e-05\n",
      "epoch: 10 step: 1801, loss is 2.4058723283815198e-05\n",
      "epoch: 10 step: 1802, loss is 0.0007492618751712143\n",
      "epoch: 10 step: 1803, loss is 0.0001559583324706182\n",
      "epoch: 10 step: 1804, loss is 0.0002263050409965217\n",
      "epoch: 10 step: 1805, loss is 0.004019096028059721\n",
      "epoch: 10 step: 1806, loss is 0.0005328608676791191\n",
      "epoch: 10 step: 1807, loss is 2.818022039718926e-05\n",
      "epoch: 10 step: 1808, loss is 2.4253180527011864e-05\n",
      "epoch: 10 step: 1809, loss is 0.01863265410065651\n",
      "epoch: 10 step: 1810, loss is 0.16780874133110046\n",
      "epoch: 10 step: 1811, loss is 0.0003495384007692337\n",
      "epoch: 10 step: 1812, loss is 0.005849966313689947\n",
      "epoch: 10 step: 1813, loss is 0.00015580796753056347\n",
      "epoch: 10 step: 1814, loss is 0.0026062880642712116\n",
      "epoch: 10 step: 1815, loss is 0.0006006135954521596\n",
      "epoch: 10 step: 1816, loss is 0.0010342095047235489\n",
      "epoch: 10 step: 1817, loss is 0.03270018473267555\n",
      "epoch: 10 step: 1818, loss is 7.713418017374352e-05\n",
      "epoch: 10 step: 1819, loss is 0.0025786643382161856\n",
      "epoch: 10 step: 1820, loss is 0.00037893810076639056\n",
      "epoch: 10 step: 1821, loss is 6.917064456501976e-05\n",
      "epoch: 10 step: 1822, loss is 0.07547997683286667\n",
      "epoch: 10 step: 1823, loss is 0.004880346357822418\n",
      "epoch: 10 step: 1824, loss is 0.03449352830648422\n",
      "epoch: 10 step: 1825, loss is 0.0027489732019603252\n",
      "epoch: 10 step: 1826, loss is 0.009445762261748314\n",
      "epoch: 10 step: 1827, loss is 0.0015957803698256612\n",
      "epoch: 10 step: 1828, loss is 0.0007916597533039749\n",
      "epoch: 10 step: 1829, loss is 0.13769277930259705\n",
      "epoch: 10 step: 1830, loss is 0.0014874722110107541\n",
      "epoch: 10 step: 1831, loss is 8.306060772156343e-05\n",
      "epoch: 10 step: 1832, loss is 0.0028276140801608562\n",
      "epoch: 10 step: 1833, loss is 0.0009172280551865697\n",
      "epoch: 10 step: 1834, loss is 0.00011141168215544894\n",
      "epoch: 10 step: 1835, loss is 0.039135612547397614\n",
      "epoch: 10 step: 1836, loss is 0.0019288979237899184\n",
      "epoch: 10 step: 1837, loss is 0.01805995963513851\n",
      "epoch: 10 step: 1838, loss is 0.0567452535033226\n",
      "epoch: 10 step: 1839, loss is 0.0005155647522769868\n",
      "epoch: 10 step: 1840, loss is 0.001921800198033452\n",
      "epoch: 10 step: 1841, loss is 0.00026383649674244225\n",
      "epoch: 10 step: 1842, loss is 0.002319350838661194\n",
      "epoch: 10 step: 1843, loss is 0.011466547846794128\n",
      "epoch: 10 step: 1844, loss is 4.7347493818961084e-05\n",
      "epoch: 10 step: 1845, loss is 0.03195395693182945\n",
      "epoch: 10 step: 1846, loss is 0.0030519247520715\n",
      "epoch: 10 step: 1847, loss is 0.0062653981149196625\n",
      "epoch: 10 step: 1848, loss is 0.006780613679438829\n",
      "epoch: 10 step: 1849, loss is 0.0008327348623424768\n",
      "epoch: 10 step: 1850, loss is 0.0030170714017003775\n",
      "epoch: 10 step: 1851, loss is 0.0011012604227289557\n",
      "epoch: 10 step: 1852, loss is 0.0007509812712669373\n",
      "epoch: 10 step: 1853, loss is 0.0019584097899496555\n",
      "epoch: 10 step: 1854, loss is 0.0005665419157594442\n",
      "epoch: 10 step: 1855, loss is 0.014228382147848606\n",
      "epoch: 10 step: 1856, loss is 0.07088790088891983\n",
      "epoch: 10 step: 1857, loss is 0.0014035714557394385\n",
      "epoch: 10 step: 1858, loss is 0.003086487064138055\n",
      "epoch: 10 step: 1859, loss is 0.0013776881387457252\n",
      "epoch: 10 step: 1860, loss is 0.003522019600495696\n",
      "epoch: 10 step: 1861, loss is 0.000122791578178294\n",
      "epoch: 10 step: 1862, loss is 0.0008922914857976139\n",
      "epoch: 10 step: 1863, loss is 9.935235721059144e-05\n",
      "epoch: 10 step: 1864, loss is 0.009399204514920712\n",
      "epoch: 10 step: 1865, loss is 4.429478849488078e-06\n",
      "epoch: 10 step: 1866, loss is 0.016015702858567238\n",
      "epoch: 10 step: 1867, loss is 0.00019743597658816725\n",
      "epoch: 10 step: 1868, loss is 0.007311656605452299\n",
      "epoch: 10 step: 1869, loss is 0.012057664804160595\n",
      "epoch: 10 step: 1870, loss is 0.0012551923282444477\n",
      "epoch: 10 step: 1871, loss is 0.00037156970938667655\n",
      "epoch: 10 step: 1872, loss is 0.011944147758185863\n",
      "epoch: 10 step: 1873, loss is 0.020017236471176147\n",
      "epoch: 10 step: 1874, loss is 0.10607068985700607\n",
      "epoch: 10 step: 1875, loss is 9.27731380215846e-05\n",
      "Train epoch time: 17236.995 ms, per step time: 9.193 ms\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision as vision\n",
    "import mindspore.dataset.transforms as transforms\n",
    "from mindspore.dataset.vision import Inter\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore.train import (\n",
    "    Accuracy,\n",
    "    LossMonitor,\n",
    "    CheckpointConfig,\n",
    "    ModelCheckpoint,\n",
    "    TimeMonitor,\n",
    "    Model,\n",
    ")\n",
    "\n",
    "ms.set_seed(1)\n",
    "\n",
    "\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1, num_parallel_workers=1):\n",
    "    \"\"\"\n",
    "    create dataset for train or test\n",
    "    \"\"\"\n",
    "    # define dataset\n",
    "    mnist_ds = ds.MnistDataset(data_path, shuffle=False)\n",
    "\n",
    "    resize_height, resize_width = 32, 32\n",
    "    rescale = 1.0 / 255.0\n",
    "    shift = 0.0\n",
    "    rescale_nml = 1 / 0.3081\n",
    "    shift_nml = -1 * 0.1307 / 0.3081\n",
    "\n",
    "    # define map operations\n",
    "    resize_op = vision.Resize(\n",
    "        (resize_height, resize_width), interpolation=Inter.LINEAR\n",
    "    )  # Bilinear mode\n",
    "    rescale_nml_op = vision.Rescale(rescale_nml, shift_nml)\n",
    "    rescale_op = vision.Rescale(rescale, shift)\n",
    "    hwc2chw_op = vision.HWC2CHW()\n",
    "    type_cast_op = transforms.TypeCast(ms.int32)\n",
    "\n",
    "    # apply map operations on images\n",
    "    mnist_ds = mnist_ds.map(\n",
    "        operations=type_cast_op,\n",
    "        input_columns=\"label\",\n",
    "        num_parallel_workers=num_parallel_workers,\n",
    "    )\n",
    "    mnist_ds = mnist_ds.map(\n",
    "        operations=resize_op,\n",
    "        input_columns=\"image\",\n",
    "        num_parallel_workers=num_parallel_workers,\n",
    "    )\n",
    "    mnist_ds = mnist_ds.map(\n",
    "        operations=rescale_op,\n",
    "        input_columns=\"image\",\n",
    "        num_parallel_workers=num_parallel_workers,\n",
    "    )\n",
    "    mnist_ds = mnist_ds.map(\n",
    "        operations=rescale_nml_op,\n",
    "        input_columns=\"image\",\n",
    "        num_parallel_workers=num_parallel_workers,\n",
    "    )\n",
    "    mnist_ds = mnist_ds.map(\n",
    "        operations=hwc2chw_op,\n",
    "        input_columns=\"image\",\n",
    "        num_parallel_workers=num_parallel_workers,\n",
    "    )\n",
    "\n",
    "    # apply DatasetOps\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(\n",
    "        buffer_size=buffer_size\n",
    "    )  # 10000 as in LeNet train script\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "    mnist_ds = mnist_ds.repeat(repeat_size)\n",
    "\n",
    "    return mnist_ds\n",
    "\n",
    "\n",
    "class LeNet5(nn.Cell):\n",
    "    \"\"\"\n",
    "    Lenet network\n",
    "\n",
    "    Args:\n",
    "        num_class (int): Number of classes. Default: 10.\n",
    "        num_channel (int): Number of channels. Default: 1.\n",
    "\n",
    "    Returns:\n",
    "        Tensor, output tensor\n",
    "    Examples:\n",
    "    LeNet(num_class=10)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class=10, num_channel=1, include_top=True):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_channel, 6, 5, pad_mode=\"valid\", weight_init=Normal(0.02)\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode=\"valid\", weight_init=Normal(0.02))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.include_top = include_top\n",
    "        if self.include_top:\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = nn.Dense(16 * 5 * 5, 120)\n",
    "            self.fc2 = nn.Dense(120, 84)\n",
    "            self.fc3 = nn.Dense(84, num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        if not self.include_top:\n",
    "            return x\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "from mindvision.dataset import Mnist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_lenet():\n",
    "    ms.set_context(mode=ms.GRAPH_MODE, device_target=\"CPU\")\n",
    "    # 下载并处理MNIST数据集\n",
    "    download_train = Mnist(\n",
    "    path=\"./mnist\",\n",
    "    split=\"train\",\n",
    "    batch_size=32,\n",
    "    repeat_num=1,\n",
    "    shuffle=True,\n",
    "    resize=32,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "    download_eval = Mnist(\n",
    "    path=\"./mnist\", split=\"test\", batch_size=32, resize=32, download=True\n",
    ")\n",
    "\n",
    "    dataset_train = download_train.run()\n",
    "    dataset_eval = download_eval.run()\n",
    "    # data_path = \n",
    "    # ds_train = create_dataset(data_path)\n",
    "    ds_train = dataset_train\n",
    "    network = LeNet5(10)\n",
    "    net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "    net_opt = nn.Momentum(network.trainable_params(), 0.01, 0.9)\n",
    "    time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n",
    "    config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)\n",
    "    model = Model(network, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
    "    summary_dir = \"./summary/lenet_test2\"\n",
    "    interval_1 = [x for x in range(1, 4)]\n",
    "    interval_2 = [x for x in range(7, 11)]\n",
    "    ##Collector landscape information\n",
    "    summary_collector = ms.SummaryCollector(\n",
    "        summary_dir,\n",
    "        keep_default_action=True,\n",
    "        collect_specified_data={\n",
    "            \"collect_landscape\": {\n",
    "                \"landscape_size\": 10,\n",
    "                \"unit\": \"epoch\",\n",
    "                \"create_landscape\": {\"train\": True, \"result\": True},\n",
    "                \"num_samples\": 512,\n",
    "                \"intervals\": [interval_1, interval_2],\n",
    "            }\n",
    "        },\n",
    "        collect_freq=1,\n",
    "    )\n",
    "\n",
    "    print(\"============== Starting Training ==============\")\n",
    "    model.train(\n",
    "        10, ds_train, callbacks=[time_cb, ckpoint_cb, LossMonitor(), summary_collector]\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_lenet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
