{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先将 图片真实值HR用Bicubic插值进行下采样，得到低分辨率的LR图像，作为超分辨率重建的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# 指定包含图像的文件夹路径\n",
    "folder_path = \"Set5/HR\"\n",
    "\n",
    "# 遍历文件夹中的所有文件\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # 构建完整的文件路径\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    # 检查文件是否为图像\n",
    "    if file_path.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\")):\n",
    "        # 读取图像\n",
    "        image = cv2.imread(file_path)\n",
    "\n",
    "        # 计算下采样的新尺寸，这里我们将宽度和高度都减半\n",
    "        down_width = int(image.shape[1] * 0.5)\n",
    "        down_height = int(image.shape[0] * 0.5)\n",
    "        down_dimensions = (down_width, down_height)\n",
    "\n",
    "        # 使用Bicubic插值进行下采样\n",
    "        down_resized = cv2.resize(image, down_dimensions, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # 计算上采样的尺寸，即原始图像的尺寸\n",
    "        up_width = image.shape[1]\n",
    "        up_height = image.shape[0]\n",
    "        up_dimensions = (up_width, up_height)\n",
    "\n",
    "        # 使用Bicubic插值进行上采样\n",
    "        up_resized = cv2.resize(\n",
    "            down_resized, up_dimensions, interpolation=cv2.INTER_CUBIC\n",
    "        )\n",
    "\n",
    "        # 如果需要，可以在这里添加更多的图像处理步骤\n",
    "\n",
    "        # 显示图像\n",
    "        # cv2.imshow(\"Processed Image\", up_resized)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        # 保存处理后的图像\n",
    "        cv2.imwrite(\"Set5/LR/\" + file_name, up_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "from mindspore.ops import operations as P\n",
    "\n",
    "class Shift8(nn.Cell):\n",
    "    def __init__(self, groups=4, stride=1, mode=\"constant\"):\n",
    "        super().__init__()\n",
    "        self.g = groups\n",
    "        self.mode = mode\n",
    "        self.stride = stride\n",
    "        self.zeros_like = ops.ZerosLike()\n",
    "        self.pad = ops.Pad(((0, 0), (0, 0), (stride, stride), (stride, stride)))\n",
    "\n",
    "    def construct(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        out = self.zeros_like(x)\n",
    "\n",
    "        pad_x = self.pad(x)\n",
    "        assert c == self.g * 8\n",
    "\n",
    "        cx, cy = self.stride, self.stride\n",
    "        stride = self.stride\n",
    "        # MindSpore does not support item assignment, so we use concatenation\n",
    "        out = ops.concat(\n",
    "            (\n",
    "                pad_x[\n",
    "                    :,\n",
    "                    0 * self.g : 1 * self.g,\n",
    "                    cx - stride : cx - stride + h,\n",
    "                    cy : cy + w,\n",
    "                ],\n",
    "                pad_x[\n",
    "                    :,\n",
    "                    1 * self.g : 2 * self.g,\n",
    "                    cx + stride : cx + stride + h,\n",
    "                    cy : cy + w,\n",
    "                ],\n",
    "                pad_x[\n",
    "                    :,\n",
    "                    2 * self.g : 3 * self.g,\n",
    "                    cx : cx + h,\n",
    "                    cy - stride : cy - stride + w,\n",
    "                ],\n",
    "                pad_x[\n",
    "                    :,\n",
    "                    3 * self.g : 4 * self.g,\n",
    "                    cx : cx + h,\n",
    "                    cy + stride : cy + stride + w,\n",
    "                ],\n",
    "                pad_x[\n",
    "                    :,\n",
    "                    4 * self.g : 5 * self.g,\n",
    "                    cx + stride : cx + stride + h,\n",
    "                    cy + stride : cy + stride + w,\n",
    "                ],\n",
    "                pad_x[\n",
    "                    :,\n",
    "                    5 * self.g : 6 * self.g,\n",
    "                    cx + stride : cx + stride + h,\n",
    "                    cy - stride : cy - stride + w,\n",
    "                ],\n",
    "                pad_x[\n",
    "                    :,\n",
    "                    6 * self.g : 7 * self.g,\n",
    "                    cx - stride : cx - stride + h,\n",
    "                    cy + stride : cy + stride + w,\n",
    "                ],\n",
    "                pad_x[\n",
    "                    :,\n",
    "                    7 * self.g : 8 * self.g,\n",
    "                    cx - stride : cx - stride + h,\n",
    "                    cy - stride : cy - stride + w,\n",
    "                ],\n",
    "            ),\n",
    "            1,\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlockShift(nn.Cell):\n",
    "    \"\"\"\n",
    "    Residual block without BN in MindSpore.\n",
    "\n",
    "    It has a style of:\n",
    "        ---Conv-Shift-ReLU-Conv-+-\n",
    "         |________________|\n",
    "\n",
    "    Args:\n",
    "        num_feat (int): Channel number of intermediate features. Default: 64.\n",
    "        res_scale (float): Residual scale. Default: 1.\n",
    "        mindspore_init (bool): If set to True, use MindSpore default init,\n",
    "            otherwise, use custom init. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feat=64, res_scale=1, mindspore_init=False):\n",
    "        super(ResidualBlockShift, self).__init__()\n",
    "        self.res_scale = res_scale\n",
    "        self.conv1 = nn.Conv2d(num_feat, num_feat, kernel_size=1, has_bias=True)\n",
    "        self.conv2 = nn.Conv2d(num_feat, num_feat, kernel_size=1, has_bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.shift = Shift8(groups=num_feat // 8, stride=1)\n",
    "\n",
    "        if not mindspore_init:\n",
    "            # Use custom initializer\n",
    "            self.conv1.weight.set_data(Normal(0.1)(self.conv1.weight.shape))\n",
    "            self.conv2.weight.set_data(Normal(0.1)(self.conv2.weight.shape))\n",
    "\n",
    "    def construct(self, x):\n",
    "        identity = x\n",
    "        out = self.conv2(self.relu(self.shift(self.conv1(x))))\n",
    "        return identity + out * self.res_scale\n",
    "\n",
    "\n",
    "class UpShiftPixelShuffle(nn.Cell):\n",
    "    \"\"\"\n",
    "    UpShiftPixelShuffle with DepthToSpace in MindSpore.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        scale (int): Upscale factor. Default: 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, scale=2):\n",
    "        super(UpShiftPixelShuffle, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=1, has_bias=True)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.02)\n",
    "        self.shift = Shift8(groups=dim // 8)\n",
    "        self.conv2 = nn.Conv2d(dim, dim * scale * scale, kernel_size=1, has_bias=True)\n",
    "        self.depth_to_space = P.DepthToSpace(block_size=scale)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.shift(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.depth_to_space(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpShiftMLP(nn.Cell):\n",
    "    \"\"\"\n",
    "    UpShiftMLP in MindSpore.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        mode (str): Upsampling mode, either 'bilinear' or 'nearest'. Default: 'bilinear'.\n",
    "        scale (int): Upscale factor. Default: 2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, mode=\"bilinear\", scale=2):\n",
    "        super(UpShiftMLP, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mode = mode\n",
    "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=1, has_bias=True)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.02)\n",
    "        self.shift = Shift8(groups=dim // 8)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=1, has_bias=True)\n",
    "        if mode == \"bilinear\":\n",
    "            self.upsample = P.ResizeBilinear(scale_factor=scale, align_corners=False)\n",
    "        else:\n",
    "            self.upsample = P.ResizeNearestNeighbor(scale_factor=scale)\n",
    "\n",
    "        self.up_layer = nn.CellList(\n",
    "            [self.upsample, self.conv1, self.leaky_relu, self.shift, self.conv2]\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        for layer in self.up_layer:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 定义 SCNet 的 MindSpore 版本\n",
    "class SCNet(nn.Cell):\n",
    "    \"\"\"\n",
    "    SCNet based on the Modified SRResNet for MindSpore.\n",
    "    Args:\n",
    "        num_in_ch (int): Channel number of inputs. Default: 3.\n",
    "        num_out_ch (int): Channel number of outputs. Default: 3.\n",
    "        num_feat (int): Channel number of intermediate features. Default: 64.\n",
    "        num_block (int): Block number in the body network. Default: 16.\n",
    "        upscale (int): Upsampling factor. Support x2, x3 and x4. Default: 4.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_in_ch=3, num_out_ch=3, num_feat=64, num_block=16, upscale=4):\n",
    "        super(SCNet, self).__init__()\n",
    "        self.upscale = upscale\n",
    "\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, num_feat, 1, has_bias=True)\n",
    "        self.body = nn.CellList(\n",
    "            [ResidualBlockShift(num_feat) for _ in range(num_block)]\n",
    "        )\n",
    "\n",
    "        # upsampling\n",
    "        if self.upscale in [2, 3]:\n",
    "            self.upconv1 = UpShiftMLP(num_feat, scale=self.upscale)\n",
    "\n",
    "        elif self.upscale == 4:\n",
    "            self.upconv1 = UpShiftMLP(num_feat)\n",
    "            self.upconv2 = UpShiftMLP(num_feat)\n",
    "        elif self.upscale == 8:\n",
    "            self.upconv1 = UpShiftMLP(num_feat)\n",
    "            self.upconv2 = UpShiftMLP(num_feat)\n",
    "            self.upconv3 = UpShiftMLP(num_feat)\n",
    "\n",
    "        self.conv_hr = nn.Conv2d(num_feat, num_feat, kernel_size=1, has_bias=True)\n",
    "        self.conv_last = nn.Conv2d(num_feat, num_out_ch, kernel_size=1, has_bias=True)\n",
    "\n",
    "        # activation function\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "        # initialization\n",
    "        for layer in [self.conv_first, self.conv_hr, self.conv_last]:\n",
    "            layer.weight.set_data(Normal(0.1)(layer.weight.shape))\n",
    "\n",
    "    def construct(self, x):\n",
    "        feat = self.lrelu(self.conv_first(x))\n",
    "        for block in self.body:\n",
    "            feat = block(feat)\n",
    "\n",
    "        if self.upscale == 4:\n",
    "            feat = self.lrelu(self.upconv1(feat))\n",
    "            feat = self.lrelu(self.upconv2(feat))\n",
    "        elif self.upscale in [2, 3]:\n",
    "            feat = self.lrelu(self.upconv1(feat))\n",
    "        elif self.upscale == 8:\n",
    "            feat = self.lrelu(self.upconv1(feat))\n",
    "            feat = self.lrelu(self.upconv2(feat))\n",
    "            feat = self.lrelu(self.upconv3(feat))\n",
    "\n",
    "        out = self.conv_last(self.lrelu(self.conv_hr(feat)))\n",
    "        base = nn.ResizeBilinear()(x, scale_factor=self.upscale)\n",
    "        out += base\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
